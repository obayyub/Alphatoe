{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from alphatoe import evals, data, game, train\n",
    "import torch\n",
    "from torch.nn.functional import cross_entropy\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformerConfig, HookedTransformer\n",
    "import json\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "gametype must be one of 'all', 'strat', or 'all minimax'. Not all minimax",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_get_all_minimax_games.ipynb Cell 2\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_get_all_minimax_games.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m _, game_list \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgen_games(\u001b[39m\"\u001b[39;49m\u001b[39mall minimax\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/p/Tic-Tac-Transformer/alphatoe/data.py:38\u001b[0m, in \u001b[0;36mgen_games\u001b[0;34m(gametype)\u001b[0m\n\u001b[1;32m     36\u001b[0m     games \u001b[39m=\u001b[39m get_all_minimax_games(board, \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgametype must be one of \u001b[39m\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrat\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mall minimax\u001b[39m\u001b[39m'\u001b[39m\u001b[39m. Not \u001b[39m\u001b[39m{\u001b[39;00mgametype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGenerated \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(games)\u001b[39m}\u001b[39;00m\u001b[39m games\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m moves \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m     45\u001b[0m     [\n\u001b[1;32m     46\u001b[0m         [\u001b[39m10\u001b[39m] \u001b[39m+\u001b[39m game\u001b[39m.\u001b[39mmoves_played \u001b[39m+\u001b[39m ([\u001b[39m9\u001b[39m] \u001b[39m*\u001b[39m (\u001b[39m10\u001b[39m \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(game\u001b[39m.\u001b[39mmoves_played)))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: gametype must be one of 'all', 'strat', or 'all minimax'. Not all minimax"
     ]
    }
   ],
   "source": [
    "_, game_list = data.gen_games(\"all minimax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10,  8,  4,  7,  6,  2,  5,  3,  0,  1,  9])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_list[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_get_all_minimax_games.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_get_all_minimax_games.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m mm_first \u001b[39m=\u001b[39m game\u001b[39m.\u001b[39;49mget_all_minimax_games([game\u001b[39m.\u001b[39;49mBoard()], \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/notebooks/testing_get_all_minimax_games.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m mm_second \u001b[39m=\u001b[39m  game\u001b[39m.\u001b[39mget_all_minimax_games([game\u001b[39m.\u001b[39mBoard()], \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/p/Tic-Tac-Transformer/alphatoe/game.py:250\u001b[0m, in \u001b[0;36mget_all_minimax_games\u001b[0;34m(boards, minimax_turn, finished_boards)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m finished_boards\n\u001b[1;32m    249\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mreturn\u001b[39;00m get_all_minimax_games(ongoing_boards, \u001b[39mnot\u001b[39;49;00m minimax_turn, finished_boards)\n",
      "File \u001b[0;32m~/p/Tic-Tac-Transformer/alphatoe/game.py:250\u001b[0m, in \u001b[0;36mget_all_minimax_games\u001b[0;34m(boards, minimax_turn, finished_boards)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m finished_boards\n\u001b[1;32m    249\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mreturn\u001b[39;00m get_all_minimax_games(ongoing_boards, \u001b[39mnot\u001b[39;49;00m minimax_turn, finished_boards)\n",
      "    \u001b[0;31m[... skipping similar frames: get_all_minimax_games at line 250 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/p/Tic-Tac-Transformer/alphatoe/game.py:250\u001b[0m, in \u001b[0;36mget_all_minimax_games\u001b[0;34m(boards, minimax_turn, finished_boards)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m finished_boards\n\u001b[1;32m    249\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mreturn\u001b[39;00m get_all_minimax_games(ongoing_boards, \u001b[39mnot\u001b[39;49;00m minimax_turn, finished_boards)\n",
      "File \u001b[0;32m~/p/Tic-Tac-Transformer/alphatoe/game.py:244\u001b[0m, in \u001b[0;36mget_all_minimax_games\u001b[0;34m(boards, minimax_turn, finished_boards)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m move_getter(board):\n\u001b[0;32m--> 244\u001b[0m         _board \u001b[39m=\u001b[39m deepcopy(board)\n\u001b[1;32m    245\u001b[0m         _board\u001b[39m.\u001b[39mmake_move(move)\n\u001b[1;32m    246\u001b[0m         ongoing_boards\u001b[39m.\u001b[39mappend(_board)\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:271\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 271\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    273\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    229\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    230\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    232\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:206\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    204\u001b[0m append \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mappend\n\u001b[1;32m    205\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m x:\n\u001b[0;32m--> 206\u001b[0m     append(deepcopy(a, memo))\n\u001b[1;32m    207\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/usr/lib/python3.10/copy.py:128\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    124\u001b[0m     d[PyStringMap] \u001b[39m=\u001b[39m PyStringMap\u001b[39m.\u001b[39mcopy\n\u001b[1;32m    126\u001b[0m \u001b[39mdel\u001b[39;00m d, t\n\u001b[0;32m--> 128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeepcopy\u001b[39m(x, memo\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, _nil\u001b[39m=\u001b[39m[]):\n\u001b[1;32m    129\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Deep copy operation on arbitrary Python objects.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[1;32m    131\u001b[0m \u001b[39m    See the module's __doc__ string for more info.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m memo \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mm_first = game.get_all_minimax_games([game.Board()], True)\n",
    "mm_second =  game.get_all_minimax_games([game.Board()], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31040\n",
      "9440\n"
     ]
    }
   ],
   "source": [
    "print(len(mm_first))\n",
    "print(len(mm_second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 2,\n",
    "    n_heads = 4,\n",
    "    d_model = 16,\n",
    "    d_head = 4,\n",
    "    d_mlp = 64,\n",
    "    act_fn = \"relu\",\n",
    "    #normalization_type=None,\n",
    "    normalization_type='LN',\n",
    "    d_vocab=11,\n",
    "    d_vocab_out=10,\n",
    "    n_ctx=10,\n",
    "    init_weights=True,\n",
    "    device=\"cuda\",\n",
    "    seed = 1337,\n",
    ")\n",
    "\n",
    "lr = 1e-5\n",
    "weight_decay = 1e-4\n",
    "test_train_split = 0.7\n",
    "epochs = 10_000\n",
    "batch_size = 4096 * 2 * 2\n",
    "minimax_is_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer(cfg).to(cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating minimax vs all...\n",
      "Generated 31040 games\n",
      "Generated array of moves\n",
      "torch.Size([31040, 10])\n",
      "Generated data and labels\n",
      "One hot encoded labels\n",
      "Generating minimax vs all...\n",
      "Generated 9440 games\n",
      "Generated array of moves\n",
      "torch.Size([9440, 10])\n",
      "Generated data and labels\n",
      "One hot encoded labels\n"
     ]
    }
   ],
   "source": [
    "minimax_first = data.gen_data(\"minimax first\", test_train_split)\n",
    "minimax_second= data.gen_data(\"minimax second\", test_train_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21728, 10, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimax_first[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = cross_entropy\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_alternating_indices(t: torch.Tensor, dim: int=0, odd:bool = True) -> torch.Tensor:\n",
    "    indices = [index for index in range(t.shape[1]) if (index + odd) % 2 != 0 ]\n",
    "    indices = torch.tensor(indices).to(t.device)\n",
    "    return torch.index_select(t, 1, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Train Loss: 2.4865224361419678 | Test Loss: 2.477419137954712\n",
      "Epoch 1 | Train Loss: 2.5231146812438965 | Test Loss: 2.5100507736206055\n",
      "Epoch 2 | Train Loss: 2.4725170135498047 | Test Loss: 2.464345932006836\n",
      "Epoch 3 | Train Loss: 2.512850522994995 | Test Loss: 2.4998228549957275\n",
      "Epoch 4 | Train Loss: 2.4594502449035645 | Test Loss: 2.4515063762664795\n",
      "Epoch 5 | Train Loss: 2.502655506134033 | Test Loss: 2.489680051803589\n",
      "Epoch 6 | Train Loss: 2.4466352462768555 | Test Loss: 2.4388341903686523\n",
      "Epoch 7 | Train Loss: 2.492593288421631 | Test Loss: 2.4796814918518066\n",
      "Epoch 8 | Train Loss: 2.433976888656616 | Test Loss: 2.426286458969116\n",
      "Epoch 9 | Train Loss: 2.482698917388916 | Test Loss: 2.469857931137085\n",
      "Epoch 10 | Train Loss: 2.421436071395874 | Test Loss: 2.4138646125793457\n",
      "Epoch 11 | Train Loss: 2.4729504585266113 | Test Loss: 2.4601500034332275\n",
      "Epoch 12 | Train Loss: 2.4090347290039062 | Test Loss: 2.4015798568725586\n",
      "Epoch 13 | Train Loss: 2.463355302810669 | Test Loss: 2.4506378173828125\n",
      "Epoch 14 | Train Loss: 2.396756887435913 | Test Loss: 2.389415740966797\n",
      "Epoch 15 | Train Loss: 2.4539589881896973 | Test Loss: 2.4413678646087646\n",
      "Epoch 16 | Train Loss: 2.384613513946533 | Test Loss: 2.3774008750915527\n",
      "Epoch 17 | Train Loss: 2.444812059402466 | Test Loss: 2.432403802871704\n",
      "Epoch 18 | Train Loss: 2.3726320266723633 | Test Loss: 2.365567445755005\n",
      "Epoch 19 | Train Loss: 2.435981035232544 | Test Loss: 2.4237048625946045\n",
      "Epoch 20 | Train Loss: 2.360813856124878 | Test Loss: 2.353872060775757\n",
      "Epoch 21 | Train Loss: 2.4273531436920166 | Test Loss: 2.415219783782959\n",
      "Epoch 22 | Train Loss: 2.349139451980591 | Test Loss: 2.3423268795013428\n",
      "Epoch 23 | Train Loss: 2.418921947479248 | Test Loss: 2.4069271087646484\n",
      "Epoch 24 | Train Loss: 2.337597608566284 | Test Loss: 2.3309037685394287\n",
      "Epoch 25 | Train Loss: 2.4107038974761963 | Test Loss: 2.3988304138183594\n",
      "Epoch 26 | Train Loss: 2.326188325881958 | Test Loss: 2.3196496963500977\n",
      "Epoch 27 | Train Loss: 2.4026432037353516 | Test Loss: 2.390918493270874\n",
      "Epoch 28 | Train Loss: 2.314971685409546 | Test Loss: 2.308579683303833\n",
      "Epoch 29 | Train Loss: 2.394787311553955 | Test Loss: 2.3832449913024902\n",
      "Epoch 30 | Train Loss: 2.3039188385009766 | Test Loss: 2.297693967819214\n",
      "Epoch 31 | Train Loss: 2.387167453765869 | Test Loss: 2.375802993774414\n",
      "Epoch 32 | Train Loss: 2.2930822372436523 | Test Loss: 2.287050247192383\n",
      "Epoch 33 | Train Loss: 2.379777669906616 | Test Loss: 2.368614673614502\n",
      "Epoch 34 | Train Loss: 2.2824630737304688 | Test Loss: 2.2766032218933105\n",
      "Epoch 35 | Train Loss: 2.3726468086242676 | Test Loss: 2.3616983890533447\n",
      "Epoch 36 | Train Loss: 2.272017002105713 | Test Loss: 2.266312599182129\n",
      "Epoch 37 | Train Loss: 2.3657755851745605 | Test Loss: 2.3550212383270264\n",
      "Epoch 38 | Train Loss: 2.2617335319519043 | Test Loss: 2.2562196254730225\n",
      "Epoch 39 | Train Loss: 2.3591301441192627 | Test Loss: 2.3485665321350098\n",
      "Epoch 40 | Train Loss: 2.251660108566284 | Test Loss: 2.2463419437408447\n",
      "Epoch 41 | Train Loss: 2.3527026176452637 | Test Loss: 2.3423333168029785\n",
      "Epoch 42 | Train Loss: 2.241814613342285 | Test Loss: 2.236707925796509\n",
      "Epoch 43 | Train Loss: 2.346480369567871 | Test Loss: 2.3363277912139893\n",
      "Epoch 44 | Train Loss: 2.2322261333465576 | Test Loss: 2.22733736038208\n",
      "Epoch 45 | Train Loss: 2.3405234813690186 | Test Loss: 2.3305838108062744\n",
      "Epoch 46 | Train Loss: 2.222895383834839 | Test Loss: 2.218214511871338\n",
      "Epoch 47 | Train Loss: 2.3348050117492676 | Test Loss: 2.325082302093506\n",
      "Epoch 48 | Train Loss: 2.2138102054595947 | Test Loss: 2.2093276977539062\n",
      "Epoch 49 | Train Loss: 2.3293380737304688 | Test Loss: 2.319838047027588\n",
      "Epoch 50 | Train Loss: 2.2049622535705566 | Test Loss: 2.20068359375\n",
      "Epoch 51 | Train Loss: 2.3241143226623535 | Test Loss: 2.3148083686828613\n",
      "Epoch 52 | Train Loss: 2.196359157562256 | Test Loss: 2.1922833919525146\n",
      "Epoch 53 | Train Loss: 2.319098711013794 | Test Loss: 2.309983253479004\n",
      "Epoch 54 | Train Loss: 2.1879982948303223 | Test Loss: 2.1841213703155518\n",
      "Epoch 55 | Train Loss: 2.3142948150634766 | Test Loss: 2.3053534030914307\n",
      "Epoch 56 | Train Loss: 2.1798720359802246 | Test Loss: 2.1761908531188965\n",
      "Epoch 57 | Train Loss: 2.309683322906494 | Test Loss: 2.3009145259857178\n",
      "Epoch 58 | Train Loss: 2.171987771987915 | Test Loss: 2.1684927940368652\n",
      "Epoch 59 | Train Loss: 2.3052587509155273 | Test Loss: 2.296661376953125\n",
      "Epoch 60 | Train Loss: 2.164332866668701 | Test Loss: 2.1610262393951416\n",
      "Epoch 61 | Train Loss: 2.3010127544403076 | Test Loss: 2.2925808429718018\n",
      "Epoch 62 | Train Loss: 2.1569135189056396 | Test Loss: 2.1537868976593018\n",
      "Epoch 63 | Train Loss: 2.2969181537628174 | Test Loss: 2.2886648178100586\n",
      "Epoch 64 | Train Loss: 2.1497185230255127 | Test Loss: 2.1467671394348145\n",
      "Epoch 65 | Train Loss: 2.292987108230591 | Test Loss: 2.284910202026367\n",
      "Epoch 66 | Train Loss: 2.142744779586792 | Test Loss: 2.139970541000366\n",
      "Epoch 67 | Train Loss: 2.2892160415649414 | Test Loss: 2.2813217639923096\n",
      "Epoch 68 | Train Loss: 2.1359879970550537 | Test Loss: 2.133376359939575\n",
      "Epoch 69 | Train Loss: 2.2856032848358154 | Test Loss: 2.2778854370117188\n",
      "Epoch 70 | Train Loss: 2.1294302940368652 | Test Loss: 2.126985549926758\n",
      "Epoch 71 | Train Loss: 2.282137155532837 | Test Loss: 2.2745871543884277\n",
      "Epoch 72 | Train Loss: 2.1230812072753906 | Test Loss: 2.1207971572875977\n",
      "Epoch 73 | Train Loss: 2.2788076400756836 | Test Loss: 2.2714200019836426\n",
      "Epoch 74 | Train Loss: 2.116933822631836 | Test Loss: 2.114806652069092\n",
      "Epoch 75 | Train Loss: 2.2756097316741943 | Test Loss: 2.2683768272399902\n",
      "Epoch 76 | Train Loss: 2.1109843254089355 | Test Loss: 2.1090080738067627\n",
      "Epoch 77 | Train Loss: 2.272534132003784 | Test Loss: 2.2654519081115723\n",
      "Epoch 78 | Train Loss: 2.1052236557006836 | Test Loss: 2.1033902168273926\n",
      "Epoch 79 | Train Loss: 2.2695698738098145 | Test Loss: 2.262627124786377\n",
      "Epoch 80 | Train Loss: 2.0996429920196533 | Test Loss: 2.0979442596435547\n",
      "Epoch 81 | Train Loss: 2.2667012214660645 | Test Loss: 2.2598869800567627\n",
      "Epoch 82 | Train Loss: 2.0942318439483643 | Test Loss: 2.092663049697876\n",
      "Epoch 83 | Train Loss: 2.263901710510254 | Test Loss: 2.2572169303894043\n",
      "Epoch 84 | Train Loss: 2.088982343673706 | Test Loss: 2.0875425338745117\n",
      "Epoch 85 | Train Loss: 2.2611942291259766 | Test Loss: 2.254638671875\n",
      "Epoch 86 | Train Loss: 2.0838921070098877 | Test Loss: 2.082571268081665\n",
      "Epoch 87 | Train Loss: 2.2585809230804443 | Test Loss: 2.2521440982818604\n",
      "Epoch 88 | Train Loss: 2.078948736190796 | Test Loss: 2.0777440071105957\n",
      "Epoch 89 | Train Loss: 2.2560503482818604 | Test Loss: 2.249727249145508\n",
      "Epoch 90 | Train Loss: 2.074145793914795 | Test Loss: 2.073045253753662\n",
      "Epoch 91 | Train Loss: 2.253601312637329 | Test Loss: 2.247389316558838\n",
      "Epoch 92 | Train Loss: 2.069472551345825 | Test Loss: 2.0684773921966553\n",
      "Epoch 93 | Train Loss: 2.251227378845215 | Test Loss: 2.245115280151367\n",
      "Epoch 94 | Train Loss: 2.0649290084838867 | Test Loss: 2.0640339851379395\n",
      "Epoch 95 | Train Loss: 2.2489137649536133 | Test Loss: 2.2429068088531494\n",
      "Epoch 96 | Train Loss: 2.060511350631714 | Test Loss: 2.0597116947174072\n",
      "Epoch 97 | Train Loss: 2.2466681003570557 | Test Loss: 2.240760087966919\n",
      "Epoch 98 | Train Loss: 2.056220054626465 | Test Loss: 2.0555145740509033\n",
      "Epoch 99 | Train Loss: 2.244483470916748 | Test Loss: 2.2386670112609863\n",
      "Epoch 100 | Train Loss: 2.052044630050659 | Test Loss: 2.051422595977783\n",
      "Epoch 101 | Train Loss: 2.242356061935425 | Test Loss: 2.2366244792938232\n",
      "Epoch 102 | Train Loss: 2.0479729175567627 | Test Loss: 2.0474345684051514\n",
      "Epoch 103 | Train Loss: 2.240283966064453 | Test Loss: 2.2346322536468506\n",
      "Epoch 104 | Train Loss: 2.044004440307617 | Test Loss: 2.0435421466827393\n",
      "Epoch 105 | Train Loss: 2.2382633686065674 | Test Loss: 2.232682466506958\n",
      "Epoch 106 | Train Loss: 2.0401251316070557 | Test Loss: 2.039731979370117\n",
      "Epoch 107 | Train Loss: 2.2362828254699707 | Test Loss: 2.2307679653167725\n",
      "Epoch 108 | Train Loss: 2.036336898803711 | Test Loss: 2.0360107421875\n",
      "Epoch 109 | Train Loss: 2.234342336654663 | Test Loss: 2.2288873195648193\n",
      "Epoch 110 | Train Loss: 2.0326344966888428 | Test Loss: 2.0323619842529297\n",
      "Epoch 111 | Train Loss: 2.232436180114746 | Test Loss: 2.2270352840423584\n",
      "Epoch 112 | Train Loss: 2.029003858566284 | Test Loss: 2.0287866592407227\n",
      "Epoch 113 | Train Loss: 2.230560064315796 | Test Loss: 2.225205659866333\n",
      "Epoch 114 | Train Loss: 2.0254526138305664 | Test Loss: 2.025285482406616\n",
      "Epoch 115 | Train Loss: 2.228703022003174 | Test Loss: 2.2233948707580566\n",
      "Epoch 116 | Train Loss: 2.0219738483428955 | Test Loss: 2.021850824356079\n",
      "Epoch 117 | Train Loss: 2.2268707752227783 | Test Loss: 2.221604585647583\n",
      "Epoch 118 | Train Loss: 2.0185627937316895 | Test Loss: 2.018481492996216\n",
      "Epoch 119 | Train Loss: 2.2250607013702393 | Test Loss: 2.219831705093384\n",
      "Epoch 120 | Train Loss: 2.015214443206787 | Test Loss: 2.0151681900024414\n",
      "Epoch 121 | Train Loss: 2.2232720851898193 | Test Loss: 2.2180771827697754\n",
      "Epoch 122 | Train Loss: 2.0119211673736572 | Test Loss: 2.0119097232818604\n",
      "Epoch 123 | Train Loss: 2.221506118774414 | Test Loss: 2.2163374423980713\n",
      "Epoch 124 | Train Loss: 2.0086846351623535 | Test Loss: 2.0087053775787354\n",
      "Epoch 125 | Train Loss: 2.2197582721710205 | Test Loss: 2.2146127223968506\n",
      "Epoch 126 | Train Loss: 2.0055012702941895 | Test Loss: 2.0055508613586426\n",
      "Epoch 127 | Train Loss: 2.2180304527282715 | Test Loss: 2.2129032611846924\n",
      "Epoch 128 | Train Loss: 2.0023698806762695 | Test Loss: 2.002446174621582\n",
      "Epoch 129 | Train Loss: 2.216320276260376 | Test Loss: 2.211207866668701\n",
      "Epoch 130 | Train Loss: 1.9992928504943848 | Test Loss: 1.999390959739685\n",
      "Epoch 131 | Train Loss: 2.2146270275115967 | Test Loss: 2.209528684616089\n",
      "Epoch 132 | Train Loss: 1.996261477470398 | Test Loss: 1.9963761568069458\n",
      "Epoch 133 | Train Loss: 2.212946891784668 | Test Loss: 2.2078559398651123\n",
      "Epoch 134 | Train Loss: 1.9932702779769897 | Test Loss: 1.993398666381836\n",
      "Epoch 135 | Train Loss: 2.2112741470336914 | Test Loss: 2.2061874866485596\n",
      "Epoch 136 | Train Loss: 1.9903175830841064 | Test Loss: 1.9904576539993286\n",
      "Epoch 137 | Train Loss: 2.2096071243286133 | Test Loss: 2.204524278640747\n",
      "Epoch 138 | Train Loss: 1.9874016046524048 | Test Loss: 1.9875496625900269\n",
      "Epoch 139 | Train Loss: 2.2079460620880127 | Test Loss: 2.202867269515991\n",
      "Epoch 140 | Train Loss: 1.9845166206359863 | Test Loss: 1.9846693277359009\n",
      "Epoch 141 | Train Loss: 2.2062911987304688 | Test Loss: 2.2012150287628174\n",
      "Epoch 142 | Train Loss: 1.9816597700119019 | Test Loss: 1.9818164110183716\n",
      "Epoch 143 | Train Loss: 2.2046446800231934 | Test Loss: 2.199570655822754\n",
      "Epoch 144 | Train Loss: 1.9788283109664917 | Test Loss: 1.9789879322052002\n",
      "Epoch 145 | Train Loss: 2.2030043601989746 | Test Loss: 2.1979281902313232\n",
      "Epoch 146 | Train Loss: 1.9760215282440186 | Test Loss: 1.9761837720870972\n",
      "Epoch 147 | Train Loss: 2.2013659477233887 | Test Loss: 2.1962814331054688\n",
      "Epoch 148 | Train Loss: 1.9732379913330078 | Test Loss: 1.9734017848968506\n",
      "Epoch 149 | Train Loss: 2.199723243713379 | Test Loss: 2.194631338119507\n",
      "Epoch 150 | Train Loss: 1.970481276512146 | Test Loss: 1.970645785331726\n",
      "Epoch 151 | Train Loss: 2.1980795860290527 | Test Loss: 2.1929800510406494\n",
      "Epoch 152 | Train Loss: 1.9677497148513794 | Test Loss: 1.9679313898086548\n",
      "Epoch 153 | Train Loss: 2.196437358856201 | Test Loss: 2.191330909729004\n",
      "Epoch 154 | Train Loss: 1.9650779962539673 | Test Loss: 1.9652563333511353\n",
      "Epoch 155 | Train Loss: 2.194798707962036 | Test Loss: 2.189687490463257\n",
      "Epoch 156 | Train Loss: 1.9624217748641968 | Test Loss: 1.9625937938690186\n",
      "Epoch 157 | Train Loss: 2.1931679248809814 | Test Loss: 2.188051700592041\n",
      "Epoch 158 | Train Loss: 1.9597793817520142 | Test Loss: 1.9599475860595703\n",
      "Epoch 159 | Train Loss: 2.1915476322174072 | Test Loss: 2.18642520904541\n",
      "Epoch 160 | Train Loss: 1.957153558731079 | Test Loss: 1.957318902015686\n",
      "Epoch 161 | Train Loss: 2.1899333000183105 | Test Loss: 2.184802532196045\n",
      "Epoch 162 | Train Loss: 1.954543948173523 | Test Loss: 1.9547072649002075\n",
      "Epoch 163 | Train Loss: 2.188321113586426 | Test Loss: 2.1831798553466797\n",
      "Epoch 164 | Train Loss: 1.9519516229629517 | Test Loss: 1.952113151550293\n",
      "Epoch 165 | Train Loss: 2.1867117881774902 | Test Loss: 2.181560754776001\n",
      "Epoch 166 | Train Loss: 1.949379801750183 | Test Loss: 1.9495387077331543\n",
      "Epoch 167 | Train Loss: 2.1851069927215576 | Test Loss: 2.1799497604370117\n",
      "Epoch 168 | Train Loss: 1.9468259811401367 | Test Loss: 1.946981430053711\n",
      "Epoch 169 | Train Loss: 2.1835103034973145 | Test Loss: 2.178345203399658\n",
      "Epoch 170 | Train Loss: 1.944289207458496 | Test Loss: 1.9444389343261719\n",
      "Epoch 171 | Train Loss: 2.1819188594818115 | Test Loss: 2.1767444610595703\n",
      "Epoch 172 | Train Loss: 1.9417656660079956 | Test Loss: 1.9419094324111938\n",
      "Epoch 173 | Train Loss: 2.1803345680236816 | Test Loss: 2.1751484870910645\n",
      "Epoch 174 | Train Loss: 1.9392540454864502 | Test Loss: 1.9393880367279053\n",
      "Epoch 175 | Train Loss: 2.178752899169922 | Test Loss: 2.1735482215881348\n",
      "Epoch 176 | Train Loss: 1.9367470741271973 | Test Loss: 1.936875581741333\n",
      "Epoch 177 | Train Loss: 2.1771702766418457 | Test Loss: 2.1719465255737305\n",
      "Epoch 178 | Train Loss: 1.9342563152313232 | Test Loss: 1.9343847036361694\n",
      "Epoch 179 | Train Loss: 2.1755893230438232 | Test Loss: 2.1703481674194336\n",
      "Epoch 180 | Train Loss: 1.9317888021469116 | Test Loss: 1.9319114685058594\n",
      "Epoch 181 | Train Loss: 2.1740143299102783 | Test Loss: 2.168755531311035\n",
      "Epoch 182 | Train Loss: 1.929332971572876 | Test Loss: 1.9294511079788208\n",
      "Epoch 183 | Train Loss: 2.1724507808685303 | Test Loss: 2.1671719551086426\n",
      "Epoch 184 | Train Loss: 1.9268920421600342 | Test Loss: 1.9270081520080566\n",
      "Epoch 185 | Train Loss: 2.1708977222442627 | Test Loss: 2.1656012535095215\n",
      "Epoch 186 | Train Loss: 1.9244670867919922 | Test Loss: 1.924581527709961\n",
      "Epoch 187 | Train Loss: 2.169355630874634 | Test Loss: 2.1640374660491943\n",
      "Epoch 188 | Train Loss: 1.9220571517944336 | Test Loss: 1.922170639038086\n",
      "Epoch 189 | Train Loss: 2.1678152084350586 | Test Loss: 2.162477493286133\n",
      "Epoch 190 | Train Loss: 1.919661045074463 | Test Loss: 1.9197728633880615\n",
      "Epoch 191 | Train Loss: 2.16627836227417 | Test Loss: 2.1609244346618652\n",
      "Epoch 192 | Train Loss: 1.9172797203063965 | Test Loss: 1.9173909425735474\n",
      "Epoch 193 | Train Loss: 2.1647467613220215 | Test Loss: 2.1593782901763916\n",
      "Epoch 194 | Train Loss: 1.914915919303894 | Test Loss: 1.9150277376174927\n",
      "Epoch 195 | Train Loss: 2.163221597671509 | Test Loss: 2.1578402519226074\n",
      "Epoch 196 | Train Loss: 1.9125690460205078 | Test Loss: 1.9126830101013184\n",
      "Epoch 197 | Train Loss: 2.1617040634155273 | Test Loss: 2.1563072204589844\n",
      "Epoch 198 | Train Loss: 1.9102391004562378 | Test Loss: 1.9103567600250244\n",
      "Epoch 199 | Train Loss: 2.160187244415283 | Test Loss: 2.1547770500183105\n",
      "Epoch 200 | Train Loss: 1.9079265594482422 | Test Loss: 1.9080489873886108\n",
      "Epoch 201 | Train Loss: 2.1586740016937256 | Test Loss: 2.153252124786377\n",
      "Epoch 202 | Train Loss: 1.905630350112915 | Test Loss: 1.9057594537734985\n",
      "Epoch 203 | Train Loss: 2.15716290473938 | Test Loss: 2.151730537414551\n",
      "Epoch 204 | Train Loss: 1.9033554792404175 | Test Loss: 1.9034912586212158\n",
      "Epoch 205 | Train Loss: 2.1556575298309326 | Test Loss: 2.1502115726470947\n",
      "Epoch 206 | Train Loss: 1.9010989665985107 | Test Loss: 1.9012408256530762\n",
      "Epoch 207 | Train Loss: 2.1541531085968018 | Test Loss: 2.1486942768096924\n",
      "Epoch 208 | Train Loss: 1.8988587856292725 | Test Loss: 1.8990037441253662\n",
      "Epoch 209 | Train Loss: 2.152639627456665 | Test Loss: 2.1471667289733887\n",
      "Epoch 210 | Train Loss: 1.8966313600540161 | Test Loss: 1.8967796564102173\n",
      "Epoch 211 | Train Loss: 2.151124954223633 | Test Loss: 2.145648956298828\n",
      "Epoch 212 | Train Loss: 1.8944213390350342 | Test Loss: 1.8945763111114502\n",
      "Epoch 213 | Train Loss: 2.1496176719665527 | Test Loss: 2.144137144088745\n",
      "Epoch 214 | Train Loss: 1.8922300338745117 | Test Loss: 1.892391324043274\n",
      "Epoch 215 | Train Loss: 2.148115634918213 | Test Loss: 2.142630100250244\n",
      "Epoch 216 | Train Loss: 1.8900560140609741 | Test Loss: 1.8902249336242676\n",
      "Epoch 217 | Train Loss: 2.146615505218506 | Test Loss: 2.1411240100860596\n",
      "Epoch 218 | Train Loss: 1.8879024982452393 | Test Loss: 1.8880797624588013\n",
      "Epoch 219 | Train Loss: 2.145118236541748 | Test Loss: 2.139626979827881\n",
      "Epoch 220 | Train Loss: 1.8857669830322266 | Test Loss: 1.885951042175293\n",
      "Epoch 221 | Train Loss: 2.1436285972595215 | Test Loss: 2.1381373405456543\n",
      "Epoch 222 | Train Loss: 1.88364839553833 | Test Loss: 1.8838391304016113\n",
      "Epoch 223 | Train Loss: 2.142148017883301 | Test Loss: 2.136660575866699\n",
      "Epoch 224 | Train Loss: 1.881546974182129 | Test Loss: 1.8817466497421265\n",
      "Epoch 225 | Train Loss: 2.1406779289245605 | Test Loss: 2.135195255279541\n",
      "Epoch 226 | Train Loss: 1.8794636726379395 | Test Loss: 1.879671573638916\n",
      "Epoch 227 | Train Loss: 2.1392195224761963 | Test Loss: 2.133739471435547\n",
      "Epoch 228 | Train Loss: 1.8773987293243408 | Test Loss: 1.8776161670684814\n",
      "Epoch 229 | Train Loss: 2.1377649307250977 | Test Loss: 2.1322898864746094\n",
      "Epoch 230 | Train Loss: 1.8753553628921509 | Test Loss: 1.8755825757980347\n",
      "Epoch 231 | Train Loss: 2.1363184452056885 | Test Loss: 2.1308507919311523\n",
      "Epoch 232 | Train Loss: 1.8733340501785278 | Test Loss: 1.8735698461532593\n",
      "Epoch 233 | Train Loss: 2.134882926940918 | Test Loss: 2.129424571990967\n",
      "Epoch 234 | Train Loss: 1.8713322877883911 | Test Loss: 1.8715753555297852\n",
      "Epoch 235 | Train Loss: 2.133457660675049 | Test Loss: 2.1280038356781006\n",
      "Epoch 236 | Train Loss: 1.8693487644195557 | Test Loss: 1.8695992231369019\n",
      "Epoch 237 | Train Loss: 2.132040023803711 | Test Loss: 2.1265926361083984\n",
      "Epoch 238 | Train Loss: 1.8673839569091797 | Test Loss: 1.8676424026489258\n",
      "Epoch 239 | Train Loss: 2.130629539489746 | Test Loss: 2.1251893043518066\n",
      "Epoch 240 | Train Loss: 1.8654379844665527 | Test Loss: 1.8657045364379883\n",
      "Epoch 241 | Train Loss: 2.129228115081787 | Test Loss: 2.1237945556640625\n",
      "Epoch 242 | Train Loss: 1.8635098934173584 | Test Loss: 1.863783836364746\n",
      "Epoch 243 | Train Loss: 2.1278367042541504 | Test Loss: 2.122410297393799\n",
      "Epoch 244 | Train Loss: 1.8615955114364624 | Test Loss: 1.8618749380111694\n",
      "Epoch 245 | Train Loss: 2.126455307006836 | Test Loss: 2.121033191680908\n",
      "Epoch 246 | Train Loss: 1.8596974611282349 | Test Loss: 1.8599857091903687\n",
      "Epoch 247 | Train Loss: 2.125082015991211 | Test Loss: 2.119664430618286\n",
      "Epoch 248 | Train Loss: 1.8578150272369385 | Test Loss: 1.858107566833496\n",
      "Epoch 249 | Train Loss: 2.1237196922302246 | Test Loss: 2.1183083057403564\n",
      "Epoch 250 | Train Loss: 1.8559435606002808 | Test Loss: 1.8562445640563965\n",
      "Epoch 251 | Train Loss: 2.122370481491089 | Test Loss: 2.1169631481170654\n",
      "Epoch 252 | Train Loss: 1.8540867567062378 | Test Loss: 1.8543956279754639\n",
      "Epoch 253 | Train Loss: 2.121032238006592 | Test Loss: 2.1156272888183594\n",
      "Epoch 254 | Train Loss: 1.8522436618804932 | Test Loss: 1.8525584936141968\n",
      "Epoch 255 | Train Loss: 2.1197025775909424 | Test Loss: 2.1142959594726562\n",
      "Epoch 256 | Train Loss: 1.8504118919372559 | Test Loss: 1.8507369756698608\n",
      "Epoch 257 | Train Loss: 2.118377685546875 | Test Loss: 2.112971544265747\n",
      "Epoch 258 | Train Loss: 1.8485970497131348 | Test Loss: 1.8489314317703247\n",
      "Epoch 259 | Train Loss: 2.1170637607574463 | Test Loss: 2.1116559505462646\n",
      "Epoch 260 | Train Loss: 1.8467962741851807 | Test Loss: 1.847142219543457\n",
      "Epoch 261 | Train Loss: 2.115757703781128 | Test Loss: 2.110346794128418\n",
      "Epoch 262 | Train Loss: 1.845010757446289 | Test Loss: 1.8453705310821533\n",
      "Epoch 263 | Train Loss: 2.114459753036499 | Test Loss: 2.109049081802368\n",
      "Epoch 264 | Train Loss: 1.843245267868042 | Test Loss: 1.8436188697814941\n",
      "Epoch 265 | Train Loss: 2.113175630569458 | Test Loss: 2.107757806777954\n",
      "Epoch 266 | Train Loss: 1.8414984941482544 | Test Loss: 1.8418856859207153\n",
      "Epoch 267 | Train Loss: 2.11189866065979 | Test Loss: 2.1064741611480713\n",
      "Epoch 268 | Train Loss: 1.8397685289382935 | Test Loss: 1.8401685953140259\n",
      "Epoch 269 | Train Loss: 2.110628843307495 | Test Loss: 2.1051952838897705\n",
      "Epoch 270 | Train Loss: 1.8380546569824219 | Test Loss: 1.8384714126586914\n",
      "Epoch 271 | Train Loss: 2.109370470046997 | Test Loss: 2.103928804397583\n",
      "Epoch 272 | Train Loss: 1.8363611698150635 | Test Loss: 1.8367915153503418\n",
      "Epoch 273 | Train Loss: 2.108123302459717 | Test Loss: 2.1026687622070312\n",
      "Epoch 274 | Train Loss: 1.8346842527389526 | Test Loss: 1.835128664970398\n",
      "Epoch 275 | Train Loss: 2.1068811416625977 | Test Loss: 2.1014130115509033\n",
      "Epoch 276 | Train Loss: 1.8330243825912476 | Test Loss: 1.8334823846817017\n",
      "Epoch 277 | Train Loss: 2.1056461334228516 | Test Loss: 2.1001687049865723\n",
      "Epoch 278 | Train Loss: 1.831383228302002 | Test Loss: 1.83185875415802\n",
      "Epoch 279 | Train Loss: 2.104421615600586 | Test Loss: 2.098935842514038\n",
      "Epoch 280 | Train Loss: 1.829759955406189 | Test Loss: 1.8302518129348755\n",
      "Epoch 281 | Train Loss: 2.103207588195801 | Test Loss: 2.0977110862731934\n",
      "Epoch 282 | Train Loss: 1.828154444694519 | Test Loss: 1.8286645412445068\n",
      "Epoch 283 | Train Loss: 2.1020007133483887 | Test Loss: 2.096496105194092\n",
      "Epoch 284 | Train Loss: 1.826568841934204 | Test Loss: 1.8270955085754395\n",
      "Epoch 285 | Train Loss: 2.1008074283599854 | Test Loss: 2.0952935218811035\n",
      "Epoch 286 | Train Loss: 1.82500159740448 | Test Loss: 1.8255438804626465\n",
      "Epoch 287 | Train Loss: 2.099626302719116 | Test Loss: 2.094102621078491\n",
      "Epoch 288 | Train Loss: 1.8234518766403198 | Test Loss: 1.8240102529525757\n",
      "Epoch 289 | Train Loss: 2.098459482192993 | Test Loss: 2.092924118041992\n",
      "Epoch 290 | Train Loss: 1.821920394897461 | Test Loss: 1.8224945068359375\n",
      "Epoch 291 | Train Loss: 2.097303628921509 | Test Loss: 2.0917561054229736\n",
      "Epoch 292 | Train Loss: 1.8204047679901123 | Test Loss: 1.8209948539733887\n",
      "Epoch 293 | Train Loss: 2.096160888671875 | Test Loss: 2.090599536895752\n",
      "Epoch 294 | Train Loss: 1.8189061880111694 | Test Loss: 1.8195146322250366\n",
      "Epoch 295 | Train Loss: 2.09502911567688 | Test Loss: 2.0894575119018555\n",
      "Epoch 296 | Train Loss: 1.8174256086349487 | Test Loss: 1.8180514574050903\n",
      "Epoch 297 | Train Loss: 2.093909978866577 | Test Loss: 2.0883283615112305\n",
      "Epoch 298 | Train Loss: 1.815960168838501 | Test Loss: 1.8166041374206543\n",
      "Epoch 299 | Train Loss: 2.0928008556365967 | Test Loss: 2.0872132778167725\n",
      "Epoch 300 | Train Loss: 1.8145089149475098 | Test Loss: 1.8151683807373047\n",
      "Epoch 301 | Train Loss: 2.0917036533355713 | Test Loss: 2.0861082077026367\n",
      "Epoch 302 | Train Loss: 1.8130666017532349 | Test Loss: 1.8137423992156982\n",
      "Epoch 303 | Train Loss: 2.090615749359131 | Test Loss: 2.085012197494507\n",
      "Epoch 304 | Train Loss: 1.811636209487915 | Test Loss: 1.812328577041626\n",
      "Epoch 305 | Train Loss: 2.08953595161438 | Test Loss: 2.083925724029541\n",
      "Epoch 306 | Train Loss: 1.810219645500183 | Test Loss: 1.8109257221221924\n",
      "Epoch 307 | Train Loss: 2.088465929031372 | Test Loss: 2.0828499794006348\n",
      "Epoch 308 | Train Loss: 1.8088141679763794 | Test Loss: 1.809535026550293\n",
      "Epoch 309 | Train Loss: 2.087404489517212 | Test Loss: 2.0817813873291016\n",
      "Epoch 310 | Train Loss: 1.8074209690093994 | Test Loss: 1.8081562519073486\n",
      "Epoch 311 | Train Loss: 2.0863468647003174 | Test Loss: 2.080719232559204\n",
      "Epoch 312 | Train Loss: 1.8060415983200073 | Test Loss: 1.8067914247512817\n",
      "Epoch 313 | Train Loss: 2.085298776626587 | Test Loss: 2.079665184020996\n",
      "Epoch 314 | Train Loss: 1.8046748638153076 | Test Loss: 1.8054386377334595\n",
      "Epoch 315 | Train Loss: 2.0842578411102295 | Test Loss: 2.0786170959472656\n",
      "Epoch 316 | Train Loss: 1.8033195734024048 | Test Loss: 1.8040995597839355\n",
      "Epoch 317 | Train Loss: 2.083223342895508 | Test Loss: 2.077575445175171\n",
      "Epoch 318 | Train Loss: 1.8019789457321167 | Test Loss: 1.8027729988098145\n",
      "Epoch 319 | Train Loss: 2.0821993350982666 | Test Loss: 2.0765492916107178\n",
      "Epoch 320 | Train Loss: 1.8006517887115479 | Test Loss: 1.801459550857544\n",
      "Epoch 321 | Train Loss: 2.0811891555786133 | Test Loss: 2.0755343437194824\n",
      "Epoch 322 | Train Loss: 1.7993402481079102 | Test Loss: 1.8001654148101807\n",
      "Epoch 323 | Train Loss: 2.0801851749420166 | Test Loss: 2.07452654838562\n",
      "Epoch 324 | Train Loss: 1.798050880432129 | Test Loss: 1.7988899946212769\n",
      "Epoch 325 | Train Loss: 2.079188108444214 | Test Loss: 2.073530673980713\n",
      "Epoch 326 | Train Loss: 1.7967759370803833 | Test Loss: 1.7976279258728027\n",
      "Epoch 327 | Train Loss: 2.0782034397125244 | Test Loss: 2.0725457668304443\n",
      "Epoch 328 | Train Loss: 1.7955148220062256 | Test Loss: 1.796378254890442\n",
      "Epoch 329 | Train Loss: 2.0772273540496826 | Test Loss: 2.0715694427490234\n",
      "Epoch 330 | Train Loss: 1.794266939163208 | Test Loss: 1.7951411008834839\n",
      "Epoch 331 | Train Loss: 2.0762574672698975 | Test Loss: 2.070600986480713\n",
      "Epoch 332 | Train Loss: 1.7930313348770142 | Test Loss: 1.7939153909683228\n",
      "Epoch 333 | Train Loss: 2.0752968788146973 | Test Loss: 2.069643020629883\n",
      "Epoch 334 | Train Loss: 1.7918078899383545 | Test Loss: 1.7927011251449585\n",
      "Epoch 335 | Train Loss: 2.074345827102661 | Test Loss: 2.0686941146850586\n",
      "Epoch 336 | Train Loss: 1.7905957698822021 | Test Loss: 1.7914992570877075\n",
      "Epoch 337 | Train Loss: 2.0734047889709473 | Test Loss: 2.0677545070648193\n",
      "Epoch 338 | Train Loss: 1.7893977165222168 | Test Loss: 1.7903114557266235\n",
      "Epoch 339 | Train Loss: 2.072469472885132 | Test Loss: 2.066821813583374\n",
      "Epoch 340 | Train Loss: 1.7882128953933716 | Test Loss: 1.7891367673873901\n",
      "Epoch 341 | Train Loss: 2.0715415477752686 | Test Loss: 2.0658986568450928\n",
      "Epoch 342 | Train Loss: 1.7870395183563232 | Test Loss: 1.787972331047058\n",
      "Epoch 343 | Train Loss: 2.0706233978271484 | Test Loss: 2.0649845600128174\n",
      "Epoch 344 | Train Loss: 1.7858777046203613 | Test Loss: 1.7868202924728394\n",
      "Epoch 345 | Train Loss: 2.0697133541107178 | Test Loss: 2.064079999923706\n",
      "Epoch 346 | Train Loss: 1.7847263813018799 | Test Loss: 1.7856775522232056\n",
      "Epoch 347 | Train Loss: 2.068814516067505 | Test Loss: 2.0631868839263916\n",
      "Epoch 348 | Train Loss: 1.783584713935852 | Test Loss: 1.7845442295074463\n",
      "Epoch 349 | Train Loss: 2.067924976348877 | Test Loss: 2.062300443649292\n",
      "Epoch 350 | Train Loss: 1.7824523448944092 | Test Loss: 1.7834203243255615\n",
      "Epoch 351 | Train Loss: 2.0670440196990967 | Test Loss: 2.061424970626831\n",
      "Epoch 352 | Train Loss: 1.7813304662704468 | Test Loss: 1.7823071479797363\n",
      "Epoch 353 | Train Loss: 2.066171169281006 | Test Loss: 2.0605602264404297\n",
      "Epoch 354 | Train Loss: 1.7802187204360962 | Test Loss: 1.7812045812606812\n",
      "Epoch 355 | Train Loss: 2.0653076171875 | Test Loss: 2.059703826904297\n",
      "Epoch 356 | Train Loss: 1.7791179418563843 | Test Loss: 1.7801132202148438\n",
      "Epoch 357 | Train Loss: 2.064450263977051 | Test Loss: 2.0588531494140625\n",
      "Epoch 358 | Train Loss: 1.7780283689498901 | Test Loss: 1.7790319919586182\n",
      "Epoch 359 | Train Loss: 2.0635986328125 | Test Loss: 2.0580077171325684\n",
      "Epoch 360 | Train Loss: 1.776948094367981 | Test Loss: 1.7779595851898193\n",
      "Epoch 361 | Train Loss: 2.062753677368164 | Test Loss: 2.057169198989868\n",
      "Epoch 362 | Train Loss: 1.775875210762024 | Test Loss: 1.7768940925598145\n",
      "Epoch 363 | Train Loss: 2.0619168281555176 | Test Loss: 2.056337356567383\n",
      "Epoch 364 | Train Loss: 1.7748090028762817 | Test Loss: 1.7758333683013916\n",
      "Epoch 365 | Train Loss: 2.061086416244507 | Test Loss: 2.0555121898651123\n",
      "Epoch 366 | Train Loss: 1.773750901222229 | Test Loss: 1.7747794389724731\n",
      "Epoch 367 | Train Loss: 2.0602667331695557 | Test Loss: 2.0546958446502686\n",
      "Epoch 368 | Train Loss: 1.772700548171997 | Test Loss: 1.773734211921692\n",
      "Epoch 369 | Train Loss: 2.0594544410705566 | Test Loss: 2.053884983062744\n",
      "Epoch 370 | Train Loss: 1.7716593742370605 | Test Loss: 1.7726976871490479\n",
      "Epoch 371 | Train Loss: 2.0586464405059814 | Test Loss: 2.0530781745910645\n",
      "Epoch 372 | Train Loss: 1.7706265449523926 | Test Loss: 1.7716699838638306\n",
      "Epoch 373 | Train Loss: 2.0578417778015137 | Test Loss: 2.052274465560913\n",
      "Epoch 374 | Train Loss: 1.7696024179458618 | Test Loss: 1.770650863647461\n",
      "Epoch 375 | Train Loss: 2.0570383071899414 | Test Loss: 2.051475763320923\n",
      "Epoch 376 | Train Loss: 1.7685859203338623 | Test Loss: 1.7696382999420166\n",
      "Epoch 377 | Train Loss: 2.056239366531372 | Test Loss: 2.050679922103882\n",
      "Epoch 378 | Train Loss: 1.7675765752792358 | Test Loss: 1.768632411956787\n",
      "Epoch 379 | Train Loss: 2.0554444789886475 | Test Loss: 2.04988956451416\n",
      "Epoch 380 | Train Loss: 1.7665749788284302 | Test Loss: 1.7676341533660889\n",
      "Epoch 381 | Train Loss: 2.0546555519104004 | Test Loss: 2.0491042137145996\n",
      "Epoch 382 | Train Loss: 1.7655830383300781 | Test Loss: 1.7666465044021606\n",
      "Epoch 383 | Train Loss: 2.0538716316223145 | Test Loss: 2.0483217239379883\n",
      "Epoch 384 | Train Loss: 1.7645995616912842 | Test Loss: 1.7656664848327637\n",
      "Epoch 385 | Train Loss: 2.053086280822754 | Test Loss: 2.0475399494171143\n",
      "Epoch 386 | Train Loss: 1.7636232376098633 | Test Loss: 1.7646934986114502\n",
      "Epoch 387 | Train Loss: 2.052302837371826 | Test Loss: 2.046762466430664\n",
      "Epoch 388 | Train Loss: 1.7626533508300781 | Test Loss: 1.7637269496917725\n",
      "Epoch 389 | Train Loss: 2.051525115966797 | Test Loss: 2.0459916591644287\n",
      "Epoch 390 | Train Loss: 1.76168954372406 | Test Loss: 1.7627677917480469\n",
      "Epoch 391 | Train Loss: 2.050753355026245 | Test Loss: 2.0452258586883545\n",
      "Epoch 392 | Train Loss: 1.760733962059021 | Test Loss: 1.7618166208267212\n",
      "Epoch 393 | Train Loss: 2.0499846935272217 | Test Loss: 2.0444629192352295\n",
      "Epoch 394 | Train Loss: 1.7597860097885132 | Test Loss: 1.7608752250671387\n",
      "Epoch 395 | Train Loss: 2.0492172241210938 | Test Loss: 2.043701648712158\n",
      "Epoch 396 | Train Loss: 1.7588480710983276 | Test Loss: 1.7599437236785889\n",
      "Epoch 397 | Train Loss: 2.048452138900757 | Test Loss: 2.04294490814209\n",
      "Epoch 398 | Train Loss: 1.7579193115234375 | Test Loss: 1.7590203285217285\n",
      "Epoch 399 | Train Loss: 2.047692060470581 | Test Loss: 2.0421907901763916\n",
      "Epoch 400 | Train Loss: 1.7569990158081055 | Test Loss: 1.7581053972244263\n",
      "Epoch 401 | Train Loss: 2.0469367504119873 | Test Loss: 2.0414397716522217\n",
      "Epoch 402 | Train Loss: 1.756089210510254 | Test Loss: 1.7572001218795776\n",
      "Epoch 403 | Train Loss: 2.0461843013763428 | Test Loss: 2.0406930446624756\n",
      "Epoch 404 | Train Loss: 1.7551863193511963 | Test Loss: 1.756300926208496\n",
      "Epoch 405 | Train Loss: 2.045436382293701 | Test Loss: 2.039952516555786\n",
      "Epoch 406 | Train Loss: 1.7542904615402222 | Test Loss: 1.7554093599319458\n",
      "Epoch 407 | Train Loss: 2.0446929931640625 | Test Loss: 2.0392165184020996\n",
      "Epoch 408 | Train Loss: 1.753401517868042 | Test Loss: 1.7545232772827148\n",
      "Epoch 409 | Train Loss: 2.0439538955688477 | Test Loss: 2.038484573364258\n",
      "Epoch 410 | Train Loss: 1.7525180578231812 | Test Loss: 1.7536427974700928\n",
      "Epoch 411 | Train Loss: 2.0432181358337402 | Test Loss: 2.0377566814422607\n",
      "Epoch 412 | Train Loss: 1.7516406774520874 | Test Loss: 1.7527680397033691\n",
      "Epoch 413 | Train Loss: 2.0424859523773193 | Test Loss: 2.0370330810546875\n",
      "Epoch 414 | Train Loss: 1.750769853591919 | Test Loss: 1.7519007921218872\n",
      "Epoch 415 | Train Loss: 2.041759490966797 | Test Loss: 2.03631591796875\n",
      "Epoch 416 | Train Loss: 1.7499077320098877 | Test Loss: 1.7510414123535156\n",
      "Epoch 417 | Train Loss: 2.04103946685791 | Test Loss: 2.0356035232543945\n",
      "Epoch 418 | Train Loss: 1.749053955078125 | Test Loss: 1.7501895427703857\n",
      "Epoch 419 | Train Loss: 2.0403237342834473 | Test Loss: 2.034895658493042\n",
      "Epoch 420 | Train Loss: 1.748207926750183 | Test Loss: 1.7493441104888916\n",
      "Epoch 421 | Train Loss: 2.03961443901062 | Test Loss: 2.034193754196167\n",
      "Epoch 422 | Train Loss: 1.7473671436309814 | Test Loss: 1.7485028505325317\n",
      "Epoch 423 | Train Loss: 2.038912296295166 | Test Loss: 2.033503293991089\n",
      "Epoch 424 | Train Loss: 1.746530532836914 | Test Loss: 1.7476651668548584\n",
      "Epoch 425 | Train Loss: 2.038219928741455 | Test Loss: 2.0328171253204346\n",
      "Epoch 426 | Train Loss: 1.745698094367981 | Test Loss: 1.7468321323394775\n",
      "Epoch 427 | Train Loss: 2.0375306606292725 | Test Loss: 2.032133102416992\n",
      "Epoch 428 | Train Loss: 1.7448701858520508 | Test Loss: 1.746004343032837\n",
      "Epoch 429 | Train Loss: 2.036846160888672 | Test Loss: 2.0314571857452393\n",
      "Epoch 430 | Train Loss: 1.7440472841262817 | Test Loss: 1.7451823949813843\n",
      "Epoch 431 | Train Loss: 2.0361673831939697 | Test Loss: 2.0307822227478027\n",
      "Epoch 432 | Train Loss: 1.7432328462600708 | Test Loss: 1.7443687915802002\n",
      "Epoch 433 | Train Loss: 2.0354843139648438 | Test Loss: 2.0301012992858887\n",
      "Epoch 434 | Train Loss: 1.7424267530441284 | Test Loss: 1.7435613870620728\n",
      "Epoch 435 | Train Loss: 2.0347986221313477 | Test Loss: 2.0294225215911865\n",
      "Epoch 436 | Train Loss: 1.7416260242462158 | Test Loss: 1.7427586317062378\n",
      "Epoch 437 | Train Loss: 2.0341148376464844 | Test Loss: 2.0287463665008545\n",
      "Epoch 438 | Train Loss: 1.7408294677734375 | Test Loss: 1.7419586181640625\n",
      "Epoch 439 | Train Loss: 2.033433437347412 | Test Loss: 2.028071880340576\n",
      "Epoch 440 | Train Loss: 1.7400381565093994 | Test Loss: 1.7411648035049438\n",
      "Epoch 441 | Train Loss: 2.03275465965271 | Test Loss: 2.0274016857147217\n",
      "Epoch 442 | Train Loss: 1.7392507791519165 | Test Loss: 1.7403748035430908\n",
      "Epoch 443 | Train Loss: 2.0320801734924316 | Test Loss: 2.02673602104187\n",
      "Epoch 444 | Train Loss: 1.7384673357009888 | Test Loss: 1.739590048789978\n",
      "Epoch 445 | Train Loss: 2.0314090251922607 | Test Loss: 2.0260725021362305\n",
      "Epoch 446 | Train Loss: 1.7376892566680908 | Test Loss: 1.7388099431991577\n",
      "Epoch 447 | Train Loss: 2.030738115310669 | Test Loss: 2.0254085063934326\n",
      "Epoch 448 | Train Loss: 1.7369147539138794 | Test Loss: 1.7380318641662598\n",
      "Epoch 449 | Train Loss: 2.0300686359405518 | Test Loss: 2.024749279022217\n",
      "Epoch 450 | Train Loss: 1.7361425161361694 | Test Loss: 1.7372552156448364\n",
      "Epoch 451 | Train Loss: 2.029402017593384 | Test Loss: 2.024095296859741\n",
      "Epoch 452 | Train Loss: 1.7353713512420654 | Test Loss: 1.7364797592163086\n",
      "Epoch 453 | Train Loss: 2.0287423133850098 | Test Loss: 2.0234463214874268\n",
      "Epoch 454 | Train Loss: 1.7346014976501465 | Test Loss: 1.7357057332992554\n",
      "Epoch 455 | Train Loss: 2.0280847549438477 | Test Loss: 2.0227999687194824\n",
      "Epoch 456 | Train Loss: 1.7338353395462036 | Test Loss: 1.7349343299865723\n",
      "Epoch 457 | Train Loss: 2.0274341106414795 | Test Loss: 2.022158622741699\n",
      "Epoch 458 | Train Loss: 1.7330721616744995 | Test Loss: 1.7341667413711548\n",
      "Epoch 459 | Train Loss: 2.026787281036377 | Test Loss: 2.021522045135498\n",
      "Epoch 460 | Train Loss: 1.7323108911514282 | Test Loss: 1.7333998680114746\n",
      "Epoch 461 | Train Loss: 2.026144027709961 | Test Loss: 2.0208873748779297\n",
      "Epoch 462 | Train Loss: 1.7315516471862793 | Test Loss: 1.7326364517211914\n",
      "Epoch 463 | Train Loss: 2.0255041122436523 | Test Loss: 2.020256519317627\n",
      "Epoch 464 | Train Loss: 1.7307970523834229 | Test Loss: 1.7318774461746216\n",
      "Epoch 465 | Train Loss: 2.0248656272888184 | Test Loss: 2.019627094268799\n",
      "Epoch 466 | Train Loss: 1.7300456762313843 | Test Loss: 1.731122374534607\n",
      "Epoch 467 | Train Loss: 2.0242295265197754 | Test Loss: 2.0190014839172363\n",
      "Epoch 468 | Train Loss: 1.7292983531951904 | Test Loss: 1.7303730249404907\n",
      "Epoch 469 | Train Loss: 2.0235960483551025 | Test Loss: 2.018380641937256\n",
      "Epoch 470 | Train Loss: 1.7285563945770264 | Test Loss: 1.7296278476715088\n",
      "Epoch 471 | Train Loss: 2.0229687690734863 | Test Loss: 2.017765760421753\n",
      "Epoch 472 | Train Loss: 1.7278178930282593 | Test Loss: 1.7288857698440552\n",
      "Epoch 473 | Train Loss: 2.0223476886749268 | Test Loss: 2.017151117324829\n",
      "Epoch 474 | Train Loss: 1.7270833253860474 | Test Loss: 1.7281467914581299\n",
      "Epoch 475 | Train Loss: 2.0217251777648926 | Test Loss: 2.016538143157959\n",
      "Epoch 476 | Train Loss: 1.7263517379760742 | Test Loss: 1.7274127006530762\n",
      "Epoch 477 | Train Loss: 2.0211048126220703 | Test Loss: 2.0159261226654053\n",
      "Epoch 478 | Train Loss: 1.7256255149841309 | Test Loss: 1.7266836166381836\n",
      "Epoch 479 | Train Loss: 2.0204832553863525 | Test Loss: 2.0153138637542725\n",
      "Epoch 480 | Train Loss: 1.7249038219451904 | Test Loss: 1.7259578704833984\n",
      "Epoch 481 | Train Loss: 2.0198652744293213 | Test Loss: 2.0147042274475098\n",
      "Epoch 482 | Train Loss: 1.7241854667663574 | Test Loss: 1.725235104560852\n",
      "Epoch 483 | Train Loss: 2.019249200820923 | Test Loss: 2.0140974521636963\n",
      "Epoch 484 | Train Loss: 1.7234697341918945 | Test Loss: 1.7245159149169922\n",
      "Epoch 485 | Train Loss: 2.0186355113983154 | Test Loss: 2.0134940147399902\n",
      "Epoch 486 | Train Loss: 1.7227582931518555 | Test Loss: 1.7238003015518188\n",
      "Epoch 487 | Train Loss: 2.01802659034729 | Test Loss: 2.01289439201355\n",
      "Epoch 488 | Train Loss: 1.7220501899719238 | Test Loss: 1.723088026046753\n",
      "Epoch 489 | Train Loss: 2.017418384552002 | Test Loss: 2.0122947692871094\n",
      "Epoch 490 | Train Loss: 1.7213457822799683 | Test Loss: 1.7223782539367676\n",
      "Epoch 491 | Train Loss: 2.016812562942505 | Test Loss: 2.011698007583618\n",
      "Epoch 492 | Train Loss: 1.7206435203552246 | Test Loss: 1.7216718196868896\n",
      "Epoch 493 | Train Loss: 2.016207218170166 | Test Loss: 2.0111024379730225\n",
      "Epoch 494 | Train Loss: 1.719944715499878 | Test Loss: 1.7209689617156982\n",
      "Epoch 495 | Train Loss: 2.0156047344207764 | Test Loss: 2.0105085372924805\n",
      "Epoch 496 | Train Loss: 1.7192504405975342 | Test Loss: 1.7202718257904053\n",
      "Epoch 497 | Train Loss: 2.015003204345703 | Test Loss: 2.009916067123413\n",
      "Epoch 498 | Train Loss: 1.7185614109039307 | Test Loss: 1.719578742980957\n",
      "Epoch 499 | Train Loss: 2.014404058456421 | Test Loss: 2.0093300342559814\n",
      "Epoch 500 | Train Loss: 1.7178754806518555 | Test Loss: 1.7188875675201416\n",
      "Epoch 501 | Train Loss: 2.013814926147461 | Test Loss: 2.0087497234344482\n",
      "Epoch 502 | Train Loss: 1.7171902656555176 | Test Loss: 1.7181988954544067\n",
      "Epoch 503 | Train Loss: 2.0132269859313965 | Test Loss: 2.008169412612915\n",
      "Epoch 504 | Train Loss: 1.716509461402893 | Test Loss: 1.7175143957138062\n",
      "Epoch 505 | Train Loss: 2.0126383304595947 | Test Loss: 2.007589101791382\n",
      "Epoch 506 | Train Loss: 1.715831995010376 | Test Loss: 1.7168326377868652\n",
      "Epoch 507 | Train Loss: 2.0120506286621094 | Test Loss: 2.0070106983184814\n",
      "Epoch 508 | Train Loss: 1.7151578664779663 | Test Loss: 1.7161531448364258\n",
      "Epoch 509 | Train Loss: 2.0114669799804688 | Test Loss: 2.006434440612793\n",
      "Epoch 510 | Train Loss: 1.714485764503479 | Test Loss: 1.7154762744903564\n",
      "Epoch 511 | Train Loss: 2.0108835697174072 | Test Loss: 2.005859375\n",
      "Epoch 512 | Train Loss: 1.7138161659240723 | Test Loss: 1.7148020267486572\n",
      "Epoch 513 | Train Loss: 2.010303258895874 | Test Loss: 2.005286931991577\n",
      "Epoch 514 | Train Loss: 1.7131496667861938 | Test Loss: 1.7141309976577759\n",
      "Epoch 515 | Train Loss: 2.0097227096557617 | Test Loss: 2.0047152042388916\n",
      "Epoch 516 | Train Loss: 1.7124865055084229 | Test Loss: 1.7134628295898438\n",
      "Epoch 517 | Train Loss: 2.0091423988342285 | Test Loss: 2.0041439533233643\n",
      "Epoch 518 | Train Loss: 1.7118271589279175 | Test Loss: 1.7127971649169922\n",
      "Epoch 519 | Train Loss: 2.0085623264312744 | Test Loss: 2.003573417663574\n",
      "Epoch 520 | Train Loss: 1.7111703157424927 | Test Loss: 1.7121332883834839\n",
      "Epoch 521 | Train Loss: 2.007983446121216 | Test Loss: 2.003005027770996\n",
      "Epoch 522 | Train Loss: 1.7105146646499634 | Test Loss: 1.711471438407898\n",
      "Epoch 523 | Train Loss: 2.0074057579040527 | Test Loss: 2.00243878364563\n",
      "Epoch 524 | Train Loss: 1.709861397743225 | Test Loss: 1.7108103036880493\n",
      "Epoch 525 | Train Loss: 2.0068318843841553 | Test Loss: 2.0018763542175293\n",
      "Epoch 526 | Train Loss: 1.7092078924179077 | Test Loss: 1.7101496458053589\n",
      "Epoch 527 | Train Loss: 2.006260633468628 | Test Loss: 2.0013155937194824\n",
      "Epoch 528 | Train Loss: 1.7085539102554321 | Test Loss: 1.7094897031784058\n",
      "Epoch 529 | Train Loss: 2.0056920051574707 | Test Loss: 2.000758409500122\n",
      "Epoch 530 | Train Loss: 1.7079017162322998 | Test Loss: 1.7088319063186646\n",
      "Epoch 531 | Train Loss: 2.005125045776367 | Test Loss: 2.000204086303711\n",
      "Epoch 532 | Train Loss: 1.707249641418457 | Test Loss: 1.7081735134124756\n",
      "Epoch 533 | Train Loss: 2.0045595169067383 | Test Loss: 1.999650478363037\n",
      "Epoch 534 | Train Loss: 1.7065978050231934 | Test Loss: 1.7075159549713135\n",
      "Epoch 535 | Train Loss: 2.003995180130005 | Test Loss: 1.9990971088409424\n",
      "Epoch 536 | Train Loss: 1.705946922302246 | Test Loss: 1.7068601846694946\n",
      "Epoch 537 | Train Loss: 2.0034313201904297 | Test Loss: 1.9985450506210327\n",
      "Epoch 538 | Train Loss: 1.7052981853485107 | Test Loss: 1.7062071561813354\n",
      "Epoch 539 | Train Loss: 2.0028693675994873 | Test Loss: 1.9979947805404663\n",
      "Epoch 540 | Train Loss: 1.704651951789856 | Test Loss: 1.7055567502975464\n",
      "Epoch 541 | Train Loss: 2.0023093223571777 | Test Loss: 1.9974457025527954\n",
      "Epoch 542 | Train Loss: 1.704007863998413 | Test Loss: 1.704909324645996\n",
      "Epoch 543 | Train Loss: 2.0017507076263428 | Test Loss: 1.9968960285186768\n",
      "Epoch 544 | Train Loss: 1.7033659219741821 | Test Loss: 1.704264521598816\n",
      "Epoch 545 | Train Loss: 2.001192092895508 | Test Loss: 1.9963462352752686\n",
      "Epoch 546 | Train Loss: 1.7027279138565063 | Test Loss: 1.7036257982254028\n",
      "Epoch 547 | Train Loss: 2.0006344318389893 | Test Loss: 1.995798110961914\n",
      "Epoch 548 | Train Loss: 1.7020955085754395 | Test Loss: 1.702991247177124\n",
      "Epoch 549 | Train Loss: 2.0000758171081543 | Test Loss: 1.9952497482299805\n",
      "Epoch 550 | Train Loss: 1.7014672756195068 | Test Loss: 1.70236074924469\n",
      "Epoch 551 | Train Loss: 1.9995174407958984 | Test Loss: 1.9947028160095215\n",
      "Epoch 552 | Train Loss: 1.7008419036865234 | Test Loss: 1.7017312049865723\n",
      "Epoch 553 | Train Loss: 1.9989615678787231 | Test Loss: 1.9941575527191162\n",
      "Epoch 554 | Train Loss: 1.7002177238464355 | Test Loss: 1.7011032104492188\n",
      "Epoch 555 | Train Loss: 1.9984076023101807 | Test Loss: 1.9936151504516602\n",
      "Epoch 556 | Train Loss: 1.6995943784713745 | Test Loss: 1.7004749774932861\n",
      "Epoch 557 | Train Loss: 1.9978562593460083 | Test Loss: 1.993074655532837\n",
      "Epoch 558 | Train Loss: 1.6989704370498657 | Test Loss: 1.699846863746643\n",
      "Epoch 559 | Train Loss: 1.9973074197769165 | Test Loss: 1.992536187171936\n",
      "Epoch 560 | Train Loss: 1.6983476877212524 | Test Loss: 1.699221134185791\n",
      "Epoch 561 | Train Loss: 1.9967591762542725 | Test Loss: 1.991998314857483\n",
      "Epoch 562 | Train Loss: 1.6977263689041138 | Test Loss: 1.698596715927124\n",
      "Epoch 563 | Train Loss: 1.9962104558944702 | Test Loss: 1.9914600849151611\n",
      "Epoch 564 | Train Loss: 1.6971062421798706 | Test Loss: 1.6979737281799316\n",
      "Epoch 565 | Train Loss: 1.995661735534668 | Test Loss: 1.9909226894378662\n",
      "Epoch 566 | Train Loss: 1.6964867115020752 | Test Loss: 1.6973516941070557\n",
      "Epoch 567 | Train Loss: 1.9951127767562866 | Test Loss: 1.9903861284255981\n",
      "Epoch 568 | Train Loss: 1.6958680152893066 | Test Loss: 1.696730613708496\n",
      "Epoch 569 | Train Loss: 1.994564414024353 | Test Loss: 1.9898498058319092\n",
      "Epoch 570 | Train Loss: 1.695250391960144 | Test Loss: 1.6961091756820679\n",
      "Epoch 571 | Train Loss: 1.994015097618103 | Test Loss: 1.989314079284668\n",
      "Epoch 572 | Train Loss: 1.6946321725845337 | Test Loss: 1.695486307144165\n",
      "Epoch 573 | Train Loss: 1.9934678077697754 | Test Loss: 1.9887791872024536\n",
      "Epoch 574 | Train Loss: 1.6940133571624756 | Test Loss: 1.6948628425598145\n",
      "Epoch 575 | Train Loss: 1.9929215908050537 | Test Loss: 1.9882445335388184\n",
      "Epoch 576 | Train Loss: 1.6933952569961548 | Test Loss: 1.6942408084869385\n",
      "Epoch 577 | Train Loss: 1.9923750162124634 | Test Loss: 1.9877103567123413\n",
      "Epoch 578 | Train Loss: 1.6927788257598877 | Test Loss: 1.693618893623352\n",
      "Epoch 579 | Train Loss: 1.9918290376663208 | Test Loss: 1.987177848815918\n",
      "Epoch 580 | Train Loss: 1.692162275314331 | Test Loss: 1.6929972171783447\n",
      "Epoch 581 | Train Loss: 1.9912837743759155 | Test Loss: 1.9866443872451782\n",
      "Epoch 582 | Train Loss: 1.6915454864501953 | Test Loss: 1.6923757791519165\n",
      "Epoch 583 | Train Loss: 1.990738868713379 | Test Loss: 1.986111044883728\n",
      "Epoch 584 | Train Loss: 1.690929889678955 | Test Loss: 1.6917564868927002\n",
      "Epoch 585 | Train Loss: 1.9901939630508423 | Test Loss: 1.9855766296386719\n",
      "Epoch 586 | Train Loss: 1.690315842628479 | Test Loss: 1.6911401748657227\n",
      "Epoch 587 | Train Loss: 1.9896477460861206 | Test Loss: 1.9850398302078247\n",
      "Epoch 588 | Train Loss: 1.6897063255310059 | Test Loss: 1.6905261278152466\n",
      "Epoch 589 | Train Loss: 1.9890986680984497 | Test Loss: 1.9845012426376343\n",
      "Epoch 590 | Train Loss: 1.6890966892242432 | Test Loss: 1.6899127960205078\n",
      "Epoch 591 | Train Loss: 1.9885482788085938 | Test Loss: 1.983961582183838\n",
      "Epoch 592 | Train Loss: 1.6884881258010864 | Test Loss: 1.6892998218536377\n",
      "Epoch 593 | Train Loss: 1.9879989624023438 | Test Loss: 1.9834222793579102\n",
      "Epoch 594 | Train Loss: 1.6878801584243774 | Test Loss: 1.6886886358261108\n",
      "Epoch 595 | Train Loss: 1.9874495267868042 | Test Loss: 1.9828821420669556\n",
      "Epoch 596 | Train Loss: 1.687273621559143 | Test Loss: 1.6880791187286377\n",
      "Epoch 597 | Train Loss: 1.9868990182876587 | Test Loss: 1.9823421239852905\n",
      "Epoch 598 | Train Loss: 1.6866681575775146 | Test Loss: 1.6874706745147705\n",
      "Epoch 599 | Train Loss: 1.9863481521606445 | Test Loss: 1.9818023443222046\n",
      "Epoch 600 | Train Loss: 1.686063528060913 | Test Loss: 1.6868619918823242\n",
      "Epoch 601 | Train Loss: 1.9857971668243408 | Test Loss: 1.9812633991241455\n",
      "Epoch 602 | Train Loss: 1.6854580640792847 | Test Loss: 1.6862534284591675\n",
      "Epoch 603 | Train Loss: 1.9852478504180908 | Test Loss: 1.980726718902588\n",
      "Epoch 604 | Train Loss: 1.684851884841919 | Test Loss: 1.685642957687378\n",
      "Epoch 605 | Train Loss: 1.9846998453140259 | Test Loss: 1.9801914691925049\n",
      "Epoch 606 | Train Loss: 1.6842442750930786 | Test Loss: 1.685031771659851\n",
      "Epoch 607 | Train Loss: 1.9841532707214355 | Test Loss: 1.979658603668213\n",
      "Epoch 608 | Train Loss: 1.683634877204895 | Test Loss: 1.6844178438186646\n",
      "Epoch 609 | Train Loss: 1.983608365058899 | Test Loss: 1.9791269302368164\n",
      "Epoch 610 | Train Loss: 1.68302321434021 | Test Loss: 1.6838040351867676\n",
      "Epoch 611 | Train Loss: 1.983062744140625 | Test Loss: 1.9785943031311035\n",
      "Epoch 612 | Train Loss: 1.6824121475219727 | Test Loss: 1.6831904649734497\n",
      "Epoch 613 | Train Loss: 1.9825166463851929 | Test Loss: 1.978061556816101\n",
      "Epoch 614 | Train Loss: 1.6818021535873413 | Test Loss: 1.6825791597366333\n",
      "Epoch 615 | Train Loss: 1.9819705486297607 | Test Loss: 1.977527141571045\n",
      "Epoch 616 | Train Loss: 1.6811943054199219 | Test Loss: 1.6819703578948975\n",
      "Epoch 617 | Train Loss: 1.9814231395721436 | Test Loss: 1.976990818977356\n",
      "Epoch 618 | Train Loss: 1.6805884838104248 | Test Loss: 1.6813632249832153\n",
      "Epoch 619 | Train Loss: 1.9808743000030518 | Test Loss: 1.9764519929885864\n",
      "Epoch 620 | Train Loss: 1.6799848079681396 | Test Loss: 1.6807581186294556\n",
      "Epoch 621 | Train Loss: 1.9803208112716675 | Test Loss: 1.9759093523025513\n",
      "Epoch 622 | Train Loss: 1.6793813705444336 | Test Loss: 1.6801528930664062\n",
      "Epoch 623 | Train Loss: 1.979765772819519 | Test Loss: 1.975366473197937\n",
      "Epoch 624 | Train Loss: 1.678778052330017 | Test Loss: 1.6795474290847778\n",
      "Epoch 625 | Train Loss: 1.9792097806930542 | Test Loss: 1.9748224020004272\n",
      "Epoch 626 | Train Loss: 1.678174614906311 | Test Loss: 1.6789417266845703\n",
      "Epoch 627 | Train Loss: 1.978653073310852 | Test Loss: 1.9742785692214966\n",
      "Epoch 628 | Train Loss: 1.67756986618042 | Test Loss: 1.6783355474472046\n",
      "Epoch 629 | Train Loss: 1.9780956506729126 | Test Loss: 1.9737350940704346\n",
      "Epoch 630 | Train Loss: 1.6769660711288452 | Test Loss: 1.6777315139770508\n",
      "Epoch 631 | Train Loss: 1.9775372743606567 | Test Loss: 1.9731899499893188\n",
      "Epoch 632 | Train Loss: 1.6763646602630615 | Test Loss: 1.6771295070648193\n",
      "Epoch 633 | Train Loss: 1.9769753217697144 | Test Loss: 1.9726420640945435\n",
      "Epoch 634 | Train Loss: 1.6757651567459106 | Test Loss: 1.6765285730361938\n",
      "Epoch 635 | Train Loss: 1.9764105081558228 | Test Loss: 1.9720919132232666\n",
      "Epoch 636 | Train Loss: 1.6751670837402344 | Test Loss: 1.6759294271469116\n",
      "Epoch 637 | Train Loss: 1.975843071937561 | Test Loss: 1.9715396165847778\n",
      "Epoch 638 | Train Loss: 1.6745702028274536 | Test Loss: 1.6753309965133667\n",
      "Epoch 639 | Train Loss: 1.9752744436264038 | Test Loss: 1.9709858894348145\n",
      "Epoch 640 | Train Loss: 1.6739740371704102 | Test Loss: 1.6747336387634277\n",
      "Epoch 641 | Train Loss: 1.974704623222351 | Test Loss: 1.9704315662384033\n",
      "Epoch 642 | Train Loss: 1.6733790636062622 | Test Loss: 1.6741373538970947\n",
      "Epoch 643 | Train Loss: 1.9741337299346924 | Test Loss: 1.9698773622512817\n",
      "Epoch 644 | Train Loss: 1.6727828979492188 | Test Loss: 1.6735410690307617\n",
      "Epoch 645 | Train Loss: 1.9735617637634277 | Test Loss: 1.9693214893341064\n",
      "Epoch 646 | Train Loss: 1.6721895933151245 | Test Loss: 1.672948956489563\n",
      "Epoch 647 | Train Loss: 1.9729877710342407 | Test Loss: 1.968761682510376\n",
      "Epoch 648 | Train Loss: 1.6715991497039795 | Test Loss: 1.6723575592041016\n",
      "Epoch 649 | Train Loss: 1.9724119901657104 | Test Loss: 1.9682005643844604\n",
      "Epoch 650 | Train Loss: 1.671008825302124 | Test Loss: 1.6717661619186401\n",
      "Epoch 651 | Train Loss: 1.971835970878601 | Test Loss: 1.9676395654678345\n",
      "Epoch 652 | Train Loss: 1.6704188585281372 | Test Loss: 1.6711753606796265\n",
      "Epoch 653 | Train Loss: 1.9712607860565186 | Test Loss: 1.9670796394348145\n",
      "Epoch 654 | Train Loss: 1.6698297262191772 | Test Loss: 1.6705849170684814\n",
      "Epoch 655 | Train Loss: 1.9706860780715942 | Test Loss: 1.9665206670761108\n",
      "Epoch 656 | Train Loss: 1.669240117073059 | Test Loss: 1.6699947118759155\n",
      "Epoch 657 | Train Loss: 1.9701132774353027 | Test Loss: 1.965962529182434\n",
      "Epoch 658 | Train Loss: 1.6686509847640991 | Test Loss: 1.6694049835205078\n",
      "Epoch 659 | Train Loss: 1.9695402383804321 | Test Loss: 1.9654045104980469\n",
      "Epoch 660 | Train Loss: 1.6680617332458496 | Test Loss: 1.6688153743743896\n",
      "Epoch 661 | Train Loss: 1.968968391418457 | Test Loss: 1.9648473262786865\n",
      "Epoch 662 | Train Loss: 1.667472004890442 | Test Loss: 1.6682239770889282\n",
      "Epoch 663 | Train Loss: 1.9683974981307983 | Test Loss: 1.9642918109893799\n",
      "Epoch 664 | Train Loss: 1.6668803691864014 | Test Loss: 1.6676307916641235\n",
      "Epoch 665 | Train Loss: 1.967828392982483 | Test Loss: 1.9637384414672852\n",
      "Epoch 666 | Train Loss: 1.6662870645523071 | Test Loss: 1.6670364141464233\n",
      "Epoch 667 | Train Loss: 1.9672602415084839 | Test Loss: 1.9631863832473755\n",
      "Epoch 668 | Train Loss: 1.6656930446624756 | Test Loss: 1.6664425134658813\n",
      "Epoch 669 | Train Loss: 1.9666917324066162 | Test Loss: 1.9626338481903076\n",
      "Epoch 670 | Train Loss: 1.6650999784469604 | Test Loss: 1.6658514738082886\n",
      "Epoch 671 | Train Loss: 1.9661227464675903 | Test Loss: 1.9620808362960815\n",
      "Epoch 672 | Train Loss: 1.6645088195800781 | Test Loss: 1.6652605533599854\n",
      "Epoch 673 | Train Loss: 1.9655544757843018 | Test Loss: 1.9615265130996704\n",
      "Epoch 674 | Train Loss: 1.663918375968933 | Test Loss: 1.6646723747253418\n",
      "Epoch 675 | Train Loss: 1.9649837017059326 | Test Loss: 1.9609702825546265\n",
      "Epoch 676 | Train Loss: 1.6633296012878418 | Test Loss: 1.6640864610671997\n",
      "Epoch 677 | Train Loss: 1.9644114971160889 | Test Loss: 1.9604142904281616\n",
      "Epoch 678 | Train Loss: 1.6627442836761475 | Test Loss: 1.663503646850586\n",
      "Epoch 679 | Train Loss: 1.963839054107666 | Test Loss: 1.9598581790924072\n",
      "Epoch 680 | Train Loss: 1.6621609926223755 | Test Loss: 1.6629220247268677\n",
      "Epoch 681 | Train Loss: 1.963266134262085 | Test Loss: 1.959303617477417\n",
      "Epoch 682 | Train Loss: 1.6615763902664185 | Test Loss: 1.6623390913009644\n",
      "Epoch 683 | Train Loss: 1.9626939296722412 | Test Loss: 1.958751916885376\n",
      "Epoch 684 | Train Loss: 1.6609904766082764 | Test Loss: 1.6617553234100342\n",
      "Epoch 685 | Train Loss: 1.962123155593872 | Test Loss: 1.9582031965255737\n",
      "Epoch 686 | Train Loss: 1.6604026556015015 | Test Loss: 1.6611696481704712\n",
      "Epoch 687 | Train Loss: 1.9615553617477417 | Test Loss: 1.9576574563980103\n",
      "Epoch 688 | Train Loss: 1.6598138809204102 | Test Loss: 1.660584568977356\n",
      "Epoch 689 | Train Loss: 1.960989236831665 | Test Loss: 1.9571146965026855\n",
      "Epoch 690 | Train Loss: 1.6592259407043457 | Test Loss: 1.659999132156372\n",
      "Epoch 691 | Train Loss: 1.9604249000549316 | Test Loss: 1.9565739631652832\n",
      "Epoch 692 | Train Loss: 1.6586376428604126 | Test Loss: 1.6594133377075195\n",
      "Epoch 693 | Train Loss: 1.9598616361618042 | Test Loss: 1.9560340642929077\n",
      "Epoch 694 | Train Loss: 1.6580497026443481 | Test Loss: 1.658827304840088\n",
      "Epoch 695 | Train Loss: 1.9592992067337036 | Test Loss: 1.9554955959320068\n",
      "Epoch 696 | Train Loss: 1.657461404800415 | Test Loss: 1.6582419872283936\n",
      "Epoch 697 | Train Loss: 1.9587370157241821 | Test Loss: 1.9549570083618164\n",
      "Epoch 698 | Train Loss: 1.6568747758865356 | Test Loss: 1.6576595306396484\n",
      "Epoch 699 | Train Loss: 1.9581732749938965 | Test Loss: 1.954417109489441\n",
      "Epoch 700 | Train Loss: 1.6562912464141846 | Test Loss: 1.6570799350738525\n",
      "Epoch 701 | Train Loss: 1.9576079845428467 | Test Loss: 1.9538766145706177\n",
      "Epoch 702 | Train Loss: 1.6557083129882812 | Test Loss: 1.656499981880188\n",
      "Epoch 703 | Train Loss: 1.9570430517196655 | Test Loss: 1.9533361196517944\n",
      "Epoch 704 | Train Loss: 1.6551240682601929 | Test Loss: 1.6559187173843384\n",
      "Epoch 705 | Train Loss: 1.9564787149429321 | Test Loss: 1.9527952671051025\n",
      "Epoch 706 | Train Loss: 1.6545385122299194 | Test Loss: 1.6553370952606201\n",
      "Epoch 707 | Train Loss: 1.9559142589569092 | Test Loss: 1.952255129814148\n",
      "Epoch 708 | Train Loss: 1.6539535522460938 | Test Loss: 1.6547563076019287\n",
      "Epoch 709 | Train Loss: 1.9553502798080444 | Test Loss: 1.9517143964767456\n",
      "Epoch 710 | Train Loss: 1.6533699035644531 | Test Loss: 1.6541767120361328\n",
      "Epoch 711 | Train Loss: 1.9547847509384155 | Test Loss: 1.951171636581421\n",
      "Epoch 712 | Train Loss: 1.652787685394287 | Test Loss: 1.6535985469818115\n",
      "Epoch 713 | Train Loss: 1.9542163610458374 | Test Loss: 1.9506268501281738\n",
      "Epoch 714 | Train Loss: 1.6522066593170166 | Test Loss: 1.6530218124389648\n",
      "Epoch 715 | Train Loss: 1.953646183013916 | Test Loss: 1.950080156326294\n",
      "Epoch 716 | Train Loss: 1.6516274213790894 | Test Loss: 1.6524467468261719\n",
      "Epoch 717 | Train Loss: 1.95307457447052 | Test Loss: 1.9495325088500977\n",
      "Epoch 718 | Train Loss: 1.6510506868362427 | Test Loss: 1.6518741846084595\n",
      "Epoch 719 | Train Loss: 1.9525021314620972 | Test Loss: 1.948983907699585\n",
      "Epoch 720 | Train Loss: 1.6504758596420288 | Test Loss: 1.6513042449951172\n",
      "Epoch 721 | Train Loss: 1.9519277811050415 | Test Loss: 1.948434829711914\n",
      "Epoch 722 | Train Loss: 1.6499018669128418 | Test Loss: 1.6507346630096436\n",
      "Epoch 723 | Train Loss: 1.9513540267944336 | Test Loss: 1.9478880167007446\n",
      "Epoch 724 | Train Loss: 1.6493265628814697 | Test Loss: 1.650162935256958\n",
      "Epoch 725 | Train Loss: 1.9507818222045898 | Test Loss: 1.9473412036895752\n",
      "Epoch 726 | Train Loss: 1.6487505435943604 | Test Loss: 1.6495931148529053\n",
      "Epoch 727 | Train Loss: 1.9502092599868774 | Test Loss: 1.9467945098876953\n",
      "Epoch 728 | Train Loss: 1.6481744050979614 | Test Loss: 1.6490219831466675\n",
      "Epoch 729 | Train Loss: 1.9496381282806396 | Test Loss: 1.946249008178711\n",
      "Epoch 730 | Train Loss: 1.6475977897644043 | Test Loss: 1.6484520435333252\n",
      "Epoch 731 | Train Loss: 1.9490684270858765 | Test Loss: 1.9457019567489624\n",
      "Epoch 732 | Train Loss: 1.6470226049423218 | Test Loss: 1.6478849649429321\n",
      "Epoch 733 | Train Loss: 1.948495864868164 | Test Loss: 1.9451543092727661\n",
      "Epoch 734 | Train Loss: 1.6464498043060303 | Test Loss: 1.64732027053833\n",
      "Epoch 735 | Train Loss: 1.947920560836792 | Test Loss: 1.9446055889129639\n",
      "Epoch 736 | Train Loss: 1.6458784341812134 | Test Loss: 1.6467567682266235\n",
      "Epoch 737 | Train Loss: 1.9473445415496826 | Test Loss: 1.944059133529663\n",
      "Epoch 738 | Train Loss: 1.6453067064285278 | Test Loss: 1.6461915969848633\n",
      "Epoch 739 | Train Loss: 1.9467709064483643 | Test Loss: 1.943515419960022\n",
      "Epoch 740 | Train Loss: 1.6447325944900513 | Test Loss: 1.6456246376037598\n",
      "Epoch 741 | Train Loss: 1.9461992979049683 | Test Loss: 1.9429731369018555\n",
      "Epoch 742 | Train Loss: 1.6441574096679688 | Test Loss: 1.6450577974319458\n",
      "Epoch 743 | Train Loss: 1.9456276893615723 | Test Loss: 1.942431926727295\n",
      "Epoch 744 | Train Loss: 1.6435824632644653 | Test Loss: 1.6444926261901855\n",
      "Epoch 745 | Train Loss: 1.9450554847717285 | Test Loss: 1.9418905973434448\n",
      "Epoch 746 | Train Loss: 1.6430091857910156 | Test Loss: 1.6439286470413208\n",
      "Epoch 747 | Train Loss: 1.944482445716858 | Test Loss: 1.9413503408432007\n",
      "Epoch 748 | Train Loss: 1.6424344778060913 | Test Loss: 1.6433628797531128\n",
      "Epoch 749 | Train Loss: 1.9439122676849365 | Test Loss: 1.9408119916915894\n",
      "Epoch 750 | Train Loss: 1.6418592929840088 | Test Loss: 1.6427981853485107\n",
      "Epoch 751 | Train Loss: 1.94334077835083 | Test Loss: 1.9402732849121094\n",
      "Epoch 752 | Train Loss: 1.6412849426269531 | Test Loss: 1.6422332525253296\n",
      "Epoch 753 | Train Loss: 1.9427701234817505 | Test Loss: 1.9397372007369995\n",
      "Epoch 754 | Train Loss: 1.6407082080841064 | Test Loss: 1.6416654586791992\n",
      "Epoch 755 | Train Loss: 1.9422043561935425 | Test Loss: 1.9392033815383911\n",
      "Epoch 756 | Train Loss: 1.6401307582855225 | Test Loss: 1.6410983800888062\n",
      "Epoch 757 | Train Loss: 1.941638469696045 | Test Loss: 1.9386706352233887\n",
      "Epoch 758 | Train Loss: 1.6395541429519653 | Test Loss: 1.640531063079834\n",
      "Epoch 759 | Train Loss: 1.941072940826416 | Test Loss: 1.9381366968154907\n",
      "Epoch 760 | Train Loss: 1.6389777660369873 | Test Loss: 1.6399643421173096\n",
      "Epoch 761 | Train Loss: 1.9405070543289185 | Test Loss: 1.9376028776168823\n",
      "Epoch 762 | Train Loss: 1.638403058052063 | Test Loss: 1.639399528503418\n",
      "Epoch 763 | Train Loss: 1.93994140625 | Test Loss: 1.9370694160461426\n",
      "Epoch 764 | Train Loss: 1.6378308534622192 | Test Loss: 1.638836145401001\n",
      "Epoch 765 | Train Loss: 1.9393764734268188 | Test Loss: 1.9365381002426147\n",
      "Epoch 766 | Train Loss: 1.6372586488723755 | Test Loss: 1.6382726430892944\n",
      "Epoch 767 | Train Loss: 1.938812017440796 | Test Loss: 1.9360079765319824\n",
      "Epoch 768 | Train Loss: 1.6366870403289795 | Test Loss: 1.6377102136611938\n",
      "Epoch 769 | Train Loss: 1.9382472038269043 | Test Loss: 1.9354780912399292\n",
      "Epoch 770 | Train Loss: 1.6361169815063477 | Test Loss: 1.6371511220932007\n",
      "Epoch 771 | Train Loss: 1.9376798868179321 | Test Loss: 1.934944748878479\n",
      "Epoch 772 | Train Loss: 1.63555109500885 | Test Loss: 1.636594533920288\n",
      "Epoch 773 | Train Loss: 1.9371099472045898 | Test Loss: 1.9344102144241333\n",
      "Epoch 774 | Train Loss: 1.6349860429763794 | Test Loss: 1.6360386610031128\n",
      "Epoch 775 | Train Loss: 1.9365395307540894 | Test Loss: 1.9338762760162354\n",
      "Epoch 776 | Train Loss: 1.6344202756881714 | Test Loss: 1.635481357574463\n",
      "Epoch 777 | Train Loss: 1.9359700679779053 | Test Loss: 1.933341145515442\n",
      "Epoch 778 | Train Loss: 1.6338545083999634 | Test Loss: 1.6349244117736816\n",
      "Epoch 779 | Train Loss: 1.9353991746902466 | Test Loss: 1.9328042268753052\n",
      "Epoch 780 | Train Loss: 1.6332889795303345 | Test Loss: 1.634367823600769\n",
      "Epoch 781 | Train Loss: 1.9348276853561401 | Test Loss: 1.9322658777236938\n",
      "Epoch 782 | Train Loss: 1.6327232122421265 | Test Loss: 1.633811116218567\n",
      "Epoch 783 | Train Loss: 1.9342557191848755 | Test Loss: 1.9317268133163452\n",
      "Epoch 784 | Train Loss: 1.6321582794189453 | Test Loss: 1.633254051208496\n",
      "Epoch 785 | Train Loss: 1.9336826801300049 | Test Loss: 1.9311881065368652\n",
      "Epoch 786 | Train Loss: 1.631591558456421 | Test Loss: 1.632695198059082\n",
      "Epoch 787 | Train Loss: 1.933112382888794 | Test Loss: 1.9306527376174927\n",
      "Epoch 788 | Train Loss: 1.631022572517395 | Test Loss: 1.6321362257003784\n",
      "Epoch 789 | Train Loss: 1.9325430393218994 | Test Loss: 1.9301170110702515\n",
      "Epoch 790 | Train Loss: 1.6304550170898438 | Test Loss: 1.6315785646438599\n",
      "Epoch 791 | Train Loss: 1.9319733381271362 | Test Loss: 1.9295812845230103\n",
      "Epoch 792 | Train Loss: 1.6298878192901611 | Test Loss: 1.6310226917266846\n",
      "Epoch 793 | Train Loss: 1.931402325630188 | Test Loss: 1.929042100906372\n",
      "Epoch 794 | Train Loss: 1.6293240785598755 | Test Loss: 1.6304689645767212\n",
      "Epoch 795 | Train Loss: 1.9308289289474487 | Test Loss: 1.9285013675689697\n",
      "Epoch 796 | Train Loss: 1.6287606954574585 | Test Loss: 1.6299163103103638\n",
      "Epoch 797 | Train Loss: 1.9302544593811035 | Test Loss: 1.9279595613479614\n",
      "Epoch 798 | Train Loss: 1.6281976699829102 | Test Loss: 1.62936532497406\n",
      "Epoch 799 | Train Loss: 1.9296784400939941 | Test Loss: 1.9274170398712158\n",
      "Epoch 800 | Train Loss: 1.6276354789733887 | Test Loss: 1.6288154125213623\n",
      "Epoch 801 | Train Loss: 1.9291013479232788 | Test Loss: 1.9268730878829956\n",
      "Epoch 802 | Train Loss: 1.6270745992660522 | Test Loss: 1.6282655000686646\n",
      "Epoch 803 | Train Loss: 1.9285244941711426 | Test Loss: 1.9263297319412231\n",
      "Epoch 804 | Train Loss: 1.6265134811401367 | Test Loss: 1.6277168989181519\n",
      "Epoch 805 | Train Loss: 1.9279465675354004 | Test Loss: 1.9257854223251343\n",
      "Epoch 806 | Train Loss: 1.625954031944275 | Test Loss: 1.627169132232666\n",
      "Epoch 807 | Train Loss: 1.9273672103881836 | Test Loss: 1.9252402782440186\n",
      "Epoch 808 | Train Loss: 1.625394582748413 | Test Loss: 1.626621961593628\n",
      "Epoch 809 | Train Loss: 1.9267871379852295 | Test Loss: 1.9246941804885864\n",
      "Epoch 810 | Train Loss: 1.6248358488082886 | Test Loss: 1.626075267791748\n",
      "Epoch 811 | Train Loss: 1.926206350326538 | Test Loss: 1.924146056175232\n",
      "Epoch 812 | Train Loss: 1.6242778301239014 | Test Loss: 1.6255285739898682\n",
      "Epoch 813 | Train Loss: 1.9256260395050049 | Test Loss: 1.9235987663269043\n",
      "Epoch 814 | Train Loss: 1.6237181425094604 | Test Loss: 1.6249828338623047\n",
      "Epoch 815 | Train Loss: 1.9250439405441284 | Test Loss: 1.9230492115020752\n",
      "Epoch 816 | Train Loss: 1.6231606006622314 | Test Loss: 1.6244391202926636\n",
      "Epoch 817 | Train Loss: 1.9244593381881714 | Test Loss: 1.9224976301193237\n",
      "Epoch 818 | Train Loss: 1.622604250907898 | Test Loss: 1.6238963603973389\n",
      "Epoch 819 | Train Loss: 1.9238725900650024 | Test Loss: 1.9219460487365723\n",
      "Epoch 820 | Train Loss: 1.6220484972000122 | Test Loss: 1.6233540773391724\n",
      "Epoch 821 | Train Loss: 1.9232844114303589 | Test Loss: 1.9213942289352417\n",
      "Epoch 822 | Train Loss: 1.6214936971664429 | Test Loss: 1.6228125095367432\n",
      "Epoch 823 | Train Loss: 1.922695517539978 | Test Loss: 1.9208425283432007\n",
      "Epoch 824 | Train Loss: 1.62093985080719 | Test Loss: 1.6222716569900513\n",
      "Epoch 825 | Train Loss: 1.9221042394638062 | Test Loss: 1.9202916622161865\n",
      "Epoch 826 | Train Loss: 1.6203866004943848 | Test Loss: 1.6217323541641235\n",
      "Epoch 827 | Train Loss: 1.921511173248291 | Test Loss: 1.9197391271591187\n",
      "Epoch 828 | Train Loss: 1.619834542274475 | Test Loss: 1.621192216873169\n",
      "Epoch 829 | Train Loss: 1.9209187030792236 | Test Loss: 1.9191895723342896\n",
      "Epoch 830 | Train Loss: 1.6192799806594849 | Test Loss: 1.6206520795822144\n",
      "Epoch 831 | Train Loss: 1.920326828956604 | Test Loss: 1.9186383485794067\n",
      "Epoch 832 | Train Loss: 1.6187273263931274 | Test Loss: 1.6201149225234985\n",
      "Epoch 833 | Train Loss: 1.9197338819503784 | Test Loss: 1.9180843830108643\n",
      "Epoch 834 | Train Loss: 1.6181772947311401 | Test Loss: 1.6195803880691528\n",
      "Epoch 835 | Train Loss: 1.91913902759552 | Test Loss: 1.9175283908843994\n",
      "Epoch 836 | Train Loss: 1.6176294088363647 | Test Loss: 1.6190485954284668\n",
      "Epoch 837 | Train Loss: 1.9185428619384766 | Test Loss: 1.9169700145721436\n",
      "Epoch 838 | Train Loss: 1.6170837879180908 | Test Loss: 1.6185190677642822\n",
      "Epoch 839 | Train Loss: 1.9179449081420898 | Test Loss: 1.9164096117019653\n",
      "Epoch 840 | Train Loss: 1.6165395975112915 | Test Loss: 1.6179893016815186\n",
      "Epoch 841 | Train Loss: 1.9173479080200195 | Test Loss: 1.915852665901184\n",
      "Epoch 842 | Train Loss: 1.615992784500122 | Test Loss: 1.6174581050872803\n",
      "Epoch 843 | Train Loss: 1.9167546033859253 | Test Loss: 1.9152956008911133\n",
      "Epoch 844 | Train Loss: 1.6154451370239258 | Test Loss: 1.6169254779815674\n",
      "Epoch 845 | Train Loss: 1.9161617755889893 | Test Loss: 1.9147400856018066\n",
      "Epoch 846 | Train Loss: 1.6148945093154907 | Test Loss: 1.6163910627365112\n",
      "Epoch 847 | Train Loss: 1.9155709743499756 | Test Loss: 1.9141873121261597\n",
      "Epoch 848 | Train Loss: 1.614342212677002 | Test Loss: 1.6158559322357178\n",
      "Epoch 849 | Train Loss: 1.9149810075759888 | Test Loss: 1.9136356115341187\n",
      "Epoch 850 | Train Loss: 1.6137897968292236 | Test Loss: 1.615321159362793\n",
      "Epoch 851 | Train Loss: 1.914389967918396 | Test Loss: 1.913083791732788\n",
      "Epoch 852 | Train Loss: 1.613237977027893 | Test Loss: 1.6147874593734741\n",
      "Epoch 853 | Train Loss: 1.9137966632843018 | Test Loss: 1.9125287532806396\n",
      "Epoch 854 | Train Loss: 1.6126885414123535 | Test Loss: 1.6142549514770508\n",
      "Epoch 855 | Train Loss: 1.9132025241851807 | Test Loss: 1.911975622177124\n",
      "Epoch 856 | Train Loss: 1.6121368408203125 | Test Loss: 1.6137213706970215\n",
      "Epoch 857 | Train Loss: 1.9126098155975342 | Test Loss: 1.9114230871200562\n",
      "Epoch 858 | Train Loss: 1.6115834712982178 | Test Loss: 1.6131863594055176\n",
      "Epoch 859 | Train Loss: 1.9120177030563354 | Test Loss: 1.9108694791793823\n",
      "Epoch 860 | Train Loss: 1.6110308170318604 | Test Loss: 1.612653374671936\n",
      "Epoch 861 | Train Loss: 1.911423921585083 | Test Loss: 1.9103134870529175\n",
      "Epoch 862 | Train Loss: 1.6104800701141357 | Test Loss: 1.6121212244033813\n",
      "Epoch 863 | Train Loss: 1.9108288288116455 | Test Loss: 1.9097554683685303\n",
      "Epoch 864 | Train Loss: 1.609930157661438 | Test Loss: 1.6115907430648804\n",
      "Epoch 865 | Train Loss: 1.910231590270996 | Test Loss: 1.9091941118240356\n",
      "Epoch 866 | Train Loss: 1.609383225440979 | Test Loss: 1.6110626459121704\n",
      "Epoch 867 | Train Loss: 1.9096318483352661 | Test Loss: 1.9086308479309082\n",
      "Epoch 868 | Train Loss: 1.608837366104126 | Test Loss: 1.610536813735962\n",
      "Epoch 869 | Train Loss: 1.9090291261672974 | Test Loss: 1.908063292503357\n",
      "Epoch 870 | Train Loss: 1.608295202255249 | Test Loss: 1.6100142002105713\n",
      "Epoch 871 | Train Loss: 1.9084241390228271 | Test Loss: 1.9074938297271729\n",
      "Epoch 872 | Train Loss: 1.6077545881271362 | Test Loss: 1.609492301940918\n",
      "Epoch 873 | Train Loss: 1.9078189134597778 | Test Loss: 1.9069230556488037\n",
      "Epoch 874 | Train Loss: 1.6072149276733398 | Test Loss: 1.6089714765548706\n",
      "Epoch 875 | Train Loss: 1.9072123765945435 | Test Loss: 1.906351089477539\n",
      "Epoch 876 | Train Loss: 1.6066762208938599 | Test Loss: 1.6084513664245605\n",
      "Epoch 877 | Train Loss: 1.9066063165664673 | Test Loss: 1.9057796001434326\n",
      "Epoch 878 | Train Loss: 1.6061378717422485 | Test Loss: 1.6079306602478027\n",
      "Epoch 879 | Train Loss: 1.905999779701233 | Test Loss: 1.9052077531814575\n",
      "Epoch 880 | Train Loss: 1.6055989265441895 | Test Loss: 1.607409119606018\n",
      "Epoch 881 | Train Loss: 1.9053932428359985 | Test Loss: 1.9046354293823242\n",
      "Epoch 882 | Train Loss: 1.6050604581832886 | Test Loss: 1.606888771057129\n",
      "Epoch 883 | Train Loss: 1.9047871828079224 | Test Loss: 1.9040653705596924\n",
      "Epoch 884 | Train Loss: 1.6045206785202026 | Test Loss: 1.6063698530197144\n",
      "Epoch 885 | Train Loss: 1.904183030128479 | Test Loss: 1.903495192527771\n",
      "Epoch 886 | Train Loss: 1.603982925415039 | Test Loss: 1.6058504581451416\n",
      "Epoch 887 | Train Loss: 1.9035794734954834 | Test Loss: 1.9029269218444824\n",
      "Epoch 888 | Train Loss: 1.6034423112869263 | Test Loss: 1.6053290367126465\n",
      "Epoch 889 | Train Loss: 1.9029781818389893 | Test Loss: 1.9023606777191162\n",
      "Epoch 890 | Train Loss: 1.6028996706008911 | Test Loss: 1.6048064231872559\n",
      "Epoch 891 | Train Loss: 1.9023789167404175 | Test Loss: 1.9017972946166992\n",
      "Epoch 892 | Train Loss: 1.6023560762405396 | Test Loss: 1.6042835712432861\n",
      "Epoch 893 | Train Loss: 1.9017815589904785 | Test Loss: 1.9012352228164673\n",
      "Epoch 894 | Train Loss: 1.6018134355545044 | Test Loss: 1.6037629842758179\n",
      "Epoch 895 | Train Loss: 1.901183009147644 | Test Loss: 1.9006717205047607\n",
      "Epoch 896 | Train Loss: 1.601273536682129 | Test Loss: 1.6032453775405884\n",
      "Epoch 897 | Train Loss: 1.9005825519561768 | Test Loss: 1.9001073837280273\n",
      "Epoch 898 | Train Loss: 1.600736379623413 | Test Loss: 1.6027297973632812\n",
      "Epoch 899 | Train Loss: 1.89998197555542 | Test Loss: 1.8995424509048462\n",
      "Epoch 900 | Train Loss: 1.6002018451690674 | Test Loss: 1.6022149324417114\n",
      "Epoch 901 | Train Loss: 1.8993816375732422 | Test Loss: 1.8989756107330322\n",
      "Epoch 902 | Train Loss: 1.5996700525283813 | Test Loss: 1.6017029285430908\n",
      "Epoch 903 | Train Loss: 1.898777723312378 | Test Loss: 1.8984076976776123\n",
      "Epoch 904 | Train Loss: 1.5991401672363281 | Test Loss: 1.6011912822723389\n",
      "Epoch 905 | Train Loss: 1.8981739282608032 | Test Loss: 1.8978410959243774\n",
      "Epoch 906 | Train Loss: 1.5986113548278809 | Test Loss: 1.6006789207458496\n",
      "Epoch 907 | Train Loss: 1.8975725173950195 | Test Loss: 1.8972772359848022\n",
      "Epoch 908 | Train Loss: 1.5980819463729858 | Test Loss: 1.6001670360565186\n",
      "Epoch 909 | Train Loss: 1.8969706296920776 | Test Loss: 1.8967134952545166\n",
      "Epoch 910 | Train Loss: 1.5975521802902222 | Test Loss: 1.5996564626693726\n",
      "Epoch 911 | Train Loss: 1.8963673114776611 | Test Loss: 1.8961483240127563\n",
      "Epoch 912 | Train Loss: 1.5970234870910645 | Test Loss: 1.599146842956543\n",
      "Epoch 913 | Train Loss: 1.89576256275177 | Test Loss: 1.8955825567245483\n",
      "Epoch 914 | Train Loss: 1.5964953899383545 | Test Loss: 1.5986366271972656\n",
      "Epoch 915 | Train Loss: 1.8951579332351685 | Test Loss: 1.8950176239013672\n",
      "Epoch 916 | Train Loss: 1.595967173576355 | Test Loss: 1.5981262922286987\n",
      "Epoch 917 | Train Loss: 1.8945518732070923 | Test Loss: 1.8944531679153442\n",
      "Epoch 918 | Train Loss: 1.5954391956329346 | Test Loss: 1.597615122795105\n",
      "Epoch 919 | Train Loss: 1.8939437866210938 | Test Loss: 1.8938896656036377\n",
      "Epoch 920 | Train Loss: 1.594910740852356 | Test Loss: 1.5971035957336426\n",
      "Epoch 921 | Train Loss: 1.8933359384536743 | Test Loss: 1.8933254480361938\n",
      "Epoch 922 | Train Loss: 1.5943819284439087 | Test Loss: 1.596592903137207\n",
      "Epoch 923 | Train Loss: 1.8927286863327026 | Test Loss: 1.8927597999572754\n",
      "Epoch 924 | Train Loss: 1.5938557386398315 | Test Loss: 1.5960854291915894\n",
      "Epoch 925 | Train Loss: 1.8921200037002563 | Test Loss: 1.8921911716461182\n",
      "Epoch 926 | Train Loss: 1.5933316946029663 | Test Loss: 1.5955805778503418\n",
      "Epoch 927 | Train Loss: 1.891510009765625 | Test Loss: 1.89162015914917\n",
      "Epoch 928 | Train Loss: 1.5928089618682861 | Test Loss: 1.5950766801834106\n",
      "Epoch 929 | Train Loss: 1.8909001350402832 | Test Loss: 1.8910470008850098\n",
      "Epoch 930 | Train Loss: 1.5922874212265015 | Test Loss: 1.5945733785629272\n",
      "Epoch 931 | Train Loss: 1.8902889490127563 | Test Loss: 1.8904744386672974\n",
      "Epoch 932 | Train Loss: 1.5917651653289795 | Test Loss: 1.594067931175232\n",
      "Epoch 933 | Train Loss: 1.8896820545196533 | Test Loss: 1.8899052143096924\n",
      "Epoch 934 | Train Loss: 1.5912400484085083 | Test Loss: 1.593560814857483\n",
      "Epoch 935 | Train Loss: 1.889076828956604 | Test Loss: 1.8893383741378784\n",
      "Epoch 936 | Train Loss: 1.5907145738601685 | Test Loss: 1.593053936958313\n",
      "Epoch 937 | Train Loss: 1.8884721994400024 | Test Loss: 1.8887734413146973\n",
      "Epoch 938 | Train Loss: 1.5901892185211182 | Test Loss: 1.5925482511520386\n",
      "Epoch 939 | Train Loss: 1.8878670930862427 | Test Loss: 1.8882102966308594\n",
      "Epoch 940 | Train Loss: 1.5896644592285156 | Test Loss: 1.5920437574386597\n",
      "Epoch 941 | Train Loss: 1.8872613906860352 | Test Loss: 1.887646198272705\n",
      "Epoch 942 | Train Loss: 1.5891419649124146 | Test Loss: 1.591542363166809\n",
      "Epoch 943 | Train Loss: 1.886654257774353 | Test Loss: 1.8870824575424194\n",
      "Epoch 944 | Train Loss: 1.5886207818984985 | Test Loss: 1.5910412073135376\n",
      "Epoch 945 | Train Loss: 1.8860480785369873 | Test Loss: 1.886519432067871\n",
      "Epoch 946 | Train Loss: 1.588099718093872 | Test Loss: 1.5905402898788452\n",
      "Epoch 947 | Train Loss: 1.885441541671753 | Test Loss: 1.8859535455703735\n",
      "Epoch 948 | Train Loss: 1.5875798463821411 | Test Loss: 1.590042233467102\n",
      "Epoch 949 | Train Loss: 1.8848340511322021 | Test Loss: 1.885384440422058\n",
      "Epoch 950 | Train Loss: 1.5870627164840698 | Test Loss: 1.5895469188690186\n",
      "Epoch 951 | Train Loss: 1.88422429561615 | Test Loss: 1.884813904762268\n",
      "Epoch 952 | Train Loss: 1.5865461826324463 | Test Loss: 1.5890496969223022\n",
      "Epoch 953 | Train Loss: 1.8836166858673096 | Test Loss: 1.8842458724975586\n",
      "Epoch 954 | Train Loss: 1.5860267877578735 | Test Loss: 1.5885491371154785\n",
      "Epoch 955 | Train Loss: 1.8830108642578125 | Test Loss: 1.8836804628372192\n",
      "Epoch 956 | Train Loss: 1.5855039358139038 | Test Loss: 1.588044285774231\n",
      "Epoch 957 | Train Loss: 1.8824079036712646 | Test Loss: 1.8831167221069336\n",
      "Epoch 958 | Train Loss: 1.5849779844284058 | Test Loss: 1.5875381231307983\n",
      "Epoch 959 | Train Loss: 1.8818063735961914 | Test Loss: 1.882554292678833\n",
      "Epoch 960 | Train Loss: 1.5844534635543823 | Test Loss: 1.587032437324524\n",
      "Epoch 961 | Train Loss: 1.8812059164047241 | Test Loss: 1.8819949626922607\n",
      "Epoch 962 | Train Loss: 1.5839272737503052 | Test Loss: 1.5865243673324585\n",
      "Epoch 963 | Train Loss: 1.8806077241897583 | Test Loss: 1.8814386129379272\n",
      "Epoch 964 | Train Loss: 1.5833990573883057 | Test Loss: 1.5860155820846558\n",
      "Epoch 965 | Train Loss: 1.8800100088119507 | Test Loss: 1.8808834552764893\n",
      "Epoch 966 | Train Loss: 1.5828702449798584 | Test Loss: 1.5855076313018799\n",
      "Epoch 967 | Train Loss: 1.8794121742248535 | Test Loss: 1.8803272247314453\n",
      "Epoch 968 | Train Loss: 1.5823428630828857 | Test Loss: 1.585002064704895\n",
      "Epoch 969 | Train Loss: 1.8788131475448608 | Test Loss: 1.8797694444656372\n",
      "Epoch 970 | Train Loss: 1.5818175077438354 | Test Loss: 1.5845001935958862\n",
      "Epoch 971 | Train Loss: 1.8782119750976562 | Test Loss: 1.8792093992233276\n",
      "Epoch 972 | Train Loss: 1.581295132637024 | Test Loss: 1.584000825881958\n",
      "Epoch 973 | Train Loss: 1.877608299255371 | Test Loss: 1.878646969795227\n",
      "Epoch 974 | Train Loss: 1.5807749032974243 | Test Loss: 1.5835028886795044\n",
      "Epoch 975 | Train Loss: 1.8770034313201904 | Test Loss: 1.8780813217163086\n",
      "Epoch 976 | Train Loss: 1.5802569389343262 | Test Loss: 1.5830062627792358\n",
      "Epoch 977 | Train Loss: 1.8763964176177979 | Test Loss: 1.8775142431259155\n",
      "Epoch 978 | Train Loss: 1.5797405242919922 | Test Loss: 1.5825119018554688\n",
      "Epoch 979 | Train Loss: 1.8757885694503784 | Test Loss: 1.8769447803497314\n",
      "Epoch 980 | Train Loss: 1.579228162765503 | Test Loss: 1.5820187330245972\n",
      "Epoch 981 | Train Loss: 1.8751801252365112 | Test Loss: 1.876377820968628\n",
      "Epoch 982 | Train Loss: 1.5787155628204346 | Test Loss: 1.5815260410308838\n",
      "Epoch 983 | Train Loss: 1.87457275390625 | Test Loss: 1.875811219215393\n",
      "Epoch 984 | Train Loss: 1.5782045125961304 | Test Loss: 1.581032633781433\n",
      "Epoch 985 | Train Loss: 1.873967170715332 | Test Loss: 1.8752480745315552\n",
      "Epoch 986 | Train Loss: 1.5776907205581665 | Test Loss: 1.580536961555481\n",
      "Epoch 987 | Train Loss: 1.8733657598495483 | Test Loss: 1.874688982963562\n",
      "Epoch 988 | Train Loss: 1.577172875404358 | Test Loss: 1.5800384283065796\n",
      "Epoch 989 | Train Loss: 1.8727666139602661 | Test Loss: 1.8741304874420166\n",
      "Epoch 990 | Train Loss: 1.5766549110412598 | Test Loss: 1.5795414447784424\n",
      "Epoch 991 | Train Loss: 1.8721667528152466 | Test Loss: 1.8735709190368652\n",
      "Epoch 992 | Train Loss: 1.5761393308639526 | Test Loss: 1.5790477991104126\n",
      "Epoch 993 | Train Loss: 1.8715639114379883 | Test Loss: 1.873010277748108\n",
      "Epoch 994 | Train Loss: 1.5756261348724365 | Test Loss: 1.578555941581726\n",
      "Epoch 995 | Train Loss: 1.8709594011306763 | Test Loss: 1.8724491596221924\n",
      "Epoch 996 | Train Loss: 1.575115442276001 | Test Loss: 1.578064203262329\n",
      "Epoch 997 | Train Loss: 1.8703564405441284 | Test Loss: 1.871888518333435\n",
      "Epoch 998 | Train Loss: 1.5746042728424072 | Test Loss: 1.577573299407959\n",
      "Epoch 999 | Train Loss: 1.8697527647018433 | Test Loss: 1.8713271617889404\n",
      "Epoch 1000 | Train Loss: 1.5740940570831299 | Test Loss: 1.5770832300186157\n",
      "Epoch 1001 | Train Loss: 1.8691474199295044 | Test Loss: 1.8707640171051025\n",
      "Epoch 1002 | Train Loss: 1.5735857486724854 | Test Loss: 1.5765947103500366\n",
      "Epoch 1003 | Train Loss: 1.8685407638549805 | Test Loss: 1.8701987266540527\n",
      "Epoch 1004 | Train Loss: 1.5730788707733154 | Test Loss: 1.5761083364486694\n",
      "Epoch 1005 | Train Loss: 1.867932677268982 | Test Loss: 1.869632363319397\n",
      "Epoch 1006 | Train Loss: 1.5725735425949097 | Test Loss: 1.5756222009658813\n",
      "Epoch 1007 | Train Loss: 1.8673226833343506 | Test Loss: 1.8690650463104248\n",
      "Epoch 1008 | Train Loss: 1.5720689296722412 | Test Loss: 1.575136423110962\n",
      "Epoch 1009 | Train Loss: 1.8667114973068237 | Test Loss: 1.8684966564178467\n",
      "Epoch 1010 | Train Loss: 1.5715638399124146 | Test Loss: 1.5746482610702515\n",
      "Epoch 1011 | Train Loss: 1.8661003112792969 | Test Loss: 1.8679338693618774\n",
      "Epoch 1012 | Train Loss: 1.5710535049438477 | Test Loss: 1.574153184890747\n",
      "Epoch 1013 | Train Loss: 1.8654929399490356 | Test Loss: 1.8673744201660156\n",
      "Epoch 1014 | Train Loss: 1.5705370903015137 | Test Loss: 1.573656439781189\n",
      "Epoch 1015 | Train Loss: 1.864884614944458 | Test Loss: 1.8668134212493896\n",
      "Epoch 1016 | Train Loss: 1.5700198411941528 | Test Loss: 1.5731602907180786\n",
      "Epoch 1017 | Train Loss: 1.864274024963379 | Test Loss: 1.866247534751892\n",
      "Epoch 1018 | Train Loss: 1.5695042610168457 | Test Loss: 1.572666049003601\n",
      "Epoch 1019 | Train Loss: 1.8636600971221924 | Test Loss: 1.8656789064407349\n",
      "Epoch 1020 | Train Loss: 1.5689882040023804 | Test Loss: 1.5721708536148071\n",
      "Epoch 1021 | Train Loss: 1.863044023513794 | Test Loss: 1.8651087284088135\n",
      "Epoch 1022 | Train Loss: 1.5684690475463867 | Test Loss: 1.5716718435287476\n",
      "Epoch 1023 | Train Loss: 1.862428069114685 | Test Loss: 1.8645397424697876\n",
      "Epoch 1024 | Train Loss: 1.567944049835205 | Test Loss: 1.571166753768921\n",
      "Epoch 1025 | Train Loss: 1.8618144989013672 | Test Loss: 1.8639708757400513\n",
      "Epoch 1026 | Train Loss: 1.567416787147522 | Test Loss: 1.5706626176834106\n",
      "Epoch 1027 | Train Loss: 1.8611990213394165 | Test Loss: 1.8634003400802612\n",
      "Epoch 1028 | Train Loss: 1.5668916702270508 | Test Loss: 1.5701602697372437\n",
      "Epoch 1029 | Train Loss: 1.8605799674987793 | Test Loss: 1.862828016281128\n",
      "Epoch 1030 | Train Loss: 1.5663683414459229 | Test Loss: 1.5696607828140259\n",
      "Epoch 1031 | Train Loss: 1.8599568605422974 | Test Loss: 1.86225426197052\n",
      "Epoch 1032 | Train Loss: 1.5658471584320068 | Test Loss: 1.5691639184951782\n",
      "Epoch 1033 | Train Loss: 1.8593298196792603 | Test Loss: 1.8616794347763062\n",
      "Epoch 1034 | Train Loss: 1.5653280019760132 | Test Loss: 1.568669080734253\n",
      "Epoch 1035 | Train Loss: 1.858699917793274 | Test Loss: 1.861104130744934\n",
      "Epoch 1036 | Train Loss: 1.5648109912872314 | Test Loss: 1.5681769847869873\n",
      "Epoch 1037 | Train Loss: 1.858066201210022 | Test Loss: 1.8605287075042725\n",
      "Epoch 1038 | Train Loss: 1.5642937421798706 | Test Loss: 1.5676826238632202\n",
      "Epoch 1039 | Train Loss: 1.8574329614639282 | Test Loss: 1.8599525690078735\n",
      "Epoch 1040 | Train Loss: 1.5637742280960083 | Test Loss: 1.567186713218689\n",
      "Epoch 1041 | Train Loss: 1.8568004369735718 | Test Loss: 1.859375238418579\n",
      "Epoch 1042 | Train Loss: 1.5632555484771729 | Test Loss: 1.5666913986206055\n",
      "Epoch 1043 | Train Loss: 1.8561673164367676 | Test Loss: 1.8587955236434937\n",
      "Epoch 1044 | Train Loss: 1.562738060951233 | Test Loss: 1.5661985874176025\n",
      "Epoch 1045 | Train Loss: 1.8555307388305664 | Test Loss: 1.858210563659668\n",
      "Epoch 1046 | Train Loss: 1.5622241497039795 | Test Loss: 1.5657087564468384\n",
      "Epoch 1047 | Train Loss: 1.8548908233642578 | Test Loss: 1.8576228618621826\n",
      "Epoch 1048 | Train Loss: 1.561711311340332 | Test Loss: 1.565217137336731\n",
      "Epoch 1049 | Train Loss: 1.854250192642212 | Test Loss: 1.8570342063903809\n",
      "Epoch 1050 | Train Loss: 1.5611978769302368 | Test Loss: 1.5647246837615967\n",
      "Epoch 1051 | Train Loss: 1.85360848903656 | Test Loss: 1.8564435243606567\n",
      "Epoch 1052 | Train Loss: 1.5606836080551147 | Test Loss: 1.5642303228378296\n",
      "Epoch 1053 | Train Loss: 1.8529671430587769 | Test Loss: 1.8558510541915894\n",
      "Epoch 1054 | Train Loss: 1.5601682662963867 | Test Loss: 1.5637367963790894\n",
      "Epoch 1055 | Train Loss: 1.8523250818252563 | Test Loss: 1.8552579879760742\n",
      "Epoch 1056 | Train Loss: 1.5596507787704468 | Test Loss: 1.563239574432373\n",
      "Epoch 1057 | Train Loss: 1.8516854047775269 | Test Loss: 1.8546664714813232\n",
      "Epoch 1058 | Train Loss: 1.559132695198059 | Test Loss: 1.562741994857788\n",
      "Epoch 1059 | Train Loss: 1.8510472774505615 | Test Loss: 1.8540743589401245\n",
      "Epoch 1060 | Train Loss: 1.5586141347885132 | Test Loss: 1.5622460842132568\n",
      "Epoch 1061 | Train Loss: 1.8504072427749634 | Test Loss: 1.853481650352478\n",
      "Epoch 1062 | Train Loss: 1.5580967664718628 | Test Loss: 1.5617477893829346\n",
      "Epoch 1063 | Train Loss: 1.8497684001922607 | Test Loss: 1.8528907299041748\n",
      "Epoch 1064 | Train Loss: 1.557578206062317 | Test Loss: 1.5612494945526123\n",
      "Epoch 1065 | Train Loss: 1.8491315841674805 | Test Loss: 1.8523000478744507\n",
      "Epoch 1066 | Train Loss: 1.5570603609085083 | Test Loss: 1.560754418373108\n",
      "Epoch 1067 | Train Loss: 1.8484927415847778 | Test Loss: 1.8517067432403564\n",
      "Epoch 1068 | Train Loss: 1.5565464496612549 | Test Loss: 1.560262680053711\n",
      "Epoch 1069 | Train Loss: 1.847851037979126 | Test Loss: 1.851111888885498\n",
      "Epoch 1070 | Train Loss: 1.5560338497161865 | Test Loss: 1.5597705841064453\n",
      "Epoch 1071 | Train Loss: 1.847210168838501 | Test Loss: 1.8505191802978516\n",
      "Epoch 1072 | Train Loss: 1.5555195808410645 | Test Loss: 1.5592774152755737\n",
      "Epoch 1073 | Train Loss: 1.8465698957443237 | Test Loss: 1.8499255180358887\n",
      "Epoch 1074 | Train Loss: 1.5550049543380737 | Test Loss: 1.5587866306304932\n",
      "Epoch 1075 | Train Loss: 1.8459274768829346 | Test Loss: 1.8493313789367676\n",
      "Epoch 1076 | Train Loss: 1.5544918775558472 | Test Loss: 1.558295488357544\n",
      "Epoch 1077 | Train Loss: 1.8452850580215454 | Test Loss: 1.8487359285354614\n",
      "Epoch 1078 | Train Loss: 1.5539790391921997 | Test Loss: 1.5578062534332275\n",
      "Epoch 1079 | Train Loss: 1.844639539718628 | Test Loss: 1.8481388092041016\n",
      "Epoch 1080 | Train Loss: 1.5534669160842896 | Test Loss: 1.5573168992996216\n",
      "Epoch 1081 | Train Loss: 1.8439931869506836 | Test Loss: 1.8475408554077148\n",
      "Epoch 1082 | Train Loss: 1.5529539585113525 | Test Loss: 1.5568251609802246\n",
      "Epoch 1083 | Train Loss: 1.843348503112793 | Test Loss: 1.8469454050064087\n",
      "Epoch 1084 | Train Loss: 1.552437663078308 | Test Loss: 1.5563298463821411\n",
      "Epoch 1085 | Train Loss: 1.8427058458328247 | Test Loss: 1.8463495969772339\n",
      "Epoch 1086 | Train Loss: 1.5519201755523682 | Test Loss: 1.5558364391326904\n",
      "Epoch 1087 | Train Loss: 1.842061161994934 | Test Loss: 1.8457508087158203\n",
      "Epoch 1088 | Train Loss: 1.5514061450958252 | Test Loss: 1.5553460121154785\n",
      "Epoch 1089 | Train Loss: 1.8414099216461182 | Test Loss: 1.8451509475708008\n",
      "Epoch 1090 | Train Loss: 1.550892949104309 | Test Loss: 1.5548546314239502\n",
      "Epoch 1091 | Train Loss: 1.8407565355300903 | Test Loss: 1.8445477485656738\n",
      "Epoch 1092 | Train Loss: 1.5503811836242676 | Test Loss: 1.554364800453186\n",
      "Epoch 1093 | Train Loss: 1.840099573135376 | Test Loss: 1.843937873840332\n",
      "Epoch 1094 | Train Loss: 1.5498719215393066 | Test Loss: 1.5538760423660278\n",
      "Epoch 1095 | Train Loss: 1.839438796043396 | Test Loss: 1.8433228731155396\n",
      "Epoch 1096 | Train Loss: 1.5493630170822144 | Test Loss: 1.5533859729766846\n",
      "Epoch 1097 | Train Loss: 1.8387776613235474 | Test Loss: 1.8427069187164307\n",
      "Epoch 1098 | Train Loss: 1.548852562904358 | Test Loss: 1.5528925657272339\n",
      "Epoch 1099 | Train Loss: 1.8381189107894897 | Test Loss: 1.8420919179916382\n",
      "Epoch 1100 | Train Loss: 1.5483391284942627 | Test Loss: 1.5523961782455444\n",
      "Epoch 1101 | Train Loss: 1.83746337890625 | Test Loss: 1.8414808511734009\n",
      "Epoch 1102 | Train Loss: 1.5478219985961914 | Test Loss: 1.551896333694458\n",
      "Epoch 1103 | Train Loss: 1.8368107080459595 | Test Loss: 1.8408701419830322\n",
      "Epoch 1104 | Train Loss: 1.5473031997680664 | Test Loss: 1.5513982772827148\n",
      "Epoch 1105 | Train Loss: 1.8361557722091675 | Test Loss: 1.8402575254440308\n",
      "Epoch 1106 | Train Loss: 1.5467864274978638 | Test Loss: 1.5509012937545776\n",
      "Epoch 1107 | Train Loss: 1.8354986906051636 | Test Loss: 1.8396443128585815\n",
      "Epoch 1108 | Train Loss: 1.546269416809082 | Test Loss: 1.5504029989242554\n",
      "Epoch 1109 | Train Loss: 1.8348424434661865 | Test Loss: 1.8390352725982666\n",
      "Epoch 1110 | Train Loss: 1.545749545097351 | Test Loss: 1.5499001741409302\n",
      "Epoch 1111 | Train Loss: 1.834189534187317 | Test Loss: 1.838431477546692\n",
      "Epoch 1112 | Train Loss: 1.545224905014038 | Test Loss: 1.5493956804275513\n",
      "Epoch 1113 | Train Loss: 1.8335373401641846 | Test Loss: 1.8378291130065918\n",
      "Epoch 1114 | Train Loss: 1.5447008609771729 | Test Loss: 1.548892855644226\n",
      "Epoch 1115 | Train Loss: 1.8328808546066284 | Test Loss: 1.837222933769226\n",
      "Epoch 1116 | Train Loss: 1.544180154800415 | Test Loss: 1.5483933687210083\n",
      "Epoch 1117 | Train Loss: 1.832219123840332 | Test Loss: 1.8366100788116455\n",
      "Epoch 1118 | Train Loss: 1.5436640977859497 | Test Loss: 1.5478988885879517\n",
      "Epoch 1119 | Train Loss: 1.831551194190979 | Test Loss: 1.8359919786453247\n",
      "Epoch 1120 | Train Loss: 1.5431509017944336 | Test Loss: 1.547405481338501\n",
      "Epoch 1121 | Train Loss: 1.8308805227279663 | Test Loss: 1.8353725671768188\n",
      "Epoch 1122 | Train Loss: 1.5426381826400757 | Test Loss: 1.5469105243682861\n",
      "Epoch 1123 | Train Loss: 1.8302102088928223 | Test Loss: 1.8347536325454712\n",
      "Epoch 1124 | Train Loss: 1.542122721672058 | Test Loss: 1.5464138984680176\n",
      "Epoch 1125 | Train Loss: 1.829541563987732 | Test Loss: 1.8341344594955444\n",
      "Epoch 1126 | Train Loss: 1.541608214378357 | Test Loss: 1.5459178686141968\n",
      "Epoch 1127 | Train Loss: 1.828871488571167 | Test Loss: 1.833512783050537\n",
      "Epoch 1128 | Train Loss: 1.5410938262939453 | Test Loss: 1.5454201698303223\n",
      "Epoch 1129 | Train Loss: 1.8282002210617065 | Test Loss: 1.8328897953033447\n",
      "Epoch 1130 | Train Loss: 1.5405775308609009 | Test Loss: 1.544921636581421\n",
      "Epoch 1131 | Train Loss: 1.8275272846221924 | Test Loss: 1.8322638273239136\n",
      "Epoch 1132 | Train Loss: 1.540062665939331 | Test Loss: 1.5444254875183105\n",
      "Epoch 1133 | Train Loss: 1.8268516063690186 | Test Loss: 1.8316351175308228\n",
      "Epoch 1134 | Train Loss: 1.5395487546920776 | Test Loss: 1.5439306497573853\n",
      "Epoch 1135 | Train Loss: 1.8261746168136597 | Test Loss: 1.8310059309005737\n",
      "Epoch 1136 | Train Loss: 1.5390337705612183 | Test Loss: 1.5434329509735107\n",
      "Epoch 1137 | Train Loss: 1.8254975080490112 | Test Loss: 1.8303776979446411\n",
      "Epoch 1138 | Train Loss: 1.5385149717330933 | Test Loss: 1.542930245399475\n",
      "Epoch 1139 | Train Loss: 1.8248238563537598 | Test Loss: 1.829749345779419\n",
      "Epoch 1140 | Train Loss: 1.537994623184204 | Test Loss: 1.5424282550811768\n",
      "Epoch 1141 | Train Loss: 1.824148178100586 | Test Loss: 1.8291183710098267\n",
      "Epoch 1142 | Train Loss: 1.537475824356079 | Test Loss: 1.5419280529022217\n",
      "Epoch 1143 | Train Loss: 1.823470115661621 | Test Loss: 1.8284837007522583\n",
      "Epoch 1144 | Train Loss: 1.536960482597351 | Test Loss: 1.541433572769165\n",
      "Epoch 1145 | Train Loss: 1.8227851390838623 | Test Loss: 1.8278471231460571\n",
      "Epoch 1146 | Train Loss: 1.5364494323730469 | Test Loss: 1.5409414768218994\n",
      "Epoch 1147 | Train Loss: 1.8220951557159424 | Test Loss: 1.8272112607955933\n",
      "Epoch 1148 | Train Loss: 1.535939335823059 | Test Loss: 1.5404493808746338\n",
      "Epoch 1149 | Train Loss: 1.8214030265808105 | Test Loss: 1.8265736103057861\n",
      "Epoch 1150 | Train Loss: 1.5354279279708862 | Test Loss: 1.5399534702301025\n",
      "Epoch 1151 | Train Loss: 1.820715069770813 | Test Loss: 1.8259345293045044\n",
      "Epoch 1152 | Train Loss: 1.5349147319793701 | Test Loss: 1.5394620895385742\n",
      "Epoch 1153 | Train Loss: 1.820021390914917 | Test Loss: 1.8252900838851929\n",
      "Epoch 1154 | Train Loss: 1.5344061851501465 | Test Loss: 1.538973331451416\n",
      "Epoch 1155 | Train Loss: 1.8193250894546509 | Test Loss: 1.8246445655822754\n",
      "Epoch 1156 | Train Loss: 1.5339006185531616 | Test Loss: 1.5384843349456787\n",
      "Epoch 1157 | Train Loss: 1.818628191947937 | Test Loss: 1.8240017890930176\n",
      "Epoch 1158 | Train Loss: 1.5333887338638306 | Test Loss: 1.5379878282546997\n",
      "Epoch 1159 | Train Loss: 1.8179348707199097 | Test Loss: 1.8233600854873657\n",
      "Epoch 1160 | Train Loss: 1.5328713655471802 | Test Loss: 1.537488579750061\n",
      "Epoch 1161 | Train Loss: 1.8172413110733032 | Test Loss: 1.8227159976959229\n",
      "Epoch 1162 | Train Loss: 1.532353401184082 | Test Loss: 1.5369924306869507\n",
      "Epoch 1163 | Train Loss: 1.8165433406829834 | Test Loss: 1.822067379951477\n",
      "Epoch 1164 | Train Loss: 1.531838297843933 | Test Loss: 1.5365022420883179\n",
      "Epoch 1165 | Train Loss: 1.8158379793167114 | Test Loss: 1.8214151859283447\n",
      "Epoch 1166 | Train Loss: 1.5313290357589722 | Test Loss: 1.536012887954712\n",
      "Epoch 1167 | Train Loss: 1.8151309490203857 | Test Loss: 1.8207658529281616\n",
      "Epoch 1168 | Train Loss: 1.5308153629302979 | Test Loss: 1.5355178117752075\n",
      "Epoch 1169 | Train Loss: 1.8144267797470093 | Test Loss: 1.8201178312301636\n",
      "Epoch 1170 | Train Loss: 1.5302958488464355 | Test Loss: 1.5350204706192017\n",
      "Epoch 1171 | Train Loss: 1.813723087310791 | Test Loss: 1.8194680213928223\n",
      "Epoch 1172 | Train Loss: 1.5297775268554688 | Test Loss: 1.5345255136489868\n",
      "Epoch 1173 | Train Loss: 1.813014030456543 | Test Loss: 1.8188142776489258\n",
      "Epoch 1174 | Train Loss: 1.5292607545852661 | Test Loss: 1.534029245376587\n",
      "Epoch 1175 | Train Loss: 1.812303066253662 | Test Loss: 1.8181580305099487\n",
      "Epoch 1176 | Train Loss: 1.5287407636642456 | Test Loss: 1.533531665802002\n",
      "Epoch 1177 | Train Loss: 1.811591386795044 | Test Loss: 1.8174985647201538\n",
      "Epoch 1178 | Train Loss: 1.528221607208252 | Test Loss: 1.5330374240875244\n",
      "Epoch 1179 | Train Loss: 1.8108733892440796 | Test Loss: 1.8168318271636963\n",
      "Epoch 1180 | Train Loss: 1.5277081727981567 | Test Loss: 1.5325496196746826\n",
      "Epoch 1181 | Train Loss: 1.8101450204849243 | Test Loss: 1.8161593675613403\n",
      "Epoch 1182 | Train Loss: 1.5272008180618286 | Test Loss: 1.5320614576339722\n",
      "Epoch 1183 | Train Loss: 1.8094143867492676 | Test Loss: 1.8154915571212769\n",
      "Epoch 1184 | Train Loss: 1.5266886949539185 | Test Loss: 1.531567931175232\n",
      "Epoch 1185 | Train Loss: 1.8086881637573242 | Test Loss: 1.8148295879364014\n",
      "Epoch 1186 | Train Loss: 1.5261675119400024 | Test Loss: 1.5310649871826172\n",
      "Epoch 1187 | Train Loss: 1.8079698085784912 | Test Loss: 1.8141716718673706\n",
      "Epoch 1188 | Train Loss: 1.5256394147872925 | Test Loss: 1.53056001663208\n",
      "Epoch 1189 | Train Loss: 1.8072514533996582 | Test Loss: 1.8135123252868652\n",
      "Epoch 1190 | Train Loss: 1.5251119136810303 | Test Loss: 1.5300517082214355\n",
      "Epoch 1191 | Train Loss: 1.8065375089645386 | Test Loss: 1.8128525018692017\n",
      "Epoch 1192 | Train Loss: 1.5245791673660278 | Test Loss: 1.529540777206421\n",
      "Epoch 1193 | Train Loss: 1.8058263063430786 | Test Loss: 1.8121917247772217\n",
      "Epoch 1194 | Train Loss: 1.5240447521209717 | Test Loss: 1.5290284156799316\n",
      "Epoch 1195 | Train Loss: 1.805114984512329 | Test Loss: 1.8115299940109253\n",
      "Epoch 1196 | Train Loss: 1.5235095024108887 | Test Loss: 1.5285165309906006\n",
      "Epoch 1197 | Train Loss: 1.8043997287750244 | Test Loss: 1.8108665943145752\n",
      "Epoch 1198 | Train Loss: 1.52297842502594 | Test Loss: 1.5280044078826904\n",
      "Epoch 1199 | Train Loss: 1.803680181503296 | Test Loss: 1.8102017641067505\n",
      "Epoch 1200 | Train Loss: 1.5224453210830688 | Test Loss: 1.5274896621704102\n",
      "Epoch 1201 | Train Loss: 1.8029621839523315 | Test Loss: 1.809535026550293\n",
      "Epoch 1202 | Train Loss: 1.5219101905822754 | Test Loss: 1.526973843574524\n",
      "Epoch 1203 | Train Loss: 1.8022422790527344 | Test Loss: 1.8088661432266235\n",
      "Epoch 1204 | Train Loss: 1.5213758945465088 | Test Loss: 1.5264601707458496\n",
      "Epoch 1205 | Train Loss: 1.8015168905258179 | Test Loss: 1.8081941604614258\n",
      "Epoch 1206 | Train Loss: 1.5208450555801392 | Test Loss: 1.5259469747543335\n",
      "Epoch 1207 | Train Loss: 1.8007880449295044 | Test Loss: 1.8075199127197266\n",
      "Epoch 1208 | Train Loss: 1.5203148126602173 | Test Loss: 1.5254303216934204\n",
      "Epoch 1209 | Train Loss: 1.8000606298446655 | Test Loss: 1.8068456649780273\n",
      "Epoch 1210 | Train Loss: 1.5197805166244507 | Test Loss: 1.5249121189117432\n",
      "Epoch 1211 | Train Loss: 1.799331784248352 | Test Loss: 1.8061680793762207\n",
      "Epoch 1212 | Train Loss: 1.5192461013793945 | Test Loss: 1.5243953466415405\n",
      "Epoch 1213 | Train Loss: 1.798600673675537 | Test Loss: 1.8054895401000977\n",
      "Epoch 1214 | Train Loss: 1.5187137126922607 | Test Loss: 1.5238808393478394\n",
      "Epoch 1215 | Train Loss: 1.7978676557540894 | Test Loss: 1.804809331893921\n",
      "Epoch 1216 | Train Loss: 1.5181831121444702 | Test Loss: 1.5233702659606934\n",
      "Epoch 1217 | Train Loss: 1.7971258163452148 | Test Loss: 1.8041242361068726\n",
      "Epoch 1218 | Train Loss: 1.517660140991211 | Test Loss: 1.5228606462478638\n",
      "Epoch 1219 | Train Loss: 1.796380877494812 | Test Loss: 1.8034392595291138\n",
      "Epoch 1220 | Train Loss: 1.5171340703964233 | Test Loss: 1.5223487615585327\n",
      "Epoch 1221 | Train Loss: 1.7956382036209106 | Test Loss: 1.802754521369934\n",
      "Epoch 1222 | Train Loss: 1.516602873802185 | Test Loss: 1.5218310356140137\n",
      "Epoch 1223 | Train Loss: 1.794899344444275 | Test Loss: 1.8020673990249634\n",
      "Epoch 1224 | Train Loss: 1.5160685777664185 | Test Loss: 1.521314024925232\n",
      "Epoch 1225 | Train Loss: 1.794156551361084 | Test Loss: 1.8013783693313599\n",
      "Epoch 1226 | Train Loss: 1.5155340433120728 | Test Loss: 1.5207947492599487\n",
      "Epoch 1227 | Train Loss: 1.7934116125106812 | Test Loss: 1.8006906509399414\n",
      "Epoch 1228 | Train Loss: 1.5149973630905151 | Test Loss: 1.5202736854553223\n",
      "Epoch 1229 | Train Loss: 1.7926660776138306 | Test Loss: 1.8000051975250244\n",
      "Epoch 1230 | Train Loss: 1.5144604444503784 | Test Loss: 1.51975417137146\n",
      "Epoch 1231 | Train Loss: 1.7919158935546875 | Test Loss: 1.799317717552185\n",
      "Epoch 1232 | Train Loss: 1.5139268636703491 | Test Loss: 1.5192347764968872\n",
      "Epoch 1233 | Train Loss: 1.791161298751831 | Test Loss: 1.798627495765686\n",
      "Epoch 1234 | Train Loss: 1.5133895874023438 | Test Loss: 1.518709659576416\n",
      "Epoch 1235 | Train Loss: 1.7904095649719238 | Test Loss: 1.7979317903518677\n",
      "Epoch 1236 | Train Loss: 1.5128511190414429 | Test Loss: 1.5181890726089478\n",
      "Epoch 1237 | Train Loss: 1.789649248123169 | Test Loss: 1.797228217124939\n",
      "Epoch 1238 | Train Loss: 1.5123181343078613 | Test Loss: 1.5176693201065063\n",
      "Epoch 1239 | Train Loss: 1.788885474205017 | Test Loss: 1.796521782875061\n",
      "Epoch 1240 | Train Loss: 1.5117816925048828 | Test Loss: 1.5171478986740112\n",
      "Epoch 1241 | Train Loss: 1.7881215810775757 | Test Loss: 1.7958136796951294\n",
      "Epoch 1242 | Train Loss: 1.5112428665161133 | Test Loss: 1.5166274309158325\n",
      "Epoch 1243 | Train Loss: 1.7873547077178955 | Test Loss: 1.7951027154922485\n",
      "Epoch 1244 | Train Loss: 1.5107088088989258 | Test Loss: 1.5161073207855225\n",
      "Epoch 1245 | Train Loss: 1.786586880683899 | Test Loss: 1.7943904399871826\n",
      "Epoch 1246 | Train Loss: 1.5101706981658936 | Test Loss: 1.515583872795105\n",
      "Epoch 1247 | Train Loss: 1.7858197689056396 | Test Loss: 1.7936753034591675\n",
      "Epoch 1248 | Train Loss: 1.509629487991333 | Test Loss: 1.5150587558746338\n",
      "Epoch 1249 | Train Loss: 1.785053014755249 | Test Loss: 1.7929580211639404\n",
      "Epoch 1250 | Train Loss: 1.5090887546539307 | Test Loss: 1.5145360231399536\n",
      "Epoch 1251 | Train Loss: 1.7842820882797241 | Test Loss: 1.7922366857528687\n",
      "Epoch 1252 | Train Loss: 1.5085535049438477 | Test Loss: 1.5140177011489868\n",
      "Epoch 1253 | Train Loss: 1.783503770828247 | Test Loss: 1.7915152311325073\n",
      "Epoch 1254 | Train Loss: 1.5080221891403198 | Test Loss: 1.51349675655365\n",
      "Epoch 1255 | Train Loss: 1.7827287912368774 | Test Loss: 1.790797233581543\n",
      "Epoch 1256 | Train Loss: 1.507481575012207 | Test Loss: 1.512964129447937\n",
      "Epoch 1257 | Train Loss: 1.7819643020629883 | Test Loss: 1.790083408355713\n",
      "Epoch 1258 | Train Loss: 1.5069310665130615 | Test Loss: 1.5124272108078003\n",
      "Epoch 1259 | Train Loss: 1.7812055349349976 | Test Loss: 1.7893688678741455\n",
      "Epoch 1260 | Train Loss: 1.5063780546188354 | Test Loss: 1.5118927955627441\n",
      "Epoch 1261 | Train Loss: 1.7804406881332397 | Test Loss: 1.7886497974395752\n",
      "Epoch 1262 | Train Loss: 1.5058314800262451 | Test Loss: 1.511364221572876\n",
      "Epoch 1263 | Train Loss: 1.7796653509140015 | Test Loss: 1.7879276275634766\n",
      "Epoch 1264 | Train Loss: 1.5052908658981323 | Test Loss: 1.5108344554901123\n",
      "Epoch 1265 | Train Loss: 1.7788870334625244 | Test Loss: 1.7872081995010376\n",
      "Epoch 1266 | Train Loss: 1.5047452449798584 | Test Loss: 1.510297179222107\n",
      "Epoch 1267 | Train Loss: 1.7781082391738892 | Test Loss: 1.786488652229309\n",
      "Epoch 1268 | Train Loss: 1.5041958093643188 | Test Loss: 1.5097599029541016\n",
      "Epoch 1269 | Train Loss: 1.777325987815857 | Test Loss: 1.7857624292373657\n",
      "Epoch 1270 | Train Loss: 1.5036485195159912 | Test Loss: 1.5092275142669678\n",
      "Epoch 1271 | Train Loss: 1.7765346765518188 | Test Loss: 1.7850255966186523\n",
      "Epoch 1272 | Train Loss: 1.5031110048294067 | Test Loss: 1.508701205253601\n",
      "Epoch 1273 | Train Loss: 1.7757314443588257 | Test Loss: 1.7842845916748047\n",
      "Epoch 1274 | Train Loss: 1.5025752782821655 | Test Loss: 1.5081729888916016\n",
      "Epoch 1275 | Train Loss: 1.774927020072937 | Test Loss: 1.783545970916748\n",
      "Epoch 1276 | Train Loss: 1.5020356178283691 | Test Loss: 1.507638692855835\n",
      "Epoch 1277 | Train Loss: 1.7741233110427856 | Test Loss: 1.7828119993209839\n",
      "Epoch 1278 | Train Loss: 1.5014901161193848 | Test Loss: 1.5071009397506714\n",
      "Epoch 1279 | Train Loss: 1.7733241319656372 | Test Loss: 1.7820823192596436\n",
      "Epoch 1280 | Train Loss: 1.5009453296661377 | Test Loss: 1.5065687894821167\n",
      "Epoch 1281 | Train Loss: 1.7725204229354858 | Test Loss: 1.7813507318496704\n",
      "Epoch 1282 | Train Loss: 1.5004061460494995 | Test Loss: 1.5060423612594604\n",
      "Epoch 1283 | Train Loss: 1.771711826324463 | Test Loss: 1.7806150913238525\n",
      "Epoch 1284 | Train Loss: 1.499869704246521 | Test Loss: 1.505513310432434\n",
      "Epoch 1285 | Train Loss: 1.7709083557128906 | Test Loss: 1.779870867729187\n",
      "Epoch 1286 | Train Loss: 1.4993281364440918 | Test Loss: 1.504981517791748\n",
      "Epoch 1287 | Train Loss: 1.7701107263565063 | Test Loss: 1.779120683670044\n",
      "Epoch 1288 | Train Loss: 1.4987852573394775 | Test Loss: 1.5044536590576172\n",
      "Epoch 1289 | Train Loss: 1.7693089246749878 | Test Loss: 1.7783676385879517\n",
      "Epoch 1290 | Train Loss: 1.4982473850250244 | Test Loss: 1.5039275884628296\n",
      "Epoch 1291 | Train Loss: 1.768504023551941 | Test Loss: 1.7776175737380981\n",
      "Epoch 1292 | Train Loss: 1.4977134466171265 | Test Loss: 1.5034022331237793\n",
      "Epoch 1293 | Train Loss: 1.7676995992660522 | Test Loss: 1.776870608329773\n",
      "Epoch 1294 | Train Loss: 1.4971750974655151 | Test Loss: 1.5028738975524902\n",
      "Epoch 1295 | Train Loss: 1.766898274421692 | Test Loss: 1.776123046875\n",
      "Epoch 1296 | Train Loss: 1.49663507938385 | Test Loss: 1.502347469329834\n",
      "Epoch 1297 | Train Loss: 1.7660967111587524 | Test Loss: 1.7753697633743286\n",
      "Epoch 1298 | Train Loss: 1.4960979223251343 | Test Loss: 1.5018256902694702\n",
      "Epoch 1299 | Train Loss: 1.7652878761291504 | Test Loss: 1.7746086120605469\n",
      "Epoch 1300 | Train Loss: 1.495568037033081 | Test Loss: 1.5013071298599243\n",
      "Epoch 1301 | Train Loss: 1.7644727230072021 | Test Loss: 1.7738488912582397\n",
      "Epoch 1302 | Train Loss: 1.4950382709503174 | Test Loss: 1.500787377357483\n",
      "Epoch 1303 | Train Loss: 1.7636573314666748 | Test Loss: 1.7730919122695923\n",
      "Epoch 1304 | Train Loss: 1.494507908821106 | Test Loss: 1.500266432762146\n",
      "Epoch 1305 | Train Loss: 1.762843370437622 | Test Loss: 1.7723379135131836\n",
      "Epoch 1306 | Train Loss: 1.4939758777618408 | Test Loss: 1.4997446537017822\n",
      "Epoch 1307 | Train Loss: 1.7620304822921753 | Test Loss: 1.771584153175354\n",
      "Epoch 1308 | Train Loss: 1.4934433698654175 | Test Loss: 1.4992213249206543\n",
      "Epoch 1309 | Train Loss: 1.7612202167510986 | Test Loss: 1.7708271741867065\n",
      "Epoch 1310 | Train Loss: 1.492913007736206 | Test Loss: 1.4987024068832397\n",
      "Epoch 1311 | Train Loss: 1.7604045867919922 | Test Loss: 1.7700705528259277\n",
      "Epoch 1312 | Train Loss: 1.4923877716064453 | Test Loss: 1.4981831312179565\n",
      "Epoch 1313 | Train Loss: 1.7595913410186768 | Test Loss: 1.769317388534546\n",
      "Epoch 1314 | Train Loss: 1.4918566942214966 | Test Loss: 1.4976545572280884\n",
      "Epoch 1315 | Train Loss: 1.7587862014770508 | Test Loss: 1.7685656547546387\n",
      "Epoch 1316 | Train Loss: 1.4913196563720703 | Test Loss: 1.4971239566802979\n",
      "Epoch 1317 | Train Loss: 1.7579846382141113 | Test Loss: 1.7678130865097046\n",
      "Epoch 1318 | Train Loss: 1.4907827377319336 | Test Loss: 1.4965959787368774\n",
      "Epoch 1319 | Train Loss: 1.757181167602539 | Test Loss: 1.7670570611953735\n",
      "Epoch 1320 | Train Loss: 1.4902514219284058 | Test Loss: 1.4960721731185913\n",
      "Epoch 1321 | Train Loss: 1.756373643875122 | Test Loss: 1.766302227973938\n",
      "Epoch 1322 | Train Loss: 1.4897232055664062 | Test Loss: 1.4955469369888306\n",
      "Epoch 1323 | Train Loss: 1.755569577217102 | Test Loss: 1.7655525207519531\n",
      "Epoch 1324 | Train Loss: 1.4891910552978516 | Test Loss: 1.495020866394043\n",
      "Epoch 1325 | Train Loss: 1.754766821861267 | Test Loss: 1.7648032903671265\n",
      "Epoch 1326 | Train Loss: 1.4886624813079834 | Test Loss: 1.4944968223571777\n",
      "Epoch 1327 | Train Loss: 1.7539650201797485 | Test Loss: 1.7640522718429565\n",
      "Epoch 1328 | Train Loss: 1.4881330728530884 | Test Loss: 1.4939744472503662\n",
      "Epoch 1329 | Train Loss: 1.7531628608703613 | Test Loss: 1.7632989883422852\n",
      "Epoch 1330 | Train Loss: 1.4876056909561157 | Test Loss: 1.4934537410736084\n",
      "Epoch 1331 | Train Loss: 1.7523605823516846 | Test Loss: 1.7625486850738525\n",
      "Epoch 1332 | Train Loss: 1.4870773553848267 | Test Loss: 1.4929287433624268\n",
      "Epoch 1333 | Train Loss: 1.7515649795532227 | Test Loss: 1.7618050575256348\n",
      "Epoch 1334 | Train Loss: 1.486542820930481 | Test Loss: 1.4924018383026123\n",
      "Epoch 1335 | Train Loss: 1.7507741451263428 | Test Loss: 1.7610584497451782\n",
      "Epoch 1336 | Train Loss: 1.4860103130340576 | Test Loss: 1.491876244544983\n",
      "Epoch 1337 | Train Loss: 1.7499804496765137 | Test Loss: 1.7603076696395874\n",
      "Epoch 1338 | Train Loss: 1.4854787588119507 | Test Loss: 1.491349458694458\n",
      "Epoch 1339 | Train Loss: 1.7491891384124756 | Test Loss: 1.7595574855804443\n",
      "Epoch 1340 | Train Loss: 1.4849457740783691 | Test Loss: 1.4908230304718018\n",
      "Epoch 1341 | Train Loss: 1.748397946357727 | Test Loss: 1.7588037252426147\n",
      "Epoch 1342 | Train Loss: 1.48441743850708 | Test Loss: 1.4902994632720947\n",
      "Epoch 1343 | Train Loss: 1.7476046085357666 | Test Loss: 1.7580486536026\n",
      "Epoch 1344 | Train Loss: 1.4838883876800537 | Test Loss: 1.489777684211731\n",
      "Epoch 1345 | Train Loss: 1.7468092441558838 | Test Loss: 1.7572938203811646\n",
      "Epoch 1346 | Train Loss: 1.4833623170852661 | Test Loss: 1.489257574081421\n",
      "Epoch 1347 | Train Loss: 1.7460135221481323 | Test Loss: 1.7565417289733887\n",
      "Epoch 1348 | Train Loss: 1.4828391075134277 | Test Loss: 1.4887382984161377\n",
      "Epoch 1349 | Train Loss: 1.745220422744751 | Test Loss: 1.755793571472168\n",
      "Epoch 1350 | Train Loss: 1.482313632965088 | Test Loss: 1.4882179498672485\n",
      "Epoch 1351 | Train Loss: 1.7444307804107666 | Test Loss: 1.7550468444824219\n",
      "Epoch 1352 | Train Loss: 1.4817872047424316 | Test Loss: 1.4876985549926758\n",
      "Epoch 1353 | Train Loss: 1.7436418533325195 | Test Loss: 1.7542998790740967\n",
      "Epoch 1354 | Train Loss: 1.4812610149383545 | Test Loss: 1.4871786832809448\n",
      "Epoch 1355 | Train Loss: 1.742855191230774 | Test Loss: 1.7535533905029297\n",
      "Epoch 1356 | Train Loss: 1.4807372093200684 | Test Loss: 1.486660361289978\n",
      "Epoch 1357 | Train Loss: 1.7420672178268433 | Test Loss: 1.752811074256897\n",
      "Epoch 1358 | Train Loss: 1.4802156686782837 | Test Loss: 1.4861407279968262\n",
      "Epoch 1359 | Train Loss: 1.7412803173065186 | Test Loss: 1.7520713806152344\n",
      "Epoch 1360 | Train Loss: 1.479690670967102 | Test Loss: 1.4856196641921997\n",
      "Epoch 1361 | Train Loss: 1.7404987812042236 | Test Loss: 1.7513283491134644\n",
      "Epoch 1362 | Train Loss: 1.4791669845581055 | Test Loss: 1.485100507736206\n",
      "Epoch 1363 | Train Loss: 1.7397148609161377 | Test Loss: 1.7505747079849243\n",
      "Epoch 1364 | Train Loss: 1.4786465167999268 | Test Loss: 1.4845861196517944\n",
      "Epoch 1365 | Train Loss: 1.7389253377914429 | Test Loss: 1.749817967414856\n",
      "Epoch 1366 | Train Loss: 1.4781296253204346 | Test Loss: 1.4840723276138306\n",
      "Epoch 1367 | Train Loss: 1.7381360530853271 | Test Loss: 1.7490723133087158\n",
      "Epoch 1368 | Train Loss: 1.4776095151901245 | Test Loss: 1.4835559129714966\n",
      "Epoch 1369 | Train Loss: 1.7373511791229248 | Test Loss: 1.7483371496200562\n",
      "Epoch 1370 | Train Loss: 1.4770870208740234 | Test Loss: 1.4830381870269775\n",
      "Epoch 1371 | Train Loss: 1.7365704774856567 | Test Loss: 1.7476016283035278\n",
      "Epoch 1372 | Train Loss: 1.4765684604644775 | Test Loss: 1.4825271368026733\n",
      "Epoch 1373 | Train Loss: 1.7357850074768066 | Test Loss: 1.7468620538711548\n",
      "Epoch 1374 | Train Loss: 1.476056456565857 | Test Loss: 1.482020616531372\n",
      "Epoch 1375 | Train Loss: 1.7349988222122192 | Test Loss: 1.7461200952529907\n",
      "Epoch 1376 | Train Loss: 1.4755445718765259 | Test Loss: 1.4815112352371216\n",
      "Epoch 1377 | Train Loss: 1.7342194318771362 | Test Loss: 1.745381474494934\n",
      "Epoch 1378 | Train Loss: 1.4750298261642456 | Test Loss: 1.480995774269104\n",
      "Epoch 1379 | Train Loss: 1.7334485054016113 | Test Loss: 1.7446484565734863\n",
      "Epoch 1380 | Train Loss: 1.4745073318481445 | Test Loss: 1.4804785251617432\n",
      "Epoch 1381 | Train Loss: 1.7326833009719849 | Test Loss: 1.7439165115356445\n",
      "Epoch 1382 | Train Loss: 1.4739856719970703 | Test Loss: 1.479966163635254\n",
      "Epoch 1383 | Train Loss: 1.7319130897521973 | Test Loss: 1.7431753873825073\n",
      "Epoch 1384 | Train Loss: 1.473471999168396 | Test Loss: 1.4794604778289795\n",
      "Epoch 1385 | Train Loss: 1.7311358451843262 | Test Loss: 1.7424310445785522\n",
      "Epoch 1386 | Train Loss: 1.4729639291763306 | Test Loss: 1.478958010673523\n",
      "Epoch 1387 | Train Loss: 1.7303587198257446 | Test Loss: 1.741694450378418\n",
      "Epoch 1388 | Train Loss: 1.4724574089050293 | Test Loss: 1.4784518480300903\n",
      "Epoch 1389 | Train Loss: 1.7295887470245361 | Test Loss: 1.7409684658050537\n",
      "Epoch 1390 | Train Loss: 1.4719427824020386 | Test Loss: 1.4779412746429443\n",
      "Epoch 1391 | Train Loss: 1.7288252115249634 | Test Loss: 1.7402458190917969\n",
      "Epoch 1392 | Train Loss: 1.4714269638061523 | Test Loss: 1.477433443069458\n",
      "Epoch 1393 | Train Loss: 1.7280598878860474 | Test Loss: 1.739516258239746\n",
      "Epoch 1394 | Train Loss: 1.4709190130233765 | Test Loss: 1.4769319295883179\n",
      "Epoch 1395 | Train Loss: 1.7272882461547852 | Test Loss: 1.7387806177139282\n",
      "Epoch 1396 | Train Loss: 1.4704174995422363 | Test Loss: 1.4764353036880493\n",
      "Epoch 1397 | Train Loss: 1.7265114784240723 | Test Loss: 1.7380447387695312\n",
      "Epoch 1398 | Train Loss: 1.469921350479126 | Test Loss: 1.4759387969970703\n",
      "Epoch 1399 | Train Loss: 1.7257370948791504 | Test Loss: 1.7373172044754028\n",
      "Epoch 1400 | Train Loss: 1.4694198369979858 | Test Loss: 1.4754382371902466\n",
      "Epoch 1401 | Train Loss: 1.724969744682312 | Test Loss: 1.7365928888320923\n",
      "Epoch 1402 | Train Loss: 1.4689139127731323 | Test Loss: 1.4749380350112915\n",
      "Epoch 1403 | Train Loss: 1.7242047786712646 | Test Loss: 1.7358647584915161\n",
      "Epoch 1404 | Train Loss: 1.4684076309204102 | Test Loss: 1.4744350910186768\n",
      "Epoch 1405 | Train Loss: 1.7234435081481934 | Test Loss: 1.735132098197937\n",
      "Epoch 1406 | Train Loss: 1.4678987264633179 | Test Loss: 1.4739320278167725\n",
      "Epoch 1407 | Train Loss: 1.7226823568344116 | Test Loss: 1.7344026565551758\n",
      "Epoch 1408 | Train Loss: 1.4673930406570435 | Test Loss: 1.4734328985214233\n",
      "Epoch 1409 | Train Loss: 1.7219173908233643 | Test Loss: 1.733683466911316\n",
      "Epoch 1410 | Train Loss: 1.466888666152954 | Test Loss: 1.4729310274124146\n",
      "Epoch 1411 | Train Loss: 1.7211552858352661 | Test Loss: 1.7329760789871216\n",
      "Epoch 1412 | Train Loss: 1.4663827419281006 | Test Loss: 1.4724302291870117\n",
      "Epoch 1413 | Train Loss: 1.720395565032959 | Test Loss: 1.732269048690796\n",
      "Epoch 1414 | Train Loss: 1.4658770561218262 | Test Loss: 1.471929669380188\n",
      "Epoch 1415 | Train Loss: 1.7196366786956787 | Test Loss: 1.731552004814148\n",
      "Epoch 1416 | Train Loss: 1.4653723239898682 | Test Loss: 1.471429705619812\n",
      "Epoch 1417 | Train Loss: 1.7188791036605835 | Test Loss: 1.730829119682312\n",
      "Epoch 1418 | Train Loss: 1.4648693799972534 | Test Loss: 1.4709335565567017\n",
      "Epoch 1419 | Train Loss: 1.7181191444396973 | Test Loss: 1.7301093339920044\n",
      "Epoch 1420 | Train Loss: 1.4643700122833252 | Test Loss: 1.4704396724700928\n",
      "Epoch 1421 | Train Loss: 1.7173588275909424 | Test Loss: 1.7293972969055176\n",
      "Epoch 1422 | Train Loss: 1.4638725519180298 | Test Loss: 1.469944953918457\n",
      "Epoch 1423 | Train Loss: 1.7166001796722412 | Test Loss: 1.728690266609192\n",
      "Epoch 1424 | Train Loss: 1.4633721113204956 | Test Loss: 1.469448208808899\n",
      "Epoch 1425 | Train Loss: 1.7158453464508057 | Test Loss: 1.7279845476150513\n",
      "Epoch 1426 | Train Loss: 1.462870478630066 | Test Loss: 1.4689513444900513\n",
      "Epoch 1427 | Train Loss: 1.7150928974151611 | Test Loss: 1.7272785902023315\n",
      "Epoch 1428 | Train Loss: 1.4623677730560303 | Test Loss: 1.4684529304504395\n",
      "Epoch 1429 | Train Loss: 1.7143436670303345 | Test Loss: 1.7265732288360596\n",
      "Epoch 1430 | Train Loss: 1.4618667364120483 | Test Loss: 1.467957854270935\n",
      "Epoch 1431 | Train Loss: 1.7135926485061646 | Test Loss: 1.7258665561676025\n",
      "Epoch 1432 | Train Loss: 1.4613693952560425 | Test Loss: 1.4674668312072754\n",
      "Epoch 1433 | Train Loss: 1.7128374576568604 | Test Loss: 1.7251569032669067\n",
      "Epoch 1434 | Train Loss: 1.460877776145935 | Test Loss: 1.4669787883758545\n",
      "Epoch 1435 | Train Loss: 1.7120814323425293 | Test Loss: 1.7244516611099243\n",
      "Epoch 1436 | Train Loss: 1.4603866338729858 | Test Loss: 1.466491460800171\n",
      "Epoch 1437 | Train Loss: 1.711328387260437 | Test Loss: 1.7237522602081299\n",
      "Epoch 1438 | Train Loss: 1.459893822669983 | Test Loss: 1.4659982919692993\n",
      "Epoch 1439 | Train Loss: 1.710581660270691 | Test Loss: 1.7230600118637085\n",
      "Epoch 1440 | Train Loss: 1.459397315979004 | Test Loss: 1.4655039310455322\n",
      "Epoch 1441 | Train Loss: 1.7098387479782104 | Test Loss: 1.7223708629608154\n",
      "Epoch 1442 | Train Loss: 1.4589004516601562 | Test Loss: 1.4650121927261353\n",
      "Epoch 1443 | Train Loss: 1.70909583568573 | Test Loss: 1.7216801643371582\n",
      "Epoch 1444 | Train Loss: 1.458407998085022 | Test Loss: 1.4645239114761353\n",
      "Epoch 1445 | Train Loss: 1.7083529233932495 | Test Loss: 1.7209815979003906\n",
      "Epoch 1446 | Train Loss: 1.4579178094863892 | Test Loss: 1.4640377759933472\n",
      "Epoch 1447 | Train Loss: 1.7076102495193481 | Test Loss: 1.720279574394226\n",
      "Epoch 1448 | Train Loss: 1.45742666721344 | Test Loss: 1.4635459184646606\n",
      "Epoch 1449 | Train Loss: 1.7068756818771362 | Test Loss: 1.7195860147476196\n",
      "Epoch 1450 | Train Loss: 1.4569318294525146 | Test Loss: 1.4630542993545532\n",
      "Epoch 1451 | Train Loss: 1.7061424255371094 | Test Loss: 1.718908429145813\n",
      "Epoch 1452 | Train Loss: 1.456438422203064 | Test Loss: 1.4625632762908936\n",
      "Epoch 1453 | Train Loss: 1.705410122871399 | Test Loss: 1.718241572380066\n",
      "Epoch 1454 | Train Loss: 1.4559485912322998 | Test Loss: 1.4620753526687622\n",
      "Epoch 1455 | Train Loss: 1.7046748399734497 | Test Loss: 1.7175743579864502\n",
      "Epoch 1456 | Train Loss: 1.455461859703064 | Test Loss: 1.4615895748138428\n",
      "Epoch 1457 | Train Loss: 1.7039389610290527 | Test Loss: 1.7169004678726196\n",
      "Epoch 1458 | Train Loss: 1.4549776315689087 | Test Loss: 1.461105227470398\n",
      "Epoch 1459 | Train Loss: 1.7032049894332886 | Test Loss: 1.7162240743637085\n",
      "Epoch 1460 | Train Loss: 1.4544938802719116 | Test Loss: 1.4606208801269531\n",
      "Epoch 1461 | Train Loss: 1.7024732828140259 | Test Loss: 1.715552806854248\n",
      "Epoch 1462 | Train Loss: 1.454010248184204 | Test Loss: 1.460134744644165\n",
      "Epoch 1463 | Train Loss: 1.7017457485198975 | Test Loss: 1.7148903608322144\n",
      "Epoch 1464 | Train Loss: 1.4535243511199951 | Test Loss: 1.459646463394165\n",
      "Epoch 1465 | Train Loss: 1.7010209560394287 | Test Loss: 1.7142333984375\n",
      "Epoch 1466 | Train Loss: 1.4530383348464966 | Test Loss: 1.4591578245162964\n",
      "Epoch 1467 | Train Loss: 1.7002986669540405 | Test Loss: 1.7135781049728394\n",
      "Epoch 1468 | Train Loss: 1.4525541067123413 | Test Loss: 1.458670735359192\n",
      "Epoch 1469 | Train Loss: 1.6995760202407837 | Test Loss: 1.7129207849502563\n",
      "Epoch 1470 | Train Loss: 1.452069878578186 | Test Loss: 1.458183765411377\n",
      "Epoch 1471 | Train Loss: 1.698854923248291 | Test Loss: 1.712262749671936\n",
      "Epoch 1472 | Train Loss: 1.4515876770019531 | Test Loss: 1.457699179649353\n",
      "Epoch 1473 | Train Loss: 1.698132038116455 | Test Loss: 1.711607813835144\n",
      "Epoch 1474 | Train Loss: 1.4511090517044067 | Test Loss: 1.4572176933288574\n",
      "Epoch 1475 | Train Loss: 1.6974053382873535 | Test Loss: 1.7109514474868774\n",
      "Epoch 1476 | Train Loss: 1.450632929801941 | Test Loss: 1.4567372798919678\n",
      "Epoch 1477 | Train Loss: 1.6966769695281982 | Test Loss: 1.7102928161621094\n",
      "Epoch 1478 | Train Loss: 1.450156807899475 | Test Loss: 1.45625638961792\n",
      "Epoch 1479 | Train Loss: 1.6959505081176758 | Test Loss: 1.7096352577209473\n",
      "Epoch 1480 | Train Loss: 1.4496804475784302 | Test Loss: 1.4557745456695557\n",
      "Epoch 1481 | Train Loss: 1.6952284574508667 | Test Loss: 1.7089848518371582\n",
      "Epoch 1482 | Train Loss: 1.449202060699463 | Test Loss: 1.4552910327911377\n",
      "Epoch 1483 | Train Loss: 1.6945116519927979 | Test Loss: 1.708338975906372\n",
      "Epoch 1484 | Train Loss: 1.4487227201461792 | Test Loss: 1.4548076391220093\n",
      "Epoch 1485 | Train Loss: 1.6937962770462036 | Test Loss: 1.7076902389526367\n",
      "Epoch 1486 | Train Loss: 1.4482438564300537 | Test Loss: 1.454325795173645\n",
      "Epoch 1487 | Train Loss: 1.693082332611084 | Test Loss: 1.7070378065109253\n",
      "Epoch 1488 | Train Loss: 1.4477663040161133 | Test Loss: 1.453845500946045\n",
      "Epoch 1489 | Train Loss: 1.6923695802688599 | Test Loss: 1.7063894271850586\n",
      "Epoch 1490 | Train Loss: 1.447289228439331 | Test Loss: 1.453366756439209\n",
      "Epoch 1491 | Train Loss: 1.6916581392288208 | Test Loss: 1.7057491540908813\n",
      "Epoch 1492 | Train Loss: 1.446813941001892 | Test Loss: 1.4528900384902954\n",
      "Epoch 1493 | Train Loss: 1.6909486055374146 | Test Loss: 1.705112338066101\n",
      "Epoch 1494 | Train Loss: 1.4463403224945068 | Test Loss: 1.4524164199829102\n",
      "Epoch 1495 | Train Loss: 1.6902406215667725 | Test Loss: 1.7044757604599\n",
      "Epoch 1496 | Train Loss: 1.4458695650100708 | Test Loss: 1.4519457817077637\n",
      "Epoch 1497 | Train Loss: 1.6895318031311035 | Test Loss: 1.703835129737854\n",
      "Epoch 1498 | Train Loss: 1.4454014301300049 | Test Loss: 1.4514796733856201\n",
      "Epoch 1499 | Train Loss: 1.6888200044631958 | Test Loss: 1.7031855583190918\n",
      "Epoch 1500 | Train Loss: 1.444935917854309 | Test Loss: 1.4510151147842407\n",
      "Epoch 1501 | Train Loss: 1.6881084442138672 | Test Loss: 1.7025312185287476\n",
      "Epoch 1502 | Train Loss: 1.444470763206482 | Test Loss: 1.4505491256713867\n",
      "Epoch 1503 | Train Loss: 1.687400221824646 | Test Loss: 1.7018762826919556\n",
      "Epoch 1504 | Train Loss: 1.4440003633499146 | Test Loss: 1.4500787258148193\n",
      "Epoch 1505 | Train Loss: 1.6866989135742188 | Test Loss: 1.7012228965759277\n",
      "Epoch 1506 | Train Loss: 1.4435265064239502 | Test Loss: 1.4496040344238281\n",
      "Epoch 1507 | Train Loss: 1.686001181602478 | Test Loss: 1.7005805969238281\n",
      "Epoch 1508 | Train Loss: 1.4430510997772217 | Test Loss: 1.4491279125213623\n",
      "Epoch 1509 | Train Loss: 1.6853066682815552 | Test Loss: 1.6999505758285522\n",
      "Epoch 1510 | Train Loss: 1.442574381828308 | Test Loss: 1.4486497640609741\n",
      "Epoch 1511 | Train Loss: 1.6846139430999756 | Test Loss: 1.6993250846862793\n",
      "Epoch 1512 | Train Loss: 1.4420980215072632 | Test Loss: 1.4481725692749023\n",
      "Epoch 1513 | Train Loss: 1.683921456336975 | Test Loss: 1.6986948251724243\n",
      "Epoch 1514 | Train Loss: 1.441623330116272 | Test Loss: 1.4476985931396484\n",
      "Epoch 1515 | Train Loss: 1.6832282543182373 | Test Loss: 1.6980581283569336\n",
      "Epoch 1516 | Train Loss: 1.441152811050415 | Test Loss: 1.4472301006317139\n",
      "Epoch 1517 | Train Loss: 1.6825298070907593 | Test Loss: 1.6974228620529175\n",
      "Epoch 1518 | Train Loss: 1.4406853914260864 | Test Loss: 1.4467626810073853\n",
      "Epoch 1519 | Train Loss: 1.6818344593048096 | Test Loss: 1.6967986822128296\n",
      "Epoch 1520 | Train Loss: 1.440214991569519 | Test Loss: 1.4462918043136597\n",
      "Epoch 1521 | Train Loss: 1.6811442375183105 | Test Loss: 1.696179986000061\n",
      "Epoch 1522 | Train Loss: 1.4397410154342651 | Test Loss: 1.4458191394805908\n",
      "Epoch 1523 | Train Loss: 1.6804567575454712 | Test Loss: 1.6955604553222656\n",
      "Epoch 1524 | Train Loss: 1.4392651319503784 | Test Loss: 1.4453458786010742\n",
      "Epoch 1525 | Train Loss: 1.6797691583633423 | Test Loss: 1.6949418783187866\n",
      "Epoch 1526 | Train Loss: 1.4387904405593872 | Test Loss: 1.4448751211166382\n",
      "Epoch 1527 | Train Loss: 1.6790809631347656 | Test Loss: 1.6943230628967285\n",
      "Epoch 1528 | Train Loss: 1.438320517539978 | Test Loss: 1.444411039352417\n",
      "Epoch 1529 | Train Loss: 1.6783865690231323 | Test Loss: 1.6936980485916138\n",
      "Epoch 1530 | Train Loss: 1.4378563165664673 | Test Loss: 1.443953275680542\n",
      "Epoch 1531 | Train Loss: 1.6776883602142334 | Test Loss: 1.6930694580078125\n",
      "Epoch 1532 | Train Loss: 1.437395453453064 | Test Loss: 1.4434974193572998\n",
      "Epoch 1533 | Train Loss: 1.676990032196045 | Test Loss: 1.6924439668655396\n",
      "Epoch 1534 | Train Loss: 1.4369332790374756 | Test Loss: 1.443039059638977\n",
      "Epoch 1535 | Train Loss: 1.676297664642334 | Test Loss: 1.6918210983276367\n",
      "Epoch 1536 | Train Loss: 1.4364683628082275 | Test Loss: 1.4425784349441528\n",
      "Epoch 1537 | Train Loss: 1.6756105422973633 | Test Loss: 1.6911965608596802\n",
      "Epoch 1538 | Train Loss: 1.436003565788269 | Test Loss: 1.442116379737854\n",
      "Epoch 1539 | Train Loss: 1.674928903579712 | Test Loss: 1.6905747652053833\n",
      "Epoch 1540 | Train Loss: 1.4355381727218628 | Test Loss: 1.4416513442993164\n",
      "Epoch 1541 | Train Loss: 1.6742528676986694 | Test Loss: 1.6899665594100952\n",
      "Epoch 1542 | Train Loss: 1.4350686073303223 | Test Loss: 1.4411824941635132\n",
      "Epoch 1543 | Train Loss: 1.6735843420028687 | Test Loss: 1.6893658638000488\n",
      "Epoch 1544 | Train Loss: 1.4345972537994385 | Test Loss: 1.4407132863998413\n",
      "Epoch 1545 | Train Loss: 1.6729187965393066 | Test Loss: 1.6887598037719727\n",
      "Epoch 1546 | Train Loss: 1.434127926826477 | Test Loss: 1.4402469396591187\n",
      "Epoch 1547 | Train Loss: 1.67225182056427 | Test Loss: 1.6881437301635742\n",
      "Epoch 1548 | Train Loss: 1.4336631298065186 | Test Loss: 1.4397845268249512\n",
      "Epoch 1549 | Train Loss: 1.6715834140777588 | Test Loss: 1.6875276565551758\n",
      "Epoch 1550 | Train Loss: 1.4332011938095093 | Test Loss: 1.4393213987350464\n",
      "Epoch 1551 | Train Loss: 1.6709169149398804 | Test Loss: 1.6869229078292847\n",
      "Epoch 1552 | Train Loss: 1.4327386617660522 | Test Loss: 1.4388600587844849\n",
      "Epoch 1553 | Train Loss: 1.6702520847320557 | Test Loss: 1.6863232851028442\n",
      "Epoch 1554 | Train Loss: 1.4322770833969116 | Test Loss: 1.438400387763977\n",
      "Epoch 1555 | Train Loss: 1.6695876121520996 | Test Loss: 1.6857190132141113\n",
      "Epoch 1556 | Train Loss: 1.4318174123764038 | Test Loss: 1.437942385673523\n",
      "Epoch 1557 | Train Loss: 1.6689231395721436 | Test Loss: 1.6851069927215576\n",
      "Epoch 1558 | Train Loss: 1.4313603639602661 | Test Loss: 1.437485933303833\n",
      "Epoch 1559 | Train Loss: 1.6682606935501099 | Test Loss: 1.6844936609268188\n",
      "Epoch 1560 | Train Loss: 1.430902361869812 | Test Loss: 1.4370285272598267\n",
      "Epoch 1561 | Train Loss: 1.6676015853881836 | Test Loss: 1.6838852167129517\n",
      "Epoch 1562 | Train Loss: 1.4304438829421997 | Test Loss: 1.4365689754486084\n",
      "Epoch 1563 | Train Loss: 1.6669471263885498 | Test Loss: 1.6832908391952515\n",
      "Epoch 1564 | Train Loss: 1.4299834966659546 | Test Loss: 1.436107873916626\n",
      "Epoch 1565 | Train Loss: 1.666293978691101 | Test Loss: 1.6827030181884766\n",
      "Epoch 1566 | Train Loss: 1.4295237064361572 | Test Loss: 1.4356467723846436\n",
      "Epoch 1567 | Train Loss: 1.6656421422958374 | Test Loss: 1.6821203231811523\n",
      "Epoch 1568 | Train Loss: 1.4290635585784912 | Test Loss: 1.4351844787597656\n",
      "Epoch 1569 | Train Loss: 1.6649932861328125 | Test Loss: 1.6815359592437744\n",
      "Epoch 1570 | Train Loss: 1.4286022186279297 | Test Loss: 1.434722661972046\n",
      "Epoch 1571 | Train Loss: 1.6643446683883667 | Test Loss: 1.6809383630752563\n",
      "Epoch 1572 | Train Loss: 1.4281433820724487 | Test Loss: 1.4342625141143799\n",
      "Epoch 1573 | Train Loss: 1.6636953353881836 | Test Loss: 1.6803374290466309\n",
      "Epoch 1574 | Train Loss: 1.4276857376098633 | Test Loss: 1.4338037967681885\n",
      "Epoch 1575 | Train Loss: 1.663046956062317 | Test Loss: 1.6797418594360352\n",
      "Epoch 1576 | Train Loss: 1.4272297620773315 | Test Loss: 1.4333484172821045\n",
      "Epoch 1577 | Train Loss: 1.6623979806900024 | Test Loss: 1.6791547536849976\n",
      "Epoch 1578 | Train Loss: 1.4267759323120117 | Test Loss: 1.4328922033309937\n",
      "Epoch 1579 | Train Loss: 1.6617523431777954 | Test Loss: 1.6785725355148315\n",
      "Epoch 1580 | Train Loss: 1.4263198375701904 | Test Loss: 1.4324337244033813\n",
      "Epoch 1581 | Train Loss: 1.6611098051071167 | Test Loss: 1.6779875755310059\n",
      "Epoch 1582 | Train Loss: 1.4258637428283691 | Test Loss: 1.4319746494293213\n",
      "Epoch 1583 | Train Loss: 1.6604702472686768 | Test Loss: 1.6774048805236816\n",
      "Epoch 1584 | Train Loss: 1.4254066944122314 | Test Loss: 1.431516408920288\n",
      "Epoch 1585 | Train Loss: 1.6598325967788696 | Test Loss: 1.676819920539856\n",
      "Epoch 1586 | Train Loss: 1.4249509572982788 | Test Loss: 1.4310601949691772\n",
      "Epoch 1587 | Train Loss: 1.6591951847076416 | Test Loss: 1.6762378215789795\n",
      "Epoch 1588 | Train Loss: 1.4244959354400635 | Test Loss: 1.4306056499481201\n",
      "Epoch 1589 | Train Loss: 1.6585582494735718 | Test Loss: 1.6756587028503418\n",
      "Epoch 1590 | Train Loss: 1.4240425825119019 | Test Loss: 1.4301520586013794\n",
      "Epoch 1591 | Train Loss: 1.6579210758209229 | Test Loss: 1.675080418586731\n",
      "Epoch 1592 | Train Loss: 1.4235886335372925 | Test Loss: 1.429694414138794\n",
      "Epoch 1593 | Train Loss: 1.6572892665863037 | Test Loss: 1.6745105981826782\n",
      "Epoch 1594 | Train Loss: 1.423132061958313 | Test Loss: 1.4292371273040771\n",
      "Epoch 1595 | Train Loss: 1.65665864944458 | Test Loss: 1.673938512802124\n",
      "Epoch 1596 | Train Loss: 1.4226783514022827 | Test Loss: 1.4287843704223633\n",
      "Epoch 1597 | Train Loss: 1.6560250520706177 | Test Loss: 1.673366665840149\n",
      "Epoch 1598 | Train Loss: 1.4222286939620972 | Test Loss: 1.4283347129821777\n",
      "Epoch 1599 | Train Loss: 1.6553897857666016 | Test Loss: 1.6727961301803589\n",
      "Epoch 1600 | Train Loss: 1.4217795133590698 | Test Loss: 1.4278839826583862\n",
      "Epoch 1601 | Train Loss: 1.6547574996948242 | Test Loss: 1.6722320318222046\n",
      "Epoch 1602 | Train Loss: 1.4213271141052246 | Test Loss: 1.4274314641952515\n",
      "Epoch 1603 | Train Loss: 1.6541286706924438 | Test Loss: 1.6716642379760742\n",
      "Epoch 1604 | Train Loss: 1.4208753108978271 | Test Loss: 1.426979660987854\n",
      "Epoch 1605 | Train Loss: 1.6535024642944336 | Test Loss: 1.6710952520370483\n",
      "Epoch 1606 | Train Loss: 1.4204233884811401 | Test Loss: 1.4265289306640625\n",
      "Epoch 1607 | Train Loss: 1.6528770923614502 | Test Loss: 1.6705266237258911\n",
      "Epoch 1608 | Train Loss: 1.4199728965759277 | Test Loss: 1.4260826110839844\n",
      "Epoch 1609 | Train Loss: 1.6522513628005981 | Test Loss: 1.6699604988098145\n",
      "Epoch 1610 | Train Loss: 1.4195268154144287 | Test Loss: 1.4256401062011719\n",
      "Epoch 1611 | Train Loss: 1.6516249179840088 | Test Loss: 1.6693956851959229\n",
      "Epoch 1612 | Train Loss: 1.4190815687179565 | Test Loss: 1.4251973628997803\n",
      "Epoch 1613 | Train Loss: 1.6510016918182373 | Test Loss: 1.6688307523727417\n",
      "Epoch 1614 | Train Loss: 1.4186358451843262 | Test Loss: 1.4247539043426514\n",
      "Epoch 1615 | Train Loss: 1.650382161140442 | Test Loss: 1.6682674884796143\n",
      "Epoch 1616 | Train Loss: 1.4181879758834839 | Test Loss: 1.4243073463439941\n",
      "Epoch 1617 | Train Loss: 1.6497670412063599 | Test Loss: 1.6677039861679077\n",
      "Epoch 1618 | Train Loss: 1.4177381992340088 | Test Loss: 1.4238605499267578\n",
      "Epoch 1619 | Train Loss: 1.6491531133651733 | Test Loss: 1.6671417951583862\n",
      "Epoch 1620 | Train Loss: 1.41728937625885 | Test Loss: 1.4234135150909424\n",
      "Epoch 1621 | Train Loss: 1.6485414505004883 | Test Loss: 1.6665871143341064\n",
      "Epoch 1622 | Train Loss: 1.4168401956558228 | Test Loss: 1.422966480255127\n",
      "Epoch 1623 | Train Loss: 1.6479307413101196 | Test Loss: 1.6660380363464355\n",
      "Epoch 1624 | Train Loss: 1.4163918495178223 | Test Loss: 1.4225211143493652\n",
      "Epoch 1625 | Train Loss: 1.647321343421936 | Test Loss: 1.6654887199401855\n",
      "Epoch 1626 | Train Loss: 1.4159443378448486 | Test Loss: 1.4220772981643677\n",
      "Epoch 1627 | Train Loss: 1.6467112302780151 | Test Loss: 1.664931058883667\n",
      "Epoch 1628 | Train Loss: 1.4154977798461914 | Test Loss: 1.4216337203979492\n",
      "Epoch 1629 | Train Loss: 1.6461023092269897 | Test Loss: 1.664367914199829\n",
      "Epoch 1630 | Train Loss: 1.4150511026382446 | Test Loss: 1.4211872816085815\n",
      "Epoch 1631 | Train Loss: 1.645498275756836 | Test Loss: 1.6638137102127075\n",
      "Epoch 1632 | Train Loss: 1.4146015644073486 | Test Loss: 1.420742154121399\n",
      "Epoch 1633 | Train Loss: 1.6448956727981567 | Test Loss: 1.6632705926895142\n",
      "Epoch 1634 | Train Loss: 1.4141523838043213 | Test Loss: 1.4202971458435059\n",
      "Epoch 1635 | Train Loss: 1.6442948579788208 | Test Loss: 1.6627262830734253\n",
      "Epoch 1636 | Train Loss: 1.413704752922058 | Test Loss: 1.419853687286377\n",
      "Epoch 1637 | Train Loss: 1.6436948776245117 | Test Loss: 1.662178635597229\n",
      "Epoch 1638 | Train Loss: 1.4132590293884277 | Test Loss: 1.4194116592407227\n",
      "Epoch 1639 | Train Loss: 1.6430953741073608 | Test Loss: 1.661637783050537\n",
      "Epoch 1640 | Train Loss: 1.4128127098083496 | Test Loss: 1.4189692735671997\n",
      "Epoch 1641 | Train Loss: 1.6424978971481323 | Test Loss: 1.66110098361969\n",
      "Epoch 1642 | Train Loss: 1.4123684167861938 | Test Loss: 1.4185311794281006\n",
      "Epoch 1643 | Train Loss: 1.641900658607483 | Test Loss: 1.6605643033981323\n",
      "Epoch 1644 | Train Loss: 1.4119306802749634 | Test Loss: 1.4181002378463745\n",
      "Epoch 1645 | Train Loss: 1.6413003206253052 | Test Loss: 1.6600147485733032\n",
      "Epoch 1646 | Train Loss: 1.4114971160888672 | Test Loss: 1.4176719188690186\n",
      "Epoch 1647 | Train Loss: 1.6407023668289185 | Test Loss: 1.6594667434692383\n",
      "Epoch 1648 | Train Loss: 1.4110636711120605 | Test Loss: 1.4172430038452148\n",
      "Epoch 1649 | Train Loss: 1.6401076316833496 | Test Loss: 1.6589289903640747\n",
      "Epoch 1650 | Train Loss: 1.410628080368042 | Test Loss: 1.4168146848678589\n",
      "Epoch 1651 | Train Loss: 1.6395167112350464 | Test Loss: 1.6583925485610962\n",
      "Epoch 1652 | Train Loss: 1.4101930856704712 | Test Loss: 1.4163883924484253\n",
      "Epoch 1653 | Train Loss: 1.638926386833191 | Test Loss: 1.6578487157821655\n",
      "Epoch 1654 | Train Loss: 1.4097611904144287 | Test Loss: 1.4159666299819946\n",
      "Epoch 1655 | Train Loss: 1.6383343935012817 | Test Loss: 1.6573011875152588\n",
      "Epoch 1656 | Train Loss: 1.4093323945999146 | Test Loss: 1.4155436754226685\n",
      "Epoch 1657 | Train Loss: 1.63774573802948 | Test Loss: 1.6567646265029907\n",
      "Epoch 1658 | Train Loss: 1.4089022874832153 | Test Loss: 1.4151170253753662\n",
      "Epoch 1659 | Train Loss: 1.6371617317199707 | Test Loss: 1.6562365293502808\n",
      "Epoch 1660 | Train Loss: 1.4084705114364624 | Test Loss: 1.4146900177001953\n",
      "Epoch 1661 | Train Loss: 1.6365809440612793 | Test Loss: 1.6557008028030396\n",
      "Epoch 1662 | Train Loss: 1.4080413579940796 | Test Loss: 1.4142662286758423\n",
      "Epoch 1663 | Train Loss: 1.6360002756118774 | Test Loss: 1.6551573276519775\n",
      "Epoch 1664 | Train Loss: 1.407614827156067 | Test Loss: 1.4138438701629639\n",
      "Epoch 1665 | Train Loss: 1.635420560836792 | Test Loss: 1.6546190977096558\n",
      "Epoch 1666 | Train Loss: 1.4071887731552124 | Test Loss: 1.4134222269058228\n",
      "Epoch 1667 | Train Loss: 1.6348429918289185 | Test Loss: 1.6540848016738892\n",
      "Epoch 1668 | Train Loss: 1.4067641496658325 | Test Loss: 1.4130014181137085\n",
      "Epoch 1669 | Train Loss: 1.6342675685882568 | Test Loss: 1.65355384349823\n",
      "Epoch 1670 | Train Loss: 1.4063401222229004 | Test Loss: 1.4125834703445435\n",
      "Epoch 1671 | Train Loss: 1.6336921453475952 | Test Loss: 1.6530163288116455\n",
      "Epoch 1672 | Train Loss: 1.4059206247329712 | Test Loss: 1.4121718406677246\n",
      "Epoch 1673 | Train Loss: 1.6331140995025635 | Test Loss: 1.652476191520691\n",
      "Epoch 1674 | Train Loss: 1.405504584312439 | Test Loss: 1.4117628335952759\n",
      "Epoch 1675 | Train Loss: 1.6325362920761108 | Test Loss: 1.651942491531372\n",
      "Epoch 1676 | Train Loss: 1.4050902128219604 | Test Loss: 1.411352515220642\n",
      "Epoch 1677 | Train Loss: 1.6319621801376343 | Test Loss: 1.6514118909835815\n",
      "Epoch 1678 | Train Loss: 1.404671311378479 | Test Loss: 1.410940170288086\n",
      "Epoch 1679 | Train Loss: 1.6313930749893188 | Test Loss: 1.6508861780166626\n",
      "Epoch 1680 | Train Loss: 1.4042531251907349 | Test Loss: 1.4105305671691895\n",
      "Epoch 1681 | Train Loss: 1.6308249235153198 | Test Loss: 1.650359034538269\n",
      "Epoch 1682 | Train Loss: 1.4038389921188354 | Test Loss: 1.4101207256317139\n",
      "Epoch 1683 | Train Loss: 1.6302595138549805 | Test Loss: 1.6498342752456665\n",
      "Epoch 1684 | Train Loss: 1.4034217596054077 | Test Loss: 1.4097073078155518\n",
      "Epoch 1685 | Train Loss: 1.62969970703125 | Test Loss: 1.6493064165115356\n",
      "Epoch 1686 | Train Loss: 1.4030027389526367 | Test Loss: 1.4092953205108643\n",
      "Epoch 1687 | Train Loss: 1.629140853881836 | Test Loss: 1.6487795114517212\n",
      "Epoch 1688 | Train Loss: 1.4025884866714478 | Test Loss: 1.408888578414917\n",
      "Epoch 1689 | Train Loss: 1.6285794973373413 | Test Loss: 1.6482665538787842\n",
      "Epoch 1690 | Train Loss: 1.4021769762039185 | Test Loss: 1.4084842205047607\n",
      "Epoch 1691 | Train Loss: 1.6280184984207153 | Test Loss: 1.6477617025375366\n",
      "Epoch 1692 | Train Loss: 1.4017674922943115 | Test Loss: 1.408079743385315\n",
      "Epoch 1693 | Train Loss: 1.6274598836898804 | Test Loss: 1.647247076034546\n",
      "Epoch 1694 | Train Loss: 1.4013562202453613 | Test Loss: 1.40767502784729\n",
      "Epoch 1695 | Train Loss: 1.6269029378890991 | Test Loss: 1.646725058555603\n",
      "Epoch 1696 | Train Loss: 1.4009473323822021 | Test Loss: 1.407273292541504\n",
      "Epoch 1697 | Train Loss: 1.6263453960418701 | Test Loss: 1.6462042331695557\n",
      "Epoch 1698 | Train Loss: 1.4005385637283325 | Test Loss: 1.406869649887085\n",
      "Epoch 1699 | Train Loss: 1.6257942914962769 | Test Loss: 1.6456900835037231\n",
      "Epoch 1700 | Train Loss: 1.4001246690750122 | Test Loss: 1.4064620733261108\n",
      "Epoch 1701 | Train Loss: 1.6252468824386597 | Test Loss: 1.6451870203018188\n",
      "Epoch 1702 | Train Loss: 1.3997124433517456 | Test Loss: 1.4060568809509277\n",
      "Epoch 1703 | Train Loss: 1.6246984004974365 | Test Loss: 1.6446828842163086\n",
      "Epoch 1704 | Train Loss: 1.3993042707443237 | Test Loss: 1.4056540727615356\n",
      "Epoch 1705 | Train Loss: 1.624148964881897 | Test Loss: 1.6441738605499268\n",
      "Epoch 1706 | Train Loss: 1.3988978862762451 | Test Loss: 1.4052494764328003\n",
      "Epoch 1707 | Train Loss: 1.6236040592193604 | Test Loss: 1.6436669826507568\n",
      "Epoch 1708 | Train Loss: 1.3984875679016113 | Test Loss: 1.4048428535461426\n",
      "Epoch 1709 | Train Loss: 1.6230645179748535 | Test Loss: 1.643159031867981\n",
      "Epoch 1710 | Train Loss: 1.3980770111083984 | Test Loss: 1.4044402837753296\n",
      "Epoch 1711 | Train Loss: 1.6225252151489258 | Test Loss: 1.6426523923873901\n",
      "Epoch 1712 | Train Loss: 1.3976740837097168 | Test Loss: 1.4040461778640747\n",
      "Epoch 1713 | Train Loss: 1.6219813823699951 | Test Loss: 1.6421480178833008\n",
      "Epoch 1714 | Train Loss: 1.3972749710083008 | Test Loss: 1.4036500453948975\n",
      "Epoch 1715 | Train Loss: 1.6214416027069092 | Test Loss: 1.6416417360305786\n",
      "Epoch 1716 | Train Loss: 1.3968706130981445 | Test Loss: 1.4032478332519531\n",
      "Epoch 1717 | Train Loss: 1.6209087371826172 | Test Loss: 1.6411378383636475\n",
      "Epoch 1718 | Train Loss: 1.3964637517929077 | Test Loss: 1.4028477668762207\n",
      "Epoch 1719 | Train Loss: 1.6203750371932983 | Test Loss: 1.6406350135803223\n",
      "Epoch 1720 | Train Loss: 1.3960624933242798 | Test Loss: 1.4024509191513062\n",
      "Epoch 1721 | Train Loss: 1.6198410987854004 | Test Loss: 1.6401426792144775\n",
      "Epoch 1722 | Train Loss: 1.3956602811813354 | Test Loss: 1.4020545482635498\n",
      "Epoch 1723 | Train Loss: 1.6193106174468994 | Test Loss: 1.6396515369415283\n",
      "Epoch 1724 | Train Loss: 1.3952579498291016 | Test Loss: 1.4016603231430054\n",
      "Epoch 1725 | Train Loss: 1.6187812089920044 | Test Loss: 1.6391483545303345\n",
      "Epoch 1726 | Train Loss: 1.394857406616211 | Test Loss: 1.40126371383667\n",
      "Epoch 1727 | Train Loss: 1.618254542350769 | Test Loss: 1.6386330127716064\n",
      "Epoch 1728 | Train Loss: 1.3944534063339233 | Test Loss: 1.4008655548095703\n",
      "Epoch 1729 | Train Loss: 1.6177312135696411 | Test Loss: 1.638120174407959\n",
      "Epoch 1730 | Train Loss: 1.3940523862838745 | Test Loss: 1.4004716873168945\n",
      "Epoch 1731 | Train Loss: 1.617207646369934 | Test Loss: 1.6376228332519531\n",
      "Epoch 1732 | Train Loss: 1.3936545848846436 | Test Loss: 1.4000853300094604\n",
      "Epoch 1733 | Train Loss: 1.6166800260543823 | Test Loss: 1.6371339559555054\n",
      "Epoch 1734 | Train Loss: 1.3932640552520752 | Test Loss: 1.399705171585083\n",
      "Epoch 1735 | Train Loss: 1.6161491870880127 | Test Loss: 1.6366416215896606\n",
      "Epoch 1736 | Train Loss: 1.392877221107483 | Test Loss: 1.3993228673934937\n",
      "Epoch 1737 | Train Loss: 1.6156220436096191 | Test Loss: 1.636142611503601\n",
      "Epoch 1738 | Train Loss: 1.3924870491027832 | Test Loss: 1.3989346027374268\n",
      "Epoch 1739 | Train Loss: 1.615100383758545 | Test Loss: 1.6356452703475952\n",
      "Epoch 1740 | Train Loss: 1.3920955657958984 | Test Loss: 1.398545742034912\n",
      "Epoch 1741 | Train Loss: 1.6145797967910767 | Test Loss: 1.6351590156555176\n",
      "Epoch 1742 | Train Loss: 1.3917067050933838 | Test Loss: 1.3981610536575317\n",
      "Epoch 1743 | Train Loss: 1.6140574216842651 | Test Loss: 1.6346805095672607\n",
      "Epoch 1744 | Train Loss: 1.391320824623108 | Test Loss: 1.3977818489074707\n",
      "Epoch 1745 | Train Loss: 1.6135333776474 | Test Loss: 1.6341933012008667\n",
      "Epoch 1746 | Train Loss: 1.3909391164779663 | Test Loss: 1.397402048110962\n",
      "Epoch 1747 | Train Loss: 1.6130115985870361 | Test Loss: 1.6336991786956787\n",
      "Epoch 1748 | Train Loss: 1.3905532360076904 | Test Loss: 1.3970190286636353\n",
      "Epoch 1749 | Train Loss: 1.612494945526123 | Test Loss: 1.6332154273986816\n",
      "Epoch 1750 | Train Loss: 1.3901692628860474 | Test Loss: 1.3966423273086548\n",
      "Epoch 1751 | Train Loss: 1.611975908279419 | Test Loss: 1.6327459812164307\n",
      "Epoch 1752 | Train Loss: 1.3897918462753296 | Test Loss: 1.3962695598602295\n",
      "Epoch 1753 | Train Loss: 1.6114559173583984 | Test Loss: 1.6322741508483887\n",
      "Epoch 1754 | Train Loss: 1.389413833618164 | Test Loss: 1.3958953619003296\n",
      "Epoch 1755 | Train Loss: 1.610938549041748 | Test Loss: 1.631786823272705\n",
      "Epoch 1756 | Train Loss: 1.3890353441238403 | Test Loss: 1.395524263381958\n",
      "Epoch 1757 | Train Loss: 1.6104204654693604 | Test Loss: 1.6312963962554932\n",
      "Epoch 1758 | Train Loss: 1.388662576675415 | Test Loss: 1.3951560258865356\n",
      "Epoch 1759 | Train Loss: 1.609903335571289 | Test Loss: 1.6308106184005737\n",
      "Epoch 1760 | Train Loss: 1.3882874250411987 | Test Loss: 1.3947840929031372\n",
      "Epoch 1761 | Train Loss: 1.6093924045562744 | Test Loss: 1.6303350925445557\n",
      "Epoch 1762 | Train Loss: 1.3879098892211914 | Test Loss: 1.3944149017333984\n",
      "Epoch 1763 | Train Loss: 1.6088813543319702 | Test Loss: 1.6298631429672241\n",
      "Epoch 1764 | Train Loss: 1.3875375986099243 | Test Loss: 1.394047737121582\n",
      "Epoch 1765 | Train Loss: 1.6083693504333496 | Test Loss: 1.6293895244598389\n",
      "Epoch 1766 | Train Loss: 1.3871655464172363 | Test Loss: 1.3936779499053955\n",
      "Epoch 1767 | Train Loss: 1.6078605651855469 | Test Loss: 1.628915548324585\n",
      "Epoch 1768 | Train Loss: 1.3867913484573364 | Test Loss: 1.3933079242706299\n",
      "Epoch 1769 | Train Loss: 1.607353925704956 | Test Loss: 1.6284353733062744\n",
      "Epoch 1770 | Train Loss: 1.386420726776123 | Test Loss: 1.3929437398910522\n",
      "Epoch 1771 | Train Loss: 1.6068440675735474 | Test Loss: 1.6279529333114624\n",
      "Epoch 1772 | Train Loss: 1.386053204536438 | Test Loss: 1.3925824165344238\n",
      "Epoch 1773 | Train Loss: 1.6063332557678223 | Test Loss: 1.6274712085723877\n",
      "Epoch 1774 | Train Loss: 1.3856887817382812 | Test Loss: 1.3922240734100342\n",
      "Epoch 1775 | Train Loss: 1.6058218479156494 | Test Loss: 1.6269935369491577\n",
      "Epoch 1776 | Train Loss: 1.385324239730835 | Test Loss: 1.3918638229370117\n",
      "Epoch 1777 | Train Loss: 1.6053146123886108 | Test Loss: 1.6265151500701904\n",
      "Epoch 1778 | Train Loss: 1.3849583864212036 | Test Loss: 1.3914999961853027\n",
      "Epoch 1779 | Train Loss: 1.6048117876052856 | Test Loss: 1.626038908958435\n",
      "Epoch 1780 | Train Loss: 1.3845895528793335 | Test Loss: 1.3911361694335938\n",
      "Epoch 1781 | Train Loss: 1.604311227798462 | Test Loss: 1.62557053565979\n",
      "Epoch 1782 | Train Loss: 1.3842229843139648 | Test Loss: 1.3907783031463623\n",
      "Epoch 1783 | Train Loss: 1.603808045387268 | Test Loss: 1.625102162361145\n",
      "Epoch 1784 | Train Loss: 1.383863091468811 | Test Loss: 1.390424132347107\n",
      "Epoch 1785 | Train Loss: 1.6033046245574951 | Test Loss: 1.6246329545974731\n",
      "Epoch 1786 | Train Loss: 1.383502721786499 | Test Loss: 1.3900690078735352\n",
      "Epoch 1787 | Train Loss: 1.6028043031692505 | Test Loss: 1.6241625547409058\n",
      "Epoch 1788 | Train Loss: 1.3831424713134766 | Test Loss: 1.3897172212600708\n",
      "Epoch 1789 | Train Loss: 1.6023043394088745 | Test Loss: 1.6236975193023682\n",
      "Epoch 1790 | Train Loss: 1.3827836513519287 | Test Loss: 1.3893647193908691\n",
      "Epoch 1791 | Train Loss: 1.6018074750900269 | Test Loss: 1.6232352256774902\n",
      "Epoch 1792 | Train Loss: 1.3824231624603271 | Test Loss: 1.3890101909637451\n",
      "Epoch 1793 | Train Loss: 1.601314663887024 | Test Loss: 1.6227751970291138\n",
      "Epoch 1794 | Train Loss: 1.3820627927780151 | Test Loss: 1.3886590003967285\n",
      "Epoch 1795 | Train Loss: 1.6008204221725464 | Test Loss: 1.6223088502883911\n",
      "Epoch 1796 | Train Loss: 1.3817076683044434 | Test Loss: 1.3883142471313477\n",
      "Epoch 1797 | Train Loss: 1.6003214120864868 | Test Loss: 1.6218414306640625\n",
      "Epoch 1798 | Train Loss: 1.3813557624816895 | Test Loss: 1.3879694938659668\n",
      "Epoch 1799 | Train Loss: 1.5998237133026123 | Test Loss: 1.6213804483413696\n",
      "Epoch 1800 | Train Loss: 1.3810017108917236 | Test Loss: 1.3876208066940308\n",
      "Epoch 1801 | Train Loss: 1.599331259727478 | Test Loss: 1.6209241151809692\n",
      "Epoch 1802 | Train Loss: 1.380645751953125 | Test Loss: 1.387273907661438\n",
      "Epoch 1803 | Train Loss: 1.598839282989502 | Test Loss: 1.6204737424850464\n",
      "Epoch 1804 | Train Loss: 1.380294919013977 | Test Loss: 1.3869298696517944\n",
      "Epoch 1805 | Train Loss: 1.5983471870422363 | Test Loss: 1.6200196743011475\n",
      "Epoch 1806 | Train Loss: 1.3799422979354858 | Test Loss: 1.3865833282470703\n",
      "Epoch 1807 | Train Loss: 1.5978587865829468 | Test Loss: 1.6195489168167114\n",
      "Epoch 1808 | Train Loss: 1.3795886039733887 | Test Loss: 1.3862359523773193\n",
      "Epoch 1809 | Train Loss: 1.5973732471466064 | Test Loss: 1.6190778017044067\n",
      "Epoch 1810 | Train Loss: 1.3792359828948975 | Test Loss: 1.3858917951583862\n",
      "Epoch 1811 | Train Loss: 1.5968877077102661 | Test Loss: 1.6186286211013794\n",
      "Epoch 1812 | Train Loss: 1.3788862228393555 | Test Loss: 1.3855502605438232\n",
      "Epoch 1813 | Train Loss: 1.5964031219482422 | Test Loss: 1.6181917190551758\n",
      "Epoch 1814 | Train Loss: 1.378536343574524 | Test Loss: 1.3852065801620483\n",
      "Epoch 1815 | Train Loss: 1.5959233045578003 | Test Loss: 1.6177424192428589\n",
      "Epoch 1816 | Train Loss: 1.378184199333191 | Test Loss: 1.384863018989563\n",
      "Epoch 1817 | Train Loss: 1.5954437255859375 | Test Loss: 1.6172776222229004\n",
      "Epoch 1818 | Train Loss: 1.377835988998413 | Test Loss: 1.384521722793579\n",
      "Epoch 1819 | Train Loss: 1.594962477684021 | Test Loss: 1.6168100833892822\n",
      "Epoch 1820 | Train Loss: 1.377492070198059 | Test Loss: 1.3841863870620728\n",
      "Epoch 1821 | Train Loss: 1.5944780111312866 | Test Loss: 1.6163525581359863\n",
      "Epoch 1822 | Train Loss: 1.377153754234314 | Test Loss: 1.3838549852371216\n",
      "Epoch 1823 | Train Loss: 1.5939931869506836 | Test Loss: 1.6158978939056396\n",
      "Epoch 1824 | Train Loss: 1.3768157958984375 | Test Loss: 1.3835188150405884\n",
      "Epoch 1825 | Train Loss: 1.5935145616531372 | Test Loss: 1.61544668674469\n",
      "Epoch 1826 | Train Loss: 1.3764716386795044 | Test Loss: 1.3831782341003418\n",
      "Epoch 1827 | Train Loss: 1.5930424928665161 | Test Loss: 1.6150012016296387\n",
      "Epoch 1828 | Train Loss: 1.3761248588562012 | Test Loss: 1.3828341960906982\n",
      "Epoch 1829 | Train Loss: 1.5925748348236084 | Test Loss: 1.6145622730255127\n",
      "Epoch 1830 | Train Loss: 1.375777244567871 | Test Loss: 1.3824946880340576\n",
      "Epoch 1831 | Train Loss: 1.5921064615249634 | Test Loss: 1.6141185760498047\n",
      "Epoch 1832 | Train Loss: 1.37543523311615 | Test Loss: 1.382164716720581\n",
      "Epoch 1833 | Train Loss: 1.5916329622268677 | Test Loss: 1.6136630773544312\n",
      "Epoch 1834 | Train Loss: 1.3751003742218018 | Test Loss: 1.3818371295928955\n",
      "Epoch 1835 | Train Loss: 1.5911580324172974 | Test Loss: 1.6132084131240845\n",
      "Epoch 1836 | Train Loss: 1.3747667074203491 | Test Loss: 1.3815079927444458\n",
      "Epoch 1837 | Train Loss: 1.5906853675842285 | Test Loss: 1.6127618551254272\n",
      "Epoch 1838 | Train Loss: 1.374433994293213 | Test Loss: 1.3811825513839722\n",
      "Epoch 1839 | Train Loss: 1.5902104377746582 | Test Loss: 1.6123157739639282\n",
      "Epoch 1840 | Train Loss: 1.374106764793396 | Test Loss: 1.3808612823486328\n",
      "Epoch 1841 | Train Loss: 1.5897340774536133 | Test Loss: 1.6118823289871216\n",
      "Epoch 1842 | Train Loss: 1.3737804889678955 | Test Loss: 1.3805369138717651\n",
      "Epoch 1843 | Train Loss: 1.589260220527649 | Test Loss: 1.6114503145217896\n",
      "Epoch 1844 | Train Loss: 1.373451828956604 | Test Loss: 1.3802127838134766\n",
      "Epoch 1845 | Train Loss: 1.58878755569458 | Test Loss: 1.6110163927078247\n",
      "Epoch 1846 | Train Loss: 1.3731261491775513 | Test Loss: 1.379891276359558\n",
      "Epoch 1847 | Train Loss: 1.5883139371871948 | Test Loss: 1.6105725765228271\n",
      "Epoch 1848 | Train Loss: 1.372800588607788 | Test Loss: 1.3795684576034546\n",
      "Epoch 1849 | Train Loss: 1.5878428220748901 | Test Loss: 1.6101239919662476\n",
      "Epoch 1850 | Train Loss: 1.372473955154419 | Test Loss: 1.3792427778244019\n",
      "Epoch 1851 | Train Loss: 1.5873754024505615 | Test Loss: 1.6096926927566528\n",
      "Epoch 1852 | Train Loss: 1.3721437454223633 | Test Loss: 1.3789159059524536\n",
      "Epoch 1853 | Train Loss: 1.5869113206863403 | Test Loss: 1.6092697381973267\n",
      "Epoch 1854 | Train Loss: 1.3718125820159912 | Test Loss: 1.3785897493362427\n",
      "Epoch 1855 | Train Loss: 1.5864485502243042 | Test Loss: 1.6088398694992065\n",
      "Epoch 1856 | Train Loss: 1.3714841604232788 | Test Loss: 1.37826669216156\n",
      "Epoch 1857 | Train Loss: 1.5859841108322144 | Test Loss: 1.6084078550338745\n",
      "Epoch 1858 | Train Loss: 1.3711580038070679 | Test Loss: 1.377943754196167\n",
      "Epoch 1859 | Train Loss: 1.5855194330215454 | Test Loss: 1.607975721359253\n",
      "Epoch 1860 | Train Loss: 1.370833158493042 | Test Loss: 1.3776183128356934\n",
      "Epoch 1861 | Train Loss: 1.585058331489563 | Test Loss: 1.607555866241455\n",
      "Epoch 1862 | Train Loss: 1.37050461769104 | Test Loss: 1.3772928714752197\n",
      "Epoch 1863 | Train Loss: 1.5845991373062134 | Test Loss: 1.6071428060531616\n",
      "Epoch 1864 | Train Loss: 1.3701790571212769 | Test Loss: 1.3769731521606445\n",
      "Epoch 1865 | Train Loss: 1.584137201309204 | Test Loss: 1.6067150831222534\n",
      "Epoch 1866 | Train Loss: 1.3698586225509644 | Test Loss: 1.376655101776123\n",
      "Epoch 1867 | Train Loss: 1.5836752653121948 | Test Loss: 1.606276512145996\n",
      "Epoch 1868 | Train Loss: 1.3695372343063354 | Test Loss: 1.3763352632522583\n",
      "Epoch 1869 | Train Loss: 1.583215355873108 | Test Loss: 1.6058354377746582\n",
      "Epoch 1870 | Train Loss: 1.369216799736023 | Test Loss: 1.3760181665420532\n",
      "Epoch 1871 | Train Loss: 1.5827558040618896 | Test Loss: 1.6054105758666992\n",
      "Epoch 1872 | Train Loss: 1.3688985109329224 | Test Loss: 1.3757046461105347\n",
      "Epoch 1873 | Train Loss: 1.5822962522506714 | Test Loss: 1.6050045490264893\n",
      "Epoch 1874 | Train Loss: 1.3685810565948486 | Test Loss: 1.3753914833068848\n",
      "Epoch 1875 | Train Loss: 1.5818380117416382 | Test Loss: 1.6045942306518555\n",
      "Epoch 1876 | Train Loss: 1.3682630062103271 | Test Loss: 1.3750762939453125\n",
      "Epoch 1877 | Train Loss: 1.581382393836975 | Test Loss: 1.6041604280471802\n",
      "Epoch 1878 | Train Loss: 1.3679449558258057 | Test Loss: 1.3747650384902954\n",
      "Epoch 1879 | Train Loss: 1.5809255838394165 | Test Loss: 1.603725790977478\n",
      "Epoch 1880 | Train Loss: 1.3676328659057617 | Test Loss: 1.3744535446166992\n",
      "Epoch 1881 | Train Loss: 1.5804719924926758 | Test Loss: 1.6033110618591309\n",
      "Epoch 1882 | Train Loss: 1.3673160076141357 | Test Loss: 1.3741402626037598\n",
      "Epoch 1883 | Train Loss: 1.580021858215332 | Test Loss: 1.6029036045074463\n",
      "Epoch 1884 | Train Loss: 1.3670028448104858 | Test Loss: 1.3738332986831665\n",
      "Epoch 1885 | Train Loss: 1.579568862915039 | Test Loss: 1.6024789810180664\n",
      "Epoch 1886 | Train Loss: 1.3666939735412598 | Test Loss: 1.3735284805297852\n",
      "Epoch 1887 | Train Loss: 1.5791165828704834 | Test Loss: 1.6020612716674805\n",
      "Epoch 1888 | Train Loss: 1.3663822412490845 | Test Loss: 1.3732190132141113\n",
      "Epoch 1889 | Train Loss: 1.5786705017089844 | Test Loss: 1.601656198501587\n",
      "Epoch 1890 | Train Loss: 1.3660674095153809 | Test Loss: 1.3729076385498047\n",
      "Epoch 1891 | Train Loss: 1.5782287120819092 | Test Loss: 1.6012465953826904\n",
      "Epoch 1892 | Train Loss: 1.3657523393630981 | Test Loss: 1.3725993633270264\n",
      "Epoch 1893 | Train Loss: 1.5777860879898071 | Test Loss: 1.6008204221725464\n",
      "Epoch 1894 | Train Loss: 1.3654417991638184 | Test Loss: 1.3722946643829346\n",
      "Epoch 1895 | Train Loss: 1.5773416757583618 | Test Loss: 1.600404977798462\n",
      "Epoch 1896 | Train Loss: 1.3651336431503296 | Test Loss: 1.3719894886016846\n",
      "Epoch 1897 | Train Loss: 1.5768991708755493 | Test Loss: 1.600008487701416\n",
      "Epoch 1898 | Train Loss: 1.3648228645324707 | Test Loss: 1.3716840744018555\n",
      "Epoch 1899 | Train Loss: 1.5764590501785278 | Test Loss: 1.5996103286743164\n",
      "Epoch 1900 | Train Loss: 1.364514708518982 | Test Loss: 1.3713852167129517\n",
      "Epoch 1901 | Train Loss: 1.576015591621399 | Test Loss: 1.5991965532302856\n",
      "Epoch 1902 | Train Loss: 1.364210605621338 | Test Loss: 1.371086597442627\n",
      "Epoch 1903 | Train Loss: 1.5755741596221924 | Test Loss: 1.5987809896469116\n",
      "Epoch 1904 | Train Loss: 1.36390221118927 | Test Loss: 1.370782494544983\n",
      "Epoch 1905 | Train Loss: 1.5751398801803589 | Test Loss: 1.598374843597412\n",
      "Epoch 1906 | Train Loss: 1.3635913133621216 | Test Loss: 1.3704814910888672\n",
      "Epoch 1907 | Train Loss: 1.5747065544128418 | Test Loss: 1.5979758501052856\n",
      "Epoch 1908 | Train Loss: 1.363283395767212 | Test Loss: 1.3701844215393066\n",
      "Epoch 1909 | Train Loss: 1.5742714405059814 | Test Loss: 1.5975806713104248\n",
      "Epoch 1910 | Train Loss: 1.3629809617996216 | Test Loss: 1.369893193244934\n",
      "Epoch 1911 | Train Loss: 1.5738309621810913 | Test Loss: 1.5971812009811401\n",
      "Epoch 1912 | Train Loss: 1.3626843690872192 | Test Loss: 1.3696024417877197\n",
      "Epoch 1913 | Train Loss: 1.5733909606933594 | Test Loss: 1.596787929534912\n",
      "Epoch 1914 | Train Loss: 1.3623851537704468 | Test Loss: 1.3693073987960815\n",
      "Epoch 1915 | Train Loss: 1.572955846786499 | Test Loss: 1.5963871479034424\n",
      "Epoch 1916 | Train Loss: 1.362082600593567 | Test Loss: 1.3690122365951538\n",
      "Epoch 1917 | Train Loss: 1.5725226402282715 | Test Loss: 1.5959731340408325\n",
      "Epoch 1918 | Train Loss: 1.3617830276489258 | Test Loss: 1.3687233924865723\n",
      "Epoch 1919 | Train Loss: 1.5720856189727783 | Test Loss: 1.595564842224121\n",
      "Epoch 1920 | Train Loss: 1.3614873886108398 | Test Loss: 1.368435025215149\n",
      "Epoch 1921 | Train Loss: 1.5716506242752075 | Test Loss: 1.5951850414276123\n",
      "Epoch 1922 | Train Loss: 1.3611891269683838 | Test Loss: 1.3681457042694092\n",
      "Epoch 1923 | Train Loss: 1.5712170600891113 | Test Loss: 1.5948009490966797\n",
      "Epoch 1924 | Train Loss: 1.3608951568603516 | Test Loss: 1.3678592443466187\n",
      "Epoch 1925 | Train Loss: 1.570779800415039 | Test Loss: 1.5943905115127563\n",
      "Epoch 1926 | Train Loss: 1.360602855682373 | Test Loss: 1.367571234703064\n",
      "Epoch 1927 | Train Loss: 1.5703446865081787 | Test Loss: 1.5939866304397583\n",
      "Epoch 1928 | Train Loss: 1.3603098392486572 | Test Loss: 1.3672817945480347\n",
      "Epoch 1929 | Train Loss: 1.5699130296707153 | Test Loss: 1.5935990810394287\n",
      "Epoch 1930 | Train Loss: 1.3600140810012817 | Test Loss: 1.3669918775558472\n",
      "Epoch 1931 | Train Loss: 1.5694835186004639 | Test Loss: 1.5932077169418335\n",
      "Epoch 1932 | Train Loss: 1.3597209453582764 | Test Loss: 1.3667072057724\n",
      "Epoch 1933 | Train Loss: 1.5690500736236572 | Test Loss: 1.5928126573562622\n",
      "Epoch 1934 | Train Loss: 1.3594313859939575 | Test Loss: 1.3664195537567139\n",
      "Epoch 1935 | Train Loss: 1.5686205625534058 | Test Loss: 1.5924203395843506\n",
      "Epoch 1936 | Train Loss: 1.3591371774673462 | Test Loss: 1.3661298751831055\n",
      "Epoch 1937 | Train Loss: 1.5681939125061035 | Test Loss: 1.5920214653015137\n",
      "Epoch 1938 | Train Loss: 1.3588454723358154 | Test Loss: 1.3658453226089478\n",
      "Epoch 1939 | Train Loss: 1.5677646398544312 | Test Loss: 1.5916171073913574\n",
      "Epoch 1940 | Train Loss: 1.3585593700408936 | Test Loss: 1.365566611289978\n",
      "Epoch 1941 | Train Loss: 1.5673322677612305 | Test Loss: 1.591217041015625\n",
      "Epoch 1942 | Train Loss: 1.358275055885315 | Test Loss: 1.365286946296692\n",
      "Epoch 1943 | Train Loss: 1.5669020414352417 | Test Loss: 1.5908217430114746\n",
      "Epoch 1944 | Train Loss: 1.3579871654510498 | Test Loss: 1.3650017976760864\n",
      "Epoch 1945 | Train Loss: 1.5664777755737305 | Test Loss: 1.5904408693313599\n",
      "Epoch 1946 | Train Loss: 1.3576951026916504 | Test Loss: 1.364716649055481\n",
      "Epoch 1947 | Train Loss: 1.5660566091537476 | Test Loss: 1.5900574922561646\n",
      "Epoch 1948 | Train Loss: 1.3574047088623047 | Test Loss: 1.364435076713562\n",
      "Epoch 1949 | Train Loss: 1.5656347274780273 | Test Loss: 1.589666724205017\n",
      "Epoch 1950 | Train Loss: 1.3571176528930664 | Test Loss: 1.3641574382781982\n",
      "Epoch 1951 | Train Loss: 1.5652104616165161 | Test Loss: 1.5892789363861084\n",
      "Epoch 1952 | Train Loss: 1.3568345308303833 | Test Loss: 1.3638830184936523\n",
      "Epoch 1953 | Train Loss: 1.564784049987793 | Test Loss: 1.5888962745666504\n",
      "Epoch 1954 | Train Loss: 1.3565527200698853 | Test Loss: 1.3636054992675781\n",
      "Epoch 1955 | Train Loss: 1.564361572265625 | Test Loss: 1.5885156393051147\n",
      "Epoch 1956 | Train Loss: 1.356268048286438 | Test Loss: 1.3633301258087158\n",
      "Epoch 1957 | Train Loss: 1.5639389753341675 | Test Loss: 1.5881237983703613\n",
      "Epoch 1958 | Train Loss: 1.3559876680374146 | Test Loss: 1.363057255744934\n",
      "Epoch 1959 | Train Loss: 1.563516616821289 | Test Loss: 1.5877394676208496\n",
      "Epoch 1960 | Train Loss: 1.355706810951233 | Test Loss: 1.3627835512161255\n",
      "Epoch 1961 | Train Loss: 1.5630954504013062 | Test Loss: 1.5873643159866333\n",
      "Epoch 1962 | Train Loss: 1.3554275035858154 | Test Loss: 1.3625158071517944\n",
      "Epoch 1963 | Train Loss: 1.5626708269119263 | Test Loss: 1.5869847536087036\n",
      "Epoch 1964 | Train Loss: 1.3551541566848755 | Test Loss: 1.3622499704360962\n",
      "Epoch 1965 | Train Loss: 1.56224524974823 | Test Loss: 1.5865976810455322\n",
      "Epoch 1966 | Train Loss: 1.3548811674118042 | Test Loss: 1.361981987953186\n",
      "Epoch 1967 | Train Loss: 1.561822772026062 | Test Loss: 1.5862128734588623\n",
      "Epoch 1968 | Train Loss: 1.3546050786972046 | Test Loss: 1.3617119789123535\n",
      "Epoch 1969 | Train Loss: 1.5614039897918701 | Test Loss: 1.5858314037322998\n",
      "Epoch 1970 | Train Loss: 1.3543304204940796 | Test Loss: 1.361445426940918\n",
      "Epoch 1971 | Train Loss: 1.5609849691390991 | Test Loss: 1.5854504108428955\n",
      "Epoch 1972 | Train Loss: 1.354058027267456 | Test Loss: 1.3611831665039062\n",
      "Epoch 1973 | Train Loss: 1.560564398765564 | Test Loss: 1.5850740671157837\n",
      "Epoch 1974 | Train Loss: 1.353788137435913 | Test Loss: 1.3609217405319214\n",
      "Epoch 1975 | Train Loss: 1.5601451396942139 | Test Loss: 1.5847008228302002\n",
      "Epoch 1976 | Train Loss: 1.3535175323486328 | Test Loss: 1.3606570959091187\n",
      "Epoch 1977 | Train Loss: 1.559728741645813 | Test Loss: 1.5843287706375122\n",
      "Epoch 1978 | Train Loss: 1.3532449007034302 | Test Loss: 1.3603891134262085\n",
      "Epoch 1979 | Train Loss: 1.5593154430389404 | Test Loss: 1.5839537382125854\n",
      "Epoch 1980 | Train Loss: 1.3529713153839111 | Test Loss: 1.360121488571167\n",
      "Epoch 1981 | Train Loss: 1.5589038133621216 | Test Loss: 1.583573579788208\n",
      "Epoch 1982 | Train Loss: 1.3526968955993652 | Test Loss: 1.3598552942276\n",
      "Epoch 1983 | Train Loss: 1.5584932565689087 | Test Loss: 1.5831927061080933\n",
      "Epoch 1984 | Train Loss: 1.3524247407913208 | Test Loss: 1.3595921993255615\n",
      "Epoch 1985 | Train Loss: 1.5580822229385376 | Test Loss: 1.582831621170044\n",
      "Epoch 1986 | Train Loss: 1.3521528244018555 | Test Loss: 1.359328269958496\n",
      "Epoch 1987 | Train Loss: 1.5576748847961426 | Test Loss: 1.5824722051620483\n",
      "Epoch 1988 | Train Loss: 1.351881504058838 | Test Loss: 1.3590669631958008\n",
      "Epoch 1989 | Train Loss: 1.5572654008865356 | Test Loss: 1.5821012258529663\n",
      "Epoch 1990 | Train Loss: 1.3516135215759277 | Test Loss: 1.3588072061538696\n",
      "Epoch 1991 | Train Loss: 1.556855320930481 | Test Loss: 1.5817363262176514\n",
      "Epoch 1992 | Train Loss: 1.3513457775115967 | Test Loss: 1.3585468530654907\n",
      "Epoch 1993 | Train Loss: 1.5564472675323486 | Test Loss: 1.5813770294189453\n",
      "Epoch 1994 | Train Loss: 1.3510773181915283 | Test Loss: 1.3582861423492432\n",
      "Epoch 1995 | Train Loss: 1.5560399293899536 | Test Loss: 1.5810036659240723\n",
      "Epoch 1996 | Train Loss: 1.3508102893829346 | Test Loss: 1.3580244779586792\n",
      "Epoch 1997 | Train Loss: 1.5556315183639526 | Test Loss: 1.580626130104065\n",
      "Epoch 1998 | Train Loss: 1.3505412340164185 | Test Loss: 1.3577601909637451\n",
      "Epoch 1999 | Train Loss: 1.5552259683609009 | Test Loss: 1.5802643299102783\n",
      "Epoch 2000 | Train Loss: 1.3502717018127441 | Test Loss: 1.3574978113174438\n",
      "Epoch 2001 | Train Loss: 1.5548205375671387 | Test Loss: 1.579911231994629\n",
      "Epoch 2002 | Train Loss: 1.3500028848648071 | Test Loss: 1.3572392463684082\n",
      "Epoch 2003 | Train Loss: 1.5544137954711914 | Test Loss: 1.5795475244522095\n",
      "Epoch 2004 | Train Loss: 1.349738359451294 | Test Loss: 1.3569837808609009\n",
      "Epoch 2005 | Train Loss: 1.5540037155151367 | Test Loss: 1.5791773796081543\n",
      "Epoch 2006 | Train Loss: 1.3494755029678345 | Test Loss: 1.3567250967025757\n",
      "Epoch 2007 | Train Loss: 1.5535967350006104 | Test Loss: 1.5788230895996094\n",
      "Epoch 2008 | Train Loss: 1.3492109775543213 | Test Loss: 1.3564645051956177\n",
      "Epoch 2009 | Train Loss: 1.553191900253296 | Test Loss: 1.5784668922424316\n",
      "Epoch 2010 | Train Loss: 1.348947286605835 | Test Loss: 1.3562071323394775\n",
      "Epoch 2011 | Train Loss: 1.5527856349945068 | Test Loss: 1.5780894756317139\n",
      "Epoch 2012 | Train Loss: 1.34868586063385 | Test Loss: 1.355951189994812\n",
      "Epoch 2013 | Train Loss: 1.5523810386657715 | Test Loss: 1.5777186155319214\n",
      "Epoch 2014 | Train Loss: 1.3484231233596802 | Test Loss: 1.3556936979293823\n",
      "Epoch 2015 | Train Loss: 1.551979899406433 | Test Loss: 1.577368974685669\n",
      "Epoch 2016 | Train Loss: 1.3481574058532715 | Test Loss: 1.3554364442825317\n",
      "Epoch 2017 | Train Loss: 1.5515799522399902 | Test Loss: 1.577026128768921\n",
      "Epoch 2018 | Train Loss: 1.347895860671997 | Test Loss: 1.355184555053711\n",
      "Epoch 2019 | Train Loss: 1.5511746406555176 | Test Loss: 1.5766675472259521\n",
      "Epoch 2020 | Train Loss: 1.347638487815857 | Test Loss: 1.3549331426620483\n",
      "Epoch 2021 | Train Loss: 1.550766944885254 | Test Loss: 1.576303243637085\n",
      "Epoch 2022 | Train Loss: 1.3473808765411377 | Test Loss: 1.3546781539916992\n",
      "Epoch 2023 | Train Loss: 1.550362467765808 | Test Loss: 1.5759466886520386\n",
      "Epoch 2024 | Train Loss: 1.3471201658248901 | Test Loss: 1.3544223308563232\n",
      "Epoch 2025 | Train Loss: 1.549959659576416 | Test Loss: 1.5755853652954102\n",
      "Epoch 2026 | Train Loss: 1.3468602895736694 | Test Loss: 1.3541687726974487\n",
      "Epoch 2027 | Train Loss: 1.5495558977127075 | Test Loss: 1.5752193927764893\n",
      "Epoch 2028 | Train Loss: 1.3466027975082397 | Test Loss: 1.3539193868637085\n",
      "Epoch 2029 | Train Loss: 1.5491491556167603 | Test Loss: 1.5748558044433594\n",
      "Epoch 2030 | Train Loss: 1.3463491201400757 | Test Loss: 1.3536721467971802\n",
      "Epoch 2031 | Train Loss: 1.548741340637207 | Test Loss: 1.574495553970337\n",
      "Epoch 2032 | Train Loss: 1.3460958003997803 | Test Loss: 1.3534224033355713\n",
      "Epoch 2033 | Train Loss: 1.5483373403549194 | Test Loss: 1.5741437673568726\n",
      "Epoch 2034 | Train Loss: 1.3458402156829834 | Test Loss: 1.3531705141067505\n",
      "Epoch 2035 | Train Loss: 1.547936201095581 | Test Loss: 1.5737879276275635\n",
      "Epoch 2036 | Train Loss: 1.3455857038497925 | Test Loss: 1.3529205322265625\n",
      "Epoch 2037 | Train Loss: 1.5475331544876099 | Test Loss: 1.5734210014343262\n",
      "Epoch 2038 | Train Loss: 1.345335602760315 | Test Loss: 1.3526744842529297\n",
      "Epoch 2039 | Train Loss: 1.547128438949585 | Test Loss: 1.573068380355835\n",
      "Epoch 2040 | Train Loss: 1.3450847864151 | Test Loss: 1.3524267673492432\n",
      "Epoch 2041 | Train Loss: 1.546727180480957 | Test Loss: 1.572725534439087\n",
      "Epoch 2042 | Train Loss: 1.344832181930542 | Test Loss: 1.3521801233291626\n",
      "Epoch 2043 | Train Loss: 1.5463273525238037 | Test Loss: 1.5723766088485718\n",
      "Epoch 2044 | Train Loss: 1.3445796966552734 | Test Loss: 1.3519335985183716\n",
      "Epoch 2045 | Train Loss: 1.5459295511245728 | Test Loss: 1.572024941444397\n",
      "Epoch 2046 | Train Loss: 1.3443268537521362 | Test Loss: 1.3516862392425537\n",
      "Epoch 2047 | Train Loss: 1.5455321073532104 | Test Loss: 1.5716767311096191\n",
      "Epoch 2048 | Train Loss: 1.3440749645233154 | Test Loss: 1.3514394760131836\n",
      "Epoch 2049 | Train Loss: 1.5451350212097168 | Test Loss: 1.5713300704956055\n",
      "Epoch 2050 | Train Loss: 1.3438242673873901 | Test Loss: 1.3511954545974731\n",
      "Epoch 2051 | Train Loss: 1.5447372198104858 | Test Loss: 1.5709806680679321\n",
      "Epoch 2052 | Train Loss: 1.343573808670044 | Test Loss: 1.3509528636932373\n",
      "Epoch 2053 | Train Loss: 1.5443397760391235 | Test Loss: 1.570632815361023\n",
      "Epoch 2054 | Train Loss: 1.3433232307434082 | Test Loss: 1.3507074117660522\n",
      "Epoch 2055 | Train Loss: 1.5439438819885254 | Test Loss: 1.570292353630066\n",
      "Epoch 2056 | Train Loss: 1.3430695533752441 | Test Loss: 1.3504570722579956\n",
      "Epoch 2057 | Train Loss: 1.5435519218444824 | Test Loss: 1.5699474811553955\n",
      "Epoch 2058 | Train Loss: 1.3428168296813965 | Test Loss: 1.350211501121521\n",
      "Epoch 2059 | Train Loss: 1.5431567430496216 | Test Loss: 1.5695911645889282\n",
      "Epoch 2060 | Train Loss: 1.3425683975219727 | Test Loss: 1.3499730825424194\n",
      "Epoch 2061 | Train Loss: 1.5427581071853638 | Test Loss: 1.569229006767273\n",
      "Epoch 2062 | Train Loss: 1.34232497215271 | Test Loss: 1.3497382402420044\n",
      "Epoch 2063 | Train Loss: 1.5423580408096313 | Test Loss: 1.568871021270752\n",
      "Epoch 2064 | Train Loss: 1.3420807123184204 | Test Loss: 1.3494980335235596\n",
      "Epoch 2065 | Train Loss: 1.5419623851776123 | Test Loss: 1.5685268640518188\n",
      "Epoch 2066 | Train Loss: 1.3418326377868652 | Test Loss: 1.349252462387085\n",
      "Epoch 2067 | Train Loss: 1.54157292842865 | Test Loss: 1.5681886672973633\n",
      "Epoch 2068 | Train Loss: 1.341580867767334 | Test Loss: 1.3490054607391357\n",
      "Epoch 2069 | Train Loss: 1.5411854982376099 | Test Loss: 1.5678433179855347\n",
      "Epoch 2070 | Train Loss: 1.3413312435150146 | Test Loss: 1.3487651348114014\n",
      "Epoch 2071 | Train Loss: 1.5407943725585938 | Test Loss: 1.5674915313720703\n",
      "Epoch 2072 | Train Loss: 1.341086506843567 | Test Loss: 1.348528265953064\n",
      "Epoch 2073 | Train Loss: 1.5404022932052612 | Test Loss: 1.5671436786651611\n",
      "Epoch 2074 | Train Loss: 1.3408451080322266 | Test Loss: 1.3482911586761475\n",
      "Epoch 2075 | Train Loss: 1.5400099754333496 | Test Loss: 1.566815972328186\n",
      "Epoch 2076 | Train Loss: 1.3406034708023071 | Test Loss: 1.3480525016784668\n",
      "Epoch 2077 | Train Loss: 1.5396192073822021 | Test Loss: 1.5664894580841064\n",
      "Epoch 2078 | Train Loss: 1.3403626680374146 | Test Loss: 1.3478142023086548\n",
      "Epoch 2079 | Train Loss: 1.5392301082611084 | Test Loss: 1.5661391019821167\n",
      "Epoch 2080 | Train Loss: 1.3401216268539429 | Test Loss: 1.3475788831710815\n",
      "Epoch 2081 | Train Loss: 1.5388426780700684 | Test Loss: 1.5657864809036255\n",
      "Epoch 2082 | Train Loss: 1.339881420135498 | Test Loss: 1.3473455905914307\n",
      "Epoch 2083 | Train Loss: 1.538456678390503 | Test Loss: 1.565453290939331\n",
      "Epoch 2084 | Train Loss: 1.339639663696289 | Test Loss: 1.3471113443374634\n",
      "Epoch 2085 | Train Loss: 1.5380727052688599 | Test Loss: 1.5651212930679321\n",
      "Epoch 2086 | Train Loss: 1.339400053024292 | Test Loss: 1.3468793630599976\n",
      "Epoch 2087 | Train Loss: 1.5376863479614258 | Test Loss: 1.5647743940353394\n",
      "Epoch 2088 | Train Loss: 1.3391636610031128 | Test Loss: 1.3466503620147705\n",
      "Epoch 2089 | Train Loss: 1.5372987985610962 | Test Loss: 1.5644351243972778\n",
      "Epoch 2090 | Train Loss: 1.3389291763305664 | Test Loss: 1.3464175462722778\n",
      "Epoch 2091 | Train Loss: 1.5369144678115845 | Test Loss: 1.5641213655471802\n",
      "Epoch 2092 | Train Loss: 1.338689923286438 | Test Loss: 1.3461849689483643\n",
      "Epoch 2093 | Train Loss: 1.5365312099456787 | Test Loss: 1.5637961626052856\n",
      "Epoch 2094 | Train Loss: 1.3384555578231812 | Test Loss: 1.3459593057632446\n",
      "Epoch 2095 | Train Loss: 1.53614342212677 | Test Loss: 1.563446044921875\n",
      "Epoch 2096 | Train Loss: 1.3382232189178467 | Test Loss: 1.3457319736480713\n",
      "Epoch 2097 | Train Loss: 1.5357576608657837 | Test Loss: 1.5631088018417358\n",
      "Epoch 2098 | Train Loss: 1.3379873037338257 | Test Loss: 1.3454986810684204\n",
      "Epoch 2099 | Train Loss: 1.535376787185669 | Test Loss: 1.5627963542938232\n",
      "Epoch 2100 | Train Loss: 1.3377491235733032 | Test Loss: 1.3452666997909546\n",
      "Epoch 2101 | Train Loss: 1.5349961519241333 | Test Loss: 1.562469720840454\n",
      "Epoch 2102 | Train Loss: 1.3375132083892822 | Test Loss: 1.3450404405593872\n",
      "Epoch 2103 | Train Loss: 1.5346133708953857 | Test Loss: 1.5621259212493896\n",
      "Epoch 2104 | Train Loss: 1.3372812271118164 | Test Loss: 1.3448169231414795\n",
      "Epoch 2105 | Train Loss: 1.5342296361923218 | Test Loss: 1.5617843866348267\n",
      "Epoch 2106 | Train Loss: 1.3370482921600342 | Test Loss: 1.3445905447006226\n",
      "Epoch 2107 | Train Loss: 1.5338468551635742 | Test Loss: 1.561448335647583\n",
      "Epoch 2108 | Train Loss: 1.3368161916732788 | Test Loss: 1.3443657159805298\n",
      "Epoch 2109 | Train Loss: 1.5334664583206177 | Test Loss: 1.5611193180084229\n",
      "Epoch 2110 | Train Loss: 1.3365834951400757 | Test Loss: 1.3441392183303833\n",
      "Epoch 2111 | Train Loss: 1.5330872535705566 | Test Loss: 1.5607953071594238\n",
      "Epoch 2112 | Train Loss: 1.3363507986068726 | Test Loss: 1.3439148664474487\n",
      "Epoch 2113 | Train Loss: 1.5327082872390747 | Test Loss: 1.5604685544967651\n",
      "Epoch 2114 | Train Loss: 1.3361185789108276 | Test Loss: 1.3436862230300903\n",
      "Epoch 2115 | Train Loss: 1.5323315858840942 | Test Loss: 1.5601487159729004\n",
      "Epoch 2116 | Train Loss: 1.3358845710754395 | Test Loss: 1.3434550762176514\n",
      "Epoch 2117 | Train Loss: 1.5319570302963257 | Test Loss: 1.5598177909851074\n",
      "Epoch 2118 | Train Loss: 1.3356530666351318 | Test Loss: 1.3432281017303467\n",
      "Epoch 2119 | Train Loss: 1.5315797328948975 | Test Loss: 1.5594807863235474\n",
      "Epoch 2120 | Train Loss: 1.3354240655899048 | Test Loss: 1.3430051803588867\n",
      "Epoch 2121 | Train Loss: 1.5312024354934692 | Test Loss: 1.5591609477996826\n",
      "Epoch 2122 | Train Loss: 1.3351943492889404 | Test Loss: 1.3427809476852417\n",
      "Epoch 2123 | Train Loss: 1.5308284759521484 | Test Loss: 1.5588438510894775\n",
      "Epoch 2124 | Train Loss: 1.334963083267212 | Test Loss: 1.3425564765930176\n",
      "Epoch 2125 | Train Loss: 1.5304561853408813 | Test Loss: 1.558510422706604\n",
      "Epoch 2126 | Train Loss: 1.334734559059143 | Test Loss: 1.3423316478729248\n",
      "Epoch 2127 | Train Loss: 1.5300828218460083 | Test Loss: 1.5581789016723633\n",
      "Epoch 2128 | Train Loss: 1.3345057964324951 | Test Loss: 1.3421047925949097\n",
      "Epoch 2129 | Train Loss: 1.529711365699768 | Test Loss: 1.5578595399856567\n",
      "Epoch 2130 | Train Loss: 1.3342770338058472 | Test Loss: 1.3418816328048706\n",
      "Epoch 2131 | Train Loss: 1.5293382406234741 | Test Loss: 1.557527780532837\n",
      "Epoch 2132 | Train Loss: 1.3340532779693604 | Test Loss: 1.341665506362915\n",
      "Epoch 2133 | Train Loss: 1.5289613008499146 | Test Loss: 1.5571924448013306\n",
      "Epoch 2134 | Train Loss: 1.333831787109375 | Test Loss: 1.3414493799209595\n",
      "Epoch 2135 | Train Loss: 1.5285860300064087 | Test Loss: 1.5568718910217285\n",
      "Epoch 2136 | Train Loss: 1.3336066007614136 | Test Loss: 1.341225028038025\n",
      "Epoch 2137 | Train Loss: 1.5282174348831177 | Test Loss: 1.5565540790557861\n",
      "Epoch 2138 | Train Loss: 1.3333771228790283 | Test Loss: 1.341001033782959\n",
      "Epoch 2139 | Train Loss: 1.5278481245040894 | Test Loss: 1.5562130212783813\n",
      "Epoch 2140 | Train Loss: 1.3331514596939087 | Test Loss: 1.3407816886901855\n",
      "Epoch 2141 | Train Loss: 1.52747642993927 | Test Loss: 1.5558741092681885\n",
      "Epoch 2142 | Train Loss: 1.3329291343688965 | Test Loss: 1.3405636548995972\n",
      "Epoch 2143 | Train Loss: 1.5271047353744507 | Test Loss: 1.555559754371643\n",
      "Epoch 2144 | Train Loss: 1.3327044248580933 | Test Loss: 1.3403453826904297\n",
      "Epoch 2145 | Train Loss: 1.526734709739685 | Test Loss: 1.5552414655685425\n",
      "Epoch 2146 | Train Loss: 1.332482933998108 | Test Loss: 1.3401285409927368\n",
      "Epoch 2147 | Train Loss: 1.5263614654541016 | Test Loss: 1.5549033880233765\n",
      "Epoch 2148 | Train Loss: 1.332261323928833 | Test Loss: 1.339909315109253\n",
      "Epoch 2149 | Train Loss: 1.5259928703308105 | Test Loss: 1.5545772314071655\n",
      "Epoch 2150 | Train Loss: 1.3320363759994507 | Test Loss: 1.339686393737793\n",
      "Epoch 2151 | Train Loss: 1.5256294012069702 | Test Loss: 1.5542653799057007\n",
      "Epoch 2152 | Train Loss: 1.3318092823028564 | Test Loss: 1.3394665718078613\n",
      "Epoch 2153 | Train Loss: 1.5252665281295776 | Test Loss: 1.5539393424987793\n",
      "Epoch 2154 | Train Loss: 1.3315832614898682 | Test Loss: 1.3392488956451416\n",
      "Epoch 2155 | Train Loss: 1.5249022245407104 | Test Loss: 1.5535999536514282\n",
      "Epoch 2156 | Train Loss: 1.3313586711883545 | Test Loss: 1.339030146598816\n",
      "Epoch 2157 | Train Loss: 1.5245391130447388 | Test Loss: 1.553274393081665\n",
      "Epoch 2158 | Train Loss: 1.3311330080032349 | Test Loss: 1.33881413936615\n",
      "Epoch 2159 | Train Loss: 1.5241756439208984 | Test Loss: 1.55294930934906\n",
      "Epoch 2160 | Train Loss: 1.3309119939804077 | Test Loss: 1.3386045694351196\n",
      "Epoch 2161 | Train Loss: 1.5238093137741089 | Test Loss: 1.5526199340820312\n",
      "Epoch 2162 | Train Loss: 1.330693006515503 | Test Loss: 1.338392972946167\n",
      "Epoch 2163 | Train Loss: 1.5234440565109253 | Test Loss: 1.5523022413253784\n",
      "Epoch 2164 | Train Loss: 1.3304721117019653 | Test Loss: 1.3381786346435547\n",
      "Epoch 2165 | Train Loss: 1.523080825805664 | Test Loss: 1.5519824028015137\n",
      "Epoch 2166 | Train Loss: 1.3302521705627441 | Test Loss: 1.3379687070846558\n",
      "Epoch 2167 | Train Loss: 1.5227166414260864 | Test Loss: 1.5516636371612549\n",
      "Epoch 2168 | Train Loss: 1.3300340175628662 | Test Loss: 1.3377562761306763\n",
      "Epoch 2169 | Train Loss: 1.522355318069458 | Test Loss: 1.551356554031372\n",
      "Epoch 2170 | Train Loss: 1.3298125267028809 | Test Loss: 1.3375385999679565\n",
      "Epoch 2171 | Train Loss: 1.5219972133636475 | Test Loss: 1.5510380268096924\n",
      "Epoch 2172 | Train Loss: 1.3295917510986328 | Test Loss: 1.3373256921768188\n",
      "Epoch 2173 | Train Loss: 1.521638035774231 | Test Loss: 1.5507222414016724\n",
      "Epoch 2174 | Train Loss: 1.3293726444244385 | Test Loss: 1.3371156454086304\n",
      "Epoch 2175 | Train Loss: 1.521278738975525 | Test Loss: 1.5504230260849\n",
      "Epoch 2176 | Train Loss: 1.3291531801223755 | Test Loss: 1.3369046449661255\n",
      "Epoch 2177 | Train Loss: 1.5209215879440308 | Test Loss: 1.5501071214675903\n",
      "Epoch 2178 | Train Loss: 1.3289350271224976 | Test Loss: 1.3366949558258057\n",
      "Epoch 2179 | Train Loss: 1.5205636024475098 | Test Loss: 1.5497767925262451\n",
      "Epoch 2180 | Train Loss: 1.3287159204483032 | Test Loss: 1.336482048034668\n",
      "Epoch 2181 | Train Loss: 1.5202090740203857 | Test Loss: 1.5494651794433594\n",
      "Epoch 2182 | Train Loss: 1.3284958600997925 | Test Loss: 1.336272120475769\n",
      "Epoch 2183 | Train Loss: 1.519855260848999 | Test Loss: 1.5491622686386108\n",
      "Epoch 2184 | Train Loss: 1.328276515007019 | Test Loss: 1.3360657691955566\n",
      "Epoch 2185 | Train Loss: 1.5195013284683228 | Test Loss: 1.5488557815551758\n",
      "Epoch 2186 | Train Loss: 1.3280589580535889 | Test Loss: 1.3358571529388428\n",
      "Epoch 2187 | Train Loss: 1.5191471576690674 | Test Loss: 1.5485475063323975\n",
      "Epoch 2188 | Train Loss: 1.3278416395187378 | Test Loss: 1.3356438875198364\n",
      "Epoch 2189 | Train Loss: 1.5187950134277344 | Test Loss: 1.5482414960861206\n",
      "Epoch 2190 | Train Loss: 1.3276234865188599 | Test Loss: 1.335433006286621\n",
      "Epoch 2191 | Train Loss: 1.5184434652328491 | Test Loss: 1.5479196310043335\n",
      "Epoch 2192 | Train Loss: 1.327409029006958 | Test Loss: 1.3352302312850952\n",
      "Epoch 2193 | Train Loss: 1.5180881023406982 | Test Loss: 1.5476109981536865\n",
      "Epoch 2194 | Train Loss: 1.3271963596343994 | Test Loss: 1.3350262641906738\n",
      "Epoch 2195 | Train Loss: 1.5177345275878906 | Test Loss: 1.5473233461380005\n",
      "Epoch 2196 | Train Loss: 1.3269814252853394 | Test Loss: 1.3348188400268555\n",
      "Epoch 2197 | Train Loss: 1.5173840522766113 | Test Loss: 1.5470166206359863\n",
      "Epoch 2198 | Train Loss: 1.3267673254013062 | Test Loss: 1.3346143960952759\n",
      "Epoch 2199 | Train Loss: 1.5170319080352783 | Test Loss: 1.5466899871826172\n",
      "Epoch 2200 | Train Loss: 1.3265538215637207 | Test Loss: 1.3344084024429321\n",
      "Epoch 2201 | Train Loss: 1.5166813135147095 | Test Loss: 1.5463730096817017\n",
      "Epoch 2202 | Train Loss: 1.3263405561447144 | Test Loss: 1.334204912185669\n",
      "Epoch 2203 | Train Loss: 1.5163304805755615 | Test Loss: 1.5460753440856934\n",
      "Epoch 2204 | Train Loss: 1.3261277675628662 | Test Loss: 1.3339968919754028\n",
      "Epoch 2205 | Train Loss: 1.515982747077942 | Test Loss: 1.5457838773727417\n",
      "Epoch 2206 | Train Loss: 1.3259141445159912 | Test Loss: 1.3337910175323486\n",
      "Epoch 2207 | Train Loss: 1.515635371208191 | Test Loss: 1.545487403869629\n",
      "Epoch 2208 | Train Loss: 1.3257018327713013 | Test Loss: 1.3335888385772705\n",
      "Epoch 2209 | Train Loss: 1.515289068222046 | Test Loss: 1.5451935529708862\n",
      "Epoch 2210 | Train Loss: 1.325490117073059 | Test Loss: 1.3333884477615356\n",
      "Epoch 2211 | Train Loss: 1.514940857887268 | Test Loss: 1.5448952913284302\n",
      "Epoch 2212 | Train Loss: 1.3252816200256348 | Test Loss: 1.3331879377365112\n",
      "Epoch 2213 | Train Loss: 1.514593482017517 | Test Loss: 1.5445910692214966\n",
      "Epoch 2214 | Train Loss: 1.3250728845596313 | Test Loss: 1.3329863548278809\n",
      "Epoch 2215 | Train Loss: 1.5142476558685303 | Test Loss: 1.5442814826965332\n",
      "Epoch 2216 | Train Loss: 1.3248649835586548 | Test Loss: 1.3327887058258057\n",
      "Epoch 2217 | Train Loss: 1.5139000415802002 | Test Loss: 1.5439648628234863\n",
      "Epoch 2218 | Train Loss: 1.3246585130691528 | Test Loss: 1.3325941562652588\n",
      "Epoch 2219 | Train Loss: 1.5135533809661865 | Test Loss: 1.5436629056930542\n",
      "Epoch 2220 | Train Loss: 1.3244514465332031 | Test Loss: 1.3323954343795776\n",
      "Epoch 2221 | Train Loss: 1.5132116079330444 | Test Loss: 1.543370008468628\n",
      "Epoch 2222 | Train Loss: 1.3242422342300415 | Test Loss: 1.332194209098816\n",
      "Epoch 2223 | Train Loss: 1.512870192527771 | Test Loss: 1.5430619716644287\n",
      "Epoch 2224 | Train Loss: 1.3240339756011963 | Test Loss: 1.331993579864502\n",
      "Epoch 2225 | Train Loss: 1.512527346611023 | Test Loss: 1.5427603721618652\n",
      "Epoch 2226 | Train Loss: 1.3238251209259033 | Test Loss: 1.3317910432815552\n",
      "Epoch 2227 | Train Loss: 1.5121872425079346 | Test Loss: 1.5424628257751465\n",
      "Epoch 2228 | Train Loss: 1.3236151933670044 | Test Loss: 1.3315874338150024\n",
      "Epoch 2229 | Train Loss: 1.5118494033813477 | Test Loss: 1.5421730279922485\n",
      "Epoch 2230 | Train Loss: 1.323401689529419 | Test Loss: 1.3313783407211304\n",
      "Epoch 2231 | Train Loss: 1.5115156173706055 | Test Loss: 1.5418790578842163\n",
      "Epoch 2232 | Train Loss: 1.3231889009475708 | Test Loss: 1.3311736583709717\n",
      "Epoch 2233 | Train Loss: 1.5111792087554932 | Test Loss: 1.5415889024734497\n",
      "Epoch 2234 | Train Loss: 1.322980284690857 | Test Loss: 1.3309756517410278\n",
      "Epoch 2235 | Train Loss: 1.5108387470245361 | Test Loss: 1.541292428970337\n",
      "Epoch 2236 | Train Loss: 1.3227769136428833 | Test Loss: 1.3307831287384033\n",
      "Epoch 2237 | Train Loss: 1.5104949474334717 | Test Loss: 1.540988802909851\n",
      "Epoch 2238 | Train Loss: 1.322574496269226 | Test Loss: 1.3305844068527222\n",
      "Epoch 2239 | Train Loss: 1.5101547241210938 | Test Loss: 1.5406925678253174\n",
      "Epoch 2240 | Train Loss: 1.3223674297332764 | Test Loss: 1.3303802013397217\n",
      "Epoch 2241 | Train Loss: 1.5098192691802979 | Test Loss: 1.5404009819030762\n",
      "Epoch 2242 | Train Loss: 1.3221594095230103 | Test Loss: 1.3301782608032227\n",
      "Epoch 2243 | Train Loss: 1.5094834566116333 | Test Loss: 1.5401102304458618\n",
      "Epoch 2244 | Train Loss: 1.321951150894165 | Test Loss: 1.3299806118011475\n",
      "Epoch 2245 | Train Loss: 1.5091480016708374 | Test Loss: 1.5398311614990234\n",
      "Epoch 2246 | Train Loss: 1.321743130683899 | Test Loss: 1.329782485961914\n",
      "Epoch 2247 | Train Loss: 1.5088105201721191 | Test Loss: 1.5395572185516357\n",
      "Epoch 2248 | Train Loss: 1.3215371370315552 | Test Loss: 1.3295811414718628\n",
      "Epoch 2249 | Train Loss: 1.5084736347198486 | Test Loss: 1.5392695665359497\n",
      "Epoch 2250 | Train Loss: 1.3213311433792114 | Test Loss: 1.3293849229812622\n",
      "Epoch 2251 | Train Loss: 1.5081357955932617 | Test Loss: 1.5389769077301025\n",
      "Epoch 2252 | Train Loss: 1.3211253881454468 | Test Loss: 1.3291882276535034\n",
      "Epoch 2253 | Train Loss: 1.5077991485595703 | Test Loss: 1.5386868715286255\n",
      "Epoch 2254 | Train Loss: 1.3209189176559448 | Test Loss: 1.3289893865585327\n",
      "Epoch 2255 | Train Loss: 1.5074645280838013 | Test Loss: 1.5384050607681274\n",
      "Epoch 2256 | Train Loss: 1.3207117319107056 | Test Loss: 1.328790307044983\n",
      "Epoch 2257 | Train Loss: 1.5071284770965576 | Test Loss: 1.5381243228912354\n",
      "Epoch 2258 | Train Loss: 1.3205063343048096 | Test Loss: 1.3285915851593018\n",
      "Epoch 2259 | Train Loss: 1.506792664527893 | Test Loss: 1.5378371477127075\n",
      "Epoch 2260 | Train Loss: 1.3203022480010986 | Test Loss: 1.3283957242965698\n",
      "Epoch 2261 | Train Loss: 1.5064564943313599 | Test Loss: 1.5375443696975708\n",
      "Epoch 2262 | Train Loss: 1.3200997114181519 | Test Loss: 1.3282042741775513\n",
      "Epoch 2263 | Train Loss: 1.5061180591583252 | Test Loss: 1.5372490882873535\n",
      "Epoch 2264 | Train Loss: 1.3199000358581543 | Test Loss: 1.3280141353607178\n",
      "Epoch 2265 | Train Loss: 1.5057785511016846 | Test Loss: 1.5369644165039062\n",
      "Epoch 2266 | Train Loss: 1.3197005987167358 | Test Loss: 1.327824592590332\n",
      "Epoch 2267 | Train Loss: 1.5054399967193604 | Test Loss: 1.5366815328598022\n",
      "Epoch 2268 | Train Loss: 1.3194994926452637 | Test Loss: 1.3276346921920776\n",
      "Epoch 2269 | Train Loss: 1.5051047801971436 | Test Loss: 1.536390781402588\n",
      "Epoch 2270 | Train Loss: 1.3192951679229736 | Test Loss: 1.3274403810501099\n",
      "Epoch 2271 | Train Loss: 1.5047725439071655 | Test Loss: 1.536102056503296\n",
      "Epoch 2272 | Train Loss: 1.3190886974334717 | Test Loss: 1.3272401094436646\n",
      "Epoch 2273 | Train Loss: 1.504441499710083 | Test Loss: 1.535815954208374\n",
      "Epoch 2274 | Train Loss: 1.3188841342926025 | Test Loss: 1.327045202255249\n",
      "Epoch 2275 | Train Loss: 1.5041073560714722 | Test Loss: 1.5355228185653687\n",
      "Epoch 2276 | Train Loss: 1.318682074546814 | Test Loss: 1.3268585205078125\n",
      "Epoch 2277 | Train Loss: 1.5037702322006226 | Test Loss: 1.5352259874343872\n",
      "Epoch 2278 | Train Loss: 1.3184813261032104 | Test Loss: 1.3266701698303223\n",
      "Epoch 2279 | Train Loss: 1.5034358501434326 | Test Loss: 1.5349410772323608\n",
      "Epoch 2280 | Train Loss: 1.3182770013809204 | Test Loss: 1.3264753818511963\n",
      "Epoch 2281 | Train Loss: 1.5031042098999023 | Test Loss: 1.5346531867980957\n",
      "Epoch 2282 | Train Loss: 1.3180724382400513 | Test Loss: 1.3262789249420166\n",
      "Epoch 2283 | Train Loss: 1.5027724504470825 | Test Loss: 1.5343635082244873\n",
      "Epoch 2284 | Train Loss: 1.3178685903549194 | Test Loss: 1.3260846138000488\n",
      "Epoch 2285 | Train Loss: 1.5024394989013672 | Test Loss: 1.5340791940689087\n",
      "Epoch 2286 | Train Loss: 1.3176674842834473 | Test Loss: 1.3258931636810303\n",
      "Epoch 2287 | Train Loss: 1.5021064281463623 | Test Loss: 1.5337939262390137\n",
      "Epoch 2288 | Train Loss: 1.3174662590026855 | Test Loss: 1.3257007598876953\n",
      "Epoch 2289 | Train Loss: 1.5017743110656738 | Test Loss: 1.5335031747817993\n",
      "Epoch 2290 | Train Loss: 1.317264437675476 | Test Loss: 1.3255122900009155\n",
      "Epoch 2291 | Train Loss: 1.5014394521713257 | Test Loss: 1.5332196950912476\n",
      "Epoch 2292 | Train Loss: 1.3170678615570068 | Test Loss: 1.3253276348114014\n",
      "Epoch 2293 | Train Loss: 1.501102089881897 | Test Loss: 1.532940149307251\n",
      "Epoch 2294 | Train Loss: 1.3168745040893555 | Test Loss: 1.3251466751098633\n",
      "Epoch 2295 | Train Loss: 1.5007623434066772 | Test Loss: 1.5326519012451172\n",
      "Epoch 2296 | Train Loss: 1.3166818618774414 | Test Loss: 1.3249626159667969\n",
      "Epoch 2297 | Train Loss: 1.500424861907959 | Test Loss: 1.5323638916015625\n",
      "Epoch 2298 | Train Loss: 1.3164862394332886 | Test Loss: 1.3247745037078857\n",
      "Epoch 2299 | Train Loss: 1.5000914335250854 | Test Loss: 1.5320812463760376\n",
      "Epoch 2300 | Train Loss: 1.3162868022918701 | Test Loss: 1.324585199356079\n",
      "Epoch 2301 | Train Loss: 1.499761939048767 | Test Loss: 1.5318032503128052\n",
      "Epoch 2302 | Train Loss: 1.3160865306854248 | Test Loss: 1.3243967294692993\n",
      "Epoch 2303 | Train Loss: 1.4994316101074219 | Test Loss: 1.531529188156128\n",
      "Epoch 2304 | Train Loss: 1.3158875703811646 | Test Loss: 1.324208378791809\n",
      "Epoch 2305 | Train Loss: 1.499101161956787 | Test Loss: 1.5312676429748535\n",
      "Epoch 2306 | Train Loss: 1.3156903982162476 | Test Loss: 1.324021577835083\n",
      "Epoch 2307 | Train Loss: 1.4987696409225464 | Test Loss: 1.5310009717941284\n",
      "Epoch 2308 | Train Loss: 1.3154957294464111 | Test Loss: 1.323839545249939\n",
      "Epoch 2309 | Train Loss: 1.498435616493225 | Test Loss: 1.5307092666625977\n",
      "Epoch 2310 | Train Loss: 1.3153046369552612 | Test Loss: 1.323660135269165\n",
      "Epoch 2311 | Train Loss: 1.49809992313385 | Test Loss: 1.5304172039031982\n",
      "Epoch 2312 | Train Loss: 1.3151130676269531 | Test Loss: 1.3234801292419434\n",
      "Epoch 2313 | Train Loss: 1.4977655410766602 | Test Loss: 1.5301421880722046\n",
      "Epoch 2314 | Train Loss: 1.3149220943450928 | Test Loss: 1.3232977390289307\n",
      "Epoch 2315 | Train Loss: 1.4974312782287598 | Test Loss: 1.5298842191696167\n",
      "Epoch 2316 | Train Loss: 1.3147279024124146 | Test Loss: 1.3231083154678345\n",
      "Epoch 2317 | Train Loss: 1.4971035718917847 | Test Loss: 1.5296202898025513\n",
      "Epoch 2318 | Train Loss: 1.314530611038208 | Test Loss: 1.3229223489761353\n",
      "Epoch 2319 | Train Loss: 1.4967769384384155 | Test Loss: 1.529322624206543\n",
      "Epoch 2320 | Train Loss: 1.3143354654312134 | Test Loss: 1.322740077972412\n",
      "Epoch 2321 | Train Loss: 1.4964481592178345 | Test Loss: 1.529033899307251\n",
      "Epoch 2322 | Train Loss: 1.3141406774520874 | Test Loss: 1.3225516080856323\n",
      "Epoch 2323 | Train Loss: 1.4961220026016235 | Test Loss: 1.5287891626358032\n",
      "Epoch 2324 | Train Loss: 1.3139433860778809 | Test Loss: 1.3223642110824585\n",
      "Epoch 2325 | Train Loss: 1.4957958459854126 | Test Loss: 1.5285205841064453\n",
      "Epoch 2326 | Train Loss: 1.313751220703125 | Test Loss: 1.3221871852874756\n",
      "Epoch 2327 | Train Loss: 1.495463252067566 | Test Loss: 1.528230905532837\n",
      "Epoch 2328 | Train Loss: 1.3135616779327393 | Test Loss: 1.3220109939575195\n",
      "Epoch 2329 | Train Loss: 1.495133876800537 | Test Loss: 1.5279688835144043\n",
      "Epoch 2330 | Train Loss: 1.3133671283721924 | Test Loss: 1.3218269348144531\n",
      "Epoch 2331 | Train Loss: 1.494810700416565 | Test Loss: 1.5276880264282227\n",
      "Epoch 2332 | Train Loss: 1.3131723403930664 | Test Loss: 1.3216451406478882\n",
      "Epoch 2333 | Train Loss: 1.4944837093353271 | Test Loss: 1.5274016857147217\n",
      "Epoch 2334 | Train Loss: 1.3129796981811523 | Test Loss: 1.3214648962020874\n",
      "Epoch 2335 | Train Loss: 1.4941569566726685 | Test Loss: 1.5271259546279907\n",
      "Epoch 2336 | Train Loss: 1.3127901554107666 | Test Loss: 1.3212919235229492\n",
      "Epoch 2337 | Train Loss: 1.493826985359192 | Test Loss: 1.5268568992614746\n",
      "Epoch 2338 | Train Loss: 1.3125996589660645 | Test Loss: 1.3211127519607544\n",
      "Epoch 2339 | Train Loss: 1.4935009479522705 | Test Loss: 1.526585578918457\n",
      "Epoch 2340 | Train Loss: 1.3124065399169922 | Test Loss: 1.3209301233291626\n",
      "Epoch 2341 | Train Loss: 1.4931758642196655 | Test Loss: 1.5263075828552246\n",
      "Epoch 2342 | Train Loss: 1.3122141361236572 | Test Loss: 1.3207485675811768\n",
      "Epoch 2343 | Train Loss: 1.4928512573242188 | Test Loss: 1.5260426998138428\n",
      "Epoch 2344 | Train Loss: 1.3120230436325073 | Test Loss: 1.3205726146697998\n",
      "Epoch 2345 | Train Loss: 1.4925230741500854 | Test Loss: 1.5257701873779297\n",
      "Epoch 2346 | Train Loss: 1.3118362426757812 | Test Loss: 1.3203985691070557\n",
      "Epoch 2347 | Train Loss: 1.492192029953003 | Test Loss: 1.5254945755004883\n",
      "Epoch 2348 | Train Loss: 1.3116495609283447 | Test Loss: 1.3202191591262817\n",
      "Epoch 2349 | Train Loss: 1.4918622970581055 | Test Loss: 1.5252175331115723\n",
      "Epoch 2350 | Train Loss: 1.3114633560180664 | Test Loss: 1.3200404644012451\n",
      "Epoch 2351 | Train Loss: 1.491532325744629 | Test Loss: 1.5249351263046265\n",
      "Epoch 2352 | Train Loss: 1.311278223991394 | Test Loss: 1.3198637962341309\n",
      "Epoch 2353 | Train Loss: 1.4912046194076538 | Test Loss: 1.5246660709381104\n",
      "Epoch 2354 | Train Loss: 1.3110895156860352 | Test Loss: 1.319690465927124\n",
      "Epoch 2355 | Train Loss: 1.4908767938613892 | Test Loss: 1.5243957042694092\n",
      "Epoch 2356 | Train Loss: 1.310902714729309 | Test Loss: 1.3195146322250366\n",
      "Epoch 2357 | Train Loss: 1.4905492067337036 | Test Loss: 1.5241262912750244\n",
      "Epoch 2358 | Train Loss: 1.3107136487960815 | Test Loss: 1.3193336725234985\n",
      "Epoch 2359 | Train Loss: 1.4902255535125732 | Test Loss: 1.5238460302352905\n",
      "Epoch 2360 | Train Loss: 1.3105242252349854 | Test Loss: 1.3191570043563843\n",
      "Epoch 2361 | Train Loss: 1.4899007081985474 | Test Loss: 1.5235648155212402\n",
      "Epoch 2362 | Train Loss: 1.3103357553482056 | Test Loss: 1.318983793258667\n",
      "Epoch 2363 | Train Loss: 1.4895771741867065 | Test Loss: 1.523289442062378\n",
      "Epoch 2364 | Train Loss: 1.3101484775543213 | Test Loss: 1.3188129663467407\n",
      "Epoch 2365 | Train Loss: 1.4892513751983643 | Test Loss: 1.523016333580017\n",
      "Epoch 2366 | Train Loss: 1.3099632263183594 | Test Loss: 1.318640112876892\n",
      "Epoch 2367 | Train Loss: 1.4889254570007324 | Test Loss: 1.5227460861206055\n",
      "Epoch 2368 | Train Loss: 1.3097788095474243 | Test Loss: 1.3184670209884644\n",
      "Epoch 2369 | Train Loss: 1.4885995388031006 | Test Loss: 1.5224722623825073\n",
      "Epoch 2370 | Train Loss: 1.3095955848693848 | Test Loss: 1.318297266960144\n",
      "Epoch 2371 | Train Loss: 1.4882735013961792 | Test Loss: 1.5221928358078003\n",
      "Epoch 2372 | Train Loss: 1.3094111680984497 | Test Loss: 1.3181226253509521\n",
      "Epoch 2373 | Train Loss: 1.487950086593628 | Test Loss: 1.5219131708145142\n",
      "Epoch 2374 | Train Loss: 1.3092254400253296 | Test Loss: 1.317946434020996\n",
      "Epoch 2375 | Train Loss: 1.4876279830932617 | Test Loss: 1.5216498374938965\n",
      "Epoch 2376 | Train Loss: 1.309037685394287 | Test Loss: 1.3177673816680908\n",
      "Epoch 2377 | Train Loss: 1.4873080253601074 | Test Loss: 1.5213814973831177\n",
      "Epoch 2378 | Train Loss: 1.3088510036468506 | Test Loss: 1.31759512424469\n",
      "Epoch 2379 | Train Loss: 1.4869855642318726 | Test Loss: 1.5211098194122314\n",
      "Epoch 2380 | Train Loss: 1.3086655139923096 | Test Loss: 1.317421793937683\n",
      "Epoch 2381 | Train Loss: 1.4866647720336914 | Test Loss: 1.5208412408828735\n",
      "Epoch 2382 | Train Loss: 1.308479905128479 | Test Loss: 1.3172481060028076\n",
      "Epoch 2383 | Train Loss: 1.4863439798355103 | Test Loss: 1.5205540657043457\n",
      "Epoch 2384 | Train Loss: 1.3082960844039917 | Test Loss: 1.3170775175094604\n",
      "Epoch 2385 | Train Loss: 1.4860211610794067 | Test Loss: 1.5202767848968506\n",
      "Epoch 2386 | Train Loss: 1.3081132173538208 | Test Loss: 1.316909670829773\n",
      "Epoch 2387 | Train Loss: 1.4856973886489868 | Test Loss: 1.5200121402740479\n",
      "Epoch 2388 | Train Loss: 1.3079304695129395 | Test Loss: 1.3167389631271362\n",
      "Epoch 2389 | Train Loss: 1.4853752851486206 | Test Loss: 1.5197266340255737\n",
      "Epoch 2390 | Train Loss: 1.307747483253479 | Test Loss: 1.316566824913025\n",
      "Epoch 2391 | Train Loss: 1.4850528240203857 | Test Loss: 1.5194505453109741\n",
      "Epoch 2392 | Train Loss: 1.3075639009475708 | Test Loss: 1.316392421722412\n",
      "Epoch 2393 | Train Loss: 1.4847322702407837 | Test Loss: 1.5191808938980103\n",
      "Epoch 2394 | Train Loss: 1.307379126548767 | Test Loss: 1.3162192106246948\n",
      "Epoch 2395 | Train Loss: 1.484410285949707 | Test Loss: 1.518903374671936\n",
      "Epoch 2396 | Train Loss: 1.3071963787078857 | Test Loss: 1.316046953201294\n",
      "Epoch 2397 | Train Loss: 1.4840896129608154 | Test Loss: 1.5186463594436646\n",
      "Epoch 2398 | Train Loss: 1.3070086240768433 | Test Loss: 1.315871238708496\n",
      "Epoch 2399 | Train Loss: 1.4837726354599 | Test Loss: 1.5183709859848022\n",
      "Epoch 2400 | Train Loss: 1.3068242073059082 | Test Loss: 1.3157016038894653\n",
      "Epoch 2401 | Train Loss: 1.4834508895874023 | Test Loss: 1.5180944204330444\n",
      "Epoch 2402 | Train Loss: 1.306640386581421 | Test Loss: 1.3155275583267212\n",
      "Epoch 2403 | Train Loss: 1.48313307762146 | Test Loss: 1.5178334712982178\n",
      "Epoch 2404 | Train Loss: 1.3064532279968262 | Test Loss: 1.3153555393218994\n",
      "Epoch 2405 | Train Loss: 1.4828139543533325 | Test Loss: 1.5175548791885376\n",
      "Epoch 2406 | Train Loss: 1.3062684535980225 | Test Loss: 1.3151838779449463\n",
      "Epoch 2407 | Train Loss: 1.4824926853179932 | Test Loss: 1.5172874927520752\n",
      "Epoch 2408 | Train Loss: 1.3060839176177979 | Test Loss: 1.315010666847229\n",
      "Epoch 2409 | Train Loss: 1.4821736812591553 | Test Loss: 1.5170254707336426\n",
      "Epoch 2410 | Train Loss: 1.3058958053588867 | Test Loss: 1.3148351907730103\n",
      "Epoch 2411 | Train Loss: 1.4818577766418457 | Test Loss: 1.5167502164840698\n",
      "Epoch 2412 | Train Loss: 1.3057057857513428 | Test Loss: 1.3146591186523438\n",
      "Epoch 2413 | Train Loss: 1.4815436601638794 | Test Loss: 1.516497254371643\n",
      "Epoch 2414 | Train Loss: 1.305515170097351 | Test Loss: 1.3144851922988892\n",
      "Epoch 2415 | Train Loss: 1.4812266826629639 | Test Loss: 1.5162431001663208\n",
      "Epoch 2416 | Train Loss: 1.3053315877914429 | Test Loss: 1.3143168687820435\n",
      "Epoch 2417 | Train Loss: 1.4809017181396484 | Test Loss: 1.515967845916748\n",
      "Epoch 2418 | Train Loss: 1.305152416229248 | Test Loss: 1.3141486644744873\n",
      "Epoch 2419 | Train Loss: 1.4805777072906494 | Test Loss: 1.5156902074813843\n",
      "Epoch 2420 | Train Loss: 1.3049687147140503 | Test Loss: 1.313969373703003\n",
      "Epoch 2421 | Train Loss: 1.480262041091919 | Test Loss: 1.5154224634170532\n",
      "Epoch 2422 | Train Loss: 1.3047785758972168 | Test Loss: 1.313793420791626\n",
      "Epoch 2423 | Train Loss: 1.479947566986084 | Test Loss: 1.5151365995407104\n",
      "Epoch 2424 | Train Loss: 1.3045932054519653 | Test Loss: 1.3136259317398071\n",
      "Epoch 2425 | Train Loss: 1.4796286821365356 | Test Loss: 1.5148701667785645\n",
      "Epoch 2426 | Train Loss: 1.3044097423553467 | Test Loss: 1.3134567737579346\n",
      "Epoch 2427 | Train Loss: 1.4793084859848022 | Test Loss: 1.51460599899292\n",
      "Epoch 2428 | Train Loss: 1.3042268753051758 | Test Loss: 1.3132858276367188\n",
      "Epoch 2429 | Train Loss: 1.4789890050888062 | Test Loss: 1.5143101215362549\n",
      "Epoch 2430 | Train Loss: 1.3040437698364258 | Test Loss: 1.3131123781204224\n",
      "Epoch 2431 | Train Loss: 1.4786720275878906 | Test Loss: 1.5140358209609985\n",
      "Epoch 2432 | Train Loss: 1.3038557767868042 | Test Loss: 1.31293785572052\n",
      "Epoch 2433 | Train Loss: 1.4783579111099243 | Test Loss: 1.5137708187103271\n",
      "Epoch 2434 | Train Loss: 1.303671956062317 | Test Loss: 1.3127713203430176\n",
      "Epoch 2435 | Train Loss: 1.478037714958191 | Test Loss: 1.5135053396224976\n",
      "Epoch 2436 | Train Loss: 1.3034899234771729 | Test Loss: 1.312601089477539\n",
      "Epoch 2437 | Train Loss: 1.477719783782959 | Test Loss: 1.5132369995117188\n",
      "Epoch 2438 | Train Loss: 1.3033055067062378 | Test Loss: 1.3124271631240845\n",
      "Epoch 2439 | Train Loss: 1.4774043560028076 | Test Loss: 1.5129570960998535\n",
      "Epoch 2440 | Train Loss: 1.303120732307434 | Test Loss: 1.3122485876083374\n",
      "Epoch 2441 | Train Loss: 1.4770923852920532 | Test Loss: 1.5127066373825073\n",
      "Epoch 2442 | Train Loss: 1.3029311895370483 | Test Loss: 1.3120712041854858\n",
      "Epoch 2443 | Train Loss: 1.4767818450927734 | Test Loss: 1.5124270915985107\n",
      "Epoch 2444 | Train Loss: 1.3027458190917969 | Test Loss: 1.3119006156921387\n",
      "Epoch 2445 | Train Loss: 1.4764654636383057 | Test Loss: 1.5121588706970215\n",
      "Epoch 2446 | Train Loss: 1.3025622367858887 | Test Loss: 1.3117265701293945\n",
      "Epoch 2447 | Train Loss: 1.4761511087417603 | Test Loss: 1.5119125843048096\n",
      "Epoch 2448 | Train Loss: 1.3023771047592163 | Test Loss: 1.311550498008728\n",
      "Epoch 2449 | Train Loss: 1.4758392572402954 | Test Loss: 1.5116416215896606\n",
      "Epoch 2450 | Train Loss: 1.3021917343139648 | Test Loss: 1.3113757371902466\n",
      "Epoch 2451 | Train Loss: 1.4755281209945679 | Test Loss: 1.511371374130249\n",
      "Epoch 2452 | Train Loss: 1.3020042181015015 | Test Loss: 1.311204195022583\n",
      "Epoch 2453 | Train Loss: 1.4752165079116821 | Test Loss: 1.5111119747161865\n",
      "Epoch 2454 | Train Loss: 1.3018192052841187 | Test Loss: 1.3110356330871582\n",
      "Epoch 2455 | Train Loss: 1.4749019145965576 | Test Loss: 1.5108555555343628\n",
      "Epoch 2456 | Train Loss: 1.3016365766525269 | Test Loss: 1.3108664751052856\n",
      "Epoch 2457 | Train Loss: 1.4745875597000122 | Test Loss: 1.510599970817566\n",
      "Epoch 2458 | Train Loss: 1.301450252532959 | Test Loss: 1.3106918334960938\n",
      "Epoch 2459 | Train Loss: 1.4742770195007324 | Test Loss: 1.510331392288208\n",
      "Epoch 2460 | Train Loss: 1.3012624979019165 | Test Loss: 1.3105210065841675\n",
      "Epoch 2461 | Train Loss: 1.4739643335342407 | Test Loss: 1.5100716352462769\n",
      "Epoch 2462 | Train Loss: 1.301078200340271 | Test Loss: 1.3103519678115845\n",
      "Epoch 2463 | Train Loss: 1.4736477136611938 | Test Loss: 1.5098159313201904\n",
      "Epoch 2464 | Train Loss: 1.3008968830108643 | Test Loss: 1.3101814985275269\n",
      "Epoch 2465 | Train Loss: 1.473331093788147 | Test Loss: 1.509543538093567\n",
      "Epoch 2466 | Train Loss: 1.3007140159606934 | Test Loss: 1.3100061416625977\n",
      "Epoch 2467 | Train Loss: 1.4730185270309448 | Test Loss: 1.509287714958191\n",
      "Epoch 2468 | Train Loss: 1.3005247116088867 | Test Loss: 1.3098312616348267\n",
      "Epoch 2469 | Train Loss: 1.4727081060409546 | Test Loss: 1.509034514427185\n",
      "Epoch 2470 | Train Loss: 1.3003367185592651 | Test Loss: 1.3096574544906616\n",
      "Epoch 2471 | Train Loss: 1.472396969795227 | Test Loss: 1.5087741613388062\n",
      "Epoch 2472 | Train Loss: 1.300150990486145 | Test Loss: 1.3094849586486816\n",
      "Epoch 2473 | Train Loss: 1.4720830917358398 | Test Loss: 1.5084972381591797\n",
      "Epoch 2474 | Train Loss: 1.2999666929244995 | Test Loss: 1.3093092441558838\n",
      "Epoch 2475 | Train Loss: 1.4717706441879272 | Test Loss: 1.5082371234893799\n",
      "Epoch 2476 | Train Loss: 1.2997782230377197 | Test Loss: 1.3091318607330322\n",
      "Epoch 2477 | Train Loss: 1.4714611768722534 | Test Loss: 1.5079911947250366\n",
      "Epoch 2478 | Train Loss: 1.2995893955230713 | Test Loss: 1.3089576959609985\n",
      "Epoch 2479 | Train Loss: 1.47115159034729 | Test Loss: 1.5077282190322876\n",
      "Epoch 2480 | Train Loss: 1.2994024753570557 | Test Loss: 1.3087869882583618\n",
      "Epoch 2481 | Train Loss: 1.470839262008667 | Test Loss: 1.507458209991455\n",
      "Epoch 2482 | Train Loss: 1.299216866493225 | Test Loss: 1.3086116313934326\n",
      "Epoch 2483 | Train Loss: 1.4705290794372559 | Test Loss: 1.5072014331817627\n",
      "Epoch 2484 | Train Loss: 1.299028754234314 | Test Loss: 1.3084347248077393\n",
      "Epoch 2485 | Train Loss: 1.4702199697494507 | Test Loss: 1.50693941116333\n",
      "Epoch 2486 | Train Loss: 1.2988418340682983 | Test Loss: 1.3082633018493652\n",
      "Epoch 2487 | Train Loss: 1.4699081182479858 | Test Loss: 1.5066827535629272\n",
      "Epoch 2488 | Train Loss: 1.298655390739441 | Test Loss: 1.3080918788909912\n",
      "Epoch 2489 | Train Loss: 1.4695966243743896 | Test Loss: 1.5064371824264526\n",
      "Epoch 2490 | Train Loss: 1.2984682321548462 | Test Loss: 1.3079184293746948\n",
      "Epoch 2491 | Train Loss: 1.4692870378494263 | Test Loss: 1.5061827898025513\n",
      "Epoch 2492 | Train Loss: 1.298280119895935 | Test Loss: 1.3077439069747925\n",
      "Epoch 2493 | Train Loss: 1.468976616859436 | Test Loss: 1.5059125423431396\n",
      "Epoch 2494 | Train Loss: 1.2980942726135254 | Test Loss: 1.3075690269470215\n",
      "Epoch 2495 | Train Loss: 1.4686648845672607 | Test Loss: 1.505648136138916\n",
      "Epoch 2496 | Train Loss: 1.2979085445404053 | Test Loss: 1.3073921203613281\n",
      "Epoch 2497 | Train Loss: 1.4683555364608765 | Test Loss: 1.5053949356079102\n",
      "Epoch 2498 | Train Loss: 1.2977194786071777 | Test Loss: 1.307220220565796\n",
      "Epoch 2499 | Train Loss: 1.4680448770523071 | Test Loss: 1.5051355361938477\n",
      "Epoch 2500 | Train Loss: 1.2975341081619263 | Test Loss: 1.3070482015609741\n",
      "Epoch 2501 | Train Loss: 1.467732548713684 | Test Loss: 1.5048738718032837\n",
      "Epoch 2502 | Train Loss: 1.2973496913909912 | Test Loss: 1.3068771362304688\n",
      "Epoch 2503 | Train Loss: 1.4674216508865356 | Test Loss: 1.5046067237854004\n",
      "Epoch 2504 | Train Loss: 1.2971622943878174 | Test Loss: 1.3067011833190918\n",
      "Epoch 2505 | Train Loss: 1.4671142101287842 | Test Loss: 1.5043431520462036\n",
      "Epoch 2506 | Train Loss: 1.296973466873169 | Test Loss: 1.3065239191055298\n",
      "Epoch 2507 | Train Loss: 1.46680748462677 | Test Loss: 1.5040922164916992\n",
      "Epoch 2508 | Train Loss: 1.2967844009399414 | Test Loss: 1.306349277496338\n",
      "Epoch 2509 | Train Loss: 1.4665008783340454 | Test Loss: 1.5038375854492188\n",
      "Epoch 2510 | Train Loss: 1.2965953350067139 | Test Loss: 1.3061766624450684\n",
      "Epoch 2511 | Train Loss: 1.466192364692688 | Test Loss: 1.5035629272460938\n",
      "Epoch 2512 | Train Loss: 1.2964091300964355 | Test Loss: 1.3060029745101929\n",
      "Epoch 2513 | Train Loss: 1.4658821821212769 | Test Loss: 1.5032840967178345\n",
      "Epoch 2514 | Train Loss: 1.296224594116211 | Test Loss: 1.3058273792266846\n",
      "Epoch 2515 | Train Loss: 1.4655734300613403 | Test Loss: 1.5030277967453003\n",
      "Epoch 2516 | Train Loss: 1.296035647392273 | Test Loss: 1.30565345287323\n",
      "Epoch 2517 | Train Loss: 1.4652656316757202 | Test Loss: 1.5027720928192139\n",
      "Epoch 2518 | Train Loss: 1.2958487272262573 | Test Loss: 1.305482268333435\n",
      "Epoch 2519 | Train Loss: 1.4649577140808105 | Test Loss: 1.502505898475647\n",
      "Epoch 2520 | Train Loss: 1.2956610918045044 | Test Loss: 1.3053028583526611\n",
      "Epoch 2521 | Train Loss: 1.4646542072296143 | Test Loss: 1.50223970413208\n",
      "Epoch 2522 | Train Loss: 1.2954680919647217 | Test Loss: 1.3051170110702515\n",
      "Epoch 2523 | Train Loss: 1.4643523693084717 | Test Loss: 1.5019824504852295\n",
      "Epoch 2524 | Train Loss: 1.2952768802642822 | Test Loss: 1.304934024810791\n",
      "Epoch 2525 | Train Loss: 1.4640471935272217 | Test Loss: 1.5017402172088623\n",
      "Epoch 2526 | Train Loss: 1.2950893640518188 | Test Loss: 1.3047617673873901\n",
      "Epoch 2527 | Train Loss: 1.4637389183044434 | Test Loss: 1.5014829635620117\n",
      "Epoch 2528 | Train Loss: 1.294901967048645 | Test Loss: 1.3045917749404907\n",
      "Epoch 2529 | Train Loss: 1.463430643081665 | Test Loss: 1.501204013824463\n",
      "Epoch 2530 | Train Loss: 1.2947148084640503 | Test Loss: 1.30441415309906\n",
      "Epoch 2531 | Train Loss: 1.4631247520446777 | Test Loss: 1.500938892364502\n",
      "Epoch 2532 | Train Loss: 1.2945255041122437 | Test Loss: 1.304234504699707\n",
      "Epoch 2533 | Train Loss: 1.4628186225891113 | Test Loss: 1.5006847381591797\n",
      "Epoch 2534 | Train Loss: 1.2943357229232788 | Test Loss: 1.304058313369751\n",
      "Epoch 2535 | Train Loss: 1.4625110626220703 | Test Loss: 1.5004271268844604\n",
      "Epoch 2536 | Train Loss: 1.294147253036499 | Test Loss: 1.3038784265518188\n",
      "Epoch 2537 | Train Loss: 1.462205171585083 | Test Loss: 1.5001758337020874\n",
      "Epoch 2538 | Train Loss: 1.2939543724060059 | Test Loss: 1.3036960363388062\n",
      "Epoch 2539 | Train Loss: 1.4619001150131226 | Test Loss: 1.4999079704284668\n",
      "Epoch 2540 | Train Loss: 1.2937625646591187 | Test Loss: 1.3035163879394531\n",
      "Epoch 2541 | Train Loss: 1.4615933895111084 | Test Loss: 1.4996495246887207\n",
      "Epoch 2542 | Train Loss: 1.2935723066329956 | Test Loss: 1.303341031074524\n",
      "Epoch 2543 | Train Loss: 1.4612855911254883 | Test Loss: 1.499400019645691\n",
      "Epoch 2544 | Train Loss: 1.2933804988861084 | Test Loss: 1.3031651973724365\n",
      "Epoch 2545 | Train Loss: 1.4609769582748413 | Test Loss: 1.49912691116333\n",
      "Epoch 2546 | Train Loss: 1.293190598487854 | Test Loss: 1.3029855489730835\n",
      "Epoch 2547 | Train Loss: 1.4606670141220093 | Test Loss: 1.498863697052002\n",
      "Epoch 2548 | Train Loss: 1.2930017709732056 | Test Loss: 1.30280339717865\n",
      "Epoch 2549 | Train Loss: 1.4603575468063354 | Test Loss: 1.498619794845581\n",
      "Epoch 2550 | Train Loss: 1.2928087711334229 | Test Loss: 1.302624225616455\n",
      "Epoch 2551 | Train Loss: 1.4600478410720825 | Test Loss: 1.498356580734253\n",
      "Epoch 2552 | Train Loss: 1.2926180362701416 | Test Loss: 1.3024495840072632\n",
      "Epoch 2553 | Train Loss: 1.4597359895706177 | Test Loss: 1.4981045722961426\n",
      "Epoch 2554 | Train Loss: 1.292428970336914 | Test Loss: 1.3022711277008057\n",
      "Epoch 2555 | Train Loss: 1.4594252109527588 | Test Loss: 1.4978502988815308\n",
      "Epoch 2556 | Train Loss: 1.2922377586364746 | Test Loss: 1.3020908832550049\n",
      "Epoch 2557 | Train Loss: 1.459115743637085 | Test Loss: 1.4975593090057373\n",
      "Epoch 2558 | Train Loss: 1.2920461893081665 | Test Loss: 1.301914930343628\n",
      "Epoch 2559 | Train Loss: 1.4588056802749634 | Test Loss: 1.497293472290039\n",
      "Epoch 2560 | Train Loss: 1.2918565273284912 | Test Loss: 1.3017388582229614\n",
      "Epoch 2561 | Train Loss: 1.45849609375 | Test Loss: 1.4970474243164062\n",
      "Epoch 2562 | Train Loss: 1.2916629314422607 | Test Loss: 1.3015629053115845\n",
      "Epoch 2563 | Train Loss: 1.458187460899353 | Test Loss: 1.4967581033706665\n",
      "Epoch 2564 | Train Loss: 1.2914693355560303 | Test Loss: 1.3013815879821777\n",
      "Epoch 2565 | Train Loss: 1.4578808546066284 | Test Loss: 1.496503233909607\n",
      "Epoch 2566 | Train Loss: 1.2912750244140625 | Test Loss: 1.3012033700942993\n",
      "Epoch 2567 | Train Loss: 1.45756995677948 | Test Loss: 1.496250867843628\n",
      "Epoch 2568 | Train Loss: 1.2910864353179932 | Test Loss: 1.3010303974151611\n",
      "Epoch 2569 | Train Loss: 1.4572540521621704 | Test Loss: 1.4959665536880493\n",
      "Epoch 2570 | Train Loss: 1.2908974885940552 | Test Loss: 1.3008513450622559\n",
      "Epoch 2571 | Train Loss: 1.4569424390792847 | Test Loss: 1.495705485343933\n",
      "Epoch 2572 | Train Loss: 1.2907023429870605 | Test Loss: 1.3006675243377686\n",
      "Epoch 2573 | Train Loss: 1.4566329717636108 | Test Loss: 1.4954389333724976\n",
      "Epoch 2574 | Train Loss: 1.2905069589614868 | Test Loss: 1.3004850149154663\n",
      "Epoch 2575 | Train Loss: 1.4563202857971191 | Test Loss: 1.4951757192611694\n",
      "Epoch 2576 | Train Loss: 1.2903141975402832 | Test Loss: 1.300304651260376\n",
      "Epoch 2577 | Train Loss: 1.4560068845748901 | Test Loss: 1.4949232339859009\n",
      "Epoch 2578 | Train Loss: 1.2901172637939453 | Test Loss: 1.3001207113265991\n",
      "Epoch 2579 | Train Loss: 1.4556962251663208 | Test Loss: 1.4946434497833252\n",
      "Epoch 2580 | Train Loss: 1.289920687675476 | Test Loss: 1.2999382019042969\n",
      "Epoch 2581 | Train Loss: 1.4553838968276978 | Test Loss: 1.494362711906433\n",
      "Epoch 2582 | Train Loss: 1.2897273302078247 | Test Loss: 1.299759864807129\n",
      "Epoch 2583 | Train Loss: 1.4550691843032837 | Test Loss: 1.4941121339797974\n",
      "Epoch 2584 | Train Loss: 1.289533257484436 | Test Loss: 1.2995822429656982\n",
      "Epoch 2585 | Train Loss: 1.4547532796859741 | Test Loss: 1.493838906288147\n",
      "Epoch 2586 | Train Loss: 1.2893389463424683 | Test Loss: 1.299402117729187\n",
      "Epoch 2587 | Train Loss: 1.454439401626587 | Test Loss: 1.4935740232467651\n",
      "Epoch 2588 | Train Loss: 1.289141297340393 | Test Loss: 1.2992134094238281\n",
      "Epoch 2589 | Train Loss: 1.4541298151016235 | Test Loss: 1.4933264255523682\n",
      "Epoch 2590 | Train Loss: 1.2889386415481567 | Test Loss: 1.299025058746338\n",
      "Epoch 2591 | Train Loss: 1.4538217782974243 | Test Loss: 1.4930464029312134\n",
      "Epoch 2592 | Train Loss: 1.2887372970581055 | Test Loss: 1.298841118812561\n",
      "Epoch 2593 | Train Loss: 1.4535105228424072 | Test Loss: 1.4927886724472046\n",
      "Epoch 2594 | Train Loss: 1.2885386943817139 | Test Loss: 1.2986575365066528\n",
      "Epoch 2595 | Train Loss: 1.4531983137130737 | Test Loss: 1.4925395250320435\n",
      "Epoch 2596 | Train Loss: 1.2883414030075073 | Test Loss: 1.2984755039215088\n",
      "Epoch 2597 | Train Loss: 1.4528815746307373 | Test Loss: 1.4922473430633545\n",
      "Epoch 2598 | Train Loss: 1.2881478071212769 | Test Loss: 1.2982937097549438\n",
      "Epoch 2599 | Train Loss: 1.4525636434555054 | Test Loss: 1.4919861555099487\n",
      "Epoch 2600 | Train Loss: 1.2879520654678345 | Test Loss: 1.2981064319610596\n",
      "Epoch 2601 | Train Loss: 1.4522501230239868 | Test Loss: 1.4917372465133667\n",
      "Epoch 2602 | Train Loss: 1.28775155544281 | Test Loss: 1.2979211807250977\n",
      "Epoch 2603 | Train Loss: 1.4519375562667847 | Test Loss: 1.4914442300796509\n",
      "Epoch 2604 | Train Loss: 1.2875516414642334 | Test Loss: 1.2977399826049805\n",
      "Epoch 2605 | Train Loss: 1.4516232013702393 | Test Loss: 1.4911831617355347\n",
      "Epoch 2606 | Train Loss: 1.2873529195785522 | Test Loss: 1.2975587844848633\n",
      "Epoch 2607 | Train Loss: 1.4513061046600342 | Test Loss: 1.4909330606460571\n",
      "Epoch 2608 | Train Loss: 1.2871544361114502 | Test Loss: 1.2973737716674805\n",
      "Epoch 2609 | Train Loss: 1.4509916305541992 | Test Loss: 1.490649700164795\n",
      "Epoch 2610 | Train Loss: 1.2869536876678467 | Test Loss: 1.2971885204315186\n",
      "Epoch 2611 | Train Loss: 1.4506756067276 | Test Loss: 1.490368127822876\n",
      "Epoch 2612 | Train Loss: 1.2867562770843506 | Test Loss: 1.297011375427246\n",
      "Epoch 2613 | Train Loss: 1.4503529071807861 | Test Loss: 1.4901156425476074\n",
      "Epoch 2614 | Train Loss: 1.2865630388259888 | Test Loss: 1.2968353033065796\n",
      "Epoch 2615 | Train Loss: 1.4500306844711304 | Test Loss: 1.4898394346237183\n",
      "Epoch 2616 | Train Loss: 1.2863656282424927 | Test Loss: 1.296655535697937\n",
      "Epoch 2617 | Train Loss: 1.4497112035751343 | Test Loss: 1.4895448684692383\n",
      "Epoch 2618 | Train Loss: 1.2861677408218384 | Test Loss: 1.2964777946472168\n",
      "Epoch 2619 | Train Loss: 1.449392557144165 | Test Loss: 1.4892902374267578\n",
      "Epoch 2620 | Train Loss: 1.2859694957733154 | Test Loss: 1.2962982654571533\n",
      "Epoch 2621 | Train Loss: 1.4490739107131958 | Test Loss: 1.4890300035476685\n",
      "Epoch 2622 | Train Loss: 1.2857706546783447 | Test Loss: 1.2961163520812988\n",
      "Epoch 2623 | Train Loss: 1.4487556219100952 | Test Loss: 1.4887443780899048\n",
      "Epoch 2624 | Train Loss: 1.2855710983276367 | Test Loss: 1.2959336042404175\n",
      "Epoch 2625 | Train Loss: 1.4484379291534424 | Test Loss: 1.4884824752807617\n",
      "Epoch 2626 | Train Loss: 1.2853727340698242 | Test Loss: 1.2957557439804077\n",
      "Epoch 2627 | Train Loss: 1.4481161832809448 | Test Loss: 1.4882173538208008\n",
      "Epoch 2628 | Train Loss: 1.2851781845092773 | Test Loss: 1.29558527469635\n",
      "Epoch 2629 | Train Loss: 1.447790503501892 | Test Loss: 1.4879189729690552\n",
      "Epoch 2630 | Train Loss: 1.2849855422973633 | Test Loss: 1.2954087257385254\n",
      "Epoch 2631 | Train Loss: 1.4474676847457886 | Test Loss: 1.4876444339752197\n",
      "Epoch 2632 | Train Loss: 1.2847861051559448 | Test Loss: 1.295224666595459\n",
      "Epoch 2633 | Train Loss: 1.4471486806869507 | Test Loss: 1.4873830080032349\n",
      "Epoch 2634 | Train Loss: 1.2845854759216309 | Test Loss: 1.2950429916381836\n",
      "Epoch 2635 | Train Loss: 1.44682776927948 | Test Loss: 1.4871057271957397\n",
      "Epoch 2636 | Train Loss: 1.2843849658966064 | Test Loss: 1.2948617935180664\n",
      "Epoch 2637 | Train Loss: 1.4465060234069824 | Test Loss: 1.4868409633636475\n",
      "Epoch 2638 | Train Loss: 1.2841850519180298 | Test Loss: 1.2946771383285522\n",
      "Epoch 2639 | Train Loss: 1.4461870193481445 | Test Loss: 1.4865673780441284\n",
      "Epoch 2640 | Train Loss: 1.2839808464050293 | Test Loss: 1.2944906949996948\n",
      "Epoch 2641 | Train Loss: 1.4458675384521484 | Test Loss: 1.486277461051941\n",
      "Epoch 2642 | Train Loss: 1.2837786674499512 | Test Loss: 1.2943092584609985\n",
      "Epoch 2643 | Train Loss: 1.4455448389053345 | Test Loss: 1.4860175848007202\n",
      "Epoch 2644 | Train Loss: 1.2835801839828491 | Test Loss: 1.2941292524337769\n",
      "Epoch 2645 | Train Loss: 1.4452177286148071 | Test Loss: 1.485761046409607\n",
      "Epoch 2646 | Train Loss: 1.2833818197250366 | Test Loss: 1.2939473390579224\n",
      "Epoch 2647 | Train Loss: 1.4448918104171753 | Test Loss: 1.4854671955108643\n",
      "Epoch 2648 | Train Loss: 1.2831807136535645 | Test Loss: 1.293764352798462\n",
      "Epoch 2649 | Train Loss: 1.4445668458938599 | Test Loss: 1.485180139541626\n",
      "Epoch 2650 | Train Loss: 1.2829794883728027 | Test Loss: 1.293581485748291\n",
      "Epoch 2651 | Train Loss: 1.444240927696228 | Test Loss: 1.4849190711975098\n",
      "Epoch 2652 | Train Loss: 1.2827781438827515 | Test Loss: 1.2933974266052246\n",
      "Epoch 2653 | Train Loss: 1.4439139366149902 | Test Loss: 1.4846628904342651\n",
      "Epoch 2654 | Train Loss: 1.2825757265090942 | Test Loss: 1.293209195137024\n",
      "Epoch 2655 | Train Loss: 1.4435895681381226 | Test Loss: 1.4843811988830566\n",
      "Epoch 2656 | Train Loss: 1.2823702096939087 | Test Loss: 1.2930198907852173\n",
      "Epoch 2657 | Train Loss: 1.4432657957077026 | Test Loss: 1.4840896129608154\n",
      "Epoch 2658 | Train Loss: 1.2821664810180664 | Test Loss: 1.292834997177124\n",
      "Epoch 2659 | Train Loss: 1.4429385662078857 | Test Loss: 1.4838263988494873\n",
      "Epoch 2660 | Train Loss: 1.2819650173187256 | Test Loss: 1.292651891708374\n",
      "Epoch 2661 | Train Loss: 1.4426101446151733 | Test Loss: 1.4835586547851562\n",
      "Epoch 2662 | Train Loss: 1.2817630767822266 | Test Loss: 1.292468547821045\n",
      "Epoch 2663 | Train Loss: 1.4422818422317505 | Test Loss: 1.4832624197006226\n",
      "Epoch 2664 | Train Loss: 1.281561017036438 | Test Loss: 1.292284369468689\n",
      "Epoch 2665 | Train Loss: 1.4419540166854858 | Test Loss: 1.4829776287078857\n",
      "Epoch 2666 | Train Loss: 1.2813588380813599 | Test Loss: 1.292098879814148\n",
      "Epoch 2667 | Train Loss: 1.4416272640228271 | Test Loss: 1.48270583152771\n",
      "Epoch 2668 | Train Loss: 1.2811545133590698 | Test Loss: 1.2919062376022339\n",
      "Epoch 2669 | Train Loss: 1.4413033723831177 | Test Loss: 1.482425332069397\n",
      "Epoch 2670 | Train Loss: 1.2809486389160156 | Test Loss: 1.2917171716690063\n",
      "Epoch 2671 | Train Loss: 1.4409782886505127 | Test Loss: 1.4821598529815674\n",
      "Epoch 2672 | Train Loss: 1.2807449102401733 | Test Loss: 1.2915314435958862\n",
      "Epoch 2673 | Train Loss: 1.440651535987854 | Test Loss: 1.4818869829177856\n",
      "Epoch 2674 | Train Loss: 1.2805407047271729 | Test Loss: 1.2913403511047363\n",
      "Epoch 2675 | Train Loss: 1.440327525138855 | Test Loss: 1.4816102981567383\n",
      "Epoch 2676 | Train Loss: 1.2803343534469604 | Test Loss: 1.2911452054977417\n",
      "Epoch 2677 | Train Loss: 1.440004587173462 | Test Loss: 1.4813448190689087\n",
      "Epoch 2678 | Train Loss: 1.2801281213760376 | Test Loss: 1.2909525632858276\n",
      "Epoch 2679 | Train Loss: 1.4396778345108032 | Test Loss: 1.481085181236267\n",
      "Epoch 2680 | Train Loss: 1.2799246311187744 | Test Loss: 1.2907602787017822\n",
      "Epoch 2681 | Train Loss: 1.4393513202667236 | Test Loss: 1.480800986289978\n",
      "Epoch 2682 | Train Loss: 1.2797170877456665 | Test Loss: 1.2905621528625488\n",
      "Epoch 2683 | Train Loss: 1.4390287399291992 | Test Loss: 1.480507731437683\n",
      "Epoch 2684 | Train Loss: 1.2795109748840332 | Test Loss: 1.2903692722320557\n",
      "Epoch 2685 | Train Loss: 1.4387017488479614 | Test Loss: 1.4802372455596924\n",
      "Epoch 2686 | Train Loss: 1.2793079614639282 | Test Loss: 1.2901825904846191\n",
      "Epoch 2687 | Train Loss: 1.4383717775344849 | Test Loss: 1.4799528121948242\n",
      "Epoch 2688 | Train Loss: 1.2791080474853516 | Test Loss: 1.2899937629699707\n",
      "Epoch 2689 | Train Loss: 1.4380435943603516 | Test Loss: 1.4796595573425293\n",
      "Epoch 2690 | Train Loss: 1.2789015769958496 | Test Loss: 1.2897974252700806\n",
      "Epoch 2691 | Train Loss: 1.4377199411392212 | Test Loss: 1.479380488395691\n",
      "Epoch 2692 | Train Loss: 1.2786953449249268 | Test Loss: 1.289601445198059\n",
      "Epoch 2693 | Train Loss: 1.4373937845230103 | Test Loss: 1.4791213274002075\n",
      "Epoch 2694 | Train Loss: 1.2784911394119263 | Test Loss: 1.2894073724746704\n",
      "Epoch 2695 | Train Loss: 1.4370652437210083 | Test Loss: 1.478855848312378\n",
      "Epoch 2696 | Train Loss: 1.2782869338989258 | Test Loss: 1.2892093658447266\n",
      "Epoch 2697 | Train Loss: 1.4367399215698242 | Test Loss: 1.4785782098770142\n",
      "Epoch 2698 | Train Loss: 1.2780784368515015 | Test Loss: 1.2890119552612305\n",
      "Epoch 2699 | Train Loss: 1.4364153146743774 | Test Loss: 1.4782845973968506\n",
      "Epoch 2700 | Train Loss: 1.277872085571289 | Test Loss: 1.2888197898864746\n",
      "Epoch 2701 | Train Loss: 1.4360884428024292 | Test Loss: 1.4780088663101196\n",
      "Epoch 2702 | Train Loss: 1.2776654958724976 | Test Loss: 1.288623332977295\n",
      "Epoch 2703 | Train Loss: 1.435765027999878 | Test Loss: 1.477738857269287\n",
      "Epoch 2704 | Train Loss: 1.2774542570114136 | Test Loss: 1.2884153127670288\n",
      "Epoch 2705 | Train Loss: 1.4354482889175415 | Test Loss: 1.47746741771698\n",
      "Epoch 2706 | Train Loss: 1.2772371768951416 | Test Loss: 1.288212537765503\n",
      "Epoch 2707 | Train Loss: 1.435129165649414 | Test Loss: 1.4771931171417236\n",
      "Epoch 2708 | Train Loss: 1.2770274877548218 | Test Loss: 1.2880160808563232\n",
      "Epoch 2709 | Train Loss: 1.4348052740097046 | Test Loss: 1.4769294261932373\n",
      "Epoch 2710 | Train Loss: 1.276818871498108 | Test Loss: 1.2878167629241943\n",
      "Epoch 2711 | Train Loss: 1.4344842433929443 | Test Loss: 1.476652979850769\n",
      "Epoch 2712 | Train Loss: 1.2766064405441284 | Test Loss: 1.2876169681549072\n",
      "Epoch 2713 | Train Loss: 1.4341644048690796 | Test Loss: 1.47635817527771\n",
      "Epoch 2714 | Train Loss: 1.2763947248458862 | Test Loss: 1.2874194383621216\n",
      "Epoch 2715 | Train Loss: 1.4338428974151611 | Test Loss: 1.476095199584961\n",
      "Epoch 2716 | Train Loss: 1.2761832475662231 | Test Loss: 1.2872159481048584\n",
      "Epoch 2717 | Train Loss: 1.433525562286377 | Test Loss: 1.4758093357086182\n",
      "Epoch 2718 | Train Loss: 1.275965929031372 | Test Loss: 1.2870087623596191\n",
      "Epoch 2719 | Train Loss: 1.4332098960876465 | Test Loss: 1.475502610206604\n",
      "Epoch 2720 | Train Loss: 1.2757512331008911 | Test Loss: 1.2868105173110962\n",
      "Epoch 2721 | Train Loss: 1.4328874349594116 | Test Loss: 1.4752445220947266\n",
      "Epoch 2722 | Train Loss: 1.2755396366119385 | Test Loss: 1.2866127490997314\n",
      "Epoch 2723 | Train Loss: 1.432564616203308 | Test Loss: 1.474973201751709\n",
      "Epoch 2724 | Train Loss: 1.2753244638442993 | Test Loss: 1.2864044904708862\n",
      "Epoch 2725 | Train Loss: 1.4322478771209717 | Test Loss: 1.4746822118759155\n",
      "Epoch 2726 | Train Loss: 1.2751045227050781 | Test Loss: 1.286198377609253\n",
      "Epoch 2727 | Train Loss: 1.4319308996200562 | Test Loss: 1.474409818649292\n",
      "Epoch 2728 | Train Loss: 1.2748892307281494 | Test Loss: 1.2859975099563599\n",
      "Epoch 2729 | Train Loss: 1.4316091537475586 | Test Loss: 1.4741337299346924\n",
      "Epoch 2730 | Train Loss: 1.274673581123352 | Test Loss: 1.285793662071228\n",
      "Epoch 2731 | Train Loss: 1.4312891960144043 | Test Loss: 1.4738428592681885\n",
      "Epoch 2732 | Train Loss: 1.2744554281234741 | Test Loss: 1.2855877876281738\n",
      "Epoch 2733 | Train Loss: 1.4309709072113037 | Test Loss: 1.473572015762329\n",
      "Epoch 2734 | Train Loss: 1.2742366790771484 | Test Loss: 1.2853829860687256\n",
      "Epoch 2735 | Train Loss: 1.4306517839431763 | Test Loss: 1.4732880592346191\n",
      "Epoch 2736 | Train Loss: 1.2740179300308228 | Test Loss: 1.285179615020752\n",
      "Epoch 2737 | Train Loss: 1.430333137512207 | Test Loss: 1.473012089729309\n",
      "Epoch 2738 | Train Loss: 1.2737982273101807 | Test Loss: 1.2849719524383545\n",
      "Epoch 2739 | Train Loss: 1.4300155639648438 | Test Loss: 1.472754955291748\n",
      "Epoch 2740 | Train Loss: 1.2735774517059326 | Test Loss: 1.2847607135772705\n",
      "Epoch 2741 | Train Loss: 1.42969810962677 | Test Loss: 1.472473382949829\n",
      "Epoch 2742 | Train Loss: 1.2733592987060547 | Test Loss: 1.2845479249954224\n",
      "Epoch 2743 | Train Loss: 1.4293816089630127 | Test Loss: 1.4721781015396118\n",
      "Epoch 2744 | Train Loss: 1.2731379270553589 | Test Loss: 1.284338355064392\n",
      "Epoch 2745 | Train Loss: 1.4290647506713867 | Test Loss: 1.4718936681747437\n",
      "Epoch 2746 | Train Loss: 1.272918462753296 | Test Loss: 1.2841328382492065\n",
      "Epoch 2747 | Train Loss: 1.4287468194961548 | Test Loss: 1.4716107845306396\n",
      "Epoch 2748 | Train Loss: 1.2726969718933105 | Test Loss: 1.2839184999465942\n",
      "Epoch 2749 | Train Loss: 1.4284340143203735 | Test Loss: 1.4713398218154907\n",
      "Epoch 2750 | Train Loss: 1.27247154712677 | Test Loss: 1.2837023735046387\n",
      "Epoch 2751 | Train Loss: 1.4281214475631714 | Test Loss: 1.4710662364959717\n",
      "Epoch 2752 | Train Loss: 1.2722492218017578 | Test Loss: 1.283489465713501\n",
      "Epoch 2753 | Train Loss: 1.4278051853179932 | Test Loss: 1.4707940816879272\n",
      "Epoch 2754 | Train Loss: 1.2720292806625366 | Test Loss: 1.283278465270996\n",
      "Epoch 2755 | Train Loss: 1.4274888038635254 | Test Loss: 1.4705272912979126\n",
      "Epoch 2756 | Train Loss: 1.2718071937561035 | Test Loss: 1.2830665111541748\n",
      "Epoch 2757 | Train Loss: 1.4271728992462158 | Test Loss: 1.4702351093292236\n",
      "Epoch 2758 | Train Loss: 1.2715864181518555 | Test Loss: 1.282857060432434\n",
      "Epoch 2759 | Train Loss: 1.4268547296524048 | Test Loss: 1.4699475765228271\n",
      "Epoch 2760 | Train Loss: 1.2713699340820312 | Test Loss: 1.2826499938964844\n",
      "Epoch 2761 | Train Loss: 1.4265342950820923 | Test Loss: 1.4696694612503052\n",
      "Epoch 2762 | Train Loss: 1.271150827407837 | Test Loss: 1.2824394702911377\n",
      "Epoch 2763 | Train Loss: 1.426216721534729 | Test Loss: 1.4693756103515625\n",
      "Epoch 2764 | Train Loss: 1.2709287405014038 | Test Loss: 1.2822248935699463\n",
      "Epoch 2765 | Train Loss: 1.4259014129638672 | Test Loss: 1.4691005945205688\n",
      "Epoch 2766 | Train Loss: 1.2707064151763916 | Test Loss: 1.2820113897323608\n",
      "Epoch 2767 | Train Loss: 1.425586223602295 | Test Loss: 1.468833565711975\n",
      "Epoch 2768 | Train Loss: 1.2704826593399048 | Test Loss: 1.2817987203598022\n",
      "Epoch 2769 | Train Loss: 1.4252724647521973 | Test Loss: 1.468544363975525\n",
      "Epoch 2770 | Train Loss: 1.2702594995498657 | Test Loss: 1.2815897464752197\n",
      "Epoch 2771 | Train Loss: 1.424956202507019 | Test Loss: 1.468277096748352\n",
      "Epoch 2772 | Train Loss: 1.2700400352478027 | Test Loss: 1.2813849449157715\n",
      "Epoch 2773 | Train Loss: 1.4246388673782349 | Test Loss: 1.4680052995681763\n",
      "Epoch 2774 | Train Loss: 1.2698185443878174 | Test Loss: 1.2811778783798218\n",
      "Epoch 2775 | Train Loss: 1.4243234395980835 | Test Loss: 1.4677116870880127\n",
      "Epoch 2776 | Train Loss: 1.2695971727371216 | Test Loss: 1.2809648513793945\n",
      "Epoch 2777 | Train Loss: 1.424009084701538 | Test Loss: 1.46744966506958\n",
      "Epoch 2778 | Train Loss: 1.2693755626678467 | Test Loss: 1.2807561159133911\n",
      "Epoch 2779 | Train Loss: 1.4236910343170166 | Test Loss: 1.4671664237976074\n",
      "Epoch 2780 | Train Loss: 1.2691559791564941 | Test Loss: 1.2805476188659668\n",
      "Epoch 2781 | Train Loss: 1.4233733415603638 | Test Loss: 1.4668880701065063\n",
      "Epoch 2782 | Train Loss: 1.2689365148544312 | Test Loss: 1.2803362607955933\n",
      "Epoch 2783 | Train Loss: 1.4230576753616333 | Test Loss: 1.4666204452514648\n",
      "Epoch 2784 | Train Loss: 1.2687112092971802 | Test Loss: 1.280118465423584\n",
      "Epoch 2785 | Train Loss: 1.4227477312088013 | Test Loss: 1.4663199186325073\n",
      "Epoch 2786 | Train Loss: 1.2684839963912964 | Test Loss: 1.2799021005630493\n",
      "Epoch 2787 | Train Loss: 1.42243492603302 | Test Loss: 1.4660615921020508\n",
      "Epoch 2788 | Train Loss: 1.2682634592056274 | Test Loss: 1.279697060585022\n",
      "Epoch 2789 | Train Loss: 1.4221142530441284 | Test Loss: 1.4657936096191406\n",
      "Epoch 2790 | Train Loss: 1.2680450677871704 | Test Loss: 1.279487133026123\n",
      "Epoch 2791 | Train Loss: 1.4217984676361084 | Test Loss: 1.4655075073242188\n",
      "Epoch 2792 | Train Loss: 1.267818808555603 | Test Loss: 1.2792695760726929\n",
      "Epoch 2793 | Train Loss: 1.4214876890182495 | Test Loss: 1.465244174003601\n",
      "Epoch 2794 | Train Loss: 1.267594337463379 | Test Loss: 1.2790558338165283\n",
      "Epoch 2795 | Train Loss: 1.4211734533309937 | Test Loss: 1.4649564027786255\n",
      "Epoch 2796 | Train Loss: 1.2673735618591309 | Test Loss: 1.2788472175598145\n",
      "Epoch 2797 | Train Loss: 1.4208576679229736 | Test Loss: 1.4646687507629395\n",
      "Epoch 2798 | Train Loss: 1.2671507596969604 | Test Loss: 1.2786341905593872\n",
      "Epoch 2799 | Train Loss: 1.4205435514450073 | Test Loss: 1.4643975496292114\n",
      "Epoch 2800 | Train Loss: 1.2669261693954468 | Test Loss: 1.2784172296524048\n",
      "Epoch 2801 | Train Loss: 1.4202297925949097 | Test Loss: 1.464112639427185\n",
      "Epoch 2802 | Train Loss: 1.2667021751403809 | Test Loss: 1.2782055139541626\n",
      "Epoch 2803 | Train Loss: 1.419914722442627 | Test Loss: 1.4638361930847168\n",
      "Epoch 2804 | Train Loss: 1.2664813995361328 | Test Loss: 1.2779988050460815\n",
      "Epoch 2805 | Train Loss: 1.4195960760116577 | Test Loss: 1.4635525941848755\n",
      "Epoch 2806 | Train Loss: 1.2662619352340698 | Test Loss: 1.2777881622314453\n",
      "Epoch 2807 | Train Loss: 1.4192782640457153 | Test Loss: 1.4632760286331177\n",
      "Epoch 2808 | Train Loss: 1.2660417556762695 | Test Loss: 1.2775782346725464\n",
      "Epoch 2809 | Train Loss: 1.4189625978469849 | Test Loss: 1.4630082845687866\n",
      "Epoch 2810 | Train Loss: 1.2658181190490723 | Test Loss: 1.277364730834961\n",
      "Epoch 2811 | Train Loss: 1.4186519384384155 | Test Loss: 1.4627310037612915\n",
      "Epoch 2812 | Train Loss: 1.2655905485153198 | Test Loss: 1.277144432067871\n",
      "Epoch 2813 | Train Loss: 1.4183437824249268 | Test Loss: 1.4624418020248413\n",
      "Epoch 2814 | Train Loss: 1.2653615474700928 | Test Loss: 1.2769256830215454\n",
      "Epoch 2815 | Train Loss: 1.418031930923462 | Test Loss: 1.4621704816818237\n",
      "Epoch 2816 | Train Loss: 1.2651406526565552 | Test Loss: 1.2767144441604614\n",
      "Epoch 2817 | Train Loss: 1.4177123308181763 | Test Loss: 1.4618922472000122\n",
      "Epoch 2818 | Train Loss: 1.2649215459823608 | Test Loss: 1.2765047550201416\n",
      "Epoch 2819 | Train Loss: 1.41739821434021 | Test Loss: 1.4616039991378784\n",
      "Epoch 2820 | Train Loss: 1.264692783355713 | Test Loss: 1.2762861251831055\n",
      "Epoch 2821 | Train Loss: 1.4170914888381958 | Test Loss: 1.4613311290740967\n",
      "Epoch 2822 | Train Loss: 1.264462947845459 | Test Loss: 1.2760648727416992\n",
      "Epoch 2823 | Train Loss: 1.4167826175689697 | Test Loss: 1.461049199104309\n",
      "Epoch 2824 | Train Loss: 1.2642364501953125 | Test Loss: 1.2758480310440063\n",
      "Epoch 2825 | Train Loss: 1.4164719581604004 | Test Loss: 1.460768222808838\n",
      "Epoch 2826 | Train Loss: 1.2640095949172974 | Test Loss: 1.2756301164627075\n",
      "Epoch 2827 | Train Loss: 1.4161648750305176 | Test Loss: 1.460487723350525\n",
      "Epoch 2828 | Train Loss: 1.2637792825698853 | Test Loss: 1.2754106521606445\n",
      "Epoch 2829 | Train Loss: 1.4158625602722168 | Test Loss: 1.4602056741714478\n",
      "Epoch 2830 | Train Loss: 1.2635475397109985 | Test Loss: 1.2751941680908203\n",
      "Epoch 2831 | Train Loss: 1.4155570268630981 | Test Loss: 1.459945559501648\n",
      "Epoch 2832 | Train Loss: 1.263317584991455 | Test Loss: 1.2749754190444946\n",
      "Epoch 2833 | Train Loss: 1.4152523279190063 | Test Loss: 1.459657073020935\n",
      "Epoch 2834 | Train Loss: 1.2630873918533325 | Test Loss: 1.2747526168823242\n",
      "Epoch 2835 | Train Loss: 1.4149503707885742 | Test Loss: 1.4593851566314697\n",
      "Epoch 2836 | Train Loss: 1.2628542184829712 | Test Loss: 1.274530291557312\n",
      "Epoch 2837 | Train Loss: 1.4146462678909302 | Test Loss: 1.4591315984725952\n",
      "Epoch 2838 | Train Loss: 1.2626256942749023 | Test Loss: 1.2743120193481445\n",
      "Epoch 2839 | Train Loss: 1.4143390655517578 | Test Loss: 1.4588524103164673\n",
      "Epoch 2840 | Train Loss: 1.2623984813690186 | Test Loss: 1.2740954160690308\n",
      "Epoch 2841 | Train Loss: 1.41403329372406 | Test Loss: 1.4585856199264526\n",
      "Epoch 2842 | Train Loss: 1.262168526649475 | Test Loss: 1.2738721370697021\n",
      "Epoch 2843 | Train Loss: 1.4137314558029175 | Test Loss: 1.4583145380020142\n",
      "Epoch 2844 | Train Loss: 1.2619357109069824 | Test Loss: 1.2736475467681885\n",
      "Epoch 2845 | Train Loss: 1.41343092918396 | Test Loss: 1.4580259323120117\n",
      "Epoch 2846 | Train Loss: 1.2617048025131226 | Test Loss: 1.2734301090240479\n",
      "Epoch 2847 | Train Loss: 1.4131287336349487 | Test Loss: 1.4577704668045044\n",
      "Epoch 2848 | Train Loss: 1.2614721059799194 | Test Loss: 1.2732110023498535\n",
      "Epoch 2849 | Train Loss: 1.4128268957138062 | Test Loss: 1.4574943780899048\n",
      "Epoch 2850 | Train Loss: 1.261242151260376 | Test Loss: 1.2729905843734741\n",
      "Epoch 2851 | Train Loss: 1.4125261306762695 | Test Loss: 1.457231044769287\n",
      "Epoch 2852 | Train Loss: 1.2610085010528564 | Test Loss: 1.2727699279785156\n",
      "Epoch 2853 | Train Loss: 1.4122260808944702 | Test Loss: 1.4569628238677979\n",
      "Epoch 2854 | Train Loss: 1.2607767581939697 | Test Loss: 1.2725499868392944\n",
      "Epoch 2855 | Train Loss: 1.41192626953125 | Test Loss: 1.4566872119903564\n",
      "Epoch 2856 | Train Loss: 1.260546088218689 | Test Loss: 1.2723326683044434\n",
      "Epoch 2857 | Train Loss: 1.411628007888794 | Test Loss: 1.4564186334609985\n",
      "Epoch 2858 | Train Loss: 1.2603143453598022 | Test Loss: 1.272118330001831\n",
      "Epoch 2859 | Train Loss: 1.411329984664917 | Test Loss: 1.4561322927474976\n",
      "Epoch 2860 | Train Loss: 1.260085940361023 | Test Loss: 1.2719032764434814\n",
      "Epoch 2861 | Train Loss: 1.411031723022461 | Test Loss: 1.4558751583099365\n",
      "Epoch 2862 | Train Loss: 1.2598586082458496 | Test Loss: 1.2716917991638184\n",
      "Epoch 2863 | Train Loss: 1.4107321500778198 | Test Loss: 1.4556074142456055\n",
      "Epoch 2864 | Train Loss: 1.2596315145492554 | Test Loss: 1.2714747190475464\n",
      "Epoch 2865 | Train Loss: 1.4104344844818115 | Test Loss: 1.455336570739746\n",
      "Epoch 2866 | Train Loss: 1.2594027519226074 | Test Loss: 1.271255612373352\n",
      "Epoch 2867 | Train Loss: 1.4101399183273315 | Test Loss: 1.455075740814209\n",
      "Epoch 2868 | Train Loss: 1.2591731548309326 | Test Loss: 1.2710453271865845\n",
      "Epoch 2869 | Train Loss: 1.4098412990570068 | Test Loss: 1.454797387123108\n",
      "Epoch 2870 | Train Loss: 1.2589514255523682 | Test Loss: 1.2708426713943481\n",
      "Epoch 2871 | Train Loss: 1.4095357656478882 | Test Loss: 1.454543113708496\n",
      "Epoch 2872 | Train Loss: 1.2587329149246216 | Test Loss: 1.2706292867660522\n",
      "Epoch 2873 | Train Loss: 1.4092371463775635 | Test Loss: 1.4542663097381592\n",
      "Epoch 2874 | Train Loss: 1.258506417274475 | Test Loss: 1.2704172134399414\n",
      "Epoch 2875 | Train Loss: 1.4089412689208984 | Test Loss: 1.4539815187454224\n",
      "Epoch 2876 | Train Loss: 1.2582823038101196 | Test Loss: 1.2702131271362305\n",
      "Epoch 2877 | Train Loss: 1.4086401462554932 | Test Loss: 1.4537475109100342\n",
      "Epoch 2878 | Train Loss: 1.258061170578003 | Test Loss: 1.2699998617172241\n",
      "Epoch 2879 | Train Loss: 1.4083448648452759 | Test Loss: 1.4534707069396973\n",
      "Epoch 2880 | Train Loss: 1.2578332424163818 | Test Loss: 1.2697855234146118\n",
      "Epoch 2881 | Train Loss: 1.408052921295166 | Test Loss: 1.453202486038208\n",
      "Epoch 2882 | Train Loss: 1.2576082944869995 | Test Loss: 1.2695752382278442\n",
      "Epoch 2883 | Train Loss: 1.407754898071289 | Test Loss: 1.4529420137405396\n",
      "Epoch 2884 | Train Loss: 1.2573869228363037 | Test Loss: 1.2693641185760498\n",
      "Epoch 2885 | Train Loss: 1.4074562788009644 | Test Loss: 1.4526619911193848\n",
      "Epoch 2886 | Train Loss: 1.2571654319763184 | Test Loss: 1.2691584825515747\n",
      "Epoch 2887 | Train Loss: 1.4071604013442993 | Test Loss: 1.4523987770080566\n",
      "Epoch 2888 | Train Loss: 1.2569376230239868 | Test Loss: 1.2689467668533325\n",
      "Epoch 2889 | Train Loss: 1.4068708419799805 | Test Loss: 1.4521253108978271\n",
      "Epoch 2890 | Train Loss: 1.2567088603973389 | Test Loss: 1.2687286138534546\n",
      "Epoch 2891 | Train Loss: 1.406577706336975 | Test Loss: 1.451850175857544\n",
      "Epoch 2892 | Train Loss: 1.2564858198165894 | Test Loss: 1.268518328666687\n",
      "Epoch 2893 | Train Loss: 1.406280279159546 | Test Loss: 1.451587438583374\n",
      "Epoch 2894 | Train Loss: 1.2562646865844727 | Test Loss: 1.2683088779449463\n",
      "Epoch 2895 | Train Loss: 1.4059858322143555 | Test Loss: 1.4513161182403564\n",
      "Epoch 2896 | Train Loss: 1.2560368776321411 | Test Loss: 1.2680964469909668\n",
      "Epoch 2897 | Train Loss: 1.405694842338562 | Test Loss: 1.4510555267333984\n",
      "Epoch 2898 | Train Loss: 1.2558122873306274 | Test Loss: 1.267885684967041\n",
      "Epoch 2899 | Train Loss: 1.4053980112075806 | Test Loss: 1.4508039951324463\n",
      "Epoch 2900 | Train Loss: 1.2555921077728271 | Test Loss: 1.2676759958267212\n",
      "Epoch 2901 | Train Loss: 1.4051018953323364 | Test Loss: 1.450530767440796\n",
      "Epoch 2902 | Train Loss: 1.2553695440292358 | Test Loss: 1.2674652338027954\n",
      "Epoch 2903 | Train Loss: 1.4048113822937012 | Test Loss: 1.4502660036087036\n",
      "Epoch 2904 | Train Loss: 1.2551409006118774 | Test Loss: 1.2672522068023682\n",
      "Epoch 2905 | Train Loss: 1.4045236110687256 | Test Loss: 1.4499866962432861\n",
      "Epoch 2906 | Train Loss: 1.2549161911010742 | Test Loss: 1.2670356035232544\n",
      "Epoch 2907 | Train Loss: 1.404229760169983 | Test Loss: 1.4497299194335938\n",
      "Epoch 2908 | Train Loss: 1.2547005414962769 | Test Loss: 1.2668344974517822\n",
      "Epoch 2909 | Train Loss: 1.4039311408996582 | Test Loss: 1.4494675397872925\n",
      "Epoch 2910 | Train Loss: 1.254482388496399 | Test Loss: 1.2666329145431519\n",
      "Epoch 2911 | Train Loss: 1.4036369323730469 | Test Loss: 1.4491686820983887\n",
      "Epoch 2912 | Train Loss: 1.254262924194336 | Test Loss: 1.2664214372634888\n",
      "Epoch 2913 | Train Loss: 1.4033476114273071 | Test Loss: 1.4489109516143799\n",
      "Epoch 2914 | Train Loss: 1.254042387008667 | Test Loss: 1.2662076950073242\n",
      "Epoch 2915 | Train Loss: 1.4030590057373047 | Test Loss: 1.448651671409607\n",
      "Epoch 2916 | Train Loss: 1.2538228034973145 | Test Loss: 1.2660043239593506\n",
      "Epoch 2917 | Train Loss: 1.402766466140747 | Test Loss: 1.4483745098114014\n",
      "Epoch 2918 | Train Loss: 1.2536100149154663 | Test Loss: 1.2658003568649292\n",
      "Epoch 2919 | Train Loss: 1.4024701118469238 | Test Loss: 1.4481326341629028\n",
      "Epoch 2920 | Train Loss: 1.2533981800079346 | Test Loss: 1.2655948400497437\n",
      "Epoch 2921 | Train Loss: 1.4021776914596558 | Test Loss: 1.44784414768219\n",
      "Epoch 2922 | Train Loss: 1.2531840801239014 | Test Loss: 1.2653902769088745\n",
      "Epoch 2923 | Train Loss: 1.401890754699707 | Test Loss: 1.4476004838943481\n",
      "Epoch 2924 | Train Loss: 1.2529693841934204 | Test Loss: 1.2651901245117188\n",
      "Epoch 2925 | Train Loss: 1.4015991687774658 | Test Loss: 1.4473463296890259\n",
      "Epoch 2926 | Train Loss: 1.2527592182159424 | Test Loss: 1.2649879455566406\n",
      "Epoch 2927 | Train Loss: 1.4013092517852783 | Test Loss: 1.4470723867416382\n",
      "Epoch 2928 | Train Loss: 1.2525458335876465 | Test Loss: 1.2647799253463745\n",
      "Epoch 2929 | Train Loss: 1.4010273218154907 | Test Loss: 1.446835994720459\n",
      "Epoch 2930 | Train Loss: 1.2523245811462402 | Test Loss: 1.2645684480667114\n",
      "Epoch 2931 | Train Loss: 1.400750756263733 | Test Loss: 1.446550965309143\n",
      "Epoch 2932 | Train Loss: 1.2521058320999146 | Test Loss: 1.2643649578094482\n",
      "Epoch 2933 | Train Loss: 1.40047025680542 | Test Loss: 1.4463087320327759\n",
      "Epoch 2934 | Train Loss: 1.251894235610962 | Test Loss: 1.2641671895980835\n",
      "Epoch 2935 | Train Loss: 1.400184154510498 | Test Loss: 1.4460504055023193\n",
      "Epoch 2936 | Train Loss: 1.2516846656799316 | Test Loss: 1.263968586921692\n",
      "Epoch 2937 | Train Loss: 1.399904489517212 | Test Loss: 1.445767879486084\n",
      "Epoch 2938 | Train Loss: 1.2514663934707642 | Test Loss: 1.2637670040130615\n",
      "Epoch 2939 | Train Loss: 1.3996281623840332 | Test Loss: 1.445526361465454\n",
      "Epoch 2940 | Train Loss: 1.2512516975402832 | Test Loss: 1.2635608911514282\n",
      "Epoch 2941 | Train Loss: 1.3993481397628784 | Test Loss: 1.4452658891677856\n",
      "Epoch 2942 | Train Loss: 1.2510403394699097 | Test Loss: 1.2633609771728516\n",
      "Epoch 2943 | Train Loss: 1.399068832397461 | Test Loss: 1.4450345039367676\n",
      "Epoch 2944 | Train Loss: 1.250824213027954 | Test Loss: 1.2631598711013794\n",
      "Epoch 2945 | Train Loss: 1.3987940549850464 | Test Loss: 1.4447870254516602\n",
      "Epoch 2946 | Train Loss: 1.2506067752838135 | Test Loss: 1.262955665588379\n",
      "Epoch 2947 | Train Loss: 1.398521065711975 | Test Loss: 1.4445273876190186\n",
      "Epoch 2948 | Train Loss: 1.2503900527954102 | Test Loss: 1.2627508640289307\n",
      "Epoch 2949 | Train Loss: 1.3982467651367188 | Test Loss: 1.4442801475524902\n",
      "Epoch 2950 | Train Loss: 1.2501753568649292 | Test Loss: 1.2625519037246704\n",
      "Epoch 2951 | Train Loss: 1.3979699611663818 | Test Loss: 1.4440288543701172\n",
      "Epoch 2952 | Train Loss: 1.2499622106552124 | Test Loss: 1.2623475790023804\n",
      "Epoch 2953 | Train Loss: 1.3976973295211792 | Test Loss: 1.443781852722168\n",
      "Epoch 2954 | Train Loss: 1.2497451305389404 | Test Loss: 1.2621437311172485\n",
      "Epoch 2955 | Train Loss: 1.3974249362945557 | Test Loss: 1.44353449344635\n",
      "Epoch 2956 | Train Loss: 1.2495338916778564 | Test Loss: 1.2619463205337524\n",
      "Epoch 2957 | Train Loss: 1.397146224975586 | Test Loss: 1.4432793855667114\n",
      "Epoch 2958 | Train Loss: 1.249326229095459 | Test Loss: 1.26174795627594\n",
      "Epoch 2959 | Train Loss: 1.3968696594238281 | Test Loss: 1.4430134296417236\n",
      "Epoch 2960 | Train Loss: 1.2491155862808228 | Test Loss: 1.2615461349487305\n",
      "Epoch 2961 | Train Loss: 1.3965989351272583 | Test Loss: 1.4427731037139893\n",
      "Epoch 2962 | Train Loss: 1.2489014863967896 | Test Loss: 1.261346459388733\n",
      "Epoch 2963 | Train Loss: 1.396327257156372 | Test Loss: 1.4425092935562134\n",
      "Epoch 2964 | Train Loss: 1.2486920356750488 | Test Loss: 1.261142611503601\n",
      "Epoch 2965 | Train Loss: 1.3960548639297485 | Test Loss: 1.4422835111618042\n",
      "Epoch 2966 | Train Loss: 1.2484809160232544 | Test Loss: 1.2609398365020752\n",
      "Epoch 2967 | Train Loss: 1.3957849740982056 | Test Loss: 1.4420218467712402\n",
      "Epoch 2968 | Train Loss: 1.2482695579528809 | Test Loss: 1.2607392072677612\n",
      "Epoch 2969 | Train Loss: 1.3955156803131104 | Test Loss: 1.4417798519134521\n",
      "Epoch 2970 | Train Loss: 1.2480571269989014 | Test Loss: 1.2605340480804443\n",
      "Epoch 2971 | Train Loss: 1.3952488899230957 | Test Loss: 1.4415348768234253\n",
      "Epoch 2972 | Train Loss: 1.2478443384170532 | Test Loss: 1.2603386640548706\n",
      "Epoch 2973 | Train Loss: 1.3949779272079468 | Test Loss: 1.4412695169448853\n",
      "Epoch 2974 | Train Loss: 1.2476381063461304 | Test Loss: 1.2601453065872192\n",
      "Epoch 2975 | Train Loss: 1.3947017192840576 | Test Loss: 1.441063642501831\n",
      "Epoch 2976 | Train Loss: 1.2474315166473389 | Test Loss: 1.2599350214004517\n",
      "Epoch 2977 | Train Loss: 1.394436001777649 | Test Loss: 1.4407904148101807\n",
      "Epoch 2978 | Train Loss: 1.2472153902053833 | Test Loss: 1.2597308158874512\n",
      "Epoch 2979 | Train Loss: 1.3941730260849 | Test Loss: 1.4405566453933716\n",
      "Epoch 2980 | Train Loss: 1.2470020055770874 | Test Loss: 1.2595384120941162\n",
      "Epoch 2981 | Train Loss: 1.3939025402069092 | Test Loss: 1.4403300285339355\n",
      "Epoch 2982 | Train Loss: 1.246795654296875 | Test Loss: 1.2593387365341187\n",
      "Epoch 2983 | Train Loss: 1.3936314582824707 | Test Loss: 1.4400688409805298\n",
      "Epoch 2984 | Train Loss: 1.2465846538543701 | Test Loss: 1.259129524230957\n",
      "Epoch 2985 | Train Loss: 1.3933683633804321 | Test Loss: 1.4398444890975952\n",
      "Epoch 2986 | Train Loss: 1.2463688850402832 | Test Loss: 1.2589269876480103\n",
      "Epoch 2987 | Train Loss: 1.3931097984313965 | Test Loss: 1.4395954608917236\n",
      "Epoch 2988 | Train Loss: 1.2461515665054321 | Test Loss: 1.2587318420410156\n",
      "Epoch 2989 | Train Loss: 1.392848253250122 | Test Loss: 1.4393638372421265\n",
      "Epoch 2990 | Train Loss: 1.245941400527954 | Test Loss: 1.2585303783416748\n",
      "Epoch 2991 | Train Loss: 1.3925806283950806 | Test Loss: 1.4391194581985474\n",
      "Epoch 2992 | Train Loss: 1.2457364797592163 | Test Loss: 1.25833261013031\n",
      "Epoch 2993 | Train Loss: 1.39231276512146 | Test Loss: 1.4388787746429443\n",
      "Epoch 2994 | Train Loss: 1.245527982711792 | Test Loss: 1.258137583732605\n",
      "Epoch 2995 | Train Loss: 1.392048716545105 | Test Loss: 1.4386552572250366\n",
      "Epoch 2996 | Train Loss: 1.24531888961792 | Test Loss: 1.2579392194747925\n",
      "Epoch 2997 | Train Loss: 1.3917858600616455 | Test Loss: 1.4384148120880127\n",
      "Epoch 2998 | Train Loss: 1.2451119422912598 | Test Loss: 1.2577412128448486\n",
      "Epoch 2999 | Train Loss: 1.3915232419967651 | Test Loss: 1.438196063041687\n",
      "Epoch 3000 | Train Loss: 1.2449039220809937 | Test Loss: 1.257548451423645\n",
      "Epoch 3001 | Train Loss: 1.391258716583252 | Test Loss: 1.43795645236969\n",
      "Epoch 3002 | Train Loss: 1.2446998357772827 | Test Loss: 1.2573552131652832\n",
      "Epoch 3003 | Train Loss: 1.3909975290298462 | Test Loss: 1.4377162456512451\n",
      "Epoch 3004 | Train Loss: 1.2444896697998047 | Test Loss: 1.257157802581787\n",
      "Epoch 3005 | Train Loss: 1.3907408714294434 | Test Loss: 1.4374865293502808\n",
      "Epoch 3006 | Train Loss: 1.2442806959152222 | Test Loss: 1.2569602727890015\n",
      "Epoch 3007 | Train Loss: 1.3904855251312256 | Test Loss: 1.437280535697937\n",
      "Epoch 3008 | Train Loss: 1.2440701723098755 | Test Loss: 1.2567646503448486\n",
      "Epoch 3009 | Train Loss: 1.3902286291122437 | Test Loss: 1.43704354763031\n",
      "Epoch 3010 | Train Loss: 1.243864893913269 | Test Loss: 1.2565737962722778\n",
      "Epoch 3011 | Train Loss: 1.389968752861023 | Test Loss: 1.436812400817871\n",
      "Epoch 3012 | Train Loss: 1.2436614036560059 | Test Loss: 1.2563843727111816\n",
      "Epoch 3013 | Train Loss: 1.3897103071212769 | Test Loss: 1.4365735054016113\n",
      "Epoch 3014 | Train Loss: 1.243456482887268 | Test Loss: 1.256191611289978\n",
      "Epoch 3015 | Train Loss: 1.3894578218460083 | Test Loss: 1.43634033203125\n",
      "Epoch 3016 | Train Loss: 1.2432469129562378 | Test Loss: 1.2559958696365356\n",
      "Epoch 3017 | Train Loss: 1.3892064094543457 | Test Loss: 1.4361337423324585\n",
      "Epoch 3018 | Train Loss: 1.2430402040481567 | Test Loss: 1.255798101425171\n",
      "Epoch 3019 | Train Loss: 1.388952374458313 | Test Loss: 1.4359081983566284\n",
      "Epoch 3020 | Train Loss: 1.242834448814392 | Test Loss: 1.2555991411209106\n",
      "Epoch 3021 | Train Loss: 1.3887031078338623 | Test Loss: 1.4356759786605835\n",
      "Epoch 3022 | Train Loss: 1.2426220178604126 | Test Loss: 1.25540292263031\n",
      "Epoch 3023 | Train Loss: 1.3884564638137817 | Test Loss: 1.4354357719421387\n",
      "Epoch 3024 | Train Loss: 1.2424144744873047 | Test Loss: 1.2552146911621094\n",
      "Epoch 3025 | Train Loss: 1.3882046937942505 | Test Loss: 1.4352144002914429\n",
      "Epoch 3026 | Train Loss: 1.2422127723693848 | Test Loss: 1.2550288438796997\n",
      "Epoch 3027 | Train Loss: 1.3879505395889282 | Test Loss: 1.43498957157135\n",
      "Epoch 3028 | Train Loss: 1.242009162902832 | Test Loss: 1.254834771156311\n",
      "Epoch 3029 | Train Loss: 1.3877030611038208 | Test Loss: 1.434780478477478\n",
      "Epoch 3030 | Train Loss: 1.2418022155761719 | Test Loss: 1.254642367362976\n",
      "Epoch 3031 | Train Loss: 1.3874608278274536 | Test Loss: 1.434553861618042\n",
      "Epoch 3032 | Train Loss: 1.2415906190872192 | Test Loss: 1.2544516324996948\n",
      "Epoch 3033 | Train Loss: 1.3872178792953491 | Test Loss: 1.4343172311782837\n",
      "Epoch 3034 | Train Loss: 1.2413866519927979 | Test Loss: 1.2542669773101807\n",
      "Epoch 3035 | Train Loss: 1.386967658996582 | Test Loss: 1.4341166019439697\n",
      "Epoch 3036 | Train Loss: 1.241184949874878 | Test Loss: 1.2540748119354248\n",
      "Epoch 3037 | Train Loss: 1.386727213859558 | Test Loss: 1.4338644742965698\n",
      "Epoch 3038 | Train Loss: 1.2409725189208984 | Test Loss: 1.2538822889328003\n",
      "Epoch 3039 | Train Loss: 1.3864911794662476 | Test Loss: 1.4336458444595337\n",
      "Epoch 3040 | Train Loss: 1.2407666444778442 | Test Loss: 1.2536991834640503\n",
      "Epoch 3041 | Train Loss: 1.386245846748352 | Test Loss: 1.4334336519241333\n",
      "Epoch 3042 | Train Loss: 1.2405672073364258 | Test Loss: 1.2535133361816406\n",
      "Epoch 3043 | Train Loss: 1.386003851890564 | Test Loss: 1.4331998825073242\n",
      "Epoch 3044 | Train Loss: 1.2403621673583984 | Test Loss: 1.253322958946228\n",
      "Epoch 3045 | Train Loss: 1.3857640027999878 | Test Loss: 1.4329952001571655\n",
      "Epoch 3046 | Train Loss: 1.2401593923568726 | Test Loss: 1.2531344890594482\n",
      "Epoch 3047 | Train Loss: 1.3855234384536743 | Test Loss: 1.4327706098556519\n",
      "Epoch 3048 | Train Loss: 1.239957332611084 | Test Loss: 1.252949833869934\n",
      "Epoch 3049 | Train Loss: 1.3852838277816772 | Test Loss: 1.4325580596923828\n",
      "Epoch 3050 | Train Loss: 1.2397555112838745 | Test Loss: 1.2527624368667603\n",
      "Epoch 3051 | Train Loss: 1.3850454092025757 | Test Loss: 1.432348370552063\n",
      "Epoch 3052 | Train Loss: 1.2395529747009277 | Test Loss: 1.2525758743286133\n",
      "Epoch 3053 | Train Loss: 1.3848081827163696 | Test Loss: 1.4321269989013672\n",
      "Epoch 3054 | Train Loss: 1.239351749420166 | Test Loss: 1.2523950338363647\n",
      "Epoch 3055 | Train Loss: 1.3845710754394531 | Test Loss: 1.4319186210632324\n",
      "Epoch 3056 | Train Loss: 1.2391490936279297 | Test Loss: 1.2522116899490356\n",
      "Epoch 3057 | Train Loss: 1.3843337297439575 | Test Loss: 1.4316970109939575\n",
      "Epoch 3058 | Train Loss: 1.2389510869979858 | Test Loss: 1.2520270347595215\n",
      "Epoch 3059 | Train Loss: 1.3840936422348022 | Test Loss: 1.4314870834350586\n",
      "Epoch 3060 | Train Loss: 1.2387536764144897 | Test Loss: 1.2518441677093506\n",
      "Epoch 3061 | Train Loss: 1.3838567733764648 | Test Loss: 1.4312584400177002\n",
      "Epoch 3062 | Train Loss: 1.2385538816452026 | Test Loss: 1.2516634464263916\n",
      "Epoch 3063 | Train Loss: 1.3836222887039185 | Test Loss: 1.431050419807434\n",
      "Epoch 3064 | Train Loss: 1.2383553981781006 | Test Loss: 1.2514793872833252\n",
      "Epoch 3065 | Train Loss: 1.3833849430084229 | Test Loss: 1.430840253829956\n",
      "Epoch 3066 | Train Loss: 1.2381606101989746 | Test Loss: 1.2512929439544678\n",
      "Epoch 3067 | Train Loss: 1.3831496238708496 | Test Loss: 1.4306225776672363\n",
      "Epoch 3068 | Train Loss: 1.2379616498947144 | Test Loss: 1.2511122226715088\n",
      "Epoch 3069 | Train Loss: 1.3829141855239868 | Test Loss: 1.4303982257843018\n",
      "Epoch 3070 | Train Loss: 1.2377684116363525 | Test Loss: 1.2509353160858154\n",
      "Epoch 3071 | Train Loss: 1.3826732635498047 | Test Loss: 1.4301873445510864\n",
      "Epoch 3072 | Train Loss: 1.237579107284546 | Test Loss: 1.2507562637329102\n",
      "Epoch 3073 | Train Loss: 1.3824326992034912 | Test Loss: 1.4299710988998413\n",
      "Epoch 3074 | Train Loss: 1.2373863458633423 | Test Loss: 1.250566840171814\n",
      "Epoch 3075 | Train Loss: 1.382201910018921 | Test Loss: 1.4297699928283691\n",
      "Epoch 3076 | Train Loss: 1.2371854782104492 | Test Loss: 1.2503838539123535\n",
      "Epoch 3077 | Train Loss: 1.381970763206482 | Test Loss: 1.4295495748519897\n",
      "Epoch 3078 | Train Loss: 1.236992597579956 | Test Loss: 1.2502073049545288\n",
      "Epoch 3079 | Train Loss: 1.3817323446273804 | Test Loss: 1.429347038269043\n",
      "Epoch 3080 | Train Loss: 1.2368031740188599 | Test Loss: 1.2500265836715698\n",
      "Epoch 3081 | Train Loss: 1.3814985752105713 | Test Loss: 1.429127812385559\n",
      "Epoch 3082 | Train Loss: 1.2366077899932861 | Test Loss: 1.2498478889465332\n",
      "Epoch 3083 | Train Loss: 1.381271243095398 | Test Loss: 1.428912878036499\n",
      "Epoch 3084 | Train Loss: 1.2364133596420288 | Test Loss: 1.2496715784072876\n",
      "Epoch 3085 | Train Loss: 1.3810406923294067 | Test Loss: 1.4286918640136719\n",
      "Epoch 3086 | Train Loss: 1.2362233400344849 | Test Loss: 1.249493956565857\n",
      "Epoch 3087 | Train Loss: 1.3808091878890991 | Test Loss: 1.4284825325012207\n",
      "Epoch 3088 | Train Loss: 1.236031413078308 | Test Loss: 1.2493149042129517\n",
      "Epoch 3089 | Train Loss: 1.3805811405181885 | Test Loss: 1.4282594919204712\n",
      "Epoch 3090 | Train Loss: 1.235837697982788 | Test Loss: 1.249136209487915\n",
      "Epoch 3091 | Train Loss: 1.3803532123565674 | Test Loss: 1.4280462265014648\n",
      "Epoch 3092 | Train Loss: 1.2356475591659546 | Test Loss: 1.248957633972168\n",
      "Epoch 3093 | Train Loss: 1.380123257637024 | Test Loss: 1.4278303384780884\n",
      "Epoch 3094 | Train Loss: 1.2354543209075928 | Test Loss: 1.2487760782241821\n",
      "Epoch 3095 | Train Loss: 1.3799000978469849 | Test Loss: 1.4276210069656372\n",
      "Epoch 3096 | Train Loss: 1.2352601289749146 | Test Loss: 1.248597502708435\n",
      "Epoch 3097 | Train Loss: 1.3796743154525757 | Test Loss: 1.4274168014526367\n",
      "Epoch 3098 | Train Loss: 1.2350701093673706 | Test Loss: 1.248425006866455\n",
      "Epoch 3099 | Train Loss: 1.37944495677948 | Test Loss: 1.4272063970565796\n",
      "Epoch 3100 | Train Loss: 1.234883189201355 | Test Loss: 1.2482484579086304\n",
      "Epoch 3101 | Train Loss: 1.3792167901992798 | Test Loss: 1.4269930124282837\n",
      "Epoch 3102 | Train Loss: 1.2346957921981812 | Test Loss: 1.2480759620666504\n",
      "Epoch 3103 | Train Loss: 1.3789918422698975 | Test Loss: 1.4268029928207397\n",
      "Epoch 3104 | Train Loss: 1.234501838684082 | Test Loss: 1.2479009628295898\n",
      "Epoch 3105 | Train Loss: 1.3787682056427002 | Test Loss: 1.4265848398208618\n",
      "Epoch 3106 | Train Loss: 1.2343109846115112 | Test Loss: 1.2477184534072876\n",
      "Epoch 3107 | Train Loss: 1.3785457611083984 | Test Loss: 1.426390528678894\n",
      "Epoch 3108 | Train Loss: 1.2341207265853882 | Test Loss: 1.2475322484970093\n",
      "Epoch 3109 | Train Loss: 1.3783265352249146 | Test Loss: 1.4261796474456787\n",
      "Epoch 3110 | Train Loss: 1.2339258193969727 | Test Loss: 1.247361183166504\n",
      "Epoch 3111 | Train Loss: 1.3781057596206665 | Test Loss: 1.4259651899337769\n",
      "Epoch 3112 | Train Loss: 1.233738899230957 | Test Loss: 1.2471877336502075\n",
      "Epoch 3113 | Train Loss: 1.3778769969940186 | Test Loss: 1.4257770776748657\n",
      "Epoch 3114 | Train Loss: 1.2335546016693115 | Test Loss: 1.2470121383666992\n",
      "Epoch 3115 | Train Loss: 1.3776508569717407 | Test Loss: 1.425528645515442\n",
      "Epoch 3116 | Train Loss: 1.2333673238754272 | Test Loss: 1.2468326091766357\n",
      "Epoch 3117 | Train Loss: 1.3774313926696777 | Test Loss: 1.4253607988357544\n",
      "Epoch 3118 | Train Loss: 1.233176827430725 | Test Loss: 1.2466580867767334\n",
      "Epoch 3119 | Train Loss: 1.3772070407867432 | Test Loss: 1.425136923789978\n",
      "Epoch 3120 | Train Loss: 1.2329939603805542 | Test Loss: 1.24648916721344\n",
      "Epoch 3121 | Train Loss: 1.376979947090149 | Test Loss: 1.4249504804611206\n",
      "Epoch 3122 | Train Loss: 1.2328102588653564 | Test Loss: 1.2463090419769287\n",
      "Epoch 3123 | Train Loss: 1.3767586946487427 | Test Loss: 1.4247418642044067\n",
      "Epoch 3124 | Train Loss: 1.232618808746338 | Test Loss: 1.2461328506469727\n",
      "Epoch 3125 | Train Loss: 1.3765448331832886 | Test Loss: 1.4245389699935913\n",
      "Epoch 3126 | Train Loss: 1.2324270009994507 | Test Loss: 1.245955467224121\n",
      "Epoch 3127 | Train Loss: 1.37632417678833 | Test Loss: 1.4243570566177368\n",
      "Epoch 3128 | Train Loss: 1.2322440147399902 | Test Loss: 1.245783805847168\n",
      "Epoch 3129 | Train Loss: 1.3761035203933716 | Test Loss: 1.4241375923156738\n",
      "Epoch 3130 | Train Loss: 1.2320550680160522 | Test Loss: 1.2456055879592896\n",
      "Epoch 3131 | Train Loss: 1.3758864402770996 | Test Loss: 1.423936128616333\n",
      "Epoch 3132 | Train Loss: 1.2318682670593262 | Test Loss: 1.245431661605835\n",
      "Epoch 3133 | Train Loss: 1.3756680488586426 | Test Loss: 1.4237415790557861\n",
      "Epoch 3134 | Train Loss: 1.2316807508468628 | Test Loss: 1.2452555894851685\n",
      "Epoch 3135 | Train Loss: 1.375455379486084 | Test Loss: 1.4235538244247437\n",
      "Epoch 3136 | Train Loss: 1.2314872741699219 | Test Loss: 1.245079517364502\n",
      "Epoch 3137 | Train Loss: 1.3752427101135254 | Test Loss: 1.4233428239822388\n",
      "Epoch 3138 | Train Loss: 1.2313017845153809 | Test Loss: 1.2449086904525757\n",
      "Epoch 3139 | Train Loss: 1.3750228881835938 | Test Loss: 1.423168659210205\n",
      "Epoch 3140 | Train Loss: 1.2311196327209473 | Test Loss: 1.2447326183319092\n",
      "Epoch 3141 | Train Loss: 1.3748064041137695 | Test Loss: 1.4229605197906494\n",
      "Epoch 3142 | Train Loss: 1.2309321165084839 | Test Loss: 1.2445602416992188\n",
      "Epoch 3143 | Train Loss: 1.3745933771133423 | Test Loss: 1.422775387763977\n",
      "Epoch 3144 | Train Loss: 1.2307472229003906 | Test Loss: 1.2443914413452148\n",
      "Epoch 3145 | Train Loss: 1.3743746280670166 | Test Loss: 1.4225552082061768\n",
      "Epoch 3146 | Train Loss: 1.2305700778961182 | Test Loss: 1.2442312240600586\n",
      "Epoch 3147 | Train Loss: 1.3741518259048462 | Test Loss: 1.422363519668579\n",
      "Epoch 3148 | Train Loss: 1.2303920984268188 | Test Loss: 1.2440528869628906\n",
      "Epoch 3149 | Train Loss: 1.3739362955093384 | Test Loss: 1.4221731424331665\n",
      "Epoch 3150 | Train Loss: 1.2302048206329346 | Test Loss: 1.2438874244689941\n",
      "Epoch 3151 | Train Loss: 1.3737210035324097 | Test Loss: 1.4219632148742676\n",
      "Epoch 3152 | Train Loss: 1.2300275564193726 | Test Loss: 1.243725061416626\n",
      "Epoch 3153 | Train Loss: 1.3734956979751587 | Test Loss: 1.421785831451416\n",
      "Epoch 3154 | Train Loss: 1.2298508882522583 | Test Loss: 1.2435579299926758\n",
      "Epoch 3155 | Train Loss: 1.3732802867889404 | Test Loss: 1.4215575456619263\n",
      "Epoch 3156 | Train Loss: 1.2296658754348755 | Test Loss: 1.2433791160583496\n",
      "Epoch 3157 | Train Loss: 1.3730772733688354 | Test Loss: 1.4213922023773193\n",
      "Epoch 3158 | Train Loss: 1.229473352432251 | Test Loss: 1.2432078123092651\n",
      "Epoch 3159 | Train Loss: 1.3728675842285156 | Test Loss: 1.4211653470993042\n",
      "Epoch 3160 | Train Loss: 1.2292968034744263 | Test Loss: 1.243053674697876\n",
      "Epoch 3161 | Train Loss: 1.3726439476013184 | Test Loss: 1.420976996421814\n",
      "Epoch 3162 | Train Loss: 1.2291220426559448 | Test Loss: 1.242882251739502\n",
      "Epoch 3163 | Train Loss: 1.3724325895309448 | Test Loss: 1.4207768440246582\n",
      "Epoch 3164 | Train Loss: 1.2289315462112427 | Test Loss: 1.2427027225494385\n",
      "Epoch 3165 | Train Loss: 1.3722360134124756 | Test Loss: 1.4206113815307617\n",
      "Epoch 3166 | Train Loss: 1.2287380695343018 | Test Loss: 1.242526650428772\n",
      "Epoch 3167 | Train Loss: 1.372029185295105 | Test Loss: 1.420418620109558\n",
      "Epoch 3168 | Train Loss: 1.2285572290420532 | Test Loss: 1.242365837097168\n",
      "Epoch 3169 | Train Loss: 1.3718124628067017 | Test Loss: 1.4201991558074951\n",
      "Epoch 3170 | Train Loss: 1.2283796072006226 | Test Loss: 1.242194414138794\n",
      "Epoch 3171 | Train Loss: 1.371605396270752 | Test Loss: 1.42001211643219\n",
      "Epoch 3172 | Train Loss: 1.2281899452209473 | Test Loss: 1.2420259714126587\n",
      "Epoch 3173 | Train Loss: 1.3713984489440918 | Test Loss: 1.4198012351989746\n",
      "Epoch 3174 | Train Loss: 1.2280089855194092 | Test Loss: 1.2418535947799683\n",
      "Epoch 3175 | Train Loss: 1.371182918548584 | Test Loss: 1.4196181297302246\n",
      "Epoch 3176 | Train Loss: 1.2278348207473755 | Test Loss: 1.241685390472412\n",
      "Epoch 3177 | Train Loss: 1.370965838432312 | Test Loss: 1.419377088546753\n",
      "Epoch 3178 | Train Loss: 1.2276575565338135 | Test Loss: 1.2415205240249634\n",
      "Epoch 3179 | Train Loss: 1.3707584142684937 | Test Loss: 1.4192075729370117\n",
      "Epoch 3180 | Train Loss: 1.2274707555770874 | Test Loss: 1.2413452863693237\n",
      "Epoch 3181 | Train Loss: 1.3705568313598633 | Test Loss: 1.4190006256103516\n",
      "Epoch 3182 | Train Loss: 1.2272847890853882 | Test Loss: 1.2411779165267944\n",
      "Epoch 3183 | Train Loss: 1.3703529834747314 | Test Loss: 1.418851375579834\n",
      "Epoch 3184 | Train Loss: 1.2271052598953247 | Test Loss: 1.2410032749176025\n",
      "Epoch 3185 | Train Loss: 1.3701409101486206 | Test Loss: 1.4186310768127441\n",
      "Epoch 3186 | Train Loss: 1.2269285917282104 | Test Loss: 1.2408387660980225\n",
      "Epoch 3187 | Train Loss: 1.3699337244033813 | Test Loss: 1.4184211492538452\n",
      "Epoch 3188 | Train Loss: 1.226742148399353 | Test Loss: 1.24067223072052\n",
      "Epoch 3189 | Train Loss: 1.3697277307510376 | Test Loss: 1.4182469844818115\n",
      "Epoch 3190 | Train Loss: 1.2265639305114746 | Test Loss: 1.2405095100402832\n",
      "Epoch 3191 | Train Loss: 1.3695143461227417 | Test Loss: 1.4180303812026978\n",
      "Epoch 3192 | Train Loss: 1.2263880968093872 | Test Loss: 1.2403353452682495\n",
      "Epoch 3193 | Train Loss: 1.3693073987960815 | Test Loss: 1.4178688526153564\n",
      "Epoch 3194 | Train Loss: 1.226204514503479 | Test Loss: 1.2401667833328247\n",
      "Epoch 3195 | Train Loss: 1.3691051006317139 | Test Loss: 1.4176363945007324\n",
      "Epoch 3196 | Train Loss: 1.2260228395462036 | Test Loss: 1.240014672279358\n",
      "Epoch 3197 | Train Loss: 1.3688987493515015 | Test Loss: 1.4174723625183105\n",
      "Epoch 3198 | Train Loss: 1.2258473634719849 | Test Loss: 1.2398468255996704\n",
      "Epoch 3199 | Train Loss: 1.3686931133270264 | Test Loss: 1.4172711372375488\n",
      "Epoch 3200 | Train Loss: 1.2256649732589722 | Test Loss: 1.2396732568740845\n",
      "Epoch 3201 | Train Loss: 1.368491291999817 | Test Loss: 1.4170987606048584\n",
      "Epoch 3202 | Train Loss: 1.2254823446273804 | Test Loss: 1.239504098892212\n",
      "Epoch 3203 | Train Loss: 1.3682836294174194 | Test Loss: 1.4169069528579712\n",
      "Epoch 3204 | Train Loss: 1.2253080606460571 | Test Loss: 1.2393532991409302\n",
      "Epoch 3205 | Train Loss: 1.3680751323699951 | Test Loss: 1.4167269468307495\n",
      "Epoch 3206 | Train Loss: 1.2251338958740234 | Test Loss: 1.2391833066940308\n",
      "Epoch 3207 | Train Loss: 1.3678719997406006 | Test Loss: 1.4165390729904175\n",
      "Epoch 3208 | Train Loss: 1.2249516248703003 | Test Loss: 1.2390202283859253\n",
      "Epoch 3209 | Train Loss: 1.3676732778549194 | Test Loss: 1.4163262844085693\n",
      "Epoch 3210 | Train Loss: 1.2247724533081055 | Test Loss: 1.2388595342636108\n",
      "Epoch 3211 | Train Loss: 1.367468237876892 | Test Loss: 1.4161489009857178\n",
      "Epoch 3212 | Train Loss: 1.2245970964431763 | Test Loss: 1.238703966140747\n",
      "Epoch 3213 | Train Loss: 1.367262840270996 | Test Loss: 1.415950059890747\n",
      "Epoch 3214 | Train Loss: 1.2244236469268799 | Test Loss: 1.238538384437561\n",
      "Epoch 3215 | Train Loss: 1.3670613765716553 | Test Loss: 1.4157578945159912\n",
      "Epoch 3216 | Train Loss: 1.224246621131897 | Test Loss: 1.2383835315704346\n",
      "Epoch 3217 | Train Loss: 1.3668553829193115 | Test Loss: 1.415563941001892\n",
      "Epoch 3218 | Train Loss: 1.2240763902664185 | Test Loss: 1.2382216453552246\n",
      "Epoch 3219 | Train Loss: 1.3666465282440186 | Test Loss: 1.415374755859375\n",
      "Epoch 3220 | Train Loss: 1.2239032983779907 | Test Loss: 1.2380563020706177\n",
      "Epoch 3221 | Train Loss: 1.3664454221725464 | Test Loss: 1.4151860475540161\n",
      "Epoch 3222 | Train Loss: 1.2237237691879272 | Test Loss: 1.2378935813903809\n",
      "Epoch 3223 | Train Loss: 1.3662490844726562 | Test Loss: 1.414993405342102\n",
      "Epoch 3224 | Train Loss: 1.2235487699508667 | Test Loss: 1.2377467155456543\n",
      "Epoch 3225 | Train Loss: 1.3660424947738647 | Test Loss: 1.4148122072219849\n",
      "Epoch 3226 | Train Loss: 1.2233814001083374 | Test Loss: 1.2375808954238892\n",
      "Epoch 3227 | Train Loss: 1.3658348321914673 | Test Loss: 1.4146333932876587\n",
      "Epoch 3228 | Train Loss: 1.223211407661438 | Test Loss: 1.2374144792556763\n",
      "Epoch 3229 | Train Loss: 1.3656364679336548 | Test Loss: 1.4144325256347656\n",
      "Epoch 3230 | Train Loss: 1.223031759262085 | Test Loss: 1.2372641563415527\n",
      "Epoch 3231 | Train Loss: 1.3654366731643677 | Test Loss: 1.4142512083053589\n",
      "Epoch 3232 | Train Loss: 1.2228608131408691 | Test Loss: 1.2371087074279785\n",
      "Epoch 3233 | Train Loss: 1.365230917930603 | Test Loss: 1.4140589237213135\n",
      "Epoch 3234 | Train Loss: 1.2226909399032593 | Test Loss: 1.2369425296783447\n",
      "Epoch 3235 | Train Loss: 1.3650344610214233 | Test Loss: 1.4138751029968262\n",
      "Epoch 3236 | Train Loss: 1.2225102186203003 | Test Loss: 1.2367794513702393\n",
      "Epoch 3237 | Train Loss: 1.3648401498794556 | Test Loss: 1.413701057434082\n",
      "Epoch 3238 | Train Loss: 1.2223358154296875 | Test Loss: 1.2366247177124023\n",
      "Epoch 3239 | Train Loss: 1.364640474319458 | Test Loss: 1.4135091304779053\n",
      "Epoch 3240 | Train Loss: 1.2221639156341553 | Test Loss: 1.2364712953567505\n",
      "Epoch 3241 | Train Loss: 1.3644410371780396 | Test Loss: 1.4133086204528809\n",
      "Epoch 3242 | Train Loss: 1.221989631652832 | Test Loss: 1.2363160848617554\n",
      "Epoch 3243 | Train Loss: 1.3642469644546509 | Test Loss: 1.4131345748901367\n",
      "Epoch 3244 | Train Loss: 1.2218106985092163 | Test Loss: 1.2361584901809692\n",
      "Epoch 3245 | Train Loss: 1.3640539646148682 | Test Loss: 1.4129492044448853\n",
      "Epoch 3246 | Train Loss: 1.2216358184814453 | Test Loss: 1.2360050678253174\n",
      "Epoch 3247 | Train Loss: 1.3638554811477661 | Test Loss: 1.4127739667892456\n",
      "Epoch 3248 | Train Loss: 1.2214635610580444 | Test Loss: 1.2358418703079224\n",
      "Epoch 3249 | Train Loss: 1.3636642694473267 | Test Loss: 1.4125818014144897\n",
      "Epoch 3250 | Train Loss: 1.221283197402954 | Test Loss: 1.2356890439987183\n",
      "Epoch 3251 | Train Loss: 1.363471269607544 | Test Loss: 1.4124033451080322\n",
      "Epoch 3252 | Train Loss: 1.2211092710494995 | Test Loss: 1.235527753829956\n",
      "Epoch 3253 | Train Loss: 1.363274097442627 | Test Loss: 1.4122207164764404\n",
      "Epoch 3254 | Train Loss: 1.220937967300415 | Test Loss: 1.2353737354278564\n",
      "Epoch 3255 | Train Loss: 1.363075852394104 | Test Loss: 1.4120194911956787\n",
      "Epoch 3256 | Train Loss: 1.2207649946212769 | Test Loss: 1.235214352607727\n",
      "Epoch 3257 | Train Loss: 1.3628824949264526 | Test Loss: 1.4118430614471436\n",
      "Epoch 3258 | Train Loss: 1.2205902338027954 | Test Loss: 1.2350598573684692\n",
      "Epoch 3259 | Train Loss: 1.362688422203064 | Test Loss: 1.4116612672805786\n",
      "Epoch 3260 | Train Loss: 1.2204185724258423 | Test Loss: 1.2348986864089966\n",
      "Epoch 3261 | Train Loss: 1.3624954223632812 | Test Loss: 1.4114800691604614\n",
      "Epoch 3262 | Train Loss: 1.2202417850494385 | Test Loss: 1.2347420454025269\n",
      "Epoch 3263 | Train Loss: 1.3623030185699463 | Test Loss: 1.41130793094635\n",
      "Epoch 3264 | Train Loss: 1.2200684547424316 | Test Loss: 1.234583854675293\n",
      "Epoch 3265 | Train Loss: 1.3621106147766113 | Test Loss: 1.4111266136169434\n",
      "Epoch 3266 | Train Loss: 1.219892978668213 | Test Loss: 1.2344287633895874\n",
      "Epoch 3267 | Train Loss: 1.3619197607040405 | Test Loss: 1.4109402894973755\n",
      "Epoch 3268 | Train Loss: 1.2197164297103882 | Test Loss: 1.234264612197876\n",
      "Epoch 3269 | Train Loss: 1.3617303371429443 | Test Loss: 1.4107648134231567\n",
      "Epoch 3270 | Train Loss: 1.2195403575897217 | Test Loss: 1.2341078519821167\n",
      "Epoch 3271 | Train Loss: 1.3615405559539795 | Test Loss: 1.410589575767517\n",
      "Epoch 3272 | Train Loss: 1.2193642854690552 | Test Loss: 1.233948826789856\n",
      "Epoch 3273 | Train Loss: 1.361350178718567 | Test Loss: 1.4104278087615967\n",
      "Epoch 3274 | Train Loss: 1.219188928604126 | Test Loss: 1.2337853908538818\n",
      "Epoch 3275 | Train Loss: 1.3611646890640259 | Test Loss: 1.4102495908737183\n",
      "Epoch 3276 | Train Loss: 1.2190083265304565 | Test Loss: 1.2336291074752808\n",
      "Epoch 3277 | Train Loss: 1.3609790802001953 | Test Loss: 1.4100948572158813\n",
      "Epoch 3278 | Train Loss: 1.2188352346420288 | Test Loss: 1.2334686517715454\n",
      "Epoch 3279 | Train Loss: 1.3607863187789917 | Test Loss: 1.4098931550979614\n",
      "Epoch 3280 | Train Loss: 1.2186636924743652 | Test Loss: 1.2333112955093384\n",
      "Epoch 3281 | Train Loss: 1.360597014427185 | Test Loss: 1.4097208976745605\n",
      "Epoch 3282 | Train Loss: 1.2184869050979614 | Test Loss: 1.2331434488296509\n",
      "Epoch 3283 | Train Loss: 1.3604164123535156 | Test Loss: 1.4095581769943237\n",
      "Epoch 3284 | Train Loss: 1.2183053493499756 | Test Loss: 1.2329903841018677\n",
      "Epoch 3285 | Train Loss: 1.3602324724197388 | Test Loss: 1.4093714952468872\n",
      "Epoch 3286 | Train Loss: 1.2181320190429688 | Test Loss: 1.2328258752822876\n",
      "Epoch 3287 | Train Loss: 1.3600424528121948 | Test Loss: 1.4092000722885132\n",
      "Epoch 3288 | Train Loss: 1.2179584503173828 | Test Loss: 1.2326639890670776\n",
      "Epoch 3289 | Train Loss: 1.3598569631576538 | Test Loss: 1.4090163707733154\n",
      "Epoch 3290 | Train Loss: 1.2177834510803223 | Test Loss: 1.2325090169906616\n",
      "Epoch 3291 | Train Loss: 1.3596727848052979 | Test Loss: 1.4088640213012695\n",
      "Epoch 3292 | Train Loss: 1.2176092863082886 | Test Loss: 1.232359766960144\n",
      "Epoch 3293 | Train Loss: 1.3594882488250732 | Test Loss: 1.4086710214614868\n",
      "Epoch 3294 | Train Loss: 1.2174346446990967 | Test Loss: 1.2322015762329102\n",
      "Epoch 3295 | Train Loss: 1.3593043088912964 | Test Loss: 1.4085005521774292\n",
      "Epoch 3296 | Train Loss: 1.2172640562057495 | Test Loss: 1.2320395708084106\n",
      "Epoch 3297 | Train Loss: 1.3591207265853882 | Test Loss: 1.4083144664764404\n",
      "Epoch 3298 | Train Loss: 1.217089056968689 | Test Loss: 1.2318925857543945\n",
      "Epoch 3299 | Train Loss: 1.358935832977295 | Test Loss: 1.4081569910049438\n",
      "Epoch 3300 | Train Loss: 1.2169214487075806 | Test Loss: 1.2317322492599487\n",
      "Epoch 3301 | Train Loss: 1.358744502067566 | Test Loss: 1.4079610109329224\n",
      "Epoch 3302 | Train Loss: 1.2167552709579468 | Test Loss: 1.2315748929977417\n",
      "Epoch 3303 | Train Loss: 1.3585609197616577 | Test Loss: 1.4077942371368408\n",
      "Epoch 3304 | Train Loss: 1.2165777683258057 | Test Loss: 1.2314128875732422\n",
      "Epoch 3305 | Train Loss: 1.3583797216415405 | Test Loss: 1.407625675201416\n",
      "Epoch 3306 | Train Loss: 1.2164061069488525 | Test Loss: 1.231256365776062\n",
      "Epoch 3307 | Train Loss: 1.358193278312683 | Test Loss: 1.4074434041976929\n",
      "Epoch 3308 | Train Loss: 1.2162375450134277 | Test Loss: 1.2310973405838013\n",
      "Epoch 3309 | Train Loss: 1.358008623123169 | Test Loss: 1.4072847366333008\n",
      "Epoch 3310 | Train Loss: 1.216063380241394 | Test Loss: 1.2309376001358032\n",
      "Epoch 3311 | Train Loss: 1.3578298091888428 | Test Loss: 1.4071100950241089\n",
      "Epoch 3312 | Train Loss: 1.215890645980835 | Test Loss: 1.2307822704315186\n",
      "Epoch 3313 | Train Loss: 1.3576457500457764 | Test Loss: 1.4069162607192993\n",
      "Epoch 3314 | Train Loss: 1.2157251834869385 | Test Loss: 1.2306435108184814\n",
      "Epoch 3315 | Train Loss: 1.3574573993682861 | Test Loss: 1.4067543745040894\n",
      "Epoch 3316 | Train Loss: 1.2155569791793823 | Test Loss: 1.2304774522781372\n",
      "Epoch 3317 | Train Loss: 1.3572793006896973 | Test Loss: 1.4065715074539185\n",
      "Epoch 3318 | Train Loss: 1.2153818607330322 | Test Loss: 1.2303169965744019\n",
      "Epoch 3319 | Train Loss: 1.357100009918213 | Test Loss: 1.406408429145813\n",
      "Epoch 3320 | Train Loss: 1.215215802192688 | Test Loss: 1.2301623821258545\n",
      "Epoch 3321 | Train Loss: 1.3569109439849854 | Test Loss: 1.406219720840454\n",
      "Epoch 3322 | Train Loss: 1.2150503396987915 | Test Loss: 1.2300090789794922\n",
      "Epoch 3323 | Train Loss: 1.3567336797714233 | Test Loss: 1.4060529470443726\n",
      "Epoch 3324 | Train Loss: 1.2148741483688354 | Test Loss: 1.2298388481140137\n",
      "Epoch 3325 | Train Loss: 1.3565595149993896 | Test Loss: 1.405888319015503\n",
      "Epoch 3326 | Train Loss: 1.2147018909454346 | Test Loss: 1.2296892404556274\n",
      "Epoch 3327 | Train Loss: 1.3563790321350098 | Test Loss: 1.4057124853134155\n",
      "Epoch 3328 | Train Loss: 1.2145354747772217 | Test Loss: 1.2295373678207397\n",
      "Epoch 3329 | Train Loss: 1.3561946153640747 | Test Loss: 1.40555739402771\n",
      "Epoch 3330 | Train Loss: 1.2143685817718506 | Test Loss: 1.2293776273727417\n",
      "Epoch 3331 | Train Loss: 1.356016755104065 | Test Loss: 1.4053854942321777\n",
      "Epoch 3332 | Train Loss: 1.214198350906372 | Test Loss: 1.229217767715454\n",
      "Epoch 3333 | Train Loss: 1.3558380603790283 | Test Loss: 1.4052174091339111\n",
      "Epoch 3334 | Train Loss: 1.21403169631958 | Test Loss: 1.2290675640106201\n",
      "Epoch 3335 | Train Loss: 1.3556574583053589 | Test Loss: 1.4050501585006714\n",
      "Epoch 3336 | Train Loss: 1.2138636112213135 | Test Loss: 1.228904366493225\n",
      "Epoch 3337 | Train Loss: 1.3554868698120117 | Test Loss: 1.4049071073532104\n",
      "Epoch 3338 | Train Loss: 1.2136867046356201 | Test Loss: 1.2287473678588867\n",
      "Epoch 3339 | Train Loss: 1.3553136587142944 | Test Loss: 1.404727816581726\n",
      "Epoch 3340 | Train Loss: 1.213521957397461 | Test Loss: 1.2285958528518677\n",
      "Epoch 3341 | Train Loss: 1.355128526687622 | Test Loss: 1.4045602083206177\n",
      "Epoch 3342 | Train Loss: 1.2133597135543823 | Test Loss: 1.2284321784973145\n",
      "Epoch 3343 | Train Loss: 1.3549529314041138 | Test Loss: 1.4043954610824585\n",
      "Epoch 3344 | Train Loss: 1.2131861448287964 | Test Loss: 1.2282747030258179\n",
      "Epoch 3345 | Train Loss: 1.3547859191894531 | Test Loss: 1.4042404890060425\n",
      "Epoch 3346 | Train Loss: 1.213010549545288 | Test Loss: 1.2281179428100586\n",
      "Epoch 3347 | Train Loss: 1.3546149730682373 | Test Loss: 1.4040509462356567\n",
      "Epoch 3348 | Train Loss: 1.2128419876098633 | Test Loss: 1.2279598712921143\n",
      "Epoch 3349 | Train Loss: 1.3544381856918335 | Test Loss: 1.4038984775543213\n",
      "Epoch 3350 | Train Loss: 1.2126784324645996 | Test Loss: 1.2278069257736206\n",
      "Epoch 3351 | Train Loss: 1.3542606830596924 | Test Loss: 1.4037270545959473\n",
      "Epoch 3352 | Train Loss: 1.2125099897384644 | Test Loss: 1.2276636362075806\n",
      "Epoch 3353 | Train Loss: 1.354088306427002 | Test Loss: 1.4035457372665405\n",
      "Epoch 3354 | Train Loss: 1.2123404741287231 | Test Loss: 1.2274991273880005\n",
      "Epoch 3355 | Train Loss: 1.3539129495620728 | Test Loss: 1.4033660888671875\n",
      "Epoch 3356 | Train Loss: 1.2121769189834595 | Test Loss: 1.2273480892181396\n",
      "Epoch 3357 | Train Loss: 1.3537379503250122 | Test Loss: 1.4032093286514282\n",
      "Epoch 3358 | Train Loss: 1.2120060920715332 | Test Loss: 1.227199673652649\n",
      "Epoch 3359 | Train Loss: 1.353564977645874 | Test Loss: 1.4030386209487915\n",
      "Epoch 3360 | Train Loss: 1.2118395566940308 | Test Loss: 1.2270393371582031\n",
      "Epoch 3361 | Train Loss: 1.3533915281295776 | Test Loss: 1.4028741121292114\n",
      "Epoch 3362 | Train Loss: 1.2116725444793701 | Test Loss: 1.226881980895996\n",
      "Epoch 3363 | Train Loss: 1.353219985961914 | Test Loss: 1.4027019739151\n",
      "Epoch 3364 | Train Loss: 1.2115036249160767 | Test Loss: 1.2267324924468994\n",
      "Epoch 3365 | Train Loss: 1.3530514240264893 | Test Loss: 1.4025461673736572\n",
      "Epoch 3366 | Train Loss: 1.2113345861434937 | Test Loss: 1.2265790700912476\n",
      "Epoch 3367 | Train Loss: 1.3528798818588257 | Test Loss: 1.4023562669754028\n",
      "Epoch 3368 | Train Loss: 1.211168646812439 | Test Loss: 1.226426124572754\n",
      "Epoch 3369 | Train Loss: 1.3527065515518188 | Test Loss: 1.4022020101547241\n",
      "Epoch 3370 | Train Loss: 1.2110040187835693 | Test Loss: 1.22626793384552\n",
      "Epoch 3371 | Train Loss: 1.3525347709655762 | Test Loss: 1.40204656124115\n",
      "Epoch 3372 | Train Loss: 1.2108345031738281 | Test Loss: 1.226110816001892\n",
      "Epoch 3373 | Train Loss: 1.3523709774017334 | Test Loss: 1.4018967151641846\n",
      "Epoch 3374 | Train Loss: 1.2106599807739258 | Test Loss: 1.2259420156478882\n",
      "Epoch 3375 | Train Loss: 1.3522075414657593 | Test Loss: 1.4017066955566406\n",
      "Epoch 3376 | Train Loss: 1.2104920148849487 | Test Loss: 1.225798487663269\n",
      "Epoch 3377 | Train Loss: 1.3520344495773315 | Test Loss: 1.4015685319900513\n",
      "Epoch 3378 | Train Loss: 1.2103317975997925 | Test Loss: 1.225642442703247\n",
      "Epoch 3379 | Train Loss: 1.3518584966659546 | Test Loss: 1.4013924598693848\n",
      "Epoch 3380 | Train Loss: 1.2101683616638184 | Test Loss: 1.2254884243011475\n",
      "Epoch 3381 | Train Loss: 1.3516899347305298 | Test Loss: 1.4012455940246582\n",
      "Epoch 3382 | Train Loss: 1.2100019454956055 | Test Loss: 1.2253273725509644\n",
      "Epoch 3383 | Train Loss: 1.3515225648880005 | Test Loss: 1.4010624885559082\n",
      "Epoch 3384 | Train Loss: 1.209835410118103 | Test Loss: 1.2251850366592407\n",
      "Epoch 3385 | Train Loss: 1.3513555526733398 | Test Loss: 1.4009186029434204\n",
      "Epoch 3386 | Train Loss: 1.2096720933914185 | Test Loss: 1.225027084350586\n",
      "Epoch 3387 | Train Loss: 1.351183295249939 | Test Loss: 1.4007656574249268\n",
      "Epoch 3388 | Train Loss: 1.2095098495483398 | Test Loss: 1.2248849868774414\n",
      "Epoch 3389 | Train Loss: 1.3510149717330933 | Test Loss: 1.4005855321884155\n",
      "Epoch 3390 | Train Loss: 1.2093466520309448 | Test Loss: 1.2247267961502075\n",
      "Epoch 3391 | Train Loss: 1.3508434295654297 | Test Loss: 1.400449275970459\n",
      "Epoch 3392 | Train Loss: 1.2091856002807617 | Test Loss: 1.2245739698410034\n",
      "Epoch 3393 | Train Loss: 1.3506728410720825 | Test Loss: 1.400273323059082\n",
      "Epoch 3394 | Train Loss: 1.2090253829956055 | Test Loss: 1.2244181632995605\n",
      "Epoch 3395 | Train Loss: 1.3505061864852905 | Test Loss: 1.400128722190857\n",
      "Epoch 3396 | Train Loss: 1.2088590860366821 | Test Loss: 1.2242648601531982\n",
      "Epoch 3397 | Train Loss: 1.350339651107788 | Test Loss: 1.3999472856521606\n",
      "Epoch 3398 | Train Loss: 1.2086989879608154 | Test Loss: 1.2241159677505493\n",
      "Epoch 3399 | Train Loss: 1.3501710891723633 | Test Loss: 1.3998106718063354\n",
      "Epoch 3400 | Train Loss: 1.2085397243499756 | Test Loss: 1.223958134651184\n",
      "Epoch 3401 | Train Loss: 1.3500014543533325 | Test Loss: 1.3996427059173584\n",
      "Epoch 3402 | Train Loss: 1.2083802223205566 | Test Loss: 1.2238044738769531\n",
      "Epoch 3403 | Train Loss: 1.3498386144638062 | Test Loss: 1.3994868993759155\n",
      "Epoch 3404 | Train Loss: 1.208215594291687 | Test Loss: 1.2236591577529907\n",
      "Epoch 3405 | Train Loss: 1.3496699333190918 | Test Loss: 1.3993256092071533\n",
      "Epoch 3406 | Train Loss: 1.2080589532852173 | Test Loss: 1.223507046699524\n",
      "Epoch 3407 | Train Loss: 1.3494987487792969 | Test Loss: 1.3991798162460327\n",
      "Epoch 3408 | Train Loss: 1.2079002857208252 | Test Loss: 1.223348617553711\n",
      "Epoch 3409 | Train Loss: 1.349335789680481 | Test Loss: 1.3990247249603271\n",
      "Epoch 3410 | Train Loss: 1.2077348232269287 | Test Loss: 1.223204493522644\n",
      "Epoch 3411 | Train Loss: 1.3491716384887695 | Test Loss: 1.398869276046753\n",
      "Epoch 3412 | Train Loss: 1.2075738906860352 | Test Loss: 1.2230544090270996\n",
      "Epoch 3413 | Train Loss: 1.3490031957626343 | Test Loss: 1.3987082242965698\n",
      "Epoch 3414 | Train Loss: 1.2074158191680908 | Test Loss: 1.2229045629501343\n",
      "Epoch 3415 | Train Loss: 1.3488365411758423 | Test Loss: 1.398551344871521\n",
      "Epoch 3416 | Train Loss: 1.2072539329528809 | Test Loss: 1.222751259803772\n",
      "Epoch 3417 | Train Loss: 1.3486733436584473 | Test Loss: 1.3983793258666992\n",
      "Epoch 3418 | Train Loss: 1.2070910930633545 | Test Loss: 1.2225990295410156\n",
      "Epoch 3419 | Train Loss: 1.3485089540481567 | Test Loss: 1.3982452154159546\n",
      "Epoch 3420 | Train Loss: 1.2069324254989624 | Test Loss: 1.2224477529525757\n",
      "Epoch 3421 | Train Loss: 1.3483420610427856 | Test Loss: 1.3980605602264404\n",
      "Epoch 3422 | Train Loss: 1.206773042678833 | Test Loss: 1.2223010063171387\n",
      "Epoch 3423 | Train Loss: 1.3481801748275757 | Test Loss: 1.3979225158691406\n",
      "Epoch 3424 | Train Loss: 1.2066097259521484 | Test Loss: 1.2221413850784302\n",
      "Epoch 3425 | Train Loss: 1.348017692565918 | Test Loss: 1.3977243900299072\n",
      "Epoch 3426 | Train Loss: 1.2064553499221802 | Test Loss: 1.222008466720581\n",
      "Epoch 3427 | Train Loss: 1.347851276397705 | Test Loss: 1.3976093530654907\n",
      "Epoch 3428 | Train Loss: 1.206299066543579 | Test Loss: 1.2218496799468994\n",
      "Epoch 3429 | Train Loss: 1.347684383392334 | Test Loss: 1.3974071741104126\n",
      "Epoch 3430 | Train Loss: 1.2061432600021362 | Test Loss: 1.2217047214508057\n",
      "Epoch 3431 | Train Loss: 1.3475220203399658 | Test Loss: 1.3972975015640259\n",
      "Epoch 3432 | Train Loss: 1.2059825658798218 | Test Loss: 1.2215381860733032\n",
      "Epoch 3433 | Train Loss: 1.3473645448684692 | Test Loss: 1.3971176147460938\n",
      "Epoch 3434 | Train Loss: 1.2058191299438477 | Test Loss: 1.2214059829711914\n",
      "Epoch 3435 | Train Loss: 1.3472028970718384 | Test Loss: 1.3969767093658447\n",
      "Epoch 3436 | Train Loss: 1.205660104751587 | Test Loss: 1.2212450504302979\n",
      "Epoch 3437 | Train Loss: 1.347033977508545 | Test Loss: 1.3968414068222046\n",
      "Epoch 3438 | Train Loss: 1.205506443977356 | Test Loss: 1.221107840538025\n",
      "Epoch 3439 | Train Loss: 1.3468663692474365 | Test Loss: 1.3966612815856934\n",
      "Epoch 3440 | Train Loss: 1.2053433656692505 | Test Loss: 1.220942497253418\n",
      "Epoch 3441 | Train Loss: 1.3467049598693848 | Test Loss: 1.3965306282043457\n",
      "Epoch 3442 | Train Loss: 1.2051842212677002 | Test Loss: 1.220795750617981\n",
      "Epoch 3443 | Train Loss: 1.3465397357940674 | Test Loss: 1.3963431119918823\n",
      "Epoch 3444 | Train Loss: 1.2050278186798096 | Test Loss: 1.2206542491912842\n",
      "Epoch 3445 | Train Loss: 1.3463746309280396 | Test Loss: 1.396222472190857\n",
      "Epoch 3446 | Train Loss: 1.2048698663711548 | Test Loss: 1.220494031906128\n",
      "Epoch 3447 | Train Loss: 1.3462121486663818 | Test Loss: 1.3960462808609009\n",
      "Epoch 3448 | Train Loss: 1.2047113180160522 | Test Loss: 1.2203450202941895\n",
      "Epoch 3449 | Train Loss: 1.3460544347763062 | Test Loss: 1.3959136009216309\n",
      "Epoch 3450 | Train Loss: 1.2045477628707886 | Test Loss: 1.2202026844024658\n",
      "Epoch 3451 | Train Loss: 1.345893144607544 | Test Loss: 1.3957290649414062\n",
      "Epoch 3452 | Train Loss: 1.2043921947479248 | Test Loss: 1.2200493812561035\n",
      "Epoch 3453 | Train Loss: 1.345730185508728 | Test Loss: 1.3956167697906494\n",
      "Epoch 3454 | Train Loss: 1.2042332887649536 | Test Loss: 1.2198896408081055\n",
      "Epoch 3455 | Train Loss: 1.3455702066421509 | Test Loss: 1.3954401016235352\n",
      "Epoch 3456 | Train Loss: 1.2040766477584839 | Test Loss: 1.2197582721710205\n",
      "Epoch 3457 | Train Loss: 1.3454089164733887 | Test Loss: 1.3953070640563965\n",
      "Epoch 3458 | Train Loss: 1.2039198875427246 | Test Loss: 1.2196015119552612\n",
      "Epoch 3459 | Train Loss: 1.345244288444519 | Test Loss: 1.3951445817947388\n",
      "Epoch 3460 | Train Loss: 1.2037681341171265 | Test Loss: 1.2194596529006958\n",
      "Epoch 3461 | Train Loss: 1.3450782299041748 | Test Loss: 1.3949981927871704\n",
      "Epoch 3462 | Train Loss: 1.2036147117614746 | Test Loss: 1.2193036079406738\n",
      "Epoch 3463 | Train Loss: 1.344914197921753 | Test Loss: 1.3948544263839722\n",
      "Epoch 3464 | Train Loss: 1.2034565210342407 | Test Loss: 1.2191579341888428\n",
      "Epoch 3465 | Train Loss: 1.344757080078125 | Test Loss: 1.3946948051452637\n",
      "Epoch 3466 | Train Loss: 1.203291654586792 | Test Loss: 1.2189973592758179\n",
      "Epoch 3467 | Train Loss: 1.344599723815918 | Test Loss: 1.3945673704147339\n",
      "Epoch 3468 | Train Loss: 1.2031354904174805 | Test Loss: 1.2188552618026733\n",
      "Epoch 3469 | Train Loss: 1.344437599182129 | Test Loss: 1.3944144248962402\n",
      "Epoch 3470 | Train Loss: 1.2029783725738525 | Test Loss: 1.21870756149292\n",
      "Epoch 3471 | Train Loss: 1.3442835807800293 | Test Loss: 1.394268274307251\n",
      "Epoch 3472 | Train Loss: 1.2028127908706665 | Test Loss: 1.2185535430908203\n",
      "Epoch 3473 | Train Loss: 1.3441293239593506 | Test Loss: 1.3941129446029663\n",
      "Epoch 3474 | Train Loss: 1.2026565074920654 | Test Loss: 1.2184048891067505\n",
      "Epoch 3475 | Train Loss: 1.343968152999878 | Test Loss: 1.3939824104309082\n",
      "Epoch 3476 | Train Loss: 1.2025030851364136 | Test Loss: 1.2182670831680298\n",
      "Epoch 3477 | Train Loss: 1.3438056707382202 | Test Loss: 1.3938044309616089\n",
      "Epoch 3478 | Train Loss: 1.202348232269287 | Test Loss: 1.2181168794631958\n",
      "Epoch 3479 | Train Loss: 1.343651533126831 | Test Loss: 1.3936893939971924\n",
      "Epoch 3480 | Train Loss: 1.2021867036819458 | Test Loss: 1.2179571390151978\n",
      "Epoch 3481 | Train Loss: 1.3434960842132568 | Test Loss: 1.3935272693634033\n",
      "Epoch 3482 | Train Loss: 1.2020314931869507 | Test Loss: 1.2178184986114502\n",
      "Epoch 3483 | Train Loss: 1.3433361053466797 | Test Loss: 1.3933898210525513\n",
      "Epoch 3484 | Train Loss: 1.2018764019012451 | Test Loss: 1.217661738395691\n",
      "Epoch 3485 | Train Loss: 1.343183994293213 | Test Loss: 1.3932396173477173\n",
      "Epoch 3486 | Train Loss: 1.201715111732483 | Test Loss: 1.2175177335739136\n",
      "Epoch 3487 | Train Loss: 1.343030571937561 | Test Loss: 1.3931145668029785\n",
      "Epoch 3488 | Train Loss: 1.2015622854232788 | Test Loss: 1.2173722982406616\n",
      "Epoch 3489 | Train Loss: 1.3428701162338257 | Test Loss: 1.3929554224014282\n",
      "Epoch 3490 | Train Loss: 1.2014061212539673 | Test Loss: 1.2172292470932007\n",
      "Epoch 3491 | Train Loss: 1.3427222967147827 | Test Loss: 1.3928145170211792\n",
      "Epoch 3492 | Train Loss: 1.2012379169464111 | Test Loss: 1.2170604467391968\n",
      "Epoch 3493 | Train Loss: 1.3425755500793457 | Test Loss: 1.3926714658737183\n",
      "Epoch 3494 | Train Loss: 1.201086163520813 | Test Loss: 1.2169300317764282\n",
      "Epoch 3495 | Train Loss: 1.3424105644226074 | Test Loss: 1.3925586938858032\n",
      "Epoch 3496 | Train Loss: 1.200939416885376 | Test Loss: 1.2167847156524658\n",
      "Epoch 3497 | Train Loss: 1.342255711555481 | Test Loss: 1.392379641532898\n",
      "Epoch 3498 | Train Loss: 1.2007803916931152 | Test Loss: 1.2166416645050049\n",
      "Epoch 3499 | Train Loss: 1.3421090841293335 | Test Loss: 1.392254114151001\n",
      "Epoch 3500 | Train Loss: 1.2006222009658813 | Test Loss: 1.2164878845214844\n",
      "Epoch 3501 | Train Loss: 1.3419588804244995 | Test Loss: 1.3921172618865967\n",
      "Epoch 3502 | Train Loss: 1.200472116470337 | Test Loss: 1.216365098953247\n",
      "Epoch 3503 | Train Loss: 1.3417961597442627 | Test Loss: 1.3919960260391235\n",
      "Epoch 3504 | Train Loss: 1.2003229856491089 | Test Loss: 1.2162038087844849\n",
      "Epoch 3505 | Train Loss: 1.3416515588760376 | Test Loss: 1.391829490661621\n",
      "Epoch 3506 | Train Loss: 1.200160264968872 | Test Loss: 1.2160654067993164\n",
      "Epoch 3507 | Train Loss: 1.3415032625198364 | Test Loss: 1.3917152881622314\n",
      "Epoch 3508 | Train Loss: 1.2000069618225098 | Test Loss: 1.2159167528152466\n",
      "Epoch 3509 | Train Loss: 1.3413472175598145 | Test Loss: 1.3915612697601318\n",
      "Epoch 3510 | Train Loss: 1.199857234954834 | Test Loss: 1.215790867805481\n",
      "Epoch 3511 | Train Loss: 1.3411918878555298 | Test Loss: 1.3913980722427368\n",
      "Epoch 3512 | Train Loss: 1.1997008323669434 | Test Loss: 1.2156327962875366\n",
      "Epoch 3513 | Train Loss: 1.3410440683364868 | Test Loss: 1.3912698030471802\n",
      "Epoch 3514 | Train Loss: 1.1995474100112915 | Test Loss: 1.2155067920684814\n",
      "Epoch 3515 | Train Loss: 1.340890884399414 | Test Loss: 1.3911246061325073\n",
      "Epoch 3516 | Train Loss: 1.1993935108184814 | Test Loss: 1.2153526544570923\n",
      "Epoch 3517 | Train Loss: 1.340735673904419 | Test Loss: 1.3909995555877686\n",
      "Epoch 3518 | Train Loss: 1.1992460489273071 | Test Loss: 1.215206503868103\n",
      "Epoch 3519 | Train Loss: 1.340583086013794 | Test Loss: 1.390836477279663\n",
      "Epoch 3520 | Train Loss: 1.1990876197814941 | Test Loss: 1.2150617837905884\n",
      "Epoch 3521 | Train Loss: 1.3404361009597778 | Test Loss: 1.390703797340393\n",
      "Epoch 3522 | Train Loss: 1.1989350318908691 | Test Loss: 1.2149181365966797\n",
      "Epoch 3523 | Train Loss: 1.3402820825576782 | Test Loss: 1.390552282333374\n",
      "Epoch 3524 | Train Loss: 1.1987874507904053 | Test Loss: 1.214782953262329\n",
      "Epoch 3525 | Train Loss: 1.3401296138763428 | Test Loss: 1.3904447555541992\n",
      "Epoch 3526 | Train Loss: 1.1986336708068848 | Test Loss: 1.214631199836731\n",
      "Epoch 3527 | Train Loss: 1.3399841785430908 | Test Loss: 1.3902682065963745\n",
      "Epoch 3528 | Train Loss: 1.198479175567627 | Test Loss: 1.214493751525879\n",
      "Epoch 3529 | Train Loss: 1.339838981628418 | Test Loss: 1.3901513814926147\n",
      "Epoch 3530 | Train Loss: 1.1983238458633423 | Test Loss: 1.2143439054489136\n",
      "Epoch 3531 | Train Loss: 1.3396891355514526 | Test Loss: 1.3899736404418945\n",
      "Epoch 3532 | Train Loss: 1.1981772184371948 | Test Loss: 1.2142130136489868\n",
      "Epoch 3533 | Train Loss: 1.3395370244979858 | Test Loss: 1.389871597290039\n",
      "Epoch 3534 | Train Loss: 1.1980235576629639 | Test Loss: 1.21405029296875\n",
      "Epoch 3535 | Train Loss: 1.3393895626068115 | Test Loss: 1.389700174331665\n",
      "Epoch 3536 | Train Loss: 1.1978751420974731 | Test Loss: 1.2139261960983276\n",
      "Epoch 3537 | Train Loss: 1.3392382860183716 | Test Loss: 1.3895835876464844\n",
      "Epoch 3538 | Train Loss: 1.197719931602478 | Test Loss: 1.2137683629989624\n",
      "Epoch 3539 | Train Loss: 1.3390963077545166 | Test Loss: 1.3894304037094116\n",
      "Epoch 3540 | Train Loss: 1.1975704431533813 | Test Loss: 1.2136467695236206\n",
      "Epoch 3541 | Train Loss: 1.3389358520507812 | Test Loss: 1.3892905712127686\n",
      "Epoch 3542 | Train Loss: 1.1974287033081055 | Test Loss: 1.2135010957717896\n",
      "Epoch 3543 | Train Loss: 1.3387824296951294 | Test Loss: 1.3891618251800537\n",
      "Epoch 3544 | Train Loss: 1.1972795724868774 | Test Loss: 1.2133759260177612\n",
      "Epoch 3545 | Train Loss: 1.3386257886886597 | Test Loss: 1.3889952898025513\n",
      "Epoch 3546 | Train Loss: 1.1971325874328613 | Test Loss: 1.2132185697555542\n",
      "Epoch 3547 | Train Loss: 1.3384767770767212 | Test Loss: 1.3888835906982422\n",
      "Epoch 3548 | Train Loss: 1.1969821453094482 | Test Loss: 1.21309232711792\n",
      "Epoch 3549 | Train Loss: 1.338322639465332 | Test Loss: 1.3887298107147217\n",
      "Epoch 3550 | Train Loss: 1.19683837890625 | Test Loss: 1.212952733039856\n",
      "Epoch 3551 | Train Loss: 1.3381694555282593 | Test Loss: 1.388603925704956\n",
      "Epoch 3552 | Train Loss: 1.1966887712478638 | Test Loss: 1.2128081321716309\n",
      "Epoch 3553 | Train Loss: 1.3380221128463745 | Test Loss: 1.388432502746582\n",
      "Epoch 3554 | Train Loss: 1.1965395212173462 | Test Loss: 1.2126684188842773\n",
      "Epoch 3555 | Train Loss: 1.337874412536621 | Test Loss: 1.3883156776428223\n",
      "Epoch 3556 | Train Loss: 1.1963915824890137 | Test Loss: 1.2125351428985596\n",
      "Epoch 3557 | Train Loss: 1.3377203941345215 | Test Loss: 1.388161540031433\n",
      "Epoch 3558 | Train Loss: 1.1962497234344482 | Test Loss: 1.2124005556106567\n",
      "Epoch 3559 | Train Loss: 1.3375662565231323 | Test Loss: 1.3880223035812378\n",
      "Epoch 3560 | Train Loss: 1.1961047649383545 | Test Loss: 1.2122544050216675\n",
      "Epoch 3561 | Train Loss: 1.3374180793762207 | Test Loss: 1.387864589691162\n",
      "Epoch 3562 | Train Loss: 1.1959584951400757 | Test Loss: 1.2121247053146362\n",
      "Epoch 3563 | Train Loss: 1.3372650146484375 | Test Loss: 1.387755274772644\n",
      "Epoch 3564 | Train Loss: 1.195816993713379 | Test Loss: 1.2119816541671753\n",
      "Epoch 3565 | Train Loss: 1.3371179103851318 | Test Loss: 1.387589693069458\n",
      "Epoch 3566 | Train Loss: 1.1956698894500732 | Test Loss: 1.2118619680404663\n",
      "Epoch 3567 | Train Loss: 1.3369662761688232 | Test Loss: 1.3874799013137817\n",
      "Epoch 3568 | Train Loss: 1.1955291032791138 | Test Loss: 1.2117061614990234\n",
      "Epoch 3569 | Train Loss: 1.3368127346038818 | Test Loss: 1.387319803237915\n",
      "Epoch 3570 | Train Loss: 1.195390224456787 | Test Loss: 1.2115881443023682\n",
      "Epoch 3571 | Train Loss: 1.3366612195968628 | Test Loss: 1.3871928453445435\n",
      "Epoch 3572 | Train Loss: 1.1952414512634277 | Test Loss: 1.211437463760376\n",
      "Epoch 3573 | Train Loss: 1.3365132808685303 | Test Loss: 1.387067198753357\n",
      "Epoch 3574 | Train Loss: 1.1950980424880981 | Test Loss: 1.2113102674484253\n",
      "Epoch 3575 | Train Loss: 1.336361289024353 | Test Loss: 1.3869153261184692\n",
      "Epoch 3576 | Train Loss: 1.194953441619873 | Test Loss: 1.2111608982086182\n",
      "Epoch 3577 | Train Loss: 1.3362168073654175 | Test Loss: 1.3868048191070557\n",
      "Epoch 3578 | Train Loss: 1.1948050260543823 | Test Loss: 1.2110252380371094\n",
      "Epoch 3579 | Train Loss: 1.3360666036605835 | Test Loss: 1.3866612911224365\n",
      "Epoch 3580 | Train Loss: 1.1946637630462646 | Test Loss: 1.2108796834945679\n",
      "Epoch 3581 | Train Loss: 1.3359179496765137 | Test Loss: 1.3865519762039185\n",
      "Epoch 3582 | Train Loss: 1.1945186853408813 | Test Loss: 1.210754632949829\n",
      "Epoch 3583 | Train Loss: 1.3357712030410767 | Test Loss: 1.3863978385925293\n",
      "Epoch 3584 | Train Loss: 1.194371223449707 | Test Loss: 1.2106095552444458\n",
      "Epoch 3585 | Train Loss: 1.3356263637542725 | Test Loss: 1.386284589767456\n",
      "Epoch 3586 | Train Loss: 1.1942249536514282 | Test Loss: 1.2104732990264893\n",
      "Epoch 3587 | Train Loss: 1.3354809284210205 | Test Loss: 1.3861390352249146\n",
      "Epoch 3588 | Train Loss: 1.194077968597412 | Test Loss: 1.2103500366210938\n",
      "Epoch 3589 | Train Loss: 1.3353369235992432 | Test Loss: 1.3860206604003906\n",
      "Epoch 3590 | Train Loss: 1.1939295530319214 | Test Loss: 1.210202932357788\n",
      "Epoch 3591 | Train Loss: 1.3351937532424927 | Test Loss: 1.3858754634857178\n",
      "Epoch 3592 | Train Loss: 1.1937830448150635 | Test Loss: 1.210066556930542\n",
      "Epoch 3593 | Train Loss: 1.335050344467163 | Test Loss: 1.3857626914978027\n",
      "Epoch 3594 | Train Loss: 1.1936324834823608 | Test Loss: 1.209924578666687\n",
      "Epoch 3595 | Train Loss: 1.3349049091339111 | Test Loss: 1.3856232166290283\n",
      "Epoch 3596 | Train Loss: 1.1934906244277954 | Test Loss: 1.2097944021224976\n",
      "Epoch 3597 | Train Loss: 1.3347549438476562 | Test Loss: 1.3855006694793701\n",
      "Epoch 3598 | Train Loss: 1.1933479309082031 | Test Loss: 1.2096543312072754\n",
      "Epoch 3599 | Train Loss: 1.334610939025879 | Test Loss: 1.3853429555892944\n",
      "Epoch 3600 | Train Loss: 1.1932047605514526 | Test Loss: 1.2095305919647217\n",
      "Epoch 3601 | Train Loss: 1.3344662189483643 | Test Loss: 1.3852460384368896\n",
      "Epoch 3602 | Train Loss: 1.1930564641952515 | Test Loss: 1.2093729972839355\n",
      "Epoch 3603 | Train Loss: 1.3343273401260376 | Test Loss: 1.3850765228271484\n",
      "Epoch 3604 | Train Loss: 1.1929130554199219 | Test Loss: 1.209261417388916\n",
      "Epoch 3605 | Train Loss: 1.334180235862732 | Test Loss: 1.3850001096725464\n",
      "Epoch 3606 | Train Loss: 1.1927683353424072 | Test Loss: 1.2090991735458374\n",
      "Epoch 3607 | Train Loss: 1.3340424299240112 | Test Loss: 1.3848334550857544\n",
      "Epoch 3608 | Train Loss: 1.1926250457763672 | Test Loss: 1.2089835405349731\n",
      "Epoch 3609 | Train Loss: 1.3339009284973145 | Test Loss: 1.3847332000732422\n",
      "Epoch 3610 | Train Loss: 1.1924757957458496 | Test Loss: 1.208830714225769\n",
      "Epoch 3611 | Train Loss: 1.333760142326355 | Test Loss: 1.3845785856246948\n",
      "Epoch 3612 | Train Loss: 1.192337155342102 | Test Loss: 1.2087239027023315\n",
      "Epoch 3613 | Train Loss: 1.3336101770401 | Test Loss: 1.3844596147537231\n",
      "Epoch 3614 | Train Loss: 1.1921913623809814 | Test Loss: 1.2085604667663574\n",
      "Epoch 3615 | Train Loss: 1.3334685564041138 | Test Loss: 1.3843194246292114\n",
      "Epoch 3616 | Train Loss: 1.1920517683029175 | Test Loss: 1.2084516286849976\n",
      "Epoch 3617 | Train Loss: 1.3333178758621216 | Test Loss: 1.3842015266418457\n",
      "Epoch 3618 | Train Loss: 1.191910982131958 | Test Loss: 1.208298683166504\n",
      "Epoch 3619 | Train Loss: 1.333176851272583 | Test Loss: 1.3840739727020264\n",
      "Epoch 3620 | Train Loss: 1.191762924194336 | Test Loss: 1.2081772089004517\n",
      "Epoch 3621 | Train Loss: 1.3330377340316772 | Test Loss: 1.383949875831604\n",
      "Epoch 3622 | Train Loss: 1.1916172504425049 | Test Loss: 1.208025336265564\n",
      "Epoch 3623 | Train Loss: 1.3328924179077148 | Test Loss: 1.3838411569595337\n",
      "Epoch 3624 | Train Loss: 1.1914799213409424 | Test Loss: 1.207905888557434\n",
      "Epoch 3625 | Train Loss: 1.3327432870864868 | Test Loss: 1.3836851119995117\n",
      "Epoch 3626 | Train Loss: 1.191338300704956 | Test Loss: 1.2077665328979492\n",
      "Epoch 3627 | Train Loss: 1.3326022624969482 | Test Loss: 1.3835818767547607\n",
      "Epoch 3628 | Train Loss: 1.1911954879760742 | Test Loss: 1.2076293230056763\n",
      "Epoch 3629 | Train Loss: 1.332459807395935 | Test Loss: 1.383434534072876\n",
      "Epoch 3630 | Train Loss: 1.191057562828064 | Test Loss: 1.2075014114379883\n",
      "Epoch 3631 | Train Loss: 1.3323167562484741 | Test Loss: 1.3833329677581787\n",
      "Epoch 3632 | Train Loss: 1.1909146308898926 | Test Loss: 1.2073618173599243\n",
      "Epoch 3633 | Train Loss: 1.332177758216858 | Test Loss: 1.383191466331482\n",
      "Epoch 3634 | Train Loss: 1.1907751560211182 | Test Loss: 1.2072335481643677\n",
      "Epoch 3635 | Train Loss: 1.3320353031158447 | Test Loss: 1.383088231086731\n",
      "Epoch 3636 | Train Loss: 1.19063401222229 | Test Loss: 1.2070945501327515\n",
      "Epoch 3637 | Train Loss: 1.3318986892700195 | Test Loss: 1.3829346895217896\n",
      "Epoch 3638 | Train Loss: 1.1904926300048828 | Test Loss: 1.2069741487503052\n",
      "Epoch 3639 | Train Loss: 1.331757664680481 | Test Loss: 1.3828372955322266\n",
      "Epoch 3640 | Train Loss: 1.190354347229004 | Test Loss: 1.2068229913711548\n",
      "Epoch 3641 | Train Loss: 1.33161461353302 | Test Loss: 1.3826802968978882\n",
      "Epoch 3642 | Train Loss: 1.1902227401733398 | Test Loss: 1.206722617149353\n",
      "Epoch 3643 | Train Loss: 1.331472635269165 | Test Loss: 1.3826093673706055\n",
      "Epoch 3644 | Train Loss: 1.1900784969329834 | Test Loss: 1.2065602540969849\n",
      "Epoch 3645 | Train Loss: 1.3313360214233398 | Test Loss: 1.3824365139007568\n",
      "Epoch 3646 | Train Loss: 1.1899455785751343 | Test Loss: 1.2064521312713623\n",
      "Epoch 3647 | Train Loss: 1.3311891555786133 | Test Loss: 1.3823579549789429\n",
      "Epoch 3648 | Train Loss: 1.1898043155670166 | Test Loss: 1.206302285194397\n",
      "Epoch 3649 | Train Loss: 1.3310595750808716 | Test Loss: 1.3821892738342285\n",
      "Epoch 3650 | Train Loss: 1.1896661520004272 | Test Loss: 1.2062031030654907\n",
      "Epoch 3651 | Train Loss: 1.3309154510498047 | Test Loss: 1.3821319341659546\n",
      "Epoch 3652 | Train Loss: 1.1895281076431274 | Test Loss: 1.2060344219207764\n",
      "Epoch 3653 | Train Loss: 1.330786943435669 | Test Loss: 1.381964921951294\n",
      "Epoch 3654 | Train Loss: 1.1893837451934814 | Test Loss: 1.2059450149536133\n",
      "Epoch 3655 | Train Loss: 1.3306463956832886 | Test Loss: 1.3818687200546265\n",
      "Epoch 3656 | Train Loss: 1.1892428398132324 | Test Loss: 1.2057713270187378\n",
      "Epoch 3657 | Train Loss: 1.3305073976516724 | Test Loss: 1.3817389011383057\n",
      "Epoch 3658 | Train Loss: 1.1891111135482788 | Test Loss: 1.2056796550750732\n",
      "Epoch 3659 | Train Loss: 1.3303539752960205 | Test Loss: 1.3816148042678833\n",
      "Epoch 3660 | Train Loss: 1.1889723539352417 | Test Loss: 1.205520510673523\n",
      "Epoch 3661 | Train Loss: 1.3302183151245117 | Test Loss: 1.3815001249313354\n",
      "Epoch 3662 | Train Loss: 1.1888312101364136 | Test Loss: 1.205406904220581\n",
      "Epoch 3663 | Train Loss: 1.3300762176513672 | Test Loss: 1.3813753128051758\n",
      "Epoch 3664 | Train Loss: 1.188692331314087 | Test Loss: 1.205254077911377\n",
      "Epoch 3665 | Train Loss: 1.329940676689148 | Test Loss: 1.3812717199325562\n",
      "Epoch 3666 | Train Loss: 1.188551664352417 | Test Loss: 1.2051506042480469\n",
      "Epoch 3667 | Train Loss: 1.3297985792160034 | Test Loss: 1.3811339139938354\n",
      "Epoch 3668 | Train Loss: 1.1884149312973022 | Test Loss: 1.2049952745437622\n",
      "Epoch 3669 | Train Loss: 1.3296574354171753 | Test Loss: 1.3810323476791382\n",
      "Epoch 3670 | Train Loss: 1.1882801055908203 | Test Loss: 1.2048736810684204\n",
      "Epoch 3671 | Train Loss: 1.3295176029205322 | Test Loss: 1.380894660949707\n",
      "Epoch 3672 | Train Loss: 1.1881383657455444 | Test Loss: 1.2047417163848877\n",
      "Epoch 3673 | Train Loss: 1.3293858766555786 | Test Loss: 1.3808053731918335\n",
      "Epoch 3674 | Train Loss: 1.1879963874816895 | Test Loss: 1.2046087980270386\n",
      "Epoch 3675 | Train Loss: 1.3292489051818848 | Test Loss: 1.3806660175323486\n",
      "Epoch 3676 | Train Loss: 1.1878635883331299 | Test Loss: 1.204485297203064\n",
      "Epoch 3677 | Train Loss: 1.3291077613830566 | Test Loss: 1.3805744647979736\n",
      "Epoch 3678 | Train Loss: 1.187727451324463 | Test Loss: 1.204343557357788\n",
      "Epoch 3679 | Train Loss: 1.3289750814437866 | Test Loss: 1.380424976348877\n",
      "Epoch 3680 | Train Loss: 1.1875901222229004 | Test Loss: 1.2042291164398193\n",
      "Epoch 3681 | Train Loss: 1.3288373947143555 | Test Loss: 1.380327582359314\n",
      "Epoch 3682 | Train Loss: 1.1874547004699707 | Test Loss: 1.2040908336639404\n",
      "Epoch 3683 | Train Loss: 1.3287036418914795 | Test Loss: 1.3802114725112915\n",
      "Epoch 3684 | Train Loss: 1.187319278717041 | Test Loss: 1.2039693593978882\n",
      "Epoch 3685 | Train Loss: 1.3285675048828125 | Test Loss: 1.3801100254058838\n",
      "Epoch 3686 | Train Loss: 1.187179684638977 | Test Loss: 1.2038143873214722\n",
      "Epoch 3687 | Train Loss: 1.3284354209899902 | Test Loss: 1.3799527883529663\n",
      "Epoch 3688 | Train Loss: 1.1870462894439697 | Test Loss: 1.203714370727539\n",
      "Epoch 3689 | Train Loss: 1.3282984495162964 | Test Loss: 1.3798850774765015\n",
      "Epoch 3690 | Train Loss: 1.186906099319458 | Test Loss: 1.2035632133483887\n",
      "Epoch 3691 | Train Loss: 1.3281713724136353 | Test Loss: 1.3797403573989868\n",
      "Epoch 3692 | Train Loss: 1.1867655515670776 | Test Loss: 1.2034540176391602\n",
      "Epoch 3693 | Train Loss: 1.3280370235443115 | Test Loss: 1.3796361684799194\n",
      "Epoch 3694 | Train Loss: 1.186632513999939 | Test Loss: 1.2033113241195679\n",
      "Epoch 3695 | Train Loss: 1.3279019594192505 | Test Loss: 1.3795335292816162\n",
      "Epoch 3696 | Train Loss: 1.1864981651306152 | Test Loss: 1.2032029628753662\n",
      "Epoch 3697 | Train Loss: 1.327763557434082 | Test Loss: 1.3794076442718506\n",
      "Epoch 3698 | Train Loss: 1.1863597631454468 | Test Loss: 1.2030506134033203\n",
      "Epoch 3699 | Train Loss: 1.3276318311691284 | Test Loss: 1.3792765140533447\n",
      "Epoch 3700 | Train Loss: 1.186226725578308 | Test Loss: 1.202942967414856\n",
      "Epoch 3701 | Train Loss: 1.3274908065795898 | Test Loss: 1.3791866302490234\n",
      "Epoch 3702 | Train Loss: 1.186091423034668 | Test Loss: 1.20278799533844\n",
      "Epoch 3703 | Train Loss: 1.3273597955703735 | Test Loss: 1.3790736198425293\n",
      "Epoch 3704 | Train Loss: 1.1859544515609741 | Test Loss: 1.2026729583740234\n",
      "Epoch 3705 | Train Loss: 1.3272275924682617 | Test Loss: 1.3789546489715576\n",
      "Epoch 3706 | Train Loss: 1.1858173608779907 | Test Loss: 1.2025375366210938\n",
      "Epoch 3707 | Train Loss: 1.3270952701568604 | Test Loss: 1.378859281539917\n",
      "Epoch 3708 | Train Loss: 1.185680627822876 | Test Loss: 1.2024117708206177\n",
      "Epoch 3709 | Train Loss: 1.326967716217041 | Test Loss: 1.3787410259246826\n",
      "Epoch 3710 | Train Loss: 1.1855369806289673 | Test Loss: 1.2022770643234253\n",
      "Epoch 3711 | Train Loss: 1.3268418312072754 | Test Loss: 1.3786165714263916\n",
      "Epoch 3712 | Train Loss: 1.1854037046432495 | Test Loss: 1.202161431312561\n",
      "Epoch 3713 | Train Loss: 1.3267074823379517 | Test Loss: 1.3785358667373657\n",
      "Epoch 3714 | Train Loss: 1.185266375541687 | Test Loss: 1.2020139694213867\n",
      "Epoch 3715 | Train Loss: 1.3265800476074219 | Test Loss: 1.3784157037734985\n",
      "Epoch 3716 | Train Loss: 1.185128092765808 | Test Loss: 1.2018852233886719\n",
      "Epoch 3717 | Train Loss: 1.3264527320861816 | Test Loss: 1.3783087730407715\n",
      "Epoch 3718 | Train Loss: 1.184991717338562 | Test Loss: 1.2017611265182495\n",
      "Epoch 3719 | Train Loss: 1.3263188600540161 | Test Loss: 1.3782103061676025\n",
      "Epoch 3720 | Train Loss: 1.1848634481430054 | Test Loss: 1.2016394138336182\n",
      "Epoch 3721 | Train Loss: 1.3261865377426147 | Test Loss: 1.3781083822250366\n",
      "Epoch 3722 | Train Loss: 1.1847271919250488 | Test Loss: 1.2014998197555542\n",
      "Epoch 3723 | Train Loss: 1.326059341430664 | Test Loss: 1.377976894378662\n",
      "Epoch 3724 | Train Loss: 1.1845964193344116 | Test Loss: 1.2013890743255615\n",
      "Epoch 3725 | Train Loss: 1.3259230852127075 | Test Loss: 1.3779064416885376\n",
      "Epoch 3726 | Train Loss: 1.1844637393951416 | Test Loss: 1.2012478113174438\n",
      "Epoch 3727 | Train Loss: 1.3257966041564941 | Test Loss: 1.3777751922607422\n",
      "Epoch 3728 | Train Loss: 1.1843303442001343 | Test Loss: 1.2011401653289795\n",
      "Epoch 3729 | Train Loss: 1.3256670236587524 | Test Loss: 1.3776874542236328\n",
      "Epoch 3730 | Train Loss: 1.1841964721679688 | Test Loss: 1.201006531715393\n",
      "Epoch 3731 | Train Loss: 1.3255367279052734 | Test Loss: 1.3775454759597778\n",
      "Epoch 3732 | Train Loss: 1.1840696334838867 | Test Loss: 1.2009012699127197\n",
      "Epoch 3733 | Train Loss: 1.3254055976867676 | Test Loss: 1.3774595260620117\n",
      "Epoch 3734 | Train Loss: 1.1839333772659302 | Test Loss: 1.2007604837417603\n",
      "Epoch 3735 | Train Loss: 1.3252828121185303 | Test Loss: 1.3773342370986938\n",
      "Epoch 3736 | Train Loss: 1.1838014125823975 | Test Loss: 1.200653314590454\n",
      "Epoch 3737 | Train Loss: 1.3251512050628662 | Test Loss: 1.3772567510604858\n",
      "Epoch 3738 | Train Loss: 1.1836700439453125 | Test Loss: 1.2005102634429932\n",
      "Epoch 3739 | Train Loss: 1.3250240087509155 | Test Loss: 1.3771333694458008\n",
      "Epoch 3740 | Train Loss: 1.1835399866104126 | Test Loss: 1.2004092931747437\n",
      "Epoch 3741 | Train Loss: 1.3248921632766724 | Test Loss: 1.3770447969436646\n",
      "Epoch 3742 | Train Loss: 1.1834064722061157 | Test Loss: 1.2002581357955933\n",
      "Epoch 3743 | Train Loss: 1.3247718811035156 | Test Loss: 1.3769259452819824\n",
      "Epoch 3744 | Train Loss: 1.1832714080810547 | Test Loss: 1.2001638412475586\n",
      "Epoch 3745 | Train Loss: 1.3246424198150635 | Test Loss: 1.3768233060836792\n",
      "Epoch 3746 | Train Loss: 1.1831398010253906 | Test Loss: 1.2000179290771484\n",
      "Epoch 3747 | Train Loss: 1.3245189189910889 | Test Loss: 1.376714825630188\n",
      "Epoch 3748 | Train Loss: 1.1830084323883057 | Test Loss: 1.1999101638793945\n",
      "Epoch 3749 | Train Loss: 1.3243871927261353 | Test Loss: 1.3766332864761353\n",
      "Epoch 3750 | Train Loss: 1.182876706123352 | Test Loss: 1.19976806640625\n",
      "Epoch 3751 | Train Loss: 1.3242597579956055 | Test Loss: 1.376501202583313\n",
      "Epoch 3752 | Train Loss: 1.1827476024627686 | Test Loss: 1.1996673345565796\n",
      "Epoch 3753 | Train Loss: 1.3241305351257324 | Test Loss: 1.376416563987732\n",
      "Epoch 3754 | Train Loss: 1.1826140880584717 | Test Loss: 1.1995201110839844\n",
      "Epoch 3755 | Train Loss: 1.3240060806274414 | Test Loss: 1.3763172626495361\n",
      "Epoch 3756 | Train Loss: 1.1824854612350464 | Test Loss: 1.1994178295135498\n",
      "Epoch 3757 | Train Loss: 1.323879361152649 | Test Loss: 1.3762030601501465\n",
      "Epoch 3758 | Train Loss: 1.182350993156433 | Test Loss: 1.1992881298065186\n",
      "Epoch 3759 | Train Loss: 1.3237543106079102 | Test Loss: 1.3760994672775269\n",
      "Epoch 3760 | Train Loss: 1.1822245121002197 | Test Loss: 1.199169635772705\n",
      "Epoch 3761 | Train Loss: 1.3236240148544312 | Test Loss: 1.3760126829147339\n",
      "Epoch 3762 | Train Loss: 1.1820955276489258 | Test Loss: 1.1990374326705933\n",
      "Epoch 3763 | Train Loss: 1.3235012292861938 | Test Loss: 1.375889778137207\n",
      "Epoch 3764 | Train Loss: 1.1819651126861572 | Test Loss: 1.1989285945892334\n",
      "Epoch 3765 | Train Loss: 1.3233742713928223 | Test Loss: 1.3758044242858887\n",
      "Epoch 3766 | Train Loss: 1.1818369626998901 | Test Loss: 1.1987942457199097\n",
      "Epoch 3767 | Train Loss: 1.3232530355453491 | Test Loss: 1.3756870031356812\n",
      "Epoch 3768 | Train Loss: 1.181706428527832 | Test Loss: 1.1986920833587646\n",
      "Epoch 3769 | Train Loss: 1.3231281042099 | Test Loss: 1.3755979537963867\n",
      "Epoch 3770 | Train Loss: 1.181578278541565 | Test Loss: 1.1985539197921753\n",
      "Epoch 3771 | Train Loss: 1.3230078220367432 | Test Loss: 1.3754783868789673\n",
      "Epoch 3772 | Train Loss: 1.181450605392456 | Test Loss: 1.1984585523605347\n",
      "Epoch 3773 | Train Loss: 1.322883129119873 | Test Loss: 1.3753849267959595\n",
      "Epoch 3774 | Train Loss: 1.1813198328018188 | Test Loss: 1.1983176469802856\n",
      "Epoch 3775 | Train Loss: 1.3227617740631104 | Test Loss: 1.3752782344818115\n",
      "Epoch 3776 | Train Loss: 1.1811964511871338 | Test Loss: 1.1982080936431885\n",
      "Epoch 3777 | Train Loss: 1.322633981704712 | Test Loss: 1.375203013420105\n",
      "Epoch 3778 | Train Loss: 1.1810696125030518 | Test Loss: 1.1980746984481812\n",
      "Epoch 3779 | Train Loss: 1.322514295578003 | Test Loss: 1.3750770092010498\n",
      "Epoch 3780 | Train Loss: 1.1809431314468384 | Test Loss: 1.1979762315750122\n",
      "Epoch 3781 | Train Loss: 1.3223882913589478 | Test Loss: 1.375008225440979\n",
      "Epoch 3782 | Train Loss: 1.180820107460022 | Test Loss: 1.1978402137756348\n",
      "Epoch 3783 | Train Loss: 1.3222696781158447 | Test Loss: 1.3748847246170044\n",
      "Epoch 3784 | Train Loss: 1.1806933879852295 | Test Loss: 1.1977424621582031\n",
      "Epoch 3785 | Train Loss: 1.3221462965011597 | Test Loss: 1.37480628490448\n",
      "Epoch 3786 | Train Loss: 1.1805665493011475 | Test Loss: 1.1975915431976318\n",
      "Epoch 3787 | Train Loss: 1.322025179862976 | Test Loss: 1.3746966123580933\n",
      "Epoch 3788 | Train Loss: 1.180448055267334 | Test Loss: 1.1975044012069702\n",
      "Epoch 3789 | Train Loss: 1.3218994140625 | Test Loss: 1.3746289014816284\n",
      "Epoch 3790 | Train Loss: 1.1803157329559326 | Test Loss: 1.197358250617981\n",
      "Epoch 3791 | Train Loss: 1.3217880725860596 | Test Loss: 1.3744901418685913\n",
      "Epoch 3792 | Train Loss: 1.1801878213882446 | Test Loss: 1.197266936302185\n",
      "Epoch 3793 | Train Loss: 1.3216646909713745 | Test Loss: 1.3744456768035889\n",
      "Epoch 3794 | Train Loss: 1.180065393447876 | Test Loss: 1.1971241235733032\n",
      "Epoch 3795 | Train Loss: 1.3215479850769043 | Test Loss: 1.374313235282898\n",
      "Epoch 3796 | Train Loss: 1.1799418926239014 | Test Loss: 1.197038173675537\n",
      "Epoch 3797 | Train Loss: 1.32142174243927 | Test Loss: 1.3742338418960571\n",
      "Epoch 3798 | Train Loss: 1.1798136234283447 | Test Loss: 1.1968811750411987\n",
      "Epoch 3799 | Train Loss: 1.3213034868240356 | Test Loss: 1.3741326332092285\n",
      "Epoch 3800 | Train Loss: 1.179697036743164 | Test Loss: 1.1968035697937012\n",
      "Epoch 3801 | Train Loss: 1.3211731910705566 | Test Loss: 1.3740394115447998\n",
      "Epoch 3802 | Train Loss: 1.179570198059082 | Test Loss: 1.1966561079025269\n",
      "Epoch 3803 | Train Loss: 1.3210562467575073 | Test Loss: 1.3739244937896729\n",
      "Epoch 3804 | Train Loss: 1.179445743560791 | Test Loss: 1.196556806564331\n",
      "Epoch 3805 | Train Loss: 1.320928931236267 | Test Loss: 1.3738378286361694\n",
      "Epoch 3806 | Train Loss: 1.179323673248291 | Test Loss: 1.196419358253479\n",
      "Epoch 3807 | Train Loss: 1.3208075761795044 | Test Loss: 1.373725175857544\n",
      "Epoch 3808 | Train Loss: 1.1792012453079224 | Test Loss: 1.1963284015655518\n",
      "Epoch 3809 | Train Loss: 1.3206835985183716 | Test Loss: 1.3736310005187988\n",
      "Epoch 3810 | Train Loss: 1.1790733337402344 | Test Loss: 1.1961824893951416\n",
      "Epoch 3811 | Train Loss: 1.3205682039260864 | Test Loss: 1.3735328912734985\n",
      "Epoch 3812 | Train Loss: 1.1789486408233643 | Test Loss: 1.1960877180099487\n",
      "Epoch 3813 | Train Loss: 1.3204424381256104 | Test Loss: 1.373435139656067\n",
      "Epoch 3814 | Train Loss: 1.1788252592086792 | Test Loss: 1.1959530115127563\n",
      "Epoch 3815 | Train Loss: 1.3203238248825073 | Test Loss: 1.3733441829681396\n",
      "Epoch 3816 | Train Loss: 1.178700566291809 | Test Loss: 1.1958526372909546\n",
      "Epoch 3817 | Train Loss: 1.3202018737792969 | Test Loss: 1.3732359409332275\n",
      "Epoch 3818 | Train Loss: 1.1785764694213867 | Test Loss: 1.1957284212112427\n",
      "Epoch 3819 | Train Loss: 1.3200817108154297 | Test Loss: 1.373144268989563\n",
      "Epoch 3820 | Train Loss: 1.1784549951553345 | Test Loss: 1.195622444152832\n",
      "Epoch 3821 | Train Loss: 1.3199594020843506 | Test Loss: 1.3730534315109253\n",
      "Epoch 3822 | Train Loss: 1.1783289909362793 | Test Loss: 1.1955034732818604\n",
      "Epoch 3823 | Train Loss: 1.3198421001434326 | Test Loss: 1.3729305267333984\n",
      "Epoch 3824 | Train Loss: 1.178208351135254 | Test Loss: 1.195408582687378\n",
      "Epoch 3825 | Train Loss: 1.319717526435852 | Test Loss: 1.3728586435317993\n",
      "Epoch 3826 | Train Loss: 1.1780866384506226 | Test Loss: 1.1952787637710571\n",
      "Epoch 3827 | Train Loss: 1.3195977210998535 | Test Loss: 1.3727381229400635\n",
      "Epoch 3828 | Train Loss: 1.177968978881836 | Test Loss: 1.195189118385315\n",
      "Epoch 3829 | Train Loss: 1.3194764852523804 | Test Loss: 1.3726588487625122\n",
      "Epoch 3830 | Train Loss: 1.1778395175933838 | Test Loss: 1.195054531097412\n",
      "Epoch 3831 | Train Loss: 1.3193635940551758 | Test Loss: 1.372522234916687\n",
      "Epoch 3832 | Train Loss: 1.177721381187439 | Test Loss: 1.1949645280838013\n",
      "Epoch 3833 | Train Loss: 1.3192379474639893 | Test Loss: 1.3724699020385742\n",
      "Epoch 3834 | Train Loss: 1.1775997877120972 | Test Loss: 1.1948295831680298\n",
      "Epoch 3835 | Train Loss: 1.3191248178482056 | Test Loss: 1.3723275661468506\n",
      "Epoch 3836 | Train Loss: 1.1774803400039673 | Test Loss: 1.194748878479004\n",
      "Epoch 3837 | Train Loss: 1.3190029859542847 | Test Loss: 1.372278094291687\n",
      "Epoch 3838 | Train Loss: 1.1773582696914673 | Test Loss: 1.1946041584014893\n",
      "Epoch 3839 | Train Loss: 1.3188904523849487 | Test Loss: 1.3721446990966797\n",
      "Epoch 3840 | Train Loss: 1.177243709564209 | Test Loss: 1.1945397853851318\n",
      "Epoch 3841 | Train Loss: 1.3187620639801025 | Test Loss: 1.3720941543579102\n",
      "Epoch 3842 | Train Loss: 1.1771185398101807 | Test Loss: 1.1943782567977905\n",
      "Epoch 3843 | Train Loss: 1.3186579942703247 | Test Loss: 1.3719589710235596\n",
      "Epoch 3844 | Train Loss: 1.1769994497299194 | Test Loss: 1.1943100690841675\n",
      "Epoch 3845 | Train Loss: 1.3185304403305054 | Test Loss: 1.3719160556793213\n",
      "Epoch 3846 | Train Loss: 1.1768779754638672 | Test Loss: 1.194153904914856\n",
      "Epoch 3847 | Train Loss: 1.3184231519699097 | Test Loss: 1.3717751502990723\n",
      "Epoch 3848 | Train Loss: 1.1767618656158447 | Test Loss: 1.1940940618515015\n",
      "Epoch 3849 | Train Loss: 1.3182997703552246 | Test Loss: 1.3717336654663086\n",
      "Epoch 3850 | Train Loss: 1.1766360998153687 | Test Loss: 1.1939265727996826\n",
      "Epoch 3851 | Train Loss: 1.3182004690170288 | Test Loss: 1.3715819120407104\n",
      "Epoch 3852 | Train Loss: 1.176517128944397 | Test Loss: 1.1938751935958862\n",
      "Epoch 3853 | Train Loss: 1.3180710077285767 | Test Loss: 1.3715306520462036\n",
      "Epoch 3854 | Train Loss: 1.1763981580734253 | Test Loss: 1.1937096118927002\n",
      "Epoch 3855 | Train Loss: 1.317958950996399 | Test Loss: 1.3714150190353394\n",
      "Epoch 3856 | Train Loss: 1.1762882471084595 | Test Loss: 1.1936602592468262\n",
      "Epoch 3857 | Train Loss: 1.3178235292434692 | Test Loss: 1.3713257312774658\n",
      "Epoch 3858 | Train Loss: 1.1761689186096191 | Test Loss: 1.1934967041015625\n",
      "Epoch 3859 | Train Loss: 1.3177125453948975 | Test Loss: 1.3712254762649536\n",
      "Epoch 3860 | Train Loss: 1.1760560274124146 | Test Loss: 1.19344162940979\n",
      "Epoch 3861 | Train Loss: 1.3175784349441528 | Test Loss: 1.3711668252944946\n",
      "Epoch 3862 | Train Loss: 1.1759378910064697 | Test Loss: 1.1932835578918457\n",
      "Epoch 3863 | Train Loss: 1.3174686431884766 | Test Loss: 1.371029019355774\n",
      "Epoch 3864 | Train Loss: 1.1758195161819458 | Test Loss: 1.1932275295257568\n",
      "Epoch 3865 | Train Loss: 1.3173412084579468 | Test Loss: 1.3709396123886108\n",
      "Epoch 3866 | Train Loss: 1.1757093667984009 | Test Loss: 1.1930789947509766\n",
      "Epoch 3867 | Train Loss: 1.3172212839126587 | Test Loss: 1.370869755744934\n",
      "Epoch 3868 | Train Loss: 1.1755906343460083 | Test Loss: 1.1930056810379028\n",
      "Epoch 3869 | Train Loss: 1.3170955181121826 | Test Loss: 1.3707201480865479\n",
      "Epoch 3870 | Train Loss: 1.1754703521728516 | Test Loss: 1.1928727626800537\n",
      "Epoch 3871 | Train Loss: 1.3169775009155273 | Test Loss: 1.3706698417663574\n",
      "Epoch 3872 | Train Loss: 1.175359845161438 | Test Loss: 1.1927878856658936\n",
      "Epoch 3873 | Train Loss: 1.3168470859527588 | Test Loss: 1.3705434799194336\n",
      "Epoch 3874 | Train Loss: 1.1752479076385498 | Test Loss: 1.192673683166504\n",
      "Epoch 3875 | Train Loss: 1.3167321681976318 | Test Loss: 1.3704700469970703\n",
      "Epoch 3876 | Train Loss: 1.1751261949539185 | Test Loss: 1.1925687789916992\n",
      "Epoch 3877 | Train Loss: 1.3166152238845825 | Test Loss: 1.370359182357788\n",
      "Epoch 3878 | Train Loss: 1.1750143766403198 | Test Loss: 1.192461609840393\n",
      "Epoch 3879 | Train Loss: 1.3164936304092407 | Test Loss: 1.3702898025512695\n",
      "Epoch 3880 | Train Loss: 1.1748974323272705 | Test Loss: 1.1923636198043823\n",
      "Epoch 3881 | Train Loss: 1.3163789510726929 | Test Loss: 1.3701531887054443\n",
      "Epoch 3882 | Train Loss: 1.1747803688049316 | Test Loss: 1.1922553777694702\n",
      "Epoch 3883 | Train Loss: 1.3162626028060913 | Test Loss: 1.3701064586639404\n",
      "Epoch 3884 | Train Loss: 1.1746630668640137 | Test Loss: 1.1921359300613403\n",
      "Epoch 3885 | Train Loss: 1.3161530494689941 | Test Loss: 1.3699904680252075\n",
      "Epoch 3886 | Train Loss: 1.1745446920394897 | Test Loss: 1.192051649093628\n",
      "Epoch 3887 | Train Loss: 1.316035509109497 | Test Loss: 1.3699203729629517\n",
      "Epoch 3888 | Train Loss: 1.1744285821914673 | Test Loss: 1.1919200420379639\n",
      "Epoch 3889 | Train Loss: 1.3159258365631104 | Test Loss: 1.3698073625564575\n",
      "Epoch 3890 | Train Loss: 1.1743113994598389 | Test Loss: 1.191837191581726\n",
      "Epoch 3891 | Train Loss: 1.3158118724822998 | Test Loss: 1.3697484731674194\n",
      "Epoch 3892 | Train Loss: 1.1741924285888672 | Test Loss: 1.191701889038086\n",
      "Epoch 3893 | Train Loss: 1.3157002925872803 | Test Loss: 1.3696166276931763\n",
      "Epoch 3894 | Train Loss: 1.1740797758102417 | Test Loss: 1.1916323900222778\n",
      "Epoch 3895 | Train Loss: 1.3155828714370728 | Test Loss: 1.3695709705352783\n",
      "Epoch 3896 | Train Loss: 1.1739556789398193 | Test Loss: 1.191484808921814\n",
      "Epoch 3897 | Train Loss: 1.315482497215271 | Test Loss: 1.3694413900375366\n",
      "Epoch 3898 | Train Loss: 1.173842191696167 | Test Loss: 1.1914219856262207\n",
      "Epoch 3899 | Train Loss: 1.315360188484192 | Test Loss: 1.3693833351135254\n",
      "Epoch 3900 | Train Loss: 1.1737232208251953 | Test Loss: 1.1912802457809448\n",
      "Epoch 3901 | Train Loss: 1.315254807472229 | Test Loss: 1.3692585229873657\n",
      "Epoch 3902 | Train Loss: 1.1736128330230713 | Test Loss: 1.1912109851837158\n",
      "Epoch 3903 | Train Loss: 1.3151333332061768 | Test Loss: 1.3692350387573242\n",
      "Epoch 3904 | Train Loss: 1.1734956502914429 | Test Loss: 1.1910580396652222\n",
      "Epoch 3905 | Train Loss: 1.3150304555892944 | Test Loss: 1.3690444231033325\n",
      "Epoch 3906 | Train Loss: 1.1733832359313965 | Test Loss: 1.1910115480422974\n",
      "Epoch 3907 | Train Loss: 1.3149113655090332 | Test Loss: 1.3690086603164673\n",
      "Epoch 3908 | Train Loss: 1.173262119293213 | Test Loss: 1.1908376216888428\n",
      "Epoch 3909 | Train Loss: 1.3148105144500732 | Test Loss: 1.3688976764678955\n",
      "Epoch 3910 | Train Loss: 1.17314612865448 | Test Loss: 1.1907882690429688\n",
      "Epoch 3911 | Train Loss: 1.314688801765442 | Test Loss: 1.3688157796859741\n",
      "Epoch 3912 | Train Loss: 1.1730313301086426 | Test Loss: 1.1906322240829468\n",
      "Epoch 3913 | Train Loss: 1.3145793676376343 | Test Loss: 1.368726372718811\n",
      "Epoch 3914 | Train Loss: 1.1729166507720947 | Test Loss: 1.190569281578064\n",
      "Epoch 3915 | Train Loss: 1.3144574165344238 | Test Loss: 1.3686565160751343\n",
      "Epoch 3916 | Train Loss: 1.1727972030639648 | Test Loss: 1.1904009580612183\n",
      "Epoch 3917 | Train Loss: 1.3143564462661743 | Test Loss: 1.3685499429702759\n",
      "Epoch 3918 | Train Loss: 1.1726785898208618 | Test Loss: 1.190355896949768\n",
      "Epoch 3919 | Train Loss: 1.314232587814331 | Test Loss: 1.3684529066085815\n",
      "Epoch 3920 | Train Loss: 1.1725621223449707 | Test Loss: 1.190204381942749\n",
      "Epoch 3921 | Train Loss: 1.314123272895813 | Test Loss: 1.3683953285217285\n",
      "Epoch 3922 | Train Loss: 1.1724426746368408 | Test Loss: 1.1901055574417114\n",
      "Epoch 3923 | Train Loss: 1.3140074014663696 | Test Loss: 1.3682537078857422\n",
      "Epoch 3924 | Train Loss: 1.1723253726959229 | Test Loss: 1.1899909973144531\n",
      "Epoch 3925 | Train Loss: 1.3139005899429321 | Test Loss: 1.3682148456573486\n",
      "Epoch 3926 | Train Loss: 1.1722060441970825 | Test Loss: 1.1898945569992065\n",
      "Epoch 3927 | Train Loss: 1.3137844800949097 | Test Loss: 1.3680914640426636\n",
      "Epoch 3928 | Train Loss: 1.1720942258834839 | Test Loss: 1.1897743940353394\n",
      "Epoch 3929 | Train Loss: 1.3136732578277588 | Test Loss: 1.368011236190796\n",
      "Epoch 3930 | Train Loss: 1.1719764471054077 | Test Loss: 1.1896847486495972\n",
      "Epoch 3931 | Train Loss: 1.3135615587234497 | Test Loss: 1.3679139614105225\n",
      "Epoch 3932 | Train Loss: 1.171860694885254 | Test Loss: 1.1895771026611328\n",
      "Epoch 3933 | Train Loss: 1.3134516477584839 | Test Loss: 1.3678592443466187\n",
      "Epoch 3934 | Train Loss: 1.1717416048049927 | Test Loss: 1.1894543170928955\n",
      "Epoch 3935 | Train Loss: 1.313348412513733 | Test Loss: 1.3677291870117188\n",
      "Epoch 3936 | Train Loss: 1.1716251373291016 | Test Loss: 1.1893675327301025\n",
      "Epoch 3937 | Train Loss: 1.3132368326187134 | Test Loss: 1.3676865100860596\n",
      "Epoch 3938 | Train Loss: 1.1715097427368164 | Test Loss: 1.1892327070236206\n",
      "Epoch 3939 | Train Loss: 1.3131276369094849 | Test Loss: 1.3675565719604492\n",
      "Epoch 3940 | Train Loss: 1.171400785446167 | Test Loss: 1.189148187637329\n",
      "Epoch 3941 | Train Loss: 1.313014268875122 | Test Loss: 1.3675007820129395\n",
      "Epoch 3942 | Train Loss: 1.1712833642959595 | Test Loss: 1.189022421836853\n",
      "Epoch 3943 | Train Loss: 1.312907338142395 | Test Loss: 1.3673804998397827\n",
      "Epoch 3944 | Train Loss: 1.1711726188659668 | Test Loss: 1.1889381408691406\n",
      "Epoch 3945 | Train Loss: 1.3127940893173218 | Test Loss: 1.367334008216858\n",
      "Epoch 3946 | Train Loss: 1.1710586547851562 | Test Loss: 1.18880033493042\n",
      "Epoch 3947 | Train Loss: 1.3126931190490723 | Test Loss: 1.3672188520431519\n",
      "Epoch 3948 | Train Loss: 1.170944333076477 | Test Loss: 1.1887454986572266\n",
      "Epoch 3949 | Train Loss: 1.3125790357589722 | Test Loss: 1.3671813011169434\n",
      "Epoch 3950 | Train Loss: 1.1708314418792725 | Test Loss: 1.1885911226272583\n",
      "Epoch 3951 | Train Loss: 1.312479853630066 | Test Loss: 1.367033839225769\n",
      "Epoch 3952 | Train Loss: 1.170719027519226 | Test Loss: 1.1885396242141724\n",
      "Epoch 3953 | Train Loss: 1.3123672008514404 | Test Loss: 1.3669952154159546\n",
      "Epoch 3954 | Train Loss: 1.1706032752990723 | Test Loss: 1.1883869171142578\n",
      "Epoch 3955 | Train Loss: 1.312263011932373 | Test Loss: 1.366874098777771\n",
      "Epoch 3956 | Train Loss: 1.1704970598220825 | Test Loss: 1.1883398294448853\n",
      "Epoch 3957 | Train Loss: 1.3121453523635864 | Test Loss: 1.366833209991455\n",
      "Epoch 3958 | Train Loss: 1.1703782081604004 | Test Loss: 1.188171625137329\n",
      "Epoch 3959 | Train Loss: 1.3120508193969727 | Test Loss: 1.366713523864746\n",
      "Epoch 3960 | Train Loss: 1.170263409614563 | Test Loss: 1.1881203651428223\n",
      "Epoch 3961 | Train Loss: 1.311930775642395 | Test Loss: 1.3666530847549438\n",
      "Epoch 3962 | Train Loss: 1.1701489686965942 | Test Loss: 1.187964916229248\n",
      "Epoch 3963 | Train Loss: 1.3118263483047485 | Test Loss: 1.3665709495544434\n",
      "Epoch 3964 | Train Loss: 1.170034408569336 | Test Loss: 1.187902569770813\n",
      "Epoch 3965 | Train Loss: 1.3117146492004395 | Test Loss: 1.3664708137512207\n",
      "Epoch 3966 | Train Loss: 1.1699179410934448 | Test Loss: 1.1877517700195312\n",
      "Epoch 3967 | Train Loss: 1.311603307723999 | Test Loss: 1.3664073944091797\n",
      "Epoch 3968 | Train Loss: 1.169813632965088 | Test Loss: 1.187674880027771\n",
      "Epoch 3969 | Train Loss: 1.3114815950393677 | Test Loss: 1.3662946224212646\n",
      "Epoch 3970 | Train Loss: 1.1696996688842773 | Test Loss: 1.1875547170639038\n",
      "Epoch 3971 | Train Loss: 1.3113783597946167 | Test Loss: 1.3662290573120117\n",
      "Epoch 3972 | Train Loss: 1.1695854663848877 | Test Loss: 1.1874573230743408\n",
      "Epoch 3973 | Train Loss: 1.3112667798995972 | Test Loss: 1.3661415576934814\n",
      "Epoch 3974 | Train Loss: 1.1694740056991577 | Test Loss: 1.1873371601104736\n",
      "Epoch 3975 | Train Loss: 1.3111629486083984 | Test Loss: 1.3660660982131958\n",
      "Epoch 3976 | Train Loss: 1.1693569421768188 | Test Loss: 1.1872484683990479\n",
      "Epoch 3977 | Train Loss: 1.3110551834106445 | Test Loss: 1.365985631942749\n",
      "Epoch 3978 | Train Loss: 1.1692482233047485 | Test Loss: 1.1871312856674194\n",
      "Epoch 3979 | Train Loss: 1.3109474182128906 | Test Loss: 1.365897297859192\n",
      "Epoch 3980 | Train Loss: 1.1691370010375977 | Test Loss: 1.187038779258728\n",
      "Epoch 3981 | Train Loss: 1.3108402490615845 | Test Loss: 1.3658229112625122\n",
      "Epoch 3982 | Train Loss: 1.1690236330032349 | Test Loss: 1.186932921409607\n",
      "Epoch 3983 | Train Loss: 1.3107314109802246 | Test Loss: 1.3657476902008057\n",
      "Epoch 3984 | Train Loss: 1.1689159870147705 | Test Loss: 1.1868255138397217\n",
      "Epoch 3985 | Train Loss: 1.310626745223999 | Test Loss: 1.365647315979004\n",
      "Epoch 3986 | Train Loss: 1.1688002347946167 | Test Loss: 1.1867233514785767\n",
      "Epoch 3987 | Train Loss: 1.3105190992355347 | Test Loss: 1.3655791282653809\n",
      "Epoch 3988 | Train Loss: 1.1686917543411255 | Test Loss: 1.1866085529327393\n",
      "Epoch 3989 | Train Loss: 1.3104114532470703 | Test Loss: 1.3655012845993042\n",
      "Epoch 3990 | Train Loss: 1.1685813665390015 | Test Loss: 1.1865214109420776\n",
      "Epoch 3991 | Train Loss: 1.3103079795837402 | Test Loss: 1.365420937538147\n",
      "Epoch 3992 | Train Loss: 1.1684651374816895 | Test Loss: 1.1864051818847656\n",
      "Epoch 3993 | Train Loss: 1.3102065324783325 | Test Loss: 1.3653370141983032\n",
      "Epoch 3994 | Train Loss: 1.1683558225631714 | Test Loss: 1.1863048076629639\n",
      "Epoch 3995 | Train Loss: 1.3101012706756592 | Test Loss: 1.365264654159546\n",
      "Epoch 3996 | Train Loss: 1.168243169784546 | Test Loss: 1.1862057447433472\n",
      "Epoch 3997 | Train Loss: 1.3099925518035889 | Test Loss: 1.3651758432388306\n",
      "Epoch 3998 | Train Loss: 1.1681418418884277 | Test Loss: 1.1861052513122559\n",
      "Epoch 3999 | Train Loss: 1.3098816871643066 | Test Loss: 1.3651165962219238\n",
      "Epoch 4000 | Train Loss: 1.1680314540863037 | Test Loss: 1.185990571975708\n",
      "Epoch 4001 | Train Loss: 1.309781551361084 | Test Loss: 1.3650106191635132\n",
      "Epoch 4002 | Train Loss: 1.1679192781448364 | Test Loss: 1.1859095096588135\n",
      "Epoch 4003 | Train Loss: 1.3096766471862793 | Test Loss: 1.3649513721466064\n",
      "Epoch 4004 | Train Loss: 1.1678107976913452 | Test Loss: 1.185794711112976\n",
      "Epoch 4005 | Train Loss: 1.309576392173767 | Test Loss: 1.3648626804351807\n",
      "Epoch 4006 | Train Loss: 1.167698860168457 | Test Loss: 1.185700535774231\n",
      "Epoch 4007 | Train Loss: 1.3094727993011475 | Test Loss: 1.3647817373275757\n",
      "Epoch 4008 | Train Loss: 1.1675889492034912 | Test Loss: 1.1855922937393188\n",
      "Epoch 4009 | Train Loss: 1.3093653917312622 | Test Loss: 1.3647041320800781\n",
      "Epoch 4010 | Train Loss: 1.167487382888794 | Test Loss: 1.1855076551437378\n",
      "Epoch 4011 | Train Loss: 1.3092586994171143 | Test Loss: 1.3646235466003418\n",
      "Epoch 4012 | Train Loss: 1.1673729419708252 | Test Loss: 1.1853939294815063\n",
      "Epoch 4013 | Train Loss: 1.3091627359390259 | Test Loss: 1.3645237684249878\n",
      "Epoch 4014 | Train Loss: 1.1672636270523071 | Test Loss: 1.185294508934021\n",
      "Epoch 4015 | Train Loss: 1.3090583086013794 | Test Loss: 1.3644752502441406\n",
      "Epoch 4016 | Train Loss: 1.1671539545059204 | Test Loss: 1.1851940155029297\n",
      "Epoch 4017 | Train Loss: 1.3089536428451538 | Test Loss: 1.3643523454666138\n",
      "Epoch 4018 | Train Loss: 1.1670522689819336 | Test Loss: 1.185099482536316\n",
      "Epoch 4019 | Train Loss: 1.3088489770889282 | Test Loss: 1.3643174171447754\n",
      "Epoch 4020 | Train Loss: 1.1669411659240723 | Test Loss: 1.184984803199768\n",
      "Epoch 4021 | Train Loss: 1.3087440729141235 | Test Loss: 1.3641998767852783\n",
      "Epoch 4022 | Train Loss: 1.1668421030044556 | Test Loss: 1.184907078742981\n",
      "Epoch 4023 | Train Loss: 1.3086360692977905 | Test Loss: 1.3641626834869385\n",
      "Epoch 4024 | Train Loss: 1.1667307615280151 | Test Loss: 1.1847888231277466\n",
      "Epoch 4025 | Train Loss: 1.3085379600524902 | Test Loss: 1.3640424013137817\n",
      "Epoch 4026 | Train Loss: 1.1666228771209717 | Test Loss: 1.1847115755081177\n",
      "Epoch 4027 | Train Loss: 1.3084349632263184 | Test Loss: 1.364011526107788\n",
      "Epoch 4028 | Train Loss: 1.1665136814117432 | Test Loss: 1.1845965385437012\n",
      "Epoch 4029 | Train Loss: 1.3083356618881226 | Test Loss: 1.3638641834259033\n",
      "Epoch 4030 | Train Loss: 1.1664116382598877 | Test Loss: 1.1845289468765259\n",
      "Epoch 4031 | Train Loss: 1.3082281351089478 | Test Loss: 1.3638404607772827\n",
      "Epoch 4032 | Train Loss: 1.1663014888763428 | Test Loss: 1.1843841075897217\n",
      "Epoch 4033 | Train Loss: 1.3081272840499878 | Test Loss: 1.3637021780014038\n",
      "Epoch 4034 | Train Loss: 1.1662083864212036 | Test Loss: 1.1843351125717163\n",
      "Epoch 4035 | Train Loss: 1.3080127239227295 | Test Loss: 1.3636690378189087\n",
      "Epoch 4036 | Train Loss: 1.1660957336425781 | Test Loss: 1.1841951608657837\n",
      "Epoch 4037 | Train Loss: 1.3079197406768799 | Test Loss: 1.3635491132736206\n",
      "Epoch 4038 | Train Loss: 1.1659889221191406 | Test Loss: 1.1841323375701904\n",
      "Epoch 4039 | Train Loss: 1.3078131675720215 | Test Loss: 1.3635287284851074\n",
      "Epoch 4040 | Train Loss: 1.1658833026885986 | Test Loss: 1.1839944124221802\n",
      "Epoch 4041 | Train Loss: 1.3077189922332764 | Test Loss: 1.3634059429168701\n",
      "Epoch 4042 | Train Loss: 1.1657716035842896 | Test Loss: 1.1839466094970703\n",
      "Epoch 4043 | Train Loss: 1.3076118230819702 | Test Loss: 1.363355278968811\n",
      "Epoch 4044 | Train Loss: 1.1656633615493774 | Test Loss: 1.1837847232818604\n",
      "Epoch 4045 | Train Loss: 1.3075116872787476 | Test Loss: 1.3632793426513672\n",
      "Epoch 4046 | Train Loss: 1.1655608415603638 | Test Loss: 1.1837328672409058\n",
      "Epoch 4047 | Train Loss: 1.3073983192443848 | Test Loss: 1.3631726503372192\n",
      "Epoch 4048 | Train Loss: 1.1654495000839233 | Test Loss: 1.1835955381393433\n",
      "Epoch 4049 | Train Loss: 1.3072961568832397 | Test Loss: 1.3631175756454468\n",
      "Epoch 4050 | Train Loss: 1.1653481721878052 | Test Loss: 1.1835136413574219\n",
      "Epoch 4051 | Train Loss: 1.307185173034668 | Test Loss: 1.3630646467208862\n",
      "Epoch 4052 | Train Loss: 1.1652415990829468 | Test Loss: 1.183388352394104\n",
      "Epoch 4053 | Train Loss: 1.307092547416687 | Test Loss: 1.3629530668258667\n",
      "Epoch 4054 | Train Loss: 1.1651217937469482 | Test Loss: 1.1833125352859497\n",
      "Epoch 4055 | Train Loss: 1.3069971799850464 | Test Loss: 1.3629051446914673\n",
      "Epoch 4056 | Train Loss: 1.1650134325027466 | Test Loss: 1.1831715106964111\n",
      "Epoch 4057 | Train Loss: 1.3068935871124268 | Test Loss: 1.362834095954895\n",
      "Epoch 4058 | Train Loss: 1.1649127006530762 | Test Loss: 1.1831109523773193\n",
      "Epoch 4059 | Train Loss: 1.3067868947982788 | Test Loss: 1.3627475500106812\n",
      "Epoch 4060 | Train Loss: 1.1647989749908447 | Test Loss: 1.182982325553894\n",
      "Epoch 4061 | Train Loss: 1.306689739227295 | Test Loss: 1.3626857995986938\n",
      "Epoch 4062 | Train Loss: 1.164696455001831 | Test Loss: 1.1828901767730713\n",
      "Epoch 4063 | Train Loss: 1.3065788745880127 | Test Loss: 1.362612247467041\n",
      "Epoch 4064 | Train Loss: 1.164589524269104 | Test Loss: 1.182775616645813\n",
      "Epoch 4065 | Train Loss: 1.3064804077148438 | Test Loss: 1.362511157989502\n",
      "Epoch 4066 | Train Loss: 1.1644824743270874 | Test Loss: 1.1826914548873901\n",
      "Epoch 4067 | Train Loss: 1.3063766956329346 | Test Loss: 1.3624557256698608\n",
      "Epoch 4068 | Train Loss: 1.164372205734253 | Test Loss: 1.182570457458496\n",
      "Epoch 4069 | Train Loss: 1.306284785270691 | Test Loss: 1.362343668937683\n",
      "Epoch 4070 | Train Loss: 1.164259672164917 | Test Loss: 1.1824954748153687\n",
      "Epoch 4071 | Train Loss: 1.306186556816101 | Test Loss: 1.3623061180114746\n",
      "Epoch 4072 | Train Loss: 1.1641491651535034 | Test Loss: 1.1823650598526\n",
      "Epoch 4073 | Train Loss: 1.3060885667800903 | Test Loss: 1.36220383644104\n",
      "Epoch 4074 | Train Loss: 1.1640448570251465 | Test Loss: 1.1822768449783325\n",
      "Epoch 4075 | Train Loss: 1.3059871196746826 | Test Loss: 1.362158179283142\n",
      "Epoch 4076 | Train Loss: 1.1639333963394165 | Test Loss: 1.182166337966919\n",
      "Epoch 4077 | Train Loss: 1.305885910987854 | Test Loss: 1.3620549440383911\n",
      "Epoch 4078 | Train Loss: 1.163834571838379 | Test Loss: 1.1820818185806274\n",
      "Epoch 4079 | Train Loss: 1.3057838678359985 | Test Loss: 1.3620012998580933\n",
      "Epoch 4080 | Train Loss: 1.163720965385437 | Test Loss: 1.1819627285003662\n",
      "Epoch 4081 | Train Loss: 1.3056889772415161 | Test Loss: 1.3618780374526978\n",
      "Epoch 4082 | Train Loss: 1.163620948791504 | Test Loss: 1.1818903684616089\n",
      "Epoch 4083 | Train Loss: 1.3055793046951294 | Test Loss: 1.36185622215271\n",
      "Epoch 4084 | Train Loss: 1.1635171175003052 | Test Loss: 1.1817665100097656\n",
      "Epoch 4085 | Train Loss: 1.3054847717285156 | Test Loss: 1.361715316772461\n",
      "Epoch 4086 | Train Loss: 1.1634095907211304 | Test Loss: 1.1816967725753784\n",
      "Epoch 4087 | Train Loss: 1.3053874969482422 | Test Loss: 1.361709475517273\n",
      "Epoch 4088 | Train Loss: 1.1633005142211914 | Test Loss: 1.1815741062164307\n",
      "Epoch 4089 | Train Loss: 1.3052935600280762 | Test Loss: 1.3615695238113403\n",
      "Epoch 4090 | Train Loss: 1.1632006168365479 | Test Loss: 1.1815143823623657\n",
      "Epoch 4091 | Train Loss: 1.305192470550537 | Test Loss: 1.3615446090698242\n",
      "Epoch 4092 | Train Loss: 1.1630903482437134 | Test Loss: 1.181372880935669\n",
      "Epoch 4093 | Train Loss: 1.3050960302352905 | Test Loss: 1.3614134788513184\n",
      "Epoch 4094 | Train Loss: 1.1629973649978638 | Test Loss: 1.1813164949417114\n",
      "Epoch 4095 | Train Loss: 1.3049875497817993 | Test Loss: 1.3613855838775635\n",
      "Epoch 4096 | Train Loss: 1.1628879308700562 | Test Loss: 1.1811667680740356\n",
      "Epoch 4097 | Train Loss: 1.3049006462097168 | Test Loss: 1.3612266778945923\n",
      "Epoch 4098 | Train Loss: 1.1627832651138306 | Test Loss: 1.18112313747406\n",
      "Epoch 4099 | Train Loss: 1.3048022985458374 | Test Loss: 1.3612313270568848\n",
      "Epoch 4100 | Train Loss: 1.1626747846603394 | Test Loss: 1.1809664964675903\n",
      "Epoch 4101 | Train Loss: 1.3047133684158325 | Test Loss: 1.3610899448394775\n",
      "Epoch 4102 | Train Loss: 1.1625686883926392 | Test Loss: 1.180930256843567\n",
      "Epoch 4103 | Train Loss: 1.3046077489852905 | Test Loss: 1.36105477809906\n",
      "Epoch 4104 | Train Loss: 1.1624631881713867 | Test Loss: 1.1807646751403809\n",
      "Epoch 4105 | Train Loss: 1.304516315460205 | Test Loss: 1.3609554767608643\n",
      "Epoch 4106 | Train Loss: 1.1623613834381104 | Test Loss: 1.1807359457015991\n",
      "Epoch 4107 | Train Loss: 1.3044052124023438 | Test Loss: 1.3608856201171875\n",
      "Epoch 4108 | Train Loss: 1.1622551679611206 | Test Loss: 1.1805649995803833\n",
      "Epoch 4109 | Train Loss: 1.304310917854309 | Test Loss: 1.360789179801941\n",
      "Epoch 4110 | Train Loss: 1.162153959274292 | Test Loss: 1.1805187463760376\n",
      "Epoch 4111 | Train Loss: 1.3042007684707642 | Test Loss: 1.3607367277145386\n",
      "Epoch 4112 | Train Loss: 1.1620501279830933 | Test Loss: 1.180367112159729\n",
      "Epoch 4113 | Train Loss: 1.304106593132019 | Test Loss: 1.3606432676315308\n",
      "Epoch 4114 | Train Loss: 1.161939024925232 | Test Loss: 1.1803148984909058\n",
      "Epoch 4115 | Train Loss: 1.3040101528167725 | Test Loss: 1.3605704307556152\n",
      "Epoch 4116 | Train Loss: 1.1618276834487915 | Test Loss: 1.1801632642745972\n",
      "Epoch 4117 | Train Loss: 1.3039144277572632 | Test Loss: 1.3605490922927856\n",
      "Epoch 4118 | Train Loss: 1.1617273092269897 | Test Loss: 1.1800999641418457\n",
      "Epoch 4119 | Train Loss: 1.3038020133972168 | Test Loss: 1.3604316711425781\n",
      "Epoch 4120 | Train Loss: 1.161621332168579 | Test Loss: 1.1799720525741577\n",
      "Epoch 4121 | Train Loss: 1.303706169128418 | Test Loss: 1.360371708869934\n",
      "Epoch 4122 | Train Loss: 1.1615177392959595 | Test Loss: 1.1798943281173706\n",
      "Epoch 4123 | Train Loss: 1.3036003112792969 | Test Loss: 1.3603094816207886\n",
      "Epoch 4124 | Train Loss: 1.161413311958313 | Test Loss: 1.1797752380371094\n",
      "Epoch 4125 | Train Loss: 1.303504467010498 | Test Loss: 1.3602219820022583\n",
      "Epoch 4126 | Train Loss: 1.1613065004348755 | Test Loss: 1.179692029953003\n",
      "Epoch 4127 | Train Loss: 1.3034050464630127 | Test Loss: 1.360168695449829\n",
      "Epoch 4128 | Train Loss: 1.1612014770507812 | Test Loss: 1.179567575454712\n",
      "Epoch 4129 | Train Loss: 1.3033173084259033 | Test Loss: 1.3600852489471436\n",
      "Epoch 4130 | Train Loss: 1.161087989807129 | Test Loss: 1.1795026063919067\n",
      "Epoch 4131 | Train Loss: 1.3032199144363403 | Test Loss: 1.3600212335586548\n",
      "Epoch 4132 | Train Loss: 1.160984754562378 | Test Loss: 1.1793739795684814\n",
      "Epoch 4133 | Train Loss: 1.303118109703064 | Test Loss: 1.3599570989608765\n",
      "Epoch 4134 | Train Loss: 1.1608850955963135 | Test Loss: 1.1792832612991333\n",
      "Epoch 4135 | Train Loss: 1.3030188083648682 | Test Loss: 1.359859824180603\n",
      "Epoch 4136 | Train Loss: 1.1607753038406372 | Test Loss: 1.1791857481002808\n",
      "Epoch 4137 | Train Loss: 1.3029218912124634 | Test Loss: 1.3597962856292725\n",
      "Epoch 4138 | Train Loss: 1.1606758832931519 | Test Loss: 1.1790838241577148\n",
      "Epoch 4139 | Train Loss: 1.302817940711975 | Test Loss: 1.3597345352172852\n",
      "Epoch 4140 | Train Loss: 1.160569190979004 | Test Loss: 1.1789746284484863\n",
      "Epoch 4141 | Train Loss: 1.3027294874191284 | Test Loss: 1.35963773727417\n",
      "Epoch 4142 | Train Loss: 1.160455584526062 | Test Loss: 1.1788880825042725\n",
      "Epoch 4143 | Train Loss: 1.3026347160339355 | Test Loss: 1.359592080116272\n",
      "Epoch 4144 | Train Loss: 1.1603542566299438 | Test Loss: 1.1787768602371216\n",
      "Epoch 4145 | Train Loss: 1.302538275718689 | Test Loss: 1.3595027923583984\n",
      "Epoch 4146 | Train Loss: 1.1602473258972168 | Test Loss: 1.178688645362854\n",
      "Epoch 4147 | Train Loss: 1.3024400472640991 | Test Loss: 1.3594356775283813\n",
      "Epoch 4148 | Train Loss: 1.1601440906524658 | Test Loss: 1.1785686016082764\n",
      "Epoch 4149 | Train Loss: 1.3023401498794556 | Test Loss: 1.3593796491622925\n",
      "Epoch 4150 | Train Loss: 1.1600438356399536 | Test Loss: 1.1784756183624268\n",
      "Epoch 4151 | Train Loss: 1.3022477626800537 | Test Loss: 1.359290599822998\n",
      "Epoch 4152 | Train Loss: 1.1599314212799072 | Test Loss: 1.1783841848373413\n",
      "Epoch 4153 | Train Loss: 1.3021539449691772 | Test Loss: 1.359235405921936\n",
      "Epoch 4154 | Train Loss: 1.1598289012908936 | Test Loss: 1.1782654523849487\n",
      "Epoch 4155 | Train Loss: 1.3020590543746948 | Test Loss: 1.3591663837432861\n",
      "Epoch 4156 | Train Loss: 1.1597219705581665 | Test Loss: 1.178184151649475\n",
      "Epoch 4157 | Train Loss: 1.3019657135009766 | Test Loss: 1.3590880632400513\n",
      "Epoch 4158 | Train Loss: 1.159615397453308 | Test Loss: 1.1780816316604614\n",
      "Epoch 4159 | Train Loss: 1.3018708229064941 | Test Loss: 1.3590208292007446\n",
      "Epoch 4160 | Train Loss: 1.1595185995101929 | Test Loss: 1.1779820919036865\n",
      "Epoch 4161 | Train Loss: 1.3017702102661133 | Test Loss: 1.3589649200439453\n",
      "Epoch 4162 | Train Loss: 1.1594139337539673 | Test Loss: 1.1778817176818848\n",
      "Epoch 4163 | Train Loss: 1.3016765117645264 | Test Loss: 1.358865737915039\n",
      "Epoch 4164 | Train Loss: 1.1593122482299805 | Test Loss: 1.177779197692871\n",
      "Epoch 4165 | Train Loss: 1.301582932472229 | Test Loss: 1.3588438034057617\n",
      "Epoch 4166 | Train Loss: 1.159201979637146 | Test Loss: 1.1776785850524902\n",
      "Epoch 4167 | Train Loss: 1.3015003204345703 | Test Loss: 1.358728289604187\n",
      "Epoch 4168 | Train Loss: 1.1590932607650757 | Test Loss: 1.1775856018066406\n",
      "Epoch 4169 | Train Loss: 1.3014070987701416 | Test Loss: 1.3586896657943726\n",
      "Epoch 4170 | Train Loss: 1.1589919328689575 | Test Loss: 1.1774753332138062\n",
      "Epoch 4171 | Train Loss: 1.301315188407898 | Test Loss: 1.3585797548294067\n",
      "Epoch 4172 | Train Loss: 1.1588948965072632 | Test Loss: 1.177404522895813\n",
      "Epoch 4173 | Train Loss: 1.3012160062789917 | Test Loss: 1.3585726022720337\n",
      "Epoch 4174 | Train Loss: 1.1587897539138794 | Test Loss: 1.177268385887146\n",
      "Epoch 4175 | Train Loss: 1.3011305332183838 | Test Loss: 1.3584321737289429\n",
      "Epoch 4176 | Train Loss: 1.1586885452270508 | Test Loss: 1.1772136688232422\n",
      "Epoch 4177 | Train Loss: 1.3010367155075073 | Test Loss: 1.3584489822387695\n",
      "Epoch 4178 | Train Loss: 1.158586025238037 | Test Loss: 1.1770724058151245\n",
      "Epoch 4179 | Train Loss: 1.3009428977966309 | Test Loss: 1.3583043813705444\n",
      "Epoch 4180 | Train Loss: 1.1584928035736084 | Test Loss: 1.1770131587982178\n",
      "Epoch 4181 | Train Loss: 1.3008509874343872 | Test Loss: 1.3583145141601562\n",
      "Epoch 4182 | Train Loss: 1.158379077911377 | Test Loss: 1.1768728494644165\n",
      "Epoch 4183 | Train Loss: 1.300773024559021 | Test Loss: 1.3581593036651611\n",
      "Epoch 4184 | Train Loss: 1.158281922340393 | Test Loss: 1.176823616027832\n",
      "Epoch 4185 | Train Loss: 1.3006739616394043 | Test Loss: 1.3581757545471191\n",
      "Epoch 4186 | Train Loss: 1.1581801176071167 | Test Loss: 1.17665433883667\n",
      "Epoch 4187 | Train Loss: 1.3005861043930054 | Test Loss: 1.3580330610275269\n",
      "Epoch 4188 | Train Loss: 1.1580842733383179 | Test Loss: 1.176627516746521\n",
      "Epoch 4189 | Train Loss: 1.3004926443099976 | Test Loss: 1.358001708984375\n",
      "Epoch 4190 | Train Loss: 1.1579705476760864 | Test Loss: 1.1764687299728394\n",
      "Epoch 4191 | Train Loss: 1.3004076480865479 | Test Loss: 1.3579038381576538\n",
      "Epoch 4192 | Train Loss: 1.1578764915466309 | Test Loss: 1.176432490348816\n",
      "Epoch 4193 | Train Loss: 1.3002995252609253 | Test Loss: 1.3578999042510986\n",
      "Epoch 4194 | Train Loss: 1.1577726602554321 | Test Loss: 1.1762700080871582\n",
      "Epoch 4195 | Train Loss: 1.3002135753631592 | Test Loss: 1.3577791452407837\n",
      "Epoch 4196 | Train Loss: 1.157671570777893 | Test Loss: 1.1762295961380005\n",
      "Epoch 4197 | Train Loss: 1.3001173734664917 | Test Loss: 1.3577616214752197\n",
      "Epoch 4198 | Train Loss: 1.1575673818588257 | Test Loss: 1.1760724782943726\n",
      "Epoch 4199 | Train Loss: 1.3000296354293823 | Test Loss: 1.357667088508606\n",
      "Epoch 4200 | Train Loss: 1.1574689149856567 | Test Loss: 1.1760333776474\n",
      "Epoch 4201 | Train Loss: 1.2999309301376343 | Test Loss: 1.3575940132141113\n",
      "Epoch 4202 | Train Loss: 1.1573654413223267 | Test Loss: 1.1758811473846436\n",
      "Epoch 4203 | Train Loss: 1.2998353242874146 | Test Loss: 1.3575397729873657\n",
      "Epoch 4204 | Train Loss: 1.1572728157043457 | Test Loss: 1.175830364227295\n",
      "Epoch 4205 | Train Loss: 1.299731969833374 | Test Loss: 1.357453465461731\n",
      "Epoch 4206 | Train Loss: 1.1571660041809082 | Test Loss: 1.1757009029388428\n",
      "Epoch 4207 | Train Loss: 1.299641728401184 | Test Loss: 1.3574179410934448\n",
      "Epoch 4208 | Train Loss: 1.1570663452148438 | Test Loss: 1.1756234169006348\n",
      "Epoch 4209 | Train Loss: 1.2995412349700928 | Test Loss: 1.357334852218628\n",
      "Epoch 4210 | Train Loss: 1.1569691896438599 | Test Loss: 1.1755080223083496\n",
      "Epoch 4211 | Train Loss: 1.2994540929794312 | Test Loss: 1.357254147529602\n",
      "Epoch 4212 | Train Loss: 1.1568632125854492 | Test Loss: 1.1754398345947266\n",
      "Epoch 4213 | Train Loss: 1.2993577718734741 | Test Loss: 1.3571937084197998\n",
      "Epoch 4214 | Train Loss: 1.1567704677581787 | Test Loss: 1.1753116846084595\n",
      "Epoch 4215 | Train Loss: 1.2992650270462036 | Test Loss: 1.3571200370788574\n",
      "Epoch 4216 | Train Loss: 1.1566659212112427 | Test Loss: 1.175239086151123\n",
      "Epoch 4217 | Train Loss: 1.2991727590560913 | Test Loss: 1.3570475578308105\n",
      "Epoch 4218 | Train Loss: 1.1565642356872559 | Test Loss: 1.175116777420044\n",
      "Epoch 4219 | Train Loss: 1.2990814447402954 | Test Loss: 1.3569995164871216\n",
      "Epoch 4220 | Train Loss: 1.156462550163269 | Test Loss: 1.1750367879867554\n",
      "Epoch 4221 | Train Loss: 1.298994779586792 | Test Loss: 1.356894850730896\n",
      "Epoch 4222 | Train Loss: 1.156356692314148 | Test Loss: 1.174938440322876\n",
      "Epoch 4223 | Train Loss: 1.2988990545272827 | Test Loss: 1.356869101524353\n",
      "Epoch 4224 | Train Loss: 1.1562654972076416 | Test Loss: 1.1748437881469727\n",
      "Epoch 4225 | Train Loss: 1.2988042831420898 | Test Loss: 1.3567701578140259\n",
      "Epoch 4226 | Train Loss: 1.15615975856781 | Test Loss: 1.1747469902038574\n",
      "Epoch 4227 | Train Loss: 1.2987197637557983 | Test Loss: 1.356696367263794\n",
      "Epoch 4228 | Train Loss: 1.1560595035552979 | Test Loss: 1.17465341091156\n",
      "Epoch 4229 | Train Loss: 1.298621416091919 | Test Loss: 1.3566534519195557\n",
      "Epoch 4230 | Train Loss: 1.1559652090072632 | Test Loss: 1.1745482683181763\n",
      "Epoch 4231 | Train Loss: 1.298534631729126 | Test Loss: 1.3565510511398315\n",
      "Epoch 4232 | Train Loss: 1.1558588743209839 | Test Loss: 1.1744701862335205\n",
      "Epoch 4233 | Train Loss: 1.2984477281570435 | Test Loss: 1.3565171957015991\n",
      "Epoch 4234 | Train Loss: 1.155761480331421 | Test Loss: 1.174357295036316\n",
      "Epoch 4235 | Train Loss: 1.2983568906784058 | Test Loss: 1.3564273118972778\n",
      "Epoch 4236 | Train Loss: 1.1556614637374878 | Test Loss: 1.174273133277893\n",
      "Epoch 4237 | Train Loss: 1.2982746362686157 | Test Loss: 1.3563631772994995\n",
      "Epoch 4238 | Train Loss: 1.1555535793304443 | Test Loss: 1.1741669178009033\n",
      "Epoch 4239 | Train Loss: 1.298181414604187 | Test Loss: 1.3562926054000854\n",
      "Epoch 4240 | Train Loss: 1.1554653644561768 | Test Loss: 1.1740758419036865\n",
      "Epoch 4241 | Train Loss: 1.2980877161026 | Test Loss: 1.3562456369400024\n",
      "Epoch 4242 | Train Loss: 1.1553587913513184 | Test Loss: 1.173973798751831\n",
      "Epoch 4243 | Train Loss: 1.2980071306228638 | Test Loss: 1.3561455011367798\n",
      "Epoch 4244 | Train Loss: 1.1552573442459106 | Test Loss: 1.1738860607147217\n",
      "Epoch 4245 | Train Loss: 1.2979189157485962 | Test Loss: 1.356146216392517\n",
      "Epoch 4246 | Train Loss: 1.155153751373291 | Test Loss: 1.1737719774246216\n",
      "Epoch 4247 | Train Loss: 1.2978378534317017 | Test Loss: 1.3560090065002441\n",
      "Epoch 4248 | Train Loss: 1.1550577878952026 | Test Loss: 1.1737080812454224\n",
      "Epoch 4249 | Train Loss: 1.2977495193481445 | Test Loss: 1.3560131788253784\n",
      "Epoch 4250 | Train Loss: 1.154951810836792 | Test Loss: 1.1735787391662598\n",
      "Epoch 4251 | Train Loss: 1.2976638078689575 | Test Loss: 1.355870008468628\n",
      "Epoch 4252 | Train Loss: 1.154863953590393 | Test Loss: 1.173521637916565\n",
      "Epoch 4253 | Train Loss: 1.297576665878296 | Test Loss: 1.355908751487732\n",
      "Epoch 4254 | Train Loss: 1.1547516584396362 | Test Loss: 1.1733835935592651\n",
      "Epoch 4255 | Train Loss: 1.297505259513855 | Test Loss: 1.3557283878326416\n",
      "Epoch 4256 | Train Loss: 1.154658317565918 | Test Loss: 1.1733580827713013\n",
      "Epoch 4257 | Train Loss: 1.2974177598953247 | Test Loss: 1.3557953834533691\n",
      "Epoch 4258 | Train Loss: 1.154557466506958 | Test Loss: 1.1731823682785034\n",
      "Epoch 4259 | Train Loss: 1.2973498106002808 | Test Loss: 1.3556169271469116\n",
      "Epoch 4260 | Train Loss: 1.1544625759124756 | Test Loss: 1.1731793880462646\n",
      "Epoch 4261 | Train Loss: 1.2972674369812012 | Test Loss: 1.3556690216064453\n",
      "Epoch 4262 | Train Loss: 1.1543532609939575 | Test Loss: 1.1729974746704102\n",
      "Epoch 4263 | Train Loss: 1.2972064018249512 | Test Loss: 1.3554863929748535\n",
      "Epoch 4264 | Train Loss: 1.1542611122131348 | Test Loss: 1.1730082035064697\n",
      "Epoch 4265 | Train Loss: 1.2971138954162598 | Test Loss: 1.3555303812026978\n",
      "Epoch 4266 | Train Loss: 1.1541595458984375 | Test Loss: 1.1728100776672363\n",
      "Epoch 4267 | Train Loss: 1.297046422958374 | Test Loss: 1.355353593826294\n",
      "Epoch 4268 | Train Loss: 1.1540642976760864 | Test Loss: 1.1728495359420776\n",
      "Epoch 4269 | Train Loss: 1.2969505786895752 | Test Loss: 1.3553732633590698\n",
      "Epoch 4270 | Train Loss: 1.1539618968963623 | Test Loss: 1.1726154088974\n",
      "Epoch 4271 | Train Loss: 1.2968806028366089 | Test Loss: 1.3552660942077637\n",
      "Epoch 4272 | Train Loss: 1.1538625955581665 | Test Loss: 1.1726402044296265\n",
      "Epoch 4273 | Train Loss: 1.2967649698257446 | Test Loss: 1.355198860168457\n",
      "Epoch 4274 | Train Loss: 1.1537595987319946 | Test Loss: 1.1724321842193604\n",
      "Epoch 4275 | Train Loss: 1.2966727018356323 | Test Loss: 1.3551623821258545\n",
      "Epoch 4276 | Train Loss: 1.1536704301834106 | Test Loss: 1.1724116802215576\n",
      "Epoch 4277 | Train Loss: 1.2965565919876099 | Test Loss: 1.355060338973999\n",
      "Epoch 4278 | Train Loss: 1.1535673141479492 | Test Loss: 1.1722404956817627\n",
      "Epoch 4279 | Train Loss: 1.296467661857605 | Test Loss: 1.3550535440444946\n",
      "Epoch 4280 | Train Loss: 1.153467059135437 | Test Loss: 1.1721943616867065\n",
      "Epoch 4281 | Train Loss: 1.2963659763336182 | Test Loss: 1.3549354076385498\n",
      "Epoch 4282 | Train Loss: 1.1533726453781128 | Test Loss: 1.1720473766326904\n",
      "Epoch 4283 | Train Loss: 1.2962862253189087 | Test Loss: 1.3549319505691528\n",
      "Epoch 4284 | Train Loss: 1.1532618999481201 | Test Loss: 1.171991229057312\n",
      "Epoch 4285 | Train Loss: 1.2961939573287964 | Test Loss: 1.3547958135604858\n",
      "Epoch 4286 | Train Loss: 1.1531716585159302 | Test Loss: 1.1718820333480835\n",
      "Epoch 4287 | Train Loss: 1.2961031198501587 | Test Loss: 1.3548150062561035\n",
      "Epoch 4288 | Train Loss: 1.1530699729919434 | Test Loss: 1.1717801094055176\n",
      "Epoch 4289 | Train Loss: 1.2960160970687866 | Test Loss: 1.3546849489212036\n",
      "Epoch 4290 | Train Loss: 1.152976632118225 | Test Loss: 1.171701192855835\n",
      "Epoch 4291 | Train Loss: 1.295931339263916 | Test Loss: 1.3546761274337769\n",
      "Epoch 4292 | Train Loss: 1.1528723239898682 | Test Loss: 1.1715974807739258\n",
      "Epoch 4293 | Train Loss: 1.2958489656448364 | Test Loss: 1.3545657396316528\n",
      "Epoch 4294 | Train Loss: 1.152778148651123 | Test Loss: 1.1715282201766968\n",
      "Epoch 4295 | Train Loss: 1.295764446258545 | Test Loss: 1.3545533418655396\n",
      "Epoch 4296 | Train Loss: 1.1526734828948975 | Test Loss: 1.171403169631958\n",
      "Epoch 4297 | Train Loss: 1.2956851720809937 | Test Loss: 1.3544398546218872\n",
      "Epoch 4298 | Train Loss: 1.152580976486206 | Test Loss: 1.171341896057129\n",
      "Epoch 4299 | Train Loss: 1.2955985069274902 | Test Loss: 1.3544410467147827\n",
      "Epoch 4300 | Train Loss: 1.1524770259857178 | Test Loss: 1.1712150573730469\n",
      "Epoch 4301 | Train Loss: 1.295516848564148 | Test Loss: 1.3543212413787842\n",
      "Epoch 4302 | Train Loss: 1.1523865461349487 | Test Loss: 1.1711653470993042\n",
      "Epoch 4303 | Train Loss: 1.2954267263412476 | Test Loss: 1.3543065786361694\n",
      "Epoch 4304 | Train Loss: 1.1522871255874634 | Test Loss: 1.1710262298583984\n",
      "Epoch 4305 | Train Loss: 1.2953521013259888 | Test Loss: 1.354203462600708\n",
      "Epoch 4306 | Train Loss: 1.1521886587142944 | Test Loss: 1.1709908246994019\n",
      "Epoch 4307 | Train Loss: 1.2952617406845093 | Test Loss: 1.3541878461837769\n",
      "Epoch 4308 | Train Loss: 1.1520915031433105 | Test Loss: 1.1708431243896484\n",
      "Epoch 4309 | Train Loss: 1.2951866388320923 | Test Loss: 1.3540781736373901\n",
      "Epoch 4310 | Train Loss: 1.1519911289215088 | Test Loss: 1.1708036661148071\n",
      "Epoch 4311 | Train Loss: 1.295092225074768 | Test Loss: 1.3540494441986084\n",
      "Epoch 4312 | Train Loss: 1.1518957614898682 | Test Loss: 1.1706594228744507\n",
      "Epoch 4313 | Train Loss: 1.2950093746185303 | Test Loss: 1.353981614112854\n",
      "Epoch 4314 | Train Loss: 1.151800274848938 | Test Loss: 1.1706156730651855\n",
      "Epoch 4315 | Train Loss: 1.2949185371398926 | Test Loss: 1.3539113998413086\n",
      "Epoch 4316 | Train Loss: 1.151703119277954 | Test Loss: 1.1704707145690918\n",
      "Epoch 4317 | Train Loss: 1.294832706451416 | Test Loss: 1.35389244556427\n",
      "Epoch 4318 | Train Loss: 1.1516097784042358 | Test Loss: 1.1704113483428955\n",
      "Epoch 4319 | Train Loss: 1.2947386503219604 | Test Loss: 1.3537747859954834\n",
      "Epoch 4320 | Train Loss: 1.151513695716858 | Test Loss: 1.1702960729599\n",
      "Epoch 4321 | Train Loss: 1.2946631908416748 | Test Loss: 1.3537503480911255\n",
      "Epoch 4322 | Train Loss: 1.151411771774292 | Test Loss: 1.1702165603637695\n",
      "Epoch 4323 | Train Loss: 1.2945703268051147 | Test Loss: 1.3536508083343506\n",
      "Epoch 4324 | Train Loss: 1.151321291923523 | Test Loss: 1.170120358467102\n",
      "Epoch 4325 | Train Loss: 1.2944889068603516 | Test Loss: 1.3536120653152466\n",
      "Epoch 4326 | Train Loss: 1.1512207984924316 | Test Loss: 1.170035719871521\n",
      "Epoch 4327 | Train Loss: 1.2944025993347168 | Test Loss: 1.3535480499267578\n",
      "Epoch 4328 | Train Loss: 1.1511313915252686 | Test Loss: 1.1699330806732178\n",
      "Epoch 4329 | Train Loss: 1.2943181991577148 | Test Loss: 1.3535081148147583\n",
      "Epoch 4330 | Train Loss: 1.1510287523269653 | Test Loss: 1.1698551177978516\n",
      "Epoch 4331 | Train Loss: 1.2942392826080322 | Test Loss: 1.3534067869186401\n",
      "Epoch 4332 | Train Loss: 1.1509370803833008 | Test Loss: 1.1697726249694824\n",
      "Epoch 4333 | Train Loss: 1.294149398803711 | Test Loss: 1.353387475013733\n",
      "Epoch 4334 | Train Loss: 1.1508415937423706 | Test Loss: 1.1696702241897583\n",
      "Epoch 4335 | Train Loss: 1.2940726280212402 | Test Loss: 1.3533002138137817\n",
      "Epoch 4336 | Train Loss: 1.1507421731948853 | Test Loss: 1.1695976257324219\n",
      "Epoch 4337 | Train Loss: 1.2939856052398682 | Test Loss: 1.3532791137695312\n",
      "Epoch 4338 | Train Loss: 1.1506532430648804 | Test Loss: 1.1694977283477783\n",
      "Epoch 4339 | Train Loss: 1.2939026355743408 | Test Loss: 1.353204607963562\n",
      "Epoch 4340 | Train Loss: 1.150552749633789 | Test Loss: 1.1694093942642212\n",
      "Epoch 4341 | Train Loss: 1.2938263416290283 | Test Loss: 1.3531596660614014\n",
      "Epoch 4342 | Train Loss: 1.1504586935043335 | Test Loss: 1.1693166494369507\n",
      "Epoch 4343 | Train Loss: 1.293736457824707 | Test Loss: 1.3531136512756348\n",
      "Epoch 4344 | Train Loss: 1.150365948677063 | Test Loss: 1.1692328453063965\n",
      "Epoch 4345 | Train Loss: 1.2936606407165527 | Test Loss: 1.3530447483062744\n",
      "Epoch 4346 | Train Loss: 1.1502633094787598 | Test Loss: 1.1691365242004395\n",
      "Epoch 4347 | Train Loss: 1.2935770750045776 | Test Loss: 1.352990984916687\n",
      "Epoch 4348 | Train Loss: 1.1501785516738892 | Test Loss: 1.1690516471862793\n",
      "Epoch 4349 | Train Loss: 1.2934893369674683 | Test Loss: 1.3529396057128906\n",
      "Epoch 4350 | Train Loss: 1.1500757932662964 | Test Loss: 1.1689571142196655\n",
      "Epoch 4351 | Train Loss: 1.2934163808822632 | Test Loss: 1.3528677225112915\n",
      "Epoch 4352 | Train Loss: 1.14998197555542 | Test Loss: 1.1688804626464844\n",
      "Epoch 4353 | Train Loss: 1.2933290004730225 | Test Loss: 1.35282564163208\n",
      "Epoch 4354 | Train Loss: 1.1498867273330688 | Test Loss: 1.1687674522399902\n",
      "Epoch 4355 | Train Loss: 1.2932542562484741 | Test Loss: 1.3527343273162842\n",
      "Epoch 4356 | Train Loss: 1.149793028831482 | Test Loss: 1.1687005758285522\n",
      "Epoch 4357 | Train Loss: 1.2931655645370483 | Test Loss: 1.352718710899353\n",
      "Epoch 4358 | Train Loss: 1.1497015953063965 | Test Loss: 1.1686040163040161\n",
      "Epoch 4359 | Train Loss: 1.2930878400802612 | Test Loss: 1.3525936603546143\n",
      "Epoch 4360 | Train Loss: 1.1496062278747559 | Test Loss: 1.1685270071029663\n",
      "Epoch 4361 | Train Loss: 1.2930082082748413 | Test Loss: 1.3525924682617188\n",
      "Epoch 4362 | Train Loss: 1.149514079093933 | Test Loss: 1.1684280633926392\n",
      "Epoch 4363 | Train Loss: 1.29291832447052 | Test Loss: 1.352468729019165\n",
      "Epoch 4364 | Train Loss: 1.14943528175354 | Test Loss: 1.1683714389801025\n",
      "Epoch 4365 | Train Loss: 1.2928383350372314 | Test Loss: 1.3524655103683472\n",
      "Epoch 4366 | Train Loss: 1.1493291854858398 | Test Loss: 1.1682425737380981\n",
      "Epoch 4367 | Train Loss: 1.292765498161316 | Test Loss: 1.3523541688919067\n",
      "Epoch 4368 | Train Loss: 1.1492458581924438 | Test Loss: 1.1681978702545166\n",
      "Epoch 4369 | Train Loss: 1.2926712036132812 | Test Loss: 1.3523715734481812\n",
      "Epoch 4370 | Train Loss: 1.1491494178771973 | Test Loss: 1.1680653095245361\n",
      "Epoch 4371 | Train Loss: 1.2926130294799805 | Test Loss: 1.3522238731384277\n",
      "Epoch 4372 | Train Loss: 1.1490522623062134 | Test Loss: 1.1680370569229126\n",
      "Epoch 4373 | Train Loss: 1.292528510093689 | Test Loss: 1.3522632122039795\n",
      "Epoch 4374 | Train Loss: 1.1489598751068115 | Test Loss: 1.1678842306137085\n",
      "Epoch 4375 | Train Loss: 1.2924593687057495 | Test Loss: 1.3521068096160889\n",
      "Epoch 4376 | Train Loss: 1.1488757133483887 | Test Loss: 1.167880654335022\n",
      "Epoch 4377 | Train Loss: 1.2923702001571655 | Test Loss: 1.3521331548690796\n",
      "Epoch 4378 | Train Loss: 1.1487733125686646 | Test Loss: 1.167705774307251\n",
      "Epoch 4379 | Train Loss: 1.2923026084899902 | Test Loss: 1.352001667022705\n",
      "Epoch 4380 | Train Loss: 1.148690104484558 | Test Loss: 1.167715311050415\n",
      "Epoch 4381 | Train Loss: 1.2922073602676392 | Test Loss: 1.3520126342773438\n",
      "Epoch 4382 | Train Loss: 1.1485942602157593 | Test Loss: 1.1675379276275635\n",
      "Epoch 4383 | Train Loss: 1.2921497821807861 | Test Loss: 1.3518866300582886\n",
      "Epoch 4384 | Train Loss: 1.1484980583190918 | Test Loss: 1.1675430536270142\n",
      "Epoch 4385 | Train Loss: 1.2920516729354858 | Test Loss: 1.3518880605697632\n",
      "Epoch 4386 | Train Loss: 1.1484116315841675 | Test Loss: 1.1673698425292969\n",
      "Epoch 4387 | Train Loss: 1.291980266571045 | Test Loss: 1.3517926931381226\n",
      "Epoch 4388 | Train Loss: 1.148319959640503 | Test Loss: 1.1673613786697388\n",
      "Epoch 4389 | Train Loss: 1.2918789386749268 | Test Loss: 1.35173761844635\n",
      "Epoch 4390 | Train Loss: 1.1482253074645996 | Test Loss: 1.1671795845031738\n",
      "Epoch 4391 | Train Loss: 1.2918000221252441 | Test Loss: 1.3517065048217773\n",
      "Epoch 4392 | Train Loss: 1.148142695426941 | Test Loss: 1.167176365852356\n",
      "Epoch 4393 | Train Loss: 1.2916967868804932 | Test Loss: 1.3516104221343994\n",
      "Epoch 4394 | Train Loss: 1.1480441093444824 | Test Loss: 1.1670235395431519\n",
      "Epoch 4395 | Train Loss: 1.291628360748291 | Test Loss: 1.3515632152557373\n",
      "Epoch 4396 | Train Loss: 1.1479501724243164 | Test Loss: 1.166981816291809\n",
      "Epoch 4397 | Train Loss: 1.2915310859680176 | Test Loss: 1.3515125513076782\n",
      "Epoch 4398 | Train Loss: 1.147868275642395 | Test Loss: 1.166857123374939\n",
      "Epoch 4399 | Train Loss: 1.2914596796035767 | Test Loss: 1.3514485359191895\n",
      "Epoch 4400 | Train Loss: 1.1477655172348022 | Test Loss: 1.1668100357055664\n",
      "Epoch 4401 | Train Loss: 1.2913787364959717 | Test Loss: 1.351380705833435\n",
      "Epoch 4402 | Train Loss: 1.147678256034851 | Test Loss: 1.166678786277771\n",
      "Epoch 4403 | Train Loss: 1.2912888526916504 | Test Loss: 1.3513520956039429\n",
      "Epoch 4404 | Train Loss: 1.1475952863693237 | Test Loss: 1.1666263341903687\n",
      "Epoch 4405 | Train Loss: 1.291202425956726 | Test Loss: 1.3512585163116455\n",
      "Epoch 4406 | Train Loss: 1.1474971771240234 | Test Loss: 1.1665260791778564\n",
      "Epoch 4407 | Train Loss: 1.2911262512207031 | Test Loss: 1.3512519598007202\n",
      "Epoch 4408 | Train Loss: 1.147409439086914 | Test Loss: 1.1664451360702515\n",
      "Epoch 4409 | Train Loss: 1.2910382747650146 | Test Loss: 1.3511875867843628\n",
      "Epoch 4410 | Train Loss: 1.1473159790039062 | Test Loss: 1.166354775428772\n",
      "Epoch 4411 | Train Loss: 1.2909722328186035 | Test Loss: 1.3511003255844116\n",
      "Epoch 4412 | Train Loss: 1.147220253944397 | Test Loss: 1.1662871837615967\n",
      "Epoch 4413 | Train Loss: 1.2908837795257568 | Test Loss: 1.3511040210723877\n",
      "Epoch 4414 | Train Loss: 1.1471400260925293 | Test Loss: 1.1661725044250488\n",
      "Epoch 4415 | Train Loss: 1.2908040285110474 | Test Loss: 1.3509999513626099\n",
      "Epoch 4416 | Train Loss: 1.147046685218811 | Test Loss: 1.1661170721054077\n",
      "Epoch 4417 | Train Loss: 1.2907273769378662 | Test Loss: 1.3509646654129028\n",
      "Epoch 4418 | Train Loss: 1.1469534635543823 | Test Loss: 1.1660062074661255\n",
      "Epoch 4419 | Train Loss: 1.2906458377838135 | Test Loss: 1.3508837223052979\n",
      "Epoch 4420 | Train Loss: 1.1468690633773804 | Test Loss: 1.165937900543213\n",
      "Epoch 4421 | Train Loss: 1.2905690670013428 | Test Loss: 1.3508790731430054\n",
      "Epoch 4422 | Train Loss: 1.1467669010162354 | Test Loss: 1.1658310890197754\n",
      "Epoch 4423 | Train Loss: 1.2904983758926392 | Test Loss: 1.3507702350616455\n",
      "Epoch 4424 | Train Loss: 1.1466842889785767 | Test Loss: 1.1657822132110596\n",
      "Epoch 4425 | Train Loss: 1.2904160022735596 | Test Loss: 1.3507914543151855\n",
      "Epoch 4426 | Train Loss: 1.1465857028961182 | Test Loss: 1.1656544208526611\n",
      "Epoch 4427 | Train Loss: 1.2903544902801514 | Test Loss: 1.3506687879562378\n",
      "Epoch 4428 | Train Loss: 1.1465046405792236 | Test Loss: 1.1656363010406494\n",
      "Epoch 4429 | Train Loss: 1.290265440940857 | Test Loss: 1.3506940603256226\n",
      "Epoch 4430 | Train Loss: 1.146411418914795 | Test Loss: 1.1654932498931885\n",
      "Epoch 4431 | Train Loss: 1.2902023792266846 | Test Loss: 1.3505597114562988\n",
      "Epoch 4432 | Train Loss: 1.146328330039978 | Test Loss: 1.1654764413833618\n",
      "Epoch 4433 | Train Loss: 1.2901204824447632 | Test Loss: 1.350570797920227\n",
      "Epoch 4434 | Train Loss: 1.1462281942367554 | Test Loss: 1.1653289794921875\n",
      "Epoch 4435 | Train Loss: 1.2900563478469849 | Test Loss: 1.3504681587219238\n",
      "Epoch 4436 | Train Loss: 1.1461541652679443 | Test Loss: 1.1653218269348145\n",
      "Epoch 4437 | Train Loss: 1.2899585962295532 | Test Loss: 1.350468635559082\n",
      "Epoch 4438 | Train Loss: 1.1460530757904053 | Test Loss: 1.1651655435562134\n",
      "Epoch 4439 | Train Loss: 1.2899088859558105 | Test Loss: 1.3503508567810059\n",
      "Epoch 4440 | Train Loss: 1.145963191986084 | Test Loss: 1.1651666164398193\n",
      "Epoch 4441 | Train Loss: 1.289806842803955 | Test Loss: 1.3503344058990479\n",
      "Epoch 4442 | Train Loss: 1.1458804607391357 | Test Loss: 1.1650116443634033\n",
      "Epoch 4443 | Train Loss: 1.2897403240203857 | Test Loss: 1.3502615690231323\n",
      "Epoch 4444 | Train Loss: 1.145789384841919 | Test Loss: 1.1650023460388184\n",
      "Epoch 4445 | Train Loss: 1.2896456718444824 | Test Loss: 1.3501943349838257\n",
      "Epoch 4446 | Train Loss: 1.1457056999206543 | Test Loss: 1.1648437976837158\n",
      "Epoch 4447 | Train Loss: 1.2895734310150146 | Test Loss: 1.3501702547073364\n",
      "Epoch 4448 | Train Loss: 1.1456166505813599 | Test Loss: 1.1648246049880981\n",
      "Epoch 4449 | Train Loss: 1.289472222328186 | Test Loss: 1.3500676155090332\n",
      "Epoch 4450 | Train Loss: 1.1455347537994385 | Test Loss: 1.1647058725357056\n",
      "Epoch 4451 | Train Loss: 1.2893954515457153 | Test Loss: 1.3500562906265259\n",
      "Epoch 4452 | Train Loss: 1.1454455852508545 | Test Loss: 1.1646581888198853\n",
      "Epoch 4453 | Train Loss: 1.2893036603927612 | Test Loss: 1.3499525785446167\n",
      "Epoch 4454 | Train Loss: 1.1453677415847778 | Test Loss: 1.1645556688308716\n",
      "Epoch 4455 | Train Loss: 1.289231300354004 | Test Loss: 1.3499382734298706\n",
      "Epoch 4456 | Train Loss: 1.1452722549438477 | Test Loss: 1.1644920110702515\n",
      "Epoch 4457 | Train Loss: 1.2891432046890259 | Test Loss: 1.3498451709747314\n",
      "Epoch 4458 | Train Loss: 1.1451987028121948 | Test Loss: 1.164406418800354\n",
      "Epoch 4459 | Train Loss: 1.289062261581421 | Test Loss: 1.3498177528381348\n",
      "Epoch 4460 | Train Loss: 1.1451045274734497 | Test Loss: 1.1643247604370117\n",
      "Epoch 4461 | Train Loss: 1.2889870405197144 | Test Loss: 1.3497155904769897\n",
      "Epoch 4462 | Train Loss: 1.1450235843658447 | Test Loss: 1.164253830909729\n",
      "Epoch 4463 | Train Loss: 1.288904070854187 | Test Loss: 1.3497017621994019\n",
      "Epoch 4464 | Train Loss: 1.144934892654419 | Test Loss: 1.1641641855239868\n",
      "Epoch 4465 | Train Loss: 1.288824200630188 | Test Loss: 1.3496145009994507\n",
      "Epoch 4466 | Train Loss: 1.1448521614074707 | Test Loss: 1.1640832424163818\n",
      "Epoch 4467 | Train Loss: 1.2887489795684814 | Test Loss: 1.3495774269104004\n",
      "Epoch 4468 | Train Loss: 1.1447607278823853 | Test Loss: 1.1640006303787231\n",
      "Epoch 4469 | Train Loss: 1.288671612739563 | Test Loss: 1.3494985103607178\n",
      "Epoch 4470 | Train Loss: 1.144679307937622 | Test Loss: 1.163922905921936\n",
      "Epoch 4471 | Train Loss: 1.2885949611663818 | Test Loss: 1.3494716882705688\n",
      "Epoch 4472 | Train Loss: 1.1445865631103516 | Test Loss: 1.1638375520706177\n",
      "Epoch 4473 | Train Loss: 1.2885202169418335 | Test Loss: 1.3493834733963013\n",
      "Epoch 4474 | Train Loss: 1.144508719444275 | Test Loss: 1.1637705564498901\n",
      "Epoch 4475 | Train Loss: 1.288437843322754 | Test Loss: 1.3493505716323853\n",
      "Epoch 4476 | Train Loss: 1.1444151401519775 | Test Loss: 1.1636743545532227\n",
      "Epoch 4477 | Train Loss: 1.288364052772522 | Test Loss: 1.3492511510849\n",
      "Epoch 4478 | Train Loss: 1.144337773323059 | Test Loss: 1.1636149883270264\n",
      "Epoch 4479 | Train Loss: 1.2882813215255737 | Test Loss: 1.3492262363433838\n",
      "Epoch 4480 | Train Loss: 1.14424467086792 | Test Loss: 1.1635029315948486\n",
      "Epoch 4481 | Train Loss: 1.2882106304168701 | Test Loss: 1.3491286039352417\n",
      "Epoch 4482 | Train Loss: 1.1441656351089478 | Test Loss: 1.1634430885314941\n",
      "Epoch 4483 | Train Loss: 1.288130760192871 | Test Loss: 1.3491131067276\n",
      "Epoch 4484 | Train Loss: 1.1440705060958862 | Test Loss: 1.1633483171463013\n",
      "Epoch 4485 | Train Loss: 1.288063645362854 | Test Loss: 1.3490005731582642\n",
      "Epoch 4486 | Train Loss: 1.1439918279647827 | Test Loss: 1.1632919311523438\n",
      "Epoch 4487 | Train Loss: 1.2879739999771118 | Test Loss: 1.349001169204712\n",
      "Epoch 4488 | Train Loss: 1.1439075469970703 | Test Loss: 1.1631776094436646\n",
      "Epoch 4489 | Train Loss: 1.2879098653793335 | Test Loss: 1.3488829135894775\n",
      "Epoch 4490 | Train Loss: 1.1438194513320923 | Test Loss: 1.1631319522857666\n",
      "Epoch 4491 | Train Loss: 1.2878309488296509 | Test Loss: 1.3488909006118774\n",
      "Epoch 4492 | Train Loss: 1.1437314748764038 | Test Loss: 1.1630092859268188\n",
      "Epoch 4493 | Train Loss: 1.2877618074417114 | Test Loss: 1.348757028579712\n",
      "Epoch 4494 | Train Loss: 1.1436548233032227 | Test Loss: 1.1629785299301147\n",
      "Epoch 4495 | Train Loss: 1.2876824140548706 | Test Loss: 1.3487825393676758\n",
      "Epoch 4496 | Train Loss: 1.1435575485229492 | Test Loss: 1.1628354787826538\n",
      "Epoch 4497 | Train Loss: 1.2876191139221191 | Test Loss: 1.3486576080322266\n",
      "Epoch 4498 | Train Loss: 1.1434820890426636 | Test Loss: 1.162832498550415\n",
      "Epoch 4499 | Train Loss: 1.2875326871871948 | Test Loss: 1.3486756086349487\n",
      "Epoch 4500 | Train Loss: 1.1433864831924438 | Test Loss: 1.162675142288208\n",
      "Epoch 4501 | Train Loss: 1.2874833345413208 | Test Loss: 1.3485180139541626\n",
      "Epoch 4502 | Train Loss: 1.1433088779449463 | Test Loss: 1.1626851558685303\n",
      "Epoch 4503 | Train Loss: 1.2873938083648682 | Test Loss: 1.3485691547393799\n",
      "Epoch 4504 | Train Loss: 1.14322030544281 | Test Loss: 1.1625171899795532\n",
      "Epoch 4505 | Train Loss: 1.287325382232666 | Test Loss: 1.3484383821487427\n",
      "Epoch 4506 | Train Loss: 1.1431447267532349 | Test Loss: 1.1625088453292847\n",
      "Epoch 4507 | Train Loss: 1.2872393131256104 | Test Loss: 1.3484387397766113\n",
      "Epoch 4508 | Train Loss: 1.1430442333221436 | Test Loss: 1.1623462438583374\n",
      "Epoch 4509 | Train Loss: 1.28718101978302 | Test Loss: 1.3483330011367798\n",
      "Epoch 4510 | Train Loss: 1.1429665088653564 | Test Loss: 1.1623471975326538\n",
      "Epoch 4511 | Train Loss: 1.2870821952819824 | Test Loss: 1.3483282327651978\n",
      "Epoch 4512 | Train Loss: 1.1428759098052979 | Test Loss: 1.1621861457824707\n",
      "Epoch 4513 | Train Loss: 1.2870309352874756 | Test Loss: 1.3482078313827515\n",
      "Epoch 4514 | Train Loss: 1.1427850723266602 | Test Loss: 1.1621854305267334\n",
      "Epoch 4515 | Train Loss: 1.2869372367858887 | Test Loss: 1.3481931686401367\n",
      "Epoch 4516 | Train Loss: 1.1427044868469238 | Test Loss: 1.1620237827301025\n",
      "Epoch 4517 | Train Loss: 1.2868572473526 | Test Loss: 1.3481428623199463\n",
      "Epoch 4518 | Train Loss: 1.142622947692871 | Test Loss: 1.1620094776153564\n",
      "Epoch 4519 | Train Loss: 1.2867690324783325 | Test Loss: 1.348051905632019\n",
      "Epoch 4520 | Train Loss: 1.1425321102142334 | Test Loss: 1.1618777513504028\n",
      "Epoch 4521 | Train Loss: 1.2866946458816528 | Test Loss: 1.3480374813079834\n",
      "Epoch 4522 | Train Loss: 1.1424541473388672 | Test Loss: 1.1618309020996094\n",
      "Epoch 4523 | Train Loss: 1.286606788635254 | Test Loss: 1.347948431968689\n",
      "Epoch 4524 | Train Loss: 1.1423606872558594 | Test Loss: 1.1617321968078613\n",
      "Epoch 4525 | Train Loss: 1.2865465879440308 | Test Loss: 1.3479104042053223\n",
      "Epoch 4526 | Train Loss: 1.1422737836837769 | Test Loss: 1.1616727113723755\n",
      "Epoch 4527 | Train Loss: 1.286458969116211 | Test Loss: 1.3478647470474243\n",
      "Epoch 4528 | Train Loss: 1.1421982049942017 | Test Loss: 1.161566138267517\n",
      "Epoch 4529 | Train Loss: 1.2863929271697998 | Test Loss: 1.3477903604507446\n",
      "Epoch 4530 | Train Loss: 1.1421040296554565 | Test Loss: 1.1615220308303833\n",
      "Epoch 4531 | Train Loss: 1.2863141298294067 | Test Loss: 1.347760796546936\n",
      "Epoch 4532 | Train Loss: 1.1420261859893799 | Test Loss: 1.1614127159118652\n",
      "Epoch 4533 | Train Loss: 1.2862412929534912 | Test Loss: 1.3476914167404175\n",
      "Epoch 4534 | Train Loss: 1.1419389247894287 | Test Loss: 1.161366581916809\n",
      "Epoch 4535 | Train Loss: 1.286167860031128 | Test Loss: 1.3476377725601196\n",
      "Epoch 4536 | Train Loss: 1.141849398612976 | Test Loss: 1.1612581014633179\n",
      "Epoch 4537 | Train Loss: 1.2860925197601318 | Test Loss: 1.3475868701934814\n",
      "Epoch 4538 | Train Loss: 1.1417763233184814 | Test Loss: 1.1612122058868408\n",
      "Epoch 4539 | Train Loss: 1.2860114574432373 | Test Loss: 1.347531795501709\n",
      "Epoch 4540 | Train Loss: 1.1416816711425781 | Test Loss: 1.1611180305480957\n",
      "Epoch 4541 | Train Loss: 1.2859445810317993 | Test Loss: 1.3474557399749756\n",
      "Epoch 4542 | Train Loss: 1.1416021585464478 | Test Loss: 1.16105318069458\n",
      "Epoch 4543 | Train Loss: 1.2858624458312988 | Test Loss: 1.34744131565094\n",
      "Epoch 4544 | Train Loss: 1.141512393951416 | Test Loss: 1.1609575748443604\n",
      "Epoch 4545 | Train Loss: 1.2857990264892578 | Test Loss: 1.3473379611968994\n",
      "Epoch 4546 | Train Loss: 1.1414265632629395 | Test Loss: 1.1609079837799072\n",
      "Epoch 4547 | Train Loss: 1.2857253551483154 | Test Loss: 1.3473330736160278\n",
      "Epoch 4548 | Train Loss: 1.1413378715515137 | Test Loss: 1.1608003377914429\n",
      "Epoch 4549 | Train Loss: 1.2856519222259521 | Test Loss: 1.3472459316253662\n",
      "Epoch 4550 | Train Loss: 1.1412633657455444 | Test Loss: 1.160754680633545\n",
      "Epoch 4551 | Train Loss: 1.2855782508850098 | Test Loss: 1.347233772277832\n",
      "Epoch 4552 | Train Loss: 1.1411641836166382 | Test Loss: 1.160651683807373\n",
      "Epoch 4553 | Train Loss: 1.2855125665664673 | Test Loss: 1.3471293449401855\n",
      "Epoch 4554 | Train Loss: 1.1410967111587524 | Test Loss: 1.1606107950210571\n",
      "Epoch 4555 | Train Loss: 1.2854281663894653 | Test Loss: 1.3471651077270508\n",
      "Epoch 4556 | Train Loss: 1.1410000324249268 | Test Loss: 1.160493016242981\n",
      "Epoch 4557 | Train Loss: 1.2853772640228271 | Test Loss: 1.3469985723495483\n",
      "Epoch 4558 | Train Loss: 1.1409194469451904 | Test Loss: 1.1604764461517334\n",
      "Epoch 4559 | Train Loss: 1.2853015661239624 | Test Loss: 1.3470702171325684\n",
      "Epoch 4560 | Train Loss: 1.14082932472229 | Test Loss: 1.1603398323059082\n",
      "Epoch 4561 | Train Loss: 1.2852483987808228 | Test Loss: 1.346899390220642\n",
      "Epoch 4562 | Train Loss: 1.1407548189163208 | Test Loss: 1.160346269607544\n",
      "Epoch 4563 | Train Loss: 1.2851717472076416 | Test Loss: 1.3469386100769043\n",
      "Epoch 4564 | Train Loss: 1.14065420627594 | Test Loss: 1.1601824760437012\n",
      "Epoch 4565 | Train Loss: 1.2851182222366333 | Test Loss: 1.3468210697174072\n",
      "Epoch 4566 | Train Loss: 1.1405868530273438 | Test Loss: 1.1601974964141846\n",
      "Epoch 4567 | Train Loss: 1.2850277423858643 | Test Loss: 1.3468338251113892\n",
      "Epoch 4568 | Train Loss: 1.1404880285263062 | Test Loss: 1.1600358486175537\n",
      "Epoch 4569 | Train Loss: 1.2849928140640259 | Test Loss: 1.3466808795928955\n",
      "Epoch 4570 | Train Loss: 1.140404224395752 | Test Loss: 1.1600682735443115\n",
      "Epoch 4571 | Train Loss: 1.284899115562439 | Test Loss: 1.3467212915420532\n",
      "Epoch 4572 | Train Loss: 1.1403247117996216 | Test Loss: 1.1598846912384033\n",
      "Epoch 4573 | Train Loss: 1.284841537475586 | Test Loss: 1.34662663936615\n",
      "Epoch 4574 | Train Loss: 1.140245795249939 | Test Loss: 1.1599198579788208\n",
      "Epoch 4575 | Train Loss: 1.2847447395324707 | Test Loss: 1.3465882539749146\n",
      "Epoch 4576 | Train Loss: 1.1401580572128296 | Test Loss: 1.1597416400909424\n",
      "Epoch 4577 | Train Loss: 1.284671425819397 | Test Loss: 1.3465484380722046\n",
      "Epoch 4578 | Train Loss: 1.1400823593139648 | Test Loss: 1.1597346067428589\n",
      "Epoch 4579 | Train Loss: 1.2845675945281982 | Test Loss: 1.3464633226394653\n",
      "Epoch 4580 | Train Loss: 1.139995813369751 | Test Loss: 1.1596120595932007\n",
      "Epoch 4581 | Train Loss: 1.2845070362091064 | Test Loss: 1.346423625946045\n",
      "Epoch 4582 | Train Loss: 1.1399121284484863 | Test Loss: 1.159570574760437\n",
      "Epoch 4583 | Train Loss: 1.2844139337539673 | Test Loss: 1.346341848373413\n",
      "Epoch 4584 | Train Loss: 1.1398426294326782 | Test Loss: 1.1594531536102295\n",
      "Epoch 4585 | Train Loss: 1.2843501567840576 | Test Loss: 1.346322774887085\n",
      "Epoch 4586 | Train Loss: 1.13974928855896 | Test Loss: 1.1594308614730835\n",
      "Epoch 4587 | Train Loss: 1.2842663526535034 | Test Loss: 1.3462399244308472\n",
      "Epoch 4588 | Train Loss: 1.1396830081939697 | Test Loss: 1.159312129020691\n",
      "Epoch 4589 | Train Loss: 1.2841925621032715 | Test Loss: 1.3462332487106323\n",
      "Epoch 4590 | Train Loss: 1.139592170715332 | Test Loss: 1.1592693328857422\n",
      "Epoch 4591 | Train Loss: 1.2841216325759888 | Test Loss: 1.346127986907959\n",
      "Epoch 4592 | Train Loss: 1.139508843421936 | Test Loss: 1.1591954231262207\n",
      "Epoch 4593 | Train Loss: 1.284047245979309 | Test Loss: 1.3461402654647827\n",
      "Epoch 4594 | Train Loss: 1.1394319534301758 | Test Loss: 1.159116506576538\n",
      "Epoch 4595 | Train Loss: 1.2839716672897339 | Test Loss: 1.3460381031036377\n",
      "Epoch 4596 | Train Loss: 1.1393508911132812 | Test Loss: 1.159053921699524\n",
      "Epoch 4597 | Train Loss: 1.2838983535766602 | Test Loss: 1.3460209369659424\n",
      "Epoch 4598 | Train Loss: 1.1392719745635986 | Test Loss: 1.158974051475525\n",
      "Epoch 4599 | Train Loss: 1.283830165863037 | Test Loss: 1.345942497253418\n",
      "Epoch 4600 | Train Loss: 1.1391868591308594 | Test Loss: 1.158907175064087\n",
      "Epoch 4601 | Train Loss: 1.2837640047073364 | Test Loss: 1.3459229469299316\n",
      "Epoch 4602 | Train Loss: 1.1391032934188843 | Test Loss: 1.1588246822357178\n",
      "Epoch 4603 | Train Loss: 1.2836874723434448 | Test Loss: 1.345860242843628\n",
      "Epoch 4604 | Train Loss: 1.1390334367752075 | Test Loss: 1.1587523221969604\n",
      "Epoch 4605 | Train Loss: 1.283620834350586 | Test Loss: 1.3458188772201538\n",
      "Epoch 4606 | Train Loss: 1.1389379501342773 | Test Loss: 1.158677101135254\n",
      "Epoch 4607 | Train Loss: 1.2835549116134644 | Test Loss: 1.3457413911819458\n",
      "Epoch 4608 | Train Loss: 1.1388665437698364 | Test Loss: 1.158610463142395\n",
      "Epoch 4609 | Train Loss: 1.2834761142730713 | Test Loss: 1.3457404375076294\n",
      "Epoch 4610 | Train Loss: 1.1387776136398315 | Test Loss: 1.1585227251052856\n",
      "Epoch 4611 | Train Loss: 1.2834198474884033 | Test Loss: 1.3456416130065918\n",
      "Epoch 4612 | Train Loss: 1.1386945247650146 | Test Loss: 1.1584670543670654\n",
      "Epoch 4613 | Train Loss: 1.2833442687988281 | Test Loss: 1.3456462621688843\n",
      "Epoch 4614 | Train Loss: 1.1386117935180664 | Test Loss: 1.1583728790283203\n",
      "Epoch 4615 | Train Loss: 1.2832834720611572 | Test Loss: 1.3455328941345215\n",
      "Epoch 4616 | Train Loss: 1.1385291814804077 | Test Loss: 1.1583389043807983\n",
      "Epoch 4617 | Train Loss: 1.2832108736038208 | Test Loss: 1.3455349206924438\n",
      "Epoch 4618 | Train Loss: 1.1384484767913818 | Test Loss: 1.1582309007644653\n",
      "Epoch 4619 | Train Loss: 1.2831437587738037 | Test Loss: 1.3454464673995972\n",
      "Epoch 4620 | Train Loss: 1.1383717060089111 | Test Loss: 1.158199667930603\n",
      "Epoch 4621 | Train Loss: 1.2830801010131836 | Test Loss: 1.3454461097717285\n",
      "Epoch 4622 | Train Loss: 1.1382797956466675 | Test Loss: 1.1581059694290161\n",
      "Epoch 4623 | Train Loss: 1.283006191253662 | Test Loss: 1.34535551071167\n",
      "Epoch 4624 | Train Loss: 1.1382179260253906 | Test Loss: 1.1580510139465332\n",
      "Epoch 4625 | Train Loss: 1.2829289436340332 | Test Loss: 1.345359206199646\n",
      "Epoch 4626 | Train Loss: 1.1381196975708008 | Test Loss: 1.1579627990722656\n",
      "Epoch 4627 | Train Loss: 1.2828866243362427 | Test Loss: 1.3452073335647583\n",
      "Epoch 4628 | Train Loss: 1.1380478143692017 | Test Loss: 1.1579363346099854\n",
      "Epoch 4629 | Train Loss: 1.2827987670898438 | Test Loss: 1.3452973365783691\n",
      "Epoch 4630 | Train Loss: 1.1379704475402832 | Test Loss: 1.157812476158142\n",
      "Epoch 4631 | Train Loss: 1.2827485799789429 | Test Loss: 1.3451327085494995\n",
      "Epoch 4632 | Train Loss: 1.137888789176941 | Test Loss: 1.1578319072723389\n",
      "Epoch 4633 | Train Loss: 1.2826786041259766 | Test Loss: 1.3451695442199707\n",
      "Epoch 4634 | Train Loss: 1.1378053426742554 | Test Loss: 1.1576764583587646\n",
      "Epoch 4635 | Train Loss: 1.2826205492019653 | Test Loss: 1.345079779624939\n",
      "Epoch 4636 | Train Loss: 1.1377454996109009 | Test Loss: 1.1576998233795166\n",
      "Epoch 4637 | Train Loss: 1.2825311422348022 | Test Loss: 1.345041036605835\n",
      "Epoch 4638 | Train Loss: 1.1376497745513916 | Test Loss: 1.1575533151626587\n",
      "Epoch 4639 | Train Loss: 1.2824870347976685 | Test Loss: 1.3449510335922241\n",
      "Epoch 4640 | Train Loss: 1.137582540512085 | Test Loss: 1.1575567722320557\n",
      "Epoch 4641 | Train Loss: 1.2823917865753174 | Test Loss: 1.3449853658676147\n",
      "Epoch 4642 | Train Loss: 1.1375030279159546 | Test Loss: 1.1574039459228516\n",
      "Epoch 4643 | Train Loss: 1.282346487045288 | Test Loss: 1.3448566198349\n",
      "Epoch 4644 | Train Loss: 1.1374180316925049 | Test Loss: 1.1574324369430542\n",
      "Epoch 4645 | Train Loss: 1.282260537147522 | Test Loss: 1.3448880910873413\n",
      "Epoch 4646 | Train Loss: 1.137343406677246 | Test Loss: 1.1572767496109009\n",
      "Epoch 4647 | Train Loss: 1.2822009325027466 | Test Loss: 1.344829797744751\n",
      "Epoch 4648 | Train Loss: 1.1372668743133545 | Test Loss: 1.1572917699813843\n",
      "Epoch 4649 | Train Loss: 1.2821078300476074 | Test Loss: 1.3447532653808594\n",
      "Epoch 4650 | Train Loss: 1.1371870040893555 | Test Loss: 1.1571489572525024\n",
      "Epoch 4651 | Train Loss: 1.2820380926132202 | Test Loss: 1.344769835472107\n",
      "Epoch 4652 | Train Loss: 1.137118935585022 | Test Loss: 1.1571366786956787\n",
      "Epoch 4653 | Train Loss: 1.2819490432739258 | Test Loss: 1.3446707725524902\n",
      "Epoch 4654 | Train Loss: 1.1370365619659424 | Test Loss: 1.1570297479629517\n",
      "Epoch 4655 | Train Loss: 1.281896710395813 | Test Loss: 1.3446409702301025\n",
      "Epoch 4656 | Train Loss: 1.1369520425796509 | Test Loss: 1.1570051908493042\n",
      "Epoch 4657 | Train Loss: 1.2818106412887573 | Test Loss: 1.344604730606079\n",
      "Epoch 4658 | Train Loss: 1.1368921995162964 | Test Loss: 1.1568939685821533\n",
      "Epoch 4659 | Train Loss: 1.281745433807373 | Test Loss: 1.344580888748169\n",
      "Epoch 4660 | Train Loss: 1.1368036270141602 | Test Loss: 1.1568630933761597\n",
      "Epoch 4661 | Train Loss: 1.2816745042800903 | Test Loss: 1.3444923162460327\n",
      "Epoch 4662 | Train Loss: 1.136731505393982 | Test Loss: 1.156774640083313\n",
      "Epoch 4663 | Train Loss: 1.2815958261489868 | Test Loss: 1.3445098400115967\n",
      "Epoch 4664 | Train Loss: 1.1366610527038574 | Test Loss: 1.156723976135254\n",
      "Epoch 4665 | Train Loss: 1.2815260887145996 | Test Loss: 1.3443857431411743\n",
      "Epoch 4666 | Train Loss: 1.1365792751312256 | Test Loss: 1.156657099723816\n",
      "Epoch 4667 | Train Loss: 1.2814593315124512 | Test Loss: 1.3443996906280518\n",
      "Epoch 4668 | Train Loss: 1.1365052461624146 | Test Loss: 1.1565805673599243\n",
      "Epoch 4669 | Train Loss: 1.2813847064971924 | Test Loss: 1.3443397283554077\n",
      "Epoch 4670 | Train Loss: 1.1364306211471558 | Test Loss: 1.1565226316452026\n",
      "Epoch 4671 | Train Loss: 1.2813224792480469 | Test Loss: 1.344274640083313\n",
      "Epoch 4672 | Train Loss: 1.1363465785980225 | Test Loss: 1.1564377546310425\n",
      "Epoch 4673 | Train Loss: 1.281252384185791 | Test Loss: 1.3442147970199585\n",
      "Epoch 4674 | Train Loss: 1.1362814903259277 | Test Loss: 1.1563842296600342\n",
      "Epoch 4675 | Train Loss: 1.281179428100586 | Test Loss: 1.3441931009292603\n",
      "Epoch 4676 | Train Loss: 1.136196494102478 | Test Loss: 1.156323790550232\n",
      "Epoch 4677 | Train Loss: 1.2811146974563599 | Test Loss: 1.344110369682312\n",
      "Epoch 4678 | Train Loss: 1.1361274719238281 | Test Loss: 1.1562587022781372\n",
      "Epoch 4679 | Train Loss: 1.2810397148132324 | Test Loss: 1.3441015481948853\n",
      "Epoch 4680 | Train Loss: 1.1360455751419067 | Test Loss: 1.156186580657959\n",
      "Epoch 4681 | Train Loss: 1.2809805870056152 | Test Loss: 1.344007968902588\n",
      "Epoch 4682 | Train Loss: 1.1359710693359375 | Test Loss: 1.1561460494995117\n",
      "Epoch 4683 | Train Loss: 1.2809076309204102 | Test Loss: 1.344031572341919\n",
      "Epoch 4684 | Train Loss: 1.135892629623413 | Test Loss: 1.1560498476028442\n",
      "Epoch 4685 | Train Loss: 1.28084135055542 | Test Loss: 1.3439334630966187\n",
      "Epoch 4686 | Train Loss: 1.1358263492584229 | Test Loss: 1.1560167074203491\n",
      "Epoch 4687 | Train Loss: 1.2807730436325073 | Test Loss: 1.343914270401001\n",
      "Epoch 4688 | Train Loss: 1.1357357501983643 | Test Loss: 1.155921220779419\n",
      "Epoch 4689 | Train Loss: 1.2807096242904663 | Test Loss: 1.3438235521316528\n",
      "Epoch 4690 | Train Loss: 1.1356780529022217 | Test Loss: 1.155890941619873\n",
      "Epoch 4691 | Train Loss: 1.2806313037872314 | Test Loss: 1.3438374996185303\n",
      "Epoch 4692 | Train Loss: 1.135585904121399 | Test Loss: 1.1557841300964355\n",
      "Epoch 4693 | Train Loss: 1.280592918395996 | Test Loss: 1.3437180519104004\n",
      "Epoch 4694 | Train Loss: 1.1355128288269043 | Test Loss: 1.155781626701355\n",
      "Epoch 4695 | Train Loss: 1.2805075645446777 | Test Loss: 1.3437762260437012\n",
      "Epoch 4696 | Train Loss: 1.1354360580444336 | Test Loss: 1.1556495428085327\n",
      "Epoch 4697 | Train Loss: 1.2804656028747559 | Test Loss: 1.3436238765716553\n",
      "Epoch 4698 | Train Loss: 1.1353585720062256 | Test Loss: 1.155670404434204\n",
      "Epoch 4699 | Train Loss: 1.2803876399993896 | Test Loss: 1.343663215637207\n",
      "Epoch 4700 | Train Loss: 1.1352795362472534 | Test Loss: 1.1555231809616089\n",
      "Epoch 4701 | Train Loss: 1.2803312540054321 | Test Loss: 1.3435567617416382\n",
      "Epoch 4702 | Train Loss: 1.135209560394287 | Test Loss: 1.1555356979370117\n",
      "Epoch 4703 | Train Loss: 1.2802459001541138 | Test Loss: 1.3435336351394653\n",
      "Epoch 4704 | Train Loss: 1.1351282596588135 | Test Loss: 1.1553927659988403\n",
      "Epoch 4705 | Train Loss: 1.280196189880371 | Test Loss: 1.3434909582138062\n",
      "Epoch 4706 | Train Loss: 1.1350563764572144 | Test Loss: 1.1554075479507446\n",
      "Epoch 4707 | Train Loss: 1.2801049947738647 | Test Loss: 1.3434420824050903\n",
      "Epoch 4708 | Train Loss: 1.1349791288375854 | Test Loss: 1.1552726030349731\n",
      "Epoch 4709 | Train Loss: 1.2800586223602295 | Test Loss: 1.3433924913406372\n",
      "Epoch 4710 | Train Loss: 1.1348979473114014 | Test Loss: 1.1552690267562866\n",
      "Epoch 4711 | Train Loss: 1.2799723148345947 | Test Loss: 1.3433587551116943\n",
      "Epoch 4712 | Train Loss: 1.1348284482955933 | Test Loss: 1.1551337242126465\n",
      "Epoch 4713 | Train Loss: 1.2799179553985596 | Test Loss: 1.3433115482330322\n",
      "Epoch 4714 | Train Loss: 1.13474440574646 | Test Loss: 1.1551369428634644\n",
      "Epoch 4715 | Train Loss: 1.2798393964767456 | Test Loss: 1.343248724937439\n",
      "Epoch 4716 | Train Loss: 1.1346750259399414 | Test Loss: 1.155001163482666\n",
      "Epoch 4717 | Train Loss: 1.279781699180603 | Test Loss: 1.3432257175445557\n",
      "Epoch 4718 | Train Loss: 1.1345943212509155 | Test Loss: 1.154994249343872\n",
      "Epoch 4719 | Train Loss: 1.2796969413757324 | Test Loss: 1.3431450128555298\n",
      "Epoch 4720 | Train Loss: 1.1345288753509521 | Test Loss: 1.154887318611145\n",
      "Epoch 4721 | Train Loss: 1.2796354293823242 | Test Loss: 1.3431636095046997\n",
      "Epoch 4722 | Train Loss: 1.1344428062438965 | Test Loss: 1.1548409461975098\n",
      "Epoch 4723 | Train Loss: 1.279565453529358 | Test Loss: 1.3430218696594238\n",
      "Epoch 4724 | Train Loss: 1.1343748569488525 | Test Loss: 1.154766321182251\n",
      "Epoch 4725 | Train Loss: 1.279491901397705 | Test Loss: 1.3430620431900024\n",
      "Epoch 4726 | Train Loss: 1.1342986822128296 | Test Loss: 1.1546956300735474\n",
      "Epoch 4727 | Train Loss: 1.2794269323349 | Test Loss: 1.3429571390151978\n",
      "Epoch 4728 | Train Loss: 1.1342217922210693 | Test Loss: 1.1546545028686523\n",
      "Epoch 4729 | Train Loss: 1.2793618440628052 | Test Loss: 1.3429347276687622\n",
      "Epoch 4730 | Train Loss: 1.1341456174850464 | Test Loss: 1.1545681953430176\n",
      "Epoch 4731 | Train Loss: 1.2792941331863403 | Test Loss: 1.3428922891616821\n",
      "Epoch 4732 | Train Loss: 1.134076476097107 | Test Loss: 1.154524564743042\n",
      "Epoch 4733 | Train Loss: 1.2792267799377441 | Test Loss: 1.342862606048584\n",
      "Epoch 4734 | Train Loss: 1.133995532989502 | Test Loss: 1.1544498205184937\n",
      "Epoch 4735 | Train Loss: 1.2791578769683838 | Test Loss: 1.342807412147522\n",
      "Epoch 4736 | Train Loss: 1.133931279182434 | Test Loss: 1.1543798446655273\n",
      "Epoch 4737 | Train Loss: 1.2790895700454712 | Test Loss: 1.3427804708480835\n",
      "Epoch 4738 | Train Loss: 1.133846402168274 | Test Loss: 1.1543185710906982\n",
      "Epoch 4739 | Train Loss: 1.2790223360061646 | Test Loss: 1.3427013158798218\n",
      "Epoch 4740 | Train Loss: 1.1337839365005493 | Test Loss: 1.1542664766311646\n",
      "Epoch 4741 | Train Loss: 1.2789497375488281 | Test Loss: 1.3427059650421143\n",
      "Epoch 4742 | Train Loss: 1.1337019205093384 | Test Loss: 1.1541680097579956\n",
      "Epoch 4743 | Train Loss: 1.278889775276184 | Test Loss: 1.3426039218902588\n",
      "Epoch 4744 | Train Loss: 1.1336358785629272 | Test Loss: 1.154144287109375\n",
      "Epoch 4745 | Train Loss: 1.2788161039352417 | Test Loss: 1.3426324129104614\n",
      "Epoch 4746 | Train Loss: 1.1335549354553223 | Test Loss: 1.1540476083755493\n",
      "Epoch 4747 | Train Loss: 1.2787585258483887 | Test Loss: 1.3425116539001465\n",
      "Epoch 4748 | Train Loss: 1.1334905624389648 | Test Loss: 1.1540215015411377\n",
      "Epoch 4749 | Train Loss: 1.2786813974380493 | Test Loss: 1.3425225019454956\n",
      "Epoch 4750 | Train Loss: 1.1334093809127808 | Test Loss: 1.1539227962493896\n",
      "Epoch 4751 | Train Loss: 1.2786284685134888 | Test Loss: 1.34242844581604\n",
      "Epoch 4752 | Train Loss: 1.13334059715271 | Test Loss: 1.153913140296936\n",
      "Epoch 4753 | Train Loss: 1.2785553932189941 | Test Loss: 1.3424495458602905\n",
      "Epoch 4754 | Train Loss: 1.1332629919052124 | Test Loss: 1.1537883281707764\n",
      "Epoch 4755 | Train Loss: 1.278489351272583 | Test Loss: 1.3423539400100708\n",
      "Epoch 4756 | Train Loss: 1.133203148841858 | Test Loss: 1.1537859439849854\n",
      "Epoch 4757 | Train Loss: 1.2784173488616943 | Test Loss: 1.342344045639038\n",
      "Epoch 4758 | Train Loss: 1.13310968875885 | Test Loss: 1.153659462928772\n",
      "Epoch 4759 | Train Loss: 1.2783730030059814 | Test Loss: 1.3422629833221436\n",
      "Epoch 4760 | Train Loss: 1.1330493688583374 | Test Loss: 1.1536673307418823\n",
      "Epoch 4761 | Train Loss: 1.2782825231552124 | Test Loss: 1.3422672748565674\n",
      "Epoch 4762 | Train Loss: 1.132972240447998 | Test Loss: 1.1535403728485107\n",
      "Epoch 4763 | Train Loss: 1.2782436609268188 | Test Loss: 1.342153787612915\n",
      "Epoch 4764 | Train Loss: 1.1328908205032349 | Test Loss: 1.1535580158233643\n",
      "Epoch 4765 | Train Loss: 1.2781636714935303 | Test Loss: 1.3421766757965088\n",
      "Epoch 4766 | Train Loss: 1.1328285932540894 | Test Loss: 1.1534115076065063\n",
      "Epoch 4767 | Train Loss: 1.2781120538711548 | Test Loss: 1.3420692682266235\n",
      "Epoch 4768 | Train Loss: 1.1327507495880127 | Test Loss: 1.1534398794174194\n",
      "Epoch 4769 | Train Loss: 1.278031826019287 | Test Loss: 1.3420339822769165\n",
      "Epoch 4770 | Train Loss: 1.1326764822006226 | Test Loss: 1.1532796621322632\n",
      "Epoch 4771 | Train Loss: 1.2779663801193237 | Test Loss: 1.3420006036758423\n",
      "Epoch 4772 | Train Loss: 1.1326138973236084 | Test Loss: 1.1532950401306152\n",
      "Epoch 4773 | Train Loss: 1.2778871059417725 | Test Loss: 1.3419455289840698\n",
      "Epoch 4774 | Train Loss: 1.1325292587280273 | Test Loss: 1.1531691551208496\n",
      "Epoch 4775 | Train Loss: 1.2778257131576538 | Test Loss: 1.3419106006622314\n",
      "Epoch 4776 | Train Loss: 1.1324721574783325 | Test Loss: 1.153152346611023\n",
      "Epoch 4777 | Train Loss: 1.2777377367019653 | Test Loss: 1.3418997526168823\n",
      "Epoch 4778 | Train Loss: 1.132397174835205 | Test Loss: 1.153051733970642\n",
      "Epoch 4779 | Train Loss: 1.2776896953582764 | Test Loss: 1.3417890071868896\n",
      "Epoch 4780 | Train Loss: 1.1323233842849731 | Test Loss: 1.153045892715454\n",
      "Epoch 4781 | Train Loss: 1.2776093482971191 | Test Loss: 1.3418231010437012\n",
      "Epoch 4782 | Train Loss: 1.1322617530822754 | Test Loss: 1.152913212776184\n",
      "Epoch 4783 | Train Loss: 1.2775561809539795 | Test Loss: 1.3417057991027832\n",
      "Epoch 4784 | Train Loss: 1.1321812868118286 | Test Loss: 1.1529356241226196\n",
      "Epoch 4785 | Train Loss: 1.277490258216858 | Test Loss: 1.3416881561279297\n",
      "Epoch 4786 | Train Loss: 1.1321076154708862 | Test Loss: 1.152794361114502\n",
      "Epoch 4787 | Train Loss: 1.2774168252944946 | Test Loss: 1.3416448831558228\n",
      "Epoch 4788 | Train Loss: 1.1320515871047974 | Test Loss: 1.152801513671875\n",
      "Epoch 4789 | Train Loss: 1.2773425579071045 | Test Loss: 1.3415833711624146\n",
      "Epoch 4790 | Train Loss: 1.1319634914398193 | Test Loss: 1.1526947021484375\n",
      "Epoch 4791 | Train Loss: 1.2772899866104126 | Test Loss: 1.3415322303771973\n",
      "Epoch 4792 | Train Loss: 1.1319048404693604 | Test Loss: 1.1526728868484497\n",
      "Epoch 4793 | Train Loss: 1.27720308303833 | Test Loss: 1.3415509462356567\n",
      "Epoch 4794 | Train Loss: 1.1318305730819702 | Test Loss: 1.1525639295578003\n",
      "Epoch 4795 | Train Loss: 1.2771506309509277 | Test Loss: 1.3414322137832642\n",
      "Epoch 4796 | Train Loss: 1.1317589282989502 | Test Loss: 1.1525660753250122\n",
      "Epoch 4797 | Train Loss: 1.2770763635635376 | Test Loss: 1.3414652347564697\n",
      "Epoch 4798 | Train Loss: 1.131691336631775 | Test Loss: 1.1524269580841064\n",
      "Epoch 4799 | Train Loss: 1.2770189046859741 | Test Loss: 1.3413540124893188\n",
      "Epoch 4800 | Train Loss: 1.1316205263137817 | Test Loss: 1.152441382408142\n",
      "Epoch 4801 | Train Loss: 1.276942253112793 | Test Loss: 1.341354489326477\n",
      "Epoch 4802 | Train Loss: 1.1315481662750244 | Test Loss: 1.152309536933899\n",
      "Epoch 4803 | Train Loss: 1.2768808603286743 | Test Loss: 1.3413017988204956\n",
      "Epoch 4804 | Train Loss: 1.1314884424209595 | Test Loss: 1.152312159538269\n",
      "Epoch 4805 | Train Loss: 1.2768036127090454 | Test Loss: 1.3412402868270874\n",
      "Epoch 4806 | Train Loss: 1.1314090490341187 | Test Loss: 1.1522085666656494\n",
      "Epoch 4807 | Train Loss: 1.276748776435852 | Test Loss: 1.341220736503601\n",
      "Epoch 4808 | Train Loss: 1.131341814994812 | Test Loss: 1.1521821022033691\n",
      "Epoch 4809 | Train Loss: 1.276670217514038 | Test Loss: 1.341172218322754\n",
      "Epoch 4810 | Train Loss: 1.1312717199325562 | Test Loss: 1.152079701423645\n",
      "Epoch 4811 | Train Loss: 1.2766183614730835 | Test Loss: 1.3411225080490112\n",
      "Epoch 4812 | Train Loss: 1.1311975717544556 | Test Loss: 1.1520699262619019\n",
      "Epoch 4813 | Train Loss: 1.276540994644165 | Test Loss: 1.3410747051239014\n",
      "Epoch 4814 | Train Loss: 1.1311349868774414 | Test Loss: 1.1519560813903809\n",
      "Epoch 4815 | Train Loss: 1.2764830589294434 | Test Loss: 1.3410124778747559\n",
      "Epoch 4816 | Train Loss: 1.1310597658157349 | Test Loss: 1.1519542932510376\n",
      "Epoch 4817 | Train Loss: 1.276413917541504 | Test Loss: 1.3409889936447144\n",
      "Epoch 4818 | Train Loss: 1.130994439125061 | Test Loss: 1.1518350839614868\n",
      "Epoch 4819 | Train Loss: 1.2763522863388062 | Test Loss: 1.3409154415130615\n",
      "Epoch 4820 | Train Loss: 1.1309200525283813 | Test Loss: 1.1518266201019287\n",
      "Epoch 4821 | Train Loss: 1.2762908935546875 | Test Loss: 1.340890645980835\n",
      "Epoch 4822 | Train Loss: 1.1308454275131226 | Test Loss: 1.151719093322754\n",
      "Epoch 4823 | Train Loss: 1.2762187719345093 | Test Loss: 1.3408371210098267\n",
      "Epoch 4824 | Train Loss: 1.130791425704956 | Test Loss: 1.1516937017440796\n",
      "Epoch 4825 | Train Loss: 1.2761460542678833 | Test Loss: 1.3408066034317017\n",
      "Epoch 4826 | Train Loss: 1.1307092905044556 | Test Loss: 1.1516029834747314\n",
      "Epoch 4827 | Train Loss: 1.2760930061340332 | Test Loss: 1.3407232761383057\n",
      "Epoch 4828 | Train Loss: 1.1306489706039429 | Test Loss: 1.1515705585479736\n",
      "Epoch 4829 | Train Loss: 1.2760111093521118 | Test Loss: 1.3407385349273682\n",
      "Epoch 4830 | Train Loss: 1.1305785179138184 | Test Loss: 1.1514778137207031\n",
      "Epoch 4831 | Train Loss: 1.2759596109390259 | Test Loss: 1.340641975402832\n",
      "Epoch 4832 | Train Loss: 1.1305041313171387 | Test Loss: 1.1514606475830078\n",
      "Epoch 4833 | Train Loss: 1.2758891582489014 | Test Loss: 1.3406367301940918\n",
      "Epoch 4834 | Train Loss: 1.13044011592865 | Test Loss: 1.1513456106185913\n",
      "Epoch 4835 | Train Loss: 1.2758299112319946 | Test Loss: 1.3405646085739136\n",
      "Epoch 4836 | Train Loss: 1.1303682327270508 | Test Loss: 1.1513413190841675\n",
      "Epoch 4837 | Train Loss: 1.2757604122161865 | Test Loss: 1.3405330181121826\n",
      "Epoch 4838 | Train Loss: 1.1303011178970337 | Test Loss: 1.1512424945831299\n",
      "Epoch 4839 | Train Loss: 1.2756978273391724 | Test Loss: 1.34047269821167\n",
      "Epoch 4840 | Train Loss: 1.130236029624939 | Test Loss: 1.1512129306793213\n",
      "Epoch 4841 | Train Loss: 1.2756295204162598 | Test Loss: 1.3404467105865479\n",
      "Epoch 4842 | Train Loss: 1.1301649808883667 | Test Loss: 1.1511274576187134\n",
      "Epoch 4843 | Train Loss: 1.2755582332611084 | Test Loss: 1.3403798341751099\n",
      "Epoch 4844 | Train Loss: 1.1301084756851196 | Test Loss: 1.1510823965072632\n",
      "Epoch 4845 | Train Loss: 1.2754907608032227 | Test Loss: 1.3403476476669312\n",
      "Epoch 4846 | Train Loss: 1.1300287246704102 | Test Loss: 1.1510050296783447\n",
      "Epoch 4847 | Train Loss: 1.2754331827163696 | Test Loss: 1.340283751487732\n",
      "Epoch 4848 | Train Loss: 1.1299666166305542 | Test Loss: 1.1509705781936646\n",
      "Epoch 4849 | Train Loss: 1.2753629684448242 | Test Loss: 1.3402689695358276\n",
      "Epoch 4850 | Train Loss: 1.1298936605453491 | Test Loss: 1.1508755683898926\n",
      "Epoch 4851 | Train Loss: 1.275304913520813 | Test Loss: 1.3401998281478882\n",
      "Epoch 4852 | Train Loss: 1.1298296451568604 | Test Loss: 1.1508506536483765\n",
      "Epoch 4853 | Train Loss: 1.2752315998077393 | Test Loss: 1.3401705026626587\n",
      "Epoch 4854 | Train Loss: 1.1297610998153687 | Test Loss: 1.1507607698440552\n",
      "Epoch 4855 | Train Loss: 1.2751741409301758 | Test Loss: 1.3401082754135132\n",
      "Epoch 4856 | Train Loss: 1.1296935081481934 | Test Loss: 1.1507368087768555\n",
      "Epoch 4857 | Train Loss: 1.275104284286499 | Test Loss: 1.340086579322815\n",
      "Epoch 4858 | Train Loss: 1.1296225786209106 | Test Loss: 1.1506367921829224\n",
      "Epoch 4859 | Train Loss: 1.2750493288040161 | Test Loss: 1.3400226831436157\n",
      "Epoch 4860 | Train Loss: 1.129553198814392 | Test Loss: 1.1506041288375854\n",
      "Epoch 4861 | Train Loss: 1.274981141090393 | Test Loss: 1.340015172958374\n",
      "Epoch 4862 | Train Loss: 1.1294848918914795 | Test Loss: 1.150520920753479\n",
      "Epoch 4863 | Train Loss: 1.2749167680740356 | Test Loss: 1.3399238586425781\n",
      "Epoch 4864 | Train Loss: 1.1294209957122803 | Test Loss: 1.1504852771759033\n",
      "Epoch 4865 | Train Loss: 1.2748537063598633 | Test Loss: 1.339920997619629\n",
      "Epoch 4866 | Train Loss: 1.1293483972549438 | Test Loss: 1.1503996849060059\n",
      "Epoch 4867 | Train Loss: 1.2747881412506104 | Test Loss: 1.339848279953003\n",
      "Epoch 4868 | Train Loss: 1.1292906999588013 | Test Loss: 1.1503649950027466\n",
      "Epoch 4869 | Train Loss: 1.2747223377227783 | Test Loss: 1.3398411273956299\n",
      "Epoch 4870 | Train Loss: 1.1292102336883545 | Test Loss: 1.1502810716629028\n",
      "Epoch 4871 | Train Loss: 1.2746676206588745 | Test Loss: 1.3397624492645264\n",
      "Epoch 4872 | Train Loss: 1.1291530132293701 | Test Loss: 1.150251030921936\n",
      "Epoch 4873 | Train Loss: 1.2745975255966187 | Test Loss: 1.3397737741470337\n",
      "Epoch 4874 | Train Loss: 1.129075288772583 | Test Loss: 1.1501579284667969\n",
      "Epoch 4875 | Train Loss: 1.2745440006256104 | Test Loss: 1.3396637439727783\n",
      "Epoch 4876 | Train Loss: 1.1290156841278076 | Test Loss: 1.1501423120498657\n",
      "Epoch 4877 | Train Loss: 1.274472951889038 | Test Loss: 1.339704990386963\n",
      "Epoch 4878 | Train Loss: 1.1289441585540771 | Test Loss: 1.1500399112701416\n",
      "Epoch 4879 | Train Loss: 1.274419903755188 | Test Loss: 1.3395812511444092\n",
      "Epoch 4880 | Train Loss: 1.1288844347000122 | Test Loss: 1.1500427722930908\n",
      "Epoch 4881 | Train Loss: 1.2743489742279053 | Test Loss: 1.339609980583191\n",
      "Epoch 4882 | Train Loss: 1.1288100481033325 | Test Loss: 1.1499146223068237\n",
      "Epoch 4883 | Train Loss: 1.274293303489685 | Test Loss: 1.33953058719635\n",
      "Epoch 4884 | Train Loss: 1.1287490129470825 | Test Loss: 1.1499265432357788\n",
      "Epoch 4885 | Train Loss: 1.2742302417755127 | Test Loss: 1.3395168781280518\n",
      "Epoch 4886 | Train Loss: 1.1286715269088745 | Test Loss: 1.1498080492019653\n",
      "Epoch 4887 | Train Loss: 1.2741748094558716 | Test Loss: 1.3394237756729126\n",
      "Epoch 4888 | Train Loss: 1.1286189556121826 | Test Loss: 1.1498064994812012\n",
      "Epoch 4889 | Train Loss: 1.2740974426269531 | Test Loss: 1.339443325996399\n",
      "Epoch 4890 | Train Loss: 1.128537654876709 | Test Loss: 1.149699330329895\n",
      "Epoch 4891 | Train Loss: 1.2740557193756104 | Test Loss: 1.3393275737762451\n",
      "Epoch 4892 | Train Loss: 1.1284747123718262 | Test Loss: 1.149696946144104\n",
      "Epoch 4893 | Train Loss: 1.2739814519882202 | Test Loss: 1.3393735885620117\n",
      "Epoch 4894 | Train Loss: 1.128409504890442 | Test Loss: 1.1495680809020996\n",
      "Epoch 4895 | Train Loss: 1.2739253044128418 | Test Loss: 1.3392560482025146\n",
      "Epoch 4896 | Train Loss: 1.1283421516418457 | Test Loss: 1.1495826244354248\n",
      "Epoch 4897 | Train Loss: 1.2738595008850098 | Test Loss: 1.3392750024795532\n",
      "Epoch 4898 | Train Loss: 1.1282694339752197 | Test Loss: 1.1494439840316772\n",
      "Epoch 4899 | Train Loss: 1.2738093137741089 | Test Loss: 1.339196801185608\n",
      "Epoch 4900 | Train Loss: 1.128210186958313 | Test Loss: 1.149469017982483\n",
      "Epoch 4901 | Train Loss: 1.273734211921692 | Test Loss: 1.339168667793274\n",
      "Epoch 4902 | Train Loss: 1.128134846687317 | Test Loss: 1.1493419408798218\n",
      "Epoch 4903 | Train Loss: 1.2736749649047852 | Test Loss: 1.3391062021255493\n",
      "Epoch 4904 | Train Loss: 1.1280790567398071 | Test Loss: 1.1493388414382935\n",
      "Epoch 4905 | Train Loss: 1.2735999822616577 | Test Loss: 1.3391097784042358\n",
      "Epoch 4906 | Train Loss: 1.1280062198638916 | Test Loss: 1.1492211818695068\n",
      "Epoch 4907 | Train Loss: 1.2735484838485718 | Test Loss: 1.339007019996643\n",
      "Epoch 4908 | Train Loss: 1.127948522567749 | Test Loss: 1.1492329835891724\n",
      "Epoch 4909 | Train Loss: 1.2734777927398682 | Test Loss: 1.339018702507019\n",
      "Epoch 4910 | Train Loss: 1.127872109413147 | Test Loss: 1.1491049528121948\n",
      "Epoch 4911 | Train Loss: 1.2734251022338867 | Test Loss: 1.3389323949813843\n",
      "Epoch 4912 | Train Loss: 1.127816915512085 | Test Loss: 1.1491262912750244\n",
      "Epoch 4913 | Train Loss: 1.2733511924743652 | Test Loss: 1.338964581489563\n",
      "Epoch 4914 | Train Loss: 1.1277490854263306 | Test Loss: 1.1489927768707275\n",
      "Epoch 4915 | Train Loss: 1.273296594619751 | Test Loss: 1.3388404846191406\n",
      "Epoch 4916 | Train Loss: 1.1276847124099731 | Test Loss: 1.149019479751587\n",
      "Epoch 4917 | Train Loss: 1.2732292413711548 | Test Loss: 1.3388595581054688\n",
      "Epoch 4918 | Train Loss: 1.127611517906189 | Test Loss: 1.148894190788269\n",
      "Epoch 4919 | Train Loss: 1.273166537284851 | Test Loss: 1.338762640953064\n",
      "Epoch 4920 | Train Loss: 1.1275638341903687 | Test Loss: 1.1489008665084839\n",
      "Epoch 4921 | Train Loss: 1.2730929851531982 | Test Loss: 1.3387744426727295\n",
      "Epoch 4922 | Train Loss: 1.1274892091751099 | Test Loss: 1.1487871408462524\n",
      "Epoch 4923 | Train Loss: 1.2730412483215332 | Test Loss: 1.3386768102645874\n",
      "Epoch 4924 | Train Loss: 1.1274311542510986 | Test Loss: 1.1487925052642822\n",
      "Epoch 4925 | Train Loss: 1.272965908050537 | Test Loss: 1.338686466217041\n",
      "Epoch 4926 | Train Loss: 1.127363920211792 | Test Loss: 1.1486696004867554\n",
      "Epoch 4927 | Train Loss: 1.2729063034057617 | Test Loss: 1.3386027812957764\n",
      "Epoch 4928 | Train Loss: 1.1273101568222046 | Test Loss: 1.1486796140670776\n",
      "Epoch 4929 | Train Loss: 1.2728369235992432 | Test Loss: 1.3386002779006958\n",
      "Epoch 4930 | Train Loss: 1.1272358894348145 | Test Loss: 1.1485494375228882\n",
      "Epoch 4931 | Train Loss: 1.2727854251861572 | Test Loss: 1.338512897491455\n",
      "Epoch 4932 | Train Loss: 1.1271820068359375 | Test Loss: 1.1485801935195923\n",
      "Epoch 4933 | Train Loss: 1.2727103233337402 | Test Loss: 1.338519811630249\n",
      "Epoch 4934 | Train Loss: 1.127111792564392 | Test Loss: 1.1484549045562744\n",
      "Epoch 4935 | Train Loss: 1.272659182548523 | Test Loss: 1.3384149074554443\n",
      "Epoch 4936 | Train Loss: 1.1270548105239868 | Test Loss: 1.1484706401824951\n",
      "Epoch 4937 | Train Loss: 1.2725880146026611 | Test Loss: 1.338448166847229\n",
      "Epoch 4938 | Train Loss: 1.1269829273223877 | Test Loss: 1.1483372449874878\n",
      "Epoch 4939 | Train Loss: 1.2725383043289185 | Test Loss: 1.3383276462554932\n",
      "Epoch 4940 | Train Loss: 1.1269296407699585 | Test Loss: 1.148363471031189\n",
      "Epoch 4941 | Train Loss: 1.272464632987976 | Test Loss: 1.338371753692627\n",
      "Epoch 4942 | Train Loss: 1.1268620491027832 | Test Loss: 1.1482279300689697\n",
      "Epoch 4943 | Train Loss: 1.2724117040634155 | Test Loss: 1.338239312171936\n",
      "Epoch 4944 | Train Loss: 1.1268056631088257 | Test Loss: 1.148271083831787\n",
      "Epoch 4945 | Train Loss: 1.272344708442688 | Test Loss: 1.3383102416992188\n",
      "Epoch 4946 | Train Loss: 1.1267369985580444 | Test Loss: 1.148121953010559\n",
      "Epoch 4947 | Train Loss: 1.272290587425232 | Test Loss: 1.3381627798080444\n",
      "Epoch 4948 | Train Loss: 1.1266883611679077 | Test Loss: 1.1481715440750122\n",
      "Epoch 4949 | Train Loss: 1.2722169160842896 | Test Loss: 1.3382019996643066\n",
      "Epoch 4950 | Train Loss: 1.126617193222046 | Test Loss: 1.1480176448822021\n",
      "Epoch 4951 | Train Loss: 1.2721651792526245 | Test Loss: 1.3380849361419678\n",
      "Epoch 4952 | Train Loss: 1.126564621925354 | Test Loss: 1.148064374923706\n",
      "Epoch 4953 | Train Loss: 1.2720874547958374 | Test Loss: 1.3381036520004272\n",
      "Epoch 4954 | Train Loss: 1.1264971494674683 | Test Loss: 1.1479053497314453\n",
      "Epoch 4955 | Train Loss: 1.272032380104065 | Test Loss: 1.338027000427246\n",
      "Epoch 4956 | Train Loss: 1.1264399290084839 | Test Loss: 1.1479586362838745\n",
      "Epoch 4957 | Train Loss: 1.2719565629959106 | Test Loss: 1.3380074501037598\n",
      "Epoch 4958 | Train Loss: 1.1263707876205444 | Test Loss: 1.1477993726730347\n",
      "Epoch 4959 | Train Loss: 1.2719062566757202 | Test Loss: 1.3379539251327515\n",
      "Epoch 4960 | Train Loss: 1.1263142824172974 | Test Loss: 1.1478452682495117\n",
      "Epoch 4961 | Train Loss: 1.2718300819396973 | Test Loss: 1.337937593460083\n",
      "Epoch 4962 | Train Loss: 1.1262511014938354 | Test Loss: 1.1476982831954956\n",
      "Epoch 4963 | Train Loss: 1.2717784643173218 | Test Loss: 1.3378551006317139\n",
      "Epoch 4964 | Train Loss: 1.1261886358261108 | Test Loss: 1.1477224826812744\n",
      "Epoch 4965 | Train Loss: 1.2717009782791138 | Test Loss: 1.3378586769104004\n",
      "Epoch 4966 | Train Loss: 1.126127004623413 | Test Loss: 1.1475995779037476\n",
      "Epoch 4967 | Train Loss: 1.2716480493545532 | Test Loss: 1.3377761840820312\n",
      "Epoch 4968 | Train Loss: 1.126064658164978 | Test Loss: 1.1476118564605713\n",
      "Epoch 4969 | Train Loss: 1.271575689315796 | Test Loss: 1.3377752304077148\n",
      "Epoch 4970 | Train Loss: 1.1260029077529907 | Test Loss: 1.1474908590316772\n",
      "Epoch 4971 | Train Loss: 1.271522879600525 | Test Loss: 1.3377047777175903\n",
      "Epoch 4972 | Train Loss: 1.1259400844573975 | Test Loss: 1.147498369216919\n",
      "Epoch 4973 | Train Loss: 1.2714513540267944 | Test Loss: 1.3376896381378174\n",
      "Epoch 4974 | Train Loss: 1.125882863998413 | Test Loss: 1.1473805904388428\n",
      "Epoch 4975 | Train Loss: 1.2713896036148071 | Test Loss: 1.3376333713531494\n",
      "Epoch 4976 | Train Loss: 1.1258213520050049 | Test Loss: 1.1473850011825562\n",
      "Epoch 4977 | Train Loss: 1.2713251113891602 | Test Loss: 1.3375860452651978\n",
      "Epoch 4978 | Train Loss: 1.1257543563842773 | Test Loss: 1.1472760438919067\n",
      "Epoch 4979 | Train Loss: 1.2712652683258057 | Test Loss: 1.3375489711761475\n",
      "Epoch 4980 | Train Loss: 1.1257052421569824 | Test Loss: 1.147270679473877\n",
      "Epoch 4981 | Train Loss: 1.2711902856826782 | Test Loss: 1.337497591972351\n",
      "Epoch 4982 | Train Loss: 1.1256409883499146 | Test Loss: 1.1471812725067139\n",
      "Epoch 4983 | Train Loss: 1.2711328268051147 | Test Loss: 1.3374824523925781\n",
      "Epoch 4984 | Train Loss: 1.1255855560302734 | Test Loss: 1.1471643447875977\n",
      "Epoch 4985 | Train Loss: 1.2710661888122559 | Test Loss: 1.3374392986297607\n",
      "Epoch 4986 | Train Loss: 1.1255220174789429 | Test Loss: 1.1470869779586792\n",
      "Epoch 4987 | Train Loss: 1.271016240119934 | Test Loss: 1.3373905420303345\n",
      "Epoch 4988 | Train Loss: 1.125458836555481 | Test Loss: 1.1470534801483154\n",
      "Epoch 4989 | Train Loss: 1.2709449529647827 | Test Loss: 1.337364912033081\n",
      "Epoch 4990 | Train Loss: 1.125404953956604 | Test Loss: 1.1469841003417969\n",
      "Epoch 4991 | Train Loss: 1.2708820104599 | Test Loss: 1.3372942209243774\n",
      "Epoch 4992 | Train Loss: 1.1253451108932495 | Test Loss: 1.1469439268112183\n",
      "Epoch 4993 | Train Loss: 1.2708204984664917 | Test Loss: 1.337275743484497\n",
      "Epoch 4994 | Train Loss: 1.125285029411316 | Test Loss: 1.1468796730041504\n",
      "Epoch 4995 | Train Loss: 1.2707594633102417 | Test Loss: 1.3372149467468262\n",
      "Epoch 4996 | Train Loss: 1.1252261400222778 | Test Loss: 1.1468394994735718\n",
      "Epoch 4997 | Train Loss: 1.2707029581069946 | Test Loss: 1.3371989727020264\n",
      "Epoch 4998 | Train Loss: 1.125160813331604 | Test Loss: 1.1467934846878052\n",
      "Epoch 4999 | Train Loss: 1.2706366777420044 | Test Loss: 1.337143063545227\n",
      "Epoch 5000 | Train Loss: 1.12511146068573 | Test Loss: 1.1467365026474\n",
      "Epoch 5001 | Train Loss: 1.2705754041671753 | Test Loss: 1.337119221687317\n",
      "Epoch 5002 | Train Loss: 1.125045657157898 | Test Loss: 1.1466913223266602\n",
      "Epoch 5003 | Train Loss: 1.270512342453003 | Test Loss: 1.337053656578064\n",
      "Epoch 5004 | Train Loss: 1.1249959468841553 | Test Loss: 1.1466373205184937\n",
      "Epoch 5005 | Train Loss: 1.2704477310180664 | Test Loss: 1.3370182514190674\n",
      "Epoch 5006 | Train Loss: 1.1249301433563232 | Test Loss: 1.1465901136398315\n",
      "Epoch 5007 | Train Loss: 1.270391821861267 | Test Loss: 1.3369786739349365\n",
      "Epoch 5008 | Train Loss: 1.1248745918273926 | Test Loss: 1.1465445756912231\n",
      "Epoch 5009 | Train Loss: 1.2703202962875366 | Test Loss: 1.336922526359558\n",
      "Epoch 5010 | Train Loss: 1.124818205833435 | Test Loss: 1.1464810371398926\n",
      "Epoch 5011 | Train Loss: 1.2702661752700806 | Test Loss: 1.3368861675262451\n",
      "Epoch 5012 | Train Loss: 1.1247587203979492 | Test Loss: 1.1464622020721436\n",
      "Epoch 5013 | Train Loss: 1.2701964378356934 | Test Loss: 1.3368595838546753\n",
      "Epoch 5014 | Train Loss: 1.1247084140777588 | Test Loss: 1.1463812589645386\n",
      "Epoch 5015 | Train Loss: 1.2701380252838135 | Test Loss: 1.336805820465088\n",
      "Epoch 5016 | Train Loss: 1.1246459484100342 | Test Loss: 1.1463658809661865\n",
      "Epoch 5017 | Train Loss: 1.2700726985931396 | Test Loss: 1.336793303489685\n",
      "Epoch 5018 | Train Loss: 1.124592661857605 | Test Loss: 1.146289348602295\n",
      "Epoch 5019 | Train Loss: 1.270010232925415 | Test Loss: 1.3367308378219604\n",
      "Epoch 5020 | Train Loss: 1.1245354413986206 | Test Loss: 1.1462628841400146\n",
      "Epoch 5021 | Train Loss: 1.2699490785598755 | Test Loss: 1.3367456197738647\n",
      "Epoch 5022 | Train Loss: 1.124472737312317 | Test Loss: 1.1461894512176514\n",
      "Epoch 5023 | Train Loss: 1.2698915004730225 | Test Loss: 1.3366390466690063\n",
      "Epoch 5024 | Train Loss: 1.1244237422943115 | Test Loss: 1.1461714506149292\n",
      "Epoch 5025 | Train Loss: 1.269827127456665 | Test Loss: 1.3366875648498535\n",
      "Epoch 5026 | Train Loss: 1.1243579387664795 | Test Loss: 1.1460962295532227\n",
      "Epoch 5027 | Train Loss: 1.2697705030441284 | Test Loss: 1.3365604877471924\n",
      "Epoch 5028 | Train Loss: 1.1243126392364502 | Test Loss: 1.1460890769958496\n",
      "Epoch 5029 | Train Loss: 1.2697067260742188 | Test Loss: 1.3366053104400635\n",
      "Epoch 5030 | Train Loss: 1.1242424249649048 | Test Loss: 1.1459866762161255\n",
      "Epoch 5031 | Train Loss: 1.2696621417999268 | Test Loss: 1.3364585638046265\n",
      "Epoch 5032 | Train Loss: 1.12419855594635 | Test Loss: 1.1460176706314087\n",
      "Epoch 5033 | Train Loss: 1.2695939540863037 | Test Loss: 1.3365287780761719\n",
      "Epoch 5034 | Train Loss: 1.1241300106048584 | Test Loss: 1.145877480506897\n",
      "Epoch 5035 | Train Loss: 1.269553542137146 | Test Loss: 1.3363869190216064\n",
      "Epoch 5036 | Train Loss: 1.124084711074829 | Test Loss: 1.1459509134292603\n",
      "Epoch 5037 | Train Loss: 1.2694848775863647 | Test Loss: 1.3364454507827759\n",
      "Epoch 5038 | Train Loss: 1.1240116357803345 | Test Loss: 1.1457796096801758\n",
      "Epoch 5039 | Train Loss: 1.2694579362869263 | Test Loss: 1.3363041877746582\n",
      "Epoch 5040 | Train Loss: 1.1239696741104126 | Test Loss: 1.1458712816238403\n",
      "Epoch 5041 | Train Loss: 1.2693800926208496 | Test Loss: 1.336359977722168\n",
      "Epoch 5042 | Train Loss: 1.1239014863967896 | Test Loss: 1.1457010507583618\n",
      "Epoch 5043 | Train Loss: 1.2693469524383545 | Test Loss: 1.3362431526184082\n",
      "Epoch 5044 | Train Loss: 1.1238539218902588 | Test Loss: 1.1457858085632324\n",
      "Epoch 5045 | Train Loss: 1.2692663669586182 | Test Loss: 1.336277961730957\n",
      "Epoch 5046 | Train Loss: 1.1237903833389282 | Test Loss: 1.1456022262573242\n",
      "Epoch 5047 | Train Loss: 1.269234299659729 | Test Loss: 1.3361819982528687\n",
      "Epoch 5048 | Train Loss: 1.1237342357635498 | Test Loss: 1.1456959247589111\n",
      "Epoch 5049 | Train Loss: 1.269152045249939 | Test Loss: 1.3361873626708984\n",
      "Epoch 5050 | Train Loss: 1.1236728429794312 | Test Loss: 1.1455072164535522\n",
      "Epoch 5051 | Train Loss: 1.269111156463623 | Test Loss: 1.3360819816589355\n",
      "Epoch 5052 | Train Loss: 1.1236193180084229 | Test Loss: 1.1455845832824707\n",
      "Epoch 5053 | Train Loss: 1.269025206565857 | Test Loss: 1.3360923528671265\n",
      "Epoch 5054 | Train Loss: 1.1235570907592773 | Test Loss: 1.145423412322998\n",
      "Epoch 5055 | Train Loss: 1.2689632177352905 | Test Loss: 1.3360365629196167\n",
      "Epoch 5056 | Train Loss: 1.1235089302062988 | Test Loss: 1.1454565525054932\n",
      "Epoch 5057 | Train Loss: 1.2688707113265991 | Test Loss: 1.3359906673431396\n",
      "Epoch 5058 | Train Loss: 1.1234486103057861 | Test Loss: 1.1453169584274292\n",
      "Epoch 5059 | Train Loss: 1.268824815750122 | Test Loss: 1.3359670639038086\n",
      "Epoch 5060 | Train Loss: 1.1233898401260376 | Test Loss: 1.1453485488891602\n",
      "Epoch 5061 | Train Loss: 1.2687422037124634 | Test Loss: 1.3359043598175049\n",
      "Epoch 5062 | Train Loss: 1.123337984085083 | Test Loss: 1.1452101469039917\n",
      "Epoch 5063 | Train Loss: 1.2686982154846191 | Test Loss: 1.335898995399475\n",
      "Epoch 5064 | Train Loss: 1.12327241897583 | Test Loss: 1.1452387571334839\n",
      "Epoch 5065 | Train Loss: 1.2686141729354858 | Test Loss: 1.3358218669891357\n",
      "Epoch 5066 | Train Loss: 1.123240351676941 | Test Loss: 1.1451282501220703\n",
      "Epoch 5067 | Train Loss: 1.2685562372207642 | Test Loss: 1.335822343826294\n",
      "Epoch 5068 | Train Loss: 1.123166561126709 | Test Loss: 1.1451133489608765\n",
      "Epoch 5069 | Train Loss: 1.2684932947158813 | Test Loss: 1.3357226848602295\n",
      "Epoch 5070 | Train Loss: 1.1231176853179932 | Test Loss: 1.1450449228286743\n",
      "Epoch 5071 | Train Loss: 1.2684322595596313 | Test Loss: 1.3357452154159546\n",
      "Epoch 5072 | Train Loss: 1.1230560541152954 | Test Loss: 1.145010232925415\n",
      "Epoch 5073 | Train Loss: 1.2683711051940918 | Test Loss: 1.3356529474258423\n",
      "Epoch 5074 | Train Loss: 1.123002052307129 | Test Loss: 1.1449581384658813\n",
      "Epoch 5075 | Train Loss: 1.2683109045028687 | Test Loss: 1.3356553316116333\n",
      "Epoch 5076 | Train Loss: 1.1229455471038818 | Test Loss: 1.1449190378189087\n",
      "Epoch 5077 | Train Loss: 1.268249750137329 | Test Loss: 1.3356083631515503\n",
      "Epoch 5078 | Train Loss: 1.1228909492492676 | Test Loss: 1.1448613405227661\n",
      "Epoch 5079 | Train Loss: 1.2681984901428223 | Test Loss: 1.335553765296936\n",
      "Epoch 5080 | Train Loss: 1.1228307485580444 | Test Loss: 1.1448323726654053\n",
      "Epoch 5081 | Train Loss: 1.2681306600570679 | Test Loss: 1.3355480432510376\n",
      "Epoch 5082 | Train Loss: 1.122785210609436 | Test Loss: 1.1447649002075195\n",
      "Epoch 5083 | Train Loss: 1.2680740356445312 | Test Loss: 1.3354663848876953\n",
      "Epoch 5084 | Train Loss: 1.122719168663025 | Test Loss: 1.14473295211792\n",
      "Epoch 5085 | Train Loss: 1.2680176496505737 | Test Loss: 1.335479974746704\n",
      "Epoch 5086 | Train Loss: 1.122666835784912 | Test Loss: 1.1446579694747925\n",
      "Epoch 5087 | Train Loss: 1.2679587602615356 | Test Loss: 1.3354227542877197\n",
      "Epoch 5088 | Train Loss: 1.1226104497909546 | Test Loss: 1.1446486711502075\n",
      "Epoch 5089 | Train Loss: 1.2679005861282349 | Test Loss: 1.3353862762451172\n",
      "Epoch 5090 | Train Loss: 1.1225506067276 | Test Loss: 1.1445579528808594\n",
      "Epoch 5091 | Train Loss: 1.267849087715149 | Test Loss: 1.335365891456604\n",
      "Epoch 5092 | Train Loss: 1.1224963665008545 | Test Loss: 1.1445499658584595\n",
      "Epoch 5093 | Train Loss: 1.267786979675293 | Test Loss: 1.3353511095046997\n",
      "Epoch 5094 | Train Loss: 1.1224408149719238 | Test Loss: 1.1444615125656128\n",
      "Epoch 5095 | Train Loss: 1.2677334547042847 | Test Loss: 1.33526611328125\n",
      "Epoch 5096 | Train Loss: 1.1223918199539185 | Test Loss: 1.1444692611694336\n",
      "Epoch 5097 | Train Loss: 1.2676677703857422 | Test Loss: 1.335299015045166\n",
      "Epoch 5098 | Train Loss: 1.1223363876342773 | Test Loss: 1.1443846225738525\n",
      "Epoch 5099 | Train Loss: 1.2676235437393188 | Test Loss: 1.3351843357086182\n",
      "Epoch 5100 | Train Loss: 1.1222853660583496 | Test Loss: 1.1443983316421509\n",
      "Epoch 5101 | Train Loss: 1.2675610780715942 | Test Loss: 1.33524751663208\n",
      "Epoch 5102 | Train Loss: 1.122230887413025 | Test Loss: 1.1442774534225464\n",
      "Epoch 5103 | Train Loss: 1.2675138711929321 | Test Loss: 1.335099458694458\n",
      "Epoch 5104 | Train Loss: 1.122179627418518 | Test Loss: 1.1443369388580322\n",
      "Epoch 5105 | Train Loss: 1.2674602270126343 | Test Loss: 1.3351523876190186\n",
      "Epoch 5106 | Train Loss: 1.1221123933792114 | Test Loss: 1.1441813707351685\n",
      "Epoch 5107 | Train Loss: 1.2674115896224976 | Test Loss: 1.335042119026184\n",
      "Epoch 5108 | Train Loss: 1.1220810413360596 | Test Loss: 1.144266128540039\n",
      "Epoch 5109 | Train Loss: 1.267342209815979 | Test Loss: 1.3350893259048462\n",
      "Epoch 5110 | Train Loss: 1.122005820274353 | Test Loss: 1.144086241722107\n",
      "Epoch 5111 | Train Loss: 1.2673190832138062 | Test Loss: 1.3349441289901733\n",
      "Epoch 5112 | Train Loss: 1.121962308883667 | Test Loss: 1.144187092781067\n",
      "Epoch 5113 | Train Loss: 1.2672393321990967 | Test Loss: 1.3350292444229126\n",
      "Epoch 5114 | Train Loss: 1.1219004392623901 | Test Loss: 1.1439898014068604\n",
      "Epoch 5115 | Train Loss: 1.2672059535980225 | Test Loss: 1.3348653316497803\n",
      "Epoch 5116 | Train Loss: 1.1218507289886475 | Test Loss: 1.1440997123718262\n",
      "Epoch 5117 | Train Loss: 1.2671321630477905 | Test Loss: 1.3349366188049316\n",
      "Epoch 5118 | Train Loss: 1.1217854022979736 | Test Loss: 1.1438974142074585\n",
      "Epoch 5119 | Train Loss: 1.2670965194702148 | Test Loss: 1.334830641746521\n",
      "Epoch 5120 | Train Loss: 1.1217360496520996 | Test Loss: 1.1439992189407349\n",
      "Epoch 5121 | Train Loss: 1.2670090198516846 | Test Loss: 1.3348299264907837\n",
      "Epoch 5122 | Train Loss: 1.1216742992401123 | Test Loss: 1.143803358078003\n",
      "Epoch 5123 | Train Loss: 1.266969919204712 | Test Loss: 1.3347606658935547\n",
      "Epoch 5124 | Train Loss: 1.1216257810592651 | Test Loss: 1.1438772678375244\n",
      "Epoch 5125 | Train Loss: 1.2668777704238892 | Test Loss: 1.3347231149673462\n",
      "Epoch 5126 | Train Loss: 1.121566891670227 | Test Loss: 1.1437156200408936\n",
      "Epoch 5127 | Train Loss: 1.2668280601501465 | Test Loss: 1.3346760272979736\n",
      "Epoch 5128 | Train Loss: 1.1215155124664307 | Test Loss: 1.1437522172927856\n",
      "Epoch 5129 | Train Loss: 1.2667447328567505 | Test Loss: 1.334641456604004\n",
      "Epoch 5130 | Train Loss: 1.1214593648910522 | Test Loss: 1.143620252609253\n",
      "Epoch 5131 | Train Loss: 1.26669442653656 | Test Loss: 1.3345991373062134\n",
      "Epoch 5132 | Train Loss: 1.1214059591293335 | Test Loss: 1.1436611413955688\n",
      "Epoch 5133 | Train Loss: 1.2666144371032715 | Test Loss: 1.334552526473999\n",
      "Epoch 5134 | Train Loss: 1.1213527917861938 | Test Loss: 1.1435275077819824\n",
      "Epoch 5135 | Train Loss: 1.2665647268295288 | Test Loss: 1.3344959020614624\n",
      "Epoch 5136 | Train Loss: 1.1212942600250244 | Test Loss: 1.1435552835464478\n",
      "Epoch 5137 | Train Loss: 1.2664917707443237 | Test Loss: 1.3344706296920776\n",
      "Epoch 5138 | Train Loss: 1.1212457418441772 | Test Loss: 1.143436312675476\n",
      "Epoch 5139 | Train Loss: 1.2664421796798706 | Test Loss: 1.3344143629074097\n",
      "Epoch 5140 | Train Loss: 1.121187686920166 | Test Loss: 1.143459677696228\n",
      "Epoch 5141 | Train Loss: 1.2663681507110596 | Test Loss: 1.3343963623046875\n",
      "Epoch 5142 | Train Loss: 1.1211447715759277 | Test Loss: 1.1433491706848145\n",
      "Epoch 5143 | Train Loss: 1.2663105726242065 | Test Loss: 1.3343654870986938\n",
      "Epoch 5144 | Train Loss: 1.1210905313491821 | Test Loss: 1.1433746814727783\n",
      "Epoch 5145 | Train Loss: 1.266245722770691 | Test Loss: 1.3343380689620972\n",
      "Epoch 5146 | Train Loss: 1.1210383176803589 | Test Loss: 1.1432640552520752\n",
      "Epoch 5147 | Train Loss: 1.2661913633346558 | Test Loss: 1.334288239479065\n",
      "Epoch 5148 | Train Loss: 1.1209880113601685 | Test Loss: 1.1432816982269287\n",
      "Epoch 5149 | Train Loss: 1.2661240100860596 | Test Loss: 1.3342784643173218\n",
      "Epoch 5150 | Train Loss: 1.120932936668396 | Test Loss: 1.1431916952133179\n",
      "Epoch 5151 | Train Loss: 1.2660675048828125 | Test Loss: 1.334200382232666\n",
      "Epoch 5152 | Train Loss: 1.1208932399749756 | Test Loss: 1.143202543258667\n",
      "Epoch 5153 | Train Loss: 1.266006588935852 | Test Loss: 1.3342044353485107\n",
      "Epoch 5154 | Train Loss: 1.1208326816558838 | Test Loss: 1.1431024074554443\n",
      "Epoch 5155 | Train Loss: 1.2659473419189453 | Test Loss: 1.3341245651245117\n",
      "Epoch 5156 | Train Loss: 1.1207913160324097 | Test Loss: 1.143115758895874\n",
      "Epoch 5157 | Train Loss: 1.2658841609954834 | Test Loss: 1.3341645002365112\n",
      "Epoch 5158 | Train Loss: 1.1207321882247925 | Test Loss: 1.1430041790008545\n",
      "Epoch 5159 | Train Loss: 1.265845537185669 | Test Loss: 1.334033489227295\n",
      "Epoch 5160 | Train Loss: 1.1206870079040527 | Test Loss: 1.1430606842041016\n",
      "Epoch 5161 | Train Loss: 1.2657830715179443 | Test Loss: 1.3341259956359863\n",
      "Epoch 5162 | Train Loss: 1.1206245422363281 | Test Loss: 1.1429011821746826\n",
      "Epoch 5163 | Train Loss: 1.265752911567688 | Test Loss: 1.3339619636535645\n",
      "Epoch 5164 | Train Loss: 1.1205823421478271 | Test Loss: 1.143000602722168\n",
      "Epoch 5165 | Train Loss: 1.2656890153884888 | Test Loss: 1.334040641784668\n",
      "Epoch 5166 | Train Loss: 1.1205193996429443 | Test Loss: 1.1428139209747314\n",
      "Epoch 5167 | Train Loss: 1.265653133392334 | Test Loss: 1.333913803100586\n",
      "Epoch 5168 | Train Loss: 1.120477557182312 | Test Loss: 1.1429299116134644\n",
      "Epoch 5169 | Train Loss: 1.265588641166687 | Test Loss: 1.333958625793457\n",
      "Epoch 5170 | Train Loss: 1.120404839515686 | Test Loss: 1.1427321434020996\n",
      "Epoch 5171 | Train Loss: 1.265565037727356 | Test Loss: 1.333854079246521\n",
      "Epoch 5172 | Train Loss: 1.1203594207763672 | Test Loss: 1.1428459882736206\n",
      "Epoch 5173 | Train Loss: 1.2654823064804077 | Test Loss: 1.3338772058486938\n",
      "Epoch 5174 | Train Loss: 1.1203006505966187 | Test Loss: 1.142646074295044\n",
      "Epoch 5175 | Train Loss: 1.2654480934143066 | Test Loss: 1.3337715864181519\n",
      "Epoch 5176 | Train Loss: 1.1202492713928223 | Test Loss: 1.1427489519119263\n",
      "Epoch 5177 | Train Loss: 1.2653619050979614 | Test Loss: 1.3337892293930054\n",
      "Epoch 5178 | Train Loss: 1.1201980113983154 | Test Loss: 1.142564058303833\n",
      "Epoch 5179 | Train Loss: 1.265320897102356 | Test Loss: 1.3337194919586182\n",
      "Epoch 5180 | Train Loss: 1.1201341152191162 | Test Loss: 1.1426362991333008\n",
      "Epoch 5181 | Train Loss: 1.2652398347854614 | Test Loss: 1.3336900472640991\n",
      "Epoch 5182 | Train Loss: 1.1200846433639526 | Test Loss: 1.1424803733825684\n",
      "Epoch 5183 | Train Loss: 1.2651814222335815 | Test Loss: 1.3337041139602661\n",
      "Epoch 5184 | Train Loss: 1.120027780532837 | Test Loss: 1.14250648021698\n",
      "Epoch 5185 | Train Loss: 1.2651076316833496 | Test Loss: 1.333611249923706\n",
      "Epoch 5186 | Train Loss: 1.1199744939804077 | Test Loss: 1.1424040794372559\n",
      "Epoch 5187 | Train Loss: 1.265051007270813 | Test Loss: 1.3336485624313354\n",
      "Epoch 5188 | Train Loss: 1.1199191808700562 | Test Loss: 1.1423979997634888\n",
      "Epoch 5189 | Train Loss: 1.2649743556976318 | Test Loss: 1.333541989326477\n",
      "Epoch 5190 | Train Loss: 1.1198755502700806 | Test Loss: 1.1423031091690063\n",
      "Epoch 5191 | Train Loss: 1.2649257183074951 | Test Loss: 1.3335785865783691\n",
      "Epoch 5192 | Train Loss: 1.119809627532959 | Test Loss: 1.142303705215454\n",
      "Epoch 5193 | Train Loss: 1.2648621797561646 | Test Loss: 1.3334988355636597\n",
      "Epoch 5194 | Train Loss: 1.1197669506072998 | Test Loss: 1.1422134637832642\n",
      "Epoch 5195 | Train Loss: 1.2648029327392578 | Test Loss: 1.3335132598876953\n",
      "Epoch 5196 | Train Loss: 1.1197047233581543 | Test Loss: 1.1421992778778076\n",
      "Epoch 5197 | Train Loss: 1.2647513151168823 | Test Loss: 1.3334709405899048\n",
      "Epoch 5198 | Train Loss: 1.1196510791778564 | Test Loss: 1.1421359777450562\n",
      "Epoch 5199 | Train Loss: 1.264697551727295 | Test Loss: 1.333484411239624\n",
      "Epoch 5200 | Train Loss: 1.1195951700210571 | Test Loss: 1.1421079635620117\n",
      "Epoch 5201 | Train Loss: 1.2646405696868896 | Test Loss: 1.3333837985992432\n",
      "Epoch 5202 | Train Loss: 1.1195447444915771 | Test Loss: 1.1420633792877197\n",
      "Epoch 5203 | Train Loss: 1.2645891904830933 | Test Loss: 1.3334044218063354\n",
      "Epoch 5204 | Train Loss: 1.1194878816604614 | Test Loss: 1.1420003175735474\n",
      "Epoch 5205 | Train Loss: 1.264530897140503 | Test Loss: 1.3333262205123901\n",
      "Epoch 5206 | Train Loss: 1.1194384098052979 | Test Loss: 1.141968011856079\n",
      "Epoch 5207 | Train Loss: 1.2644786834716797 | Test Loss: 1.3333497047424316\n",
      "Epoch 5208 | Train Loss: 1.119382381439209 | Test Loss: 1.1419256925582886\n",
      "Epoch 5209 | Train Loss: 1.2644201517105103 | Test Loss: 1.333275556564331\n",
      "Epoch 5210 | Train Loss: 1.119336724281311 | Test Loss: 1.141900897026062\n",
      "Epoch 5211 | Train Loss: 1.2643682956695557 | Test Loss: 1.3332828283309937\n",
      "Epoch 5212 | Train Loss: 1.1192721128463745 | Test Loss: 1.1418275833129883\n",
      "Epoch 5213 | Train Loss: 1.2643182277679443 | Test Loss: 1.3332040309906006\n",
      "Epoch 5214 | Train Loss: 1.1192333698272705 | Test Loss: 1.1418144702911377\n",
      "Epoch 5215 | Train Loss: 1.264255166053772 | Test Loss: 1.3332184553146362\n",
      "Epoch 5216 | Train Loss: 1.1191729307174683 | Test Loss: 1.1417369842529297\n",
      "Epoch 5217 | Train Loss: 1.2642158269882202 | Test Loss: 1.3331228494644165\n",
      "Epoch 5218 | Train Loss: 1.119124174118042 | Test Loss: 1.141741156578064\n",
      "Epoch 5219 | Train Loss: 1.2641552686691284 | Test Loss: 1.3331953287124634\n",
      "Epoch 5220 | Train Loss: 1.1190694570541382 | Test Loss: 1.1416363716125488\n",
      "Epoch 5221 | Train Loss: 1.264115333557129 | Test Loss: 1.3330706357955933\n",
      "Epoch 5222 | Train Loss: 1.1190211772918701 | Test Loss: 1.1416668891906738\n",
      "Epoch 5223 | Train Loss: 1.2640597820281982 | Test Loss: 1.333125114440918\n",
      "Epoch 5224 | Train Loss: 1.1189594268798828 | Test Loss: 1.1415263414382935\n",
      "Epoch 5225 | Train Loss: 1.2640169858932495 | Test Loss: 1.3330025672912598\n",
      "Epoch 5226 | Train Loss: 1.1189172267913818 | Test Loss: 1.1415737867355347\n",
      "Epoch 5227 | Train Loss: 1.2639561891555786 | Test Loss: 1.3330365419387817\n",
      "Epoch 5228 | Train Loss: 1.1188585758209229 | Test Loss: 1.1414529085159302\n",
      "Epoch 5229 | Train Loss: 1.2639089822769165 | Test Loss: 1.3329527378082275\n",
      "Epoch 5230 | Train Loss: 1.118816614151001 | Test Loss: 1.1414704322814941\n",
      "Epoch 5231 | Train Loss: 1.2638458013534546 | Test Loss: 1.3329726457595825\n",
      "Epoch 5232 | Train Loss: 1.1187535524368286 | Test Loss: 1.1413675546646118\n",
      "Epoch 5233 | Train Loss: 1.2638213634490967 | Test Loss: 1.3328816890716553\n",
      "Epoch 5234 | Train Loss: 1.1187045574188232 | Test Loss: 1.1414070129394531\n",
      "Epoch 5235 | Train Loss: 1.2637596130371094 | Test Loss: 1.332924485206604\n",
      "Epoch 5236 | Train Loss: 1.1186563968658447 | Test Loss: 1.1412713527679443\n",
      "Epoch 5237 | Train Loss: 1.2637251615524292 | Test Loss: 1.3327804803848267\n",
      "Epoch 5238 | Train Loss: 1.1186009645462036 | Test Loss: 1.1413414478302002\n",
      "Epoch 5239 | Train Loss: 1.2636685371398926 | Test Loss: 1.332841157913208\n",
      "Epoch 5240 | Train Loss: 1.1185487508773804 | Test Loss: 1.1411747932434082\n",
      "Epoch 5241 | Train Loss: 1.263615369796753 | Test Loss: 1.332746982574463\n",
      "Epoch 5242 | Train Loss: 1.1184989213943481 | Test Loss: 1.141255497932434\n",
      "Epoch 5243 | Train Loss: 1.2635540962219238 | Test Loss: 1.3327597379684448\n",
      "Epoch 5244 | Train Loss: 1.1184355020523071 | Test Loss: 1.1411031484603882\n",
      "Epoch 5245 | Train Loss: 1.2635133266448975 | Test Loss: 1.332708477973938\n",
      "Epoch 5246 | Train Loss: 1.1183924674987793 | Test Loss: 1.1411724090576172\n",
      "Epoch 5247 | Train Loss: 1.2634377479553223 | Test Loss: 1.332668662071228\n",
      "Epoch 5248 | Train Loss: 1.1183360815048218 | Test Loss: 1.1410152912139893\n",
      "Epoch 5249 | Train Loss: 1.2633939981460571 | Test Loss: 1.332660436630249\n",
      "Epoch 5250 | Train Loss: 1.1182875633239746 | Test Loss: 1.1410771608352661\n",
      "Epoch 5251 | Train Loss: 1.263321042060852 | Test Loss: 1.3325914144515991\n",
      "Epoch 5252 | Train Loss: 1.1182395219802856 | Test Loss: 1.1409231424331665\n",
      "Epoch 5253 | Train Loss: 1.2632774114608765 | Test Loss: 1.3326091766357422\n",
      "Epoch 5254 | Train Loss: 1.1181782484054565 | Test Loss: 1.1409860849380493\n",
      "Epoch 5255 | Train Loss: 1.2632148265838623 | Test Loss: 1.3325306177139282\n",
      "Epoch 5256 | Train Loss: 1.1181306838989258 | Test Loss: 1.1408443450927734\n",
      "Epoch 5257 | Train Loss: 1.2631655931472778 | Test Loss: 1.3325527906417847\n",
      "Epoch 5258 | Train Loss: 1.1180742979049683 | Test Loss: 1.1408770084381104\n",
      "Epoch 5259 | Train Loss: 1.263098955154419 | Test Loss: 1.332449197769165\n",
      "Epoch 5260 | Train Loss: 1.1180338859558105 | Test Loss: 1.1407724618911743\n",
      "Epoch 5261 | Train Loss: 1.2630475759506226 | Test Loss: 1.3324693441390991\n",
      "Epoch 5262 | Train Loss: 1.117972731590271 | Test Loss: 1.140769124031067\n",
      "Epoch 5263 | Train Loss: 1.2629926204681396 | Test Loss: 1.3323888778686523\n",
      "Epoch 5264 | Train Loss: 1.1179293394088745 | Test Loss: 1.1406818628311157\n",
      "Epoch 5265 | Train Loss: 1.2629400491714478 | Test Loss: 1.332404375076294\n",
      "Epoch 5266 | Train Loss: 1.1178655624389648 | Test Loss: 1.1406630277633667\n",
      "Epoch 5267 | Train Loss: 1.2628846168518066 | Test Loss: 1.3323429822921753\n",
      "Epoch 5268 | Train Loss: 1.1178206205368042 | Test Loss: 1.1406124830245972\n",
      "Epoch 5269 | Train Loss: 1.2628284692764282 | Test Loss: 1.3323622941970825\n",
      "Epoch 5270 | Train Loss: 1.11776602268219 | Test Loss: 1.1405718326568604\n",
      "Epoch 5271 | Train Loss: 1.2627809047698975 | Test Loss: 1.3323062658309937\n",
      "Epoch 5272 | Train Loss: 1.1177140474319458 | Test Loss: 1.140532374382019\n",
      "Epoch 5273 | Train Loss: 1.262725830078125 | Test Loss: 1.3322855234146118\n",
      "Epoch 5274 | Train Loss: 1.117664098739624 | Test Loss: 1.1404787302017212\n",
      "Epoch 5275 | Train Loss: 1.2626720666885376 | Test Loss: 1.3322420120239258\n",
      "Epoch 5276 | Train Loss: 1.1176155805587769 | Test Loss: 1.140451192855835\n",
      "Epoch 5277 | Train Loss: 1.2626150846481323 | Test Loss: 1.3322217464447021\n",
      "Epoch 5278 | Train Loss: 1.117563009262085 | Test Loss: 1.140398383140564\n",
      "Epoch 5279 | Train Loss: 1.2625685930252075 | Test Loss: 1.3321733474731445\n",
      "Epoch 5280 | Train Loss: 1.1175123453140259 | Test Loss: 1.140373706817627\n",
      "Epoch 5281 | Train Loss: 1.2625088691711426 | Test Loss: 1.3321548700332642\n",
      "Epoch 5282 | Train Loss: 1.1174659729003906 | Test Loss: 1.1403100490570068\n",
      "Epoch 5283 | Train Loss: 1.262460708618164 | Test Loss: 1.3321067094802856\n",
      "Epoch 5284 | Train Loss: 1.1174107789993286 | Test Loss: 1.1402956247329712\n",
      "Epoch 5285 | Train Loss: 1.2624046802520752 | Test Loss: 1.3320993185043335\n",
      "Epoch 5286 | Train Loss: 1.1173651218414307 | Test Loss: 1.1402151584625244\n",
      "Epoch 5287 | Train Loss: 1.262351632118225 | Test Loss: 1.3320294618606567\n",
      "Epoch 5288 | Train Loss: 1.117315649986267 | Test Loss: 1.140203595161438\n",
      "Epoch 5289 | Train Loss: 1.2623050212860107 | Test Loss: 1.3320231437683105\n",
      "Epoch 5290 | Train Loss: 1.1172603368759155 | Test Loss: 1.1401411294937134\n",
      "Epoch 5291 | Train Loss: 1.2622449398040771 | Test Loss: 1.331978440284729\n",
      "Epoch 5292 | Train Loss: 1.1172209978103638 | Test Loss: 1.1400964260101318\n",
      "Epoch 5293 | Train Loss: 1.262194037437439 | Test Loss: 1.3319469690322876\n",
      "Epoch 5294 | Train Loss: 1.1171625852584839 | Test Loss: 1.1400707960128784\n",
      "Epoch 5295 | Train Loss: 1.2621411085128784 | Test Loss: 1.3319417238235474\n",
      "Epoch 5296 | Train Loss: 1.1171200275421143 | Test Loss: 1.140012502670288\n",
      "Epoch 5297 | Train Loss: 1.2620820999145508 | Test Loss: 1.3318926095962524\n",
      "Epoch 5298 | Train Loss: 1.117069125175476 | Test Loss: 1.139991283416748\n",
      "Epoch 5299 | Train Loss: 1.2620348930358887 | Test Loss: 1.3318392038345337\n",
      "Epoch 5300 | Train Loss: 1.117022156715393 | Test Loss: 1.1399414539337158\n",
      "Epoch 5301 | Train Loss: 1.2619785070419312 | Test Loss: 1.3318392038345337\n",
      "Epoch 5302 | Train Loss: 1.1169712543487549 | Test Loss: 1.1399037837982178\n",
      "Epoch 5303 | Train Loss: 1.2619301080703735 | Test Loss: 1.331776738166809\n",
      "Epoch 5304 | Train Loss: 1.1169202327728271 | Test Loss: 1.1398658752441406\n",
      "Epoch 5305 | Train Loss: 1.2618801593780518 | Test Loss: 1.3317731618881226\n",
      "Epoch 5306 | Train Loss: 1.1168678998947144 | Test Loss: 1.139826774597168\n",
      "Epoch 5307 | Train Loss: 1.26183021068573 | Test Loss: 1.3317159414291382\n",
      "Epoch 5308 | Train Loss: 1.1168173551559448 | Test Loss: 1.13978111743927\n",
      "Epoch 5309 | Train Loss: 1.2617799043655396 | Test Loss: 1.3317391872406006\n",
      "Epoch 5310 | Train Loss: 1.1167680025100708 | Test Loss: 1.1397501230239868\n",
      "Epoch 5311 | Train Loss: 1.261730670928955 | Test Loss: 1.3316316604614258\n",
      "Epoch 5312 | Train Loss: 1.116723895072937 | Test Loss: 1.1396912336349487\n",
      "Epoch 5313 | Train Loss: 1.261678695678711 | Test Loss: 1.3316855430603027\n",
      "Epoch 5314 | Train Loss: 1.1166714429855347 | Test Loss: 1.139664888381958\n",
      "Epoch 5315 | Train Loss: 1.2616257667541504 | Test Loss: 1.3315800428390503\n",
      "Epoch 5316 | Train Loss: 1.1166272163391113 | Test Loss: 1.139614462852478\n",
      "Epoch 5317 | Train Loss: 1.2615724802017212 | Test Loss: 1.331602692604065\n",
      "Epoch 5318 | Train Loss: 1.1165721416473389 | Test Loss: 1.1395775079727173\n",
      "Epoch 5319 | Train Loss: 1.261522650718689 | Test Loss: 1.3315566778182983\n",
      "Epoch 5320 | Train Loss: 1.1165310144424438 | Test Loss: 1.1395543813705444\n",
      "Epoch 5321 | Train Loss: 1.2614719867706299 | Test Loss: 1.3315377235412598\n",
      "Epoch 5322 | Train Loss: 1.1164720058441162 | Test Loss: 1.1394869089126587\n",
      "Epoch 5323 | Train Loss: 1.2614208459854126 | Test Loss: 1.331495761871338\n",
      "Epoch 5324 | Train Loss: 1.1164308786392212 | Test Loss: 1.1394504308700562\n",
      "Epoch 5325 | Train Loss: 1.2613662481307983 | Test Loss: 1.3314685821533203\n",
      "Epoch 5326 | Train Loss: 1.1163707971572876 | Test Loss: 1.1394140720367432\n",
      "Epoch 5327 | Train Loss: 1.2613210678100586 | Test Loss: 1.3314310312271118\n",
      "Epoch 5328 | Train Loss: 1.1163240671157837 | Test Loss: 1.139354944229126\n",
      "Epoch 5329 | Train Loss: 1.2612584829330444 | Test Loss: 1.3314052820205688\n",
      "Epoch 5330 | Train Loss: 1.1162781715393066 | Test Loss: 1.1393451690673828\n",
      "Epoch 5331 | Train Loss: 1.261218547821045 | Test Loss: 1.3313689231872559\n",
      "Epoch 5332 | Train Loss: 1.1162179708480835 | Test Loss: 1.1392744779586792\n",
      "Epoch 5333 | Train Loss: 1.2611597776412964 | Test Loss: 1.3313575983047485\n",
      "Epoch 5334 | Train Loss: 1.1161805391311646 | Test Loss: 1.1392396688461304\n",
      "Epoch 5335 | Train Loss: 1.2611135244369507 | Test Loss: 1.331325888633728\n",
      "Epoch 5336 | Train Loss: 1.1161171197891235 | Test Loss: 1.139203429222107\n",
      "Epoch 5337 | Train Loss: 1.261065125465393 | Test Loss: 1.3312751054763794\n",
      "Epoch 5338 | Train Loss: 1.116071343421936 | Test Loss: 1.1391539573669434\n",
      "Epoch 5339 | Train Loss: 1.2610108852386475 | Test Loss: 1.3313119411468506\n",
      "Epoch 5340 | Train Loss: 1.1160221099853516 | Test Loss: 1.1391149759292603\n",
      "Epoch 5341 | Train Loss: 1.2609518766403198 | Test Loss: 1.331213116645813\n",
      "Epoch 5342 | Train Loss: 1.115979552268982 | Test Loss: 1.1390725374221802\n",
      "Epoch 5343 | Train Loss: 1.2608994245529175 | Test Loss: 1.3312326669692993\n",
      "Epoch 5344 | Train Loss: 1.1159260272979736 | Test Loss: 1.139034628868103\n",
      "Epoch 5345 | Train Loss: 1.2608453035354614 | Test Loss: 1.3311554193496704\n",
      "Epoch 5346 | Train Loss: 1.1158775091171265 | Test Loss: 1.1390084028244019\n",
      "Epoch 5347 | Train Loss: 1.260794997215271 | Test Loss: 1.331152319908142\n",
      "Epoch 5348 | Train Loss: 1.115827202796936 | Test Loss: 1.1389501094818115\n",
      "Epoch 5349 | Train Loss: 1.2607425451278687 | Test Loss: 1.3310855627059937\n",
      "Epoch 5350 | Train Loss: 1.1157833337783813 | Test Loss: 1.1389249563217163\n",
      "Epoch 5351 | Train Loss: 1.2606861591339111 | Test Loss: 1.3311121463775635\n",
      "Epoch 5352 | Train Loss: 1.1157374382019043 | Test Loss: 1.1388676166534424\n",
      "Epoch 5353 | Train Loss: 1.260629653930664 | Test Loss: 1.331037998199463\n",
      "Epoch 5354 | Train Loss: 1.115692377090454 | Test Loss: 1.1388373374938965\n",
      "Epoch 5355 | Train Loss: 1.2605839967727661 | Test Loss: 1.3310737609863281\n",
      "Epoch 5356 | Train Loss: 1.115635871887207 | Test Loss: 1.1388018131256104\n",
      "Epoch 5357 | Train Loss: 1.2605279684066772 | Test Loss: 1.330985188484192\n",
      "Epoch 5358 | Train Loss: 1.115592122077942 | Test Loss: 1.1387537717819214\n",
      "Epoch 5359 | Train Loss: 1.2604796886444092 | Test Loss: 1.3310006856918335\n",
      "Epoch 5360 | Train Loss: 1.1155389547348022 | Test Loss: 1.1387280225753784\n",
      "Epoch 5361 | Train Loss: 1.2604246139526367 | Test Loss: 1.3309255838394165\n",
      "Epoch 5362 | Train Loss: 1.1154998540878296 | Test Loss: 1.138676404953003\n",
      "Epoch 5363 | Train Loss: 1.260372281074524 | Test Loss: 1.3309208154678345\n",
      "Epoch 5364 | Train Loss: 1.1154497861862183 | Test Loss: 1.1386501789093018\n",
      "Epoch 5365 | Train Loss: 1.2603230476379395 | Test Loss: 1.3308680057525635\n",
      "Epoch 5366 | Train Loss: 1.1153995990753174 | Test Loss: 1.138584852218628\n",
      "Epoch 5367 | Train Loss: 1.2602827548980713 | Test Loss: 1.3308619260787964\n",
      "Epoch 5368 | Train Loss: 1.115346074104309 | Test Loss: 1.1385642290115356\n",
      "Epoch 5369 | Train Loss: 1.2602283954620361 | Test Loss: 1.3308087587356567\n",
      "Epoch 5370 | Train Loss: 1.1153037548065186 | Test Loss: 1.1385034322738647\n",
      "Epoch 5371 | Train Loss: 1.260184645652771 | Test Loss: 1.3308000564575195\n",
      "Epoch 5372 | Train Loss: 1.1152480840682983 | Test Loss: 1.1385011672973633\n",
      "Epoch 5373 | Train Loss: 1.2601337432861328 | Test Loss: 1.330767273902893\n",
      "Epoch 5374 | Train Loss: 1.1152092218399048 | Test Loss: 1.1384187936782837\n",
      "Epoch 5375 | Train Loss: 1.2600823640823364 | Test Loss: 1.330769419670105\n",
      "Epoch 5376 | Train Loss: 1.1151542663574219 | Test Loss: 1.1384271383285522\n",
      "Epoch 5377 | Train Loss: 1.2600377798080444 | Test Loss: 1.3306961059570312\n",
      "Epoch 5378 | Train Loss: 1.1151045560836792 | Test Loss: 1.1383640766143799\n",
      "Epoch 5379 | Train Loss: 1.2599890232086182 | Test Loss: 1.3306900262832642\n",
      "Epoch 5380 | Train Loss: 1.1150532960891724 | Test Loss: 1.1383366584777832\n",
      "Epoch 5381 | Train Loss: 1.2599387168884277 | Test Loss: 1.3306312561035156\n",
      "Epoch 5382 | Train Loss: 1.1150089502334595 | Test Loss: 1.1382917165756226\n",
      "Epoch 5383 | Train Loss: 1.2598806619644165 | Test Loss: 1.3306280374526978\n",
      "Epoch 5384 | Train Loss: 1.1149623394012451 | Test Loss: 1.138266921043396\n",
      "Epoch 5385 | Train Loss: 1.2598292827606201 | Test Loss: 1.330564260482788\n",
      "Epoch 5386 | Train Loss: 1.114912748336792 | Test Loss: 1.1382137537002563\n",
      "Epoch 5387 | Train Loss: 1.2597776651382446 | Test Loss: 1.330551028251648\n",
      "Epoch 5388 | Train Loss: 1.1148631572723389 | Test Loss: 1.1381902694702148\n",
      "Epoch 5389 | Train Loss: 1.259723424911499 | Test Loss: 1.3305470943450928\n",
      "Epoch 5390 | Train Loss: 1.114816427230835 | Test Loss: 1.138132095336914\n",
      "Epoch 5391 | Train Loss: 1.2596720457077026 | Test Loss: 1.3304957151412964\n",
      "Epoch 5392 | Train Loss: 1.1147654056549072 | Test Loss: 1.1381158828735352\n",
      "Epoch 5393 | Train Loss: 1.2596205472946167 | Test Loss: 1.3305108547210693\n",
      "Epoch 5394 | Train Loss: 1.1147202253341675 | Test Loss: 1.1380751132965088\n",
      "Epoch 5395 | Train Loss: 1.2595677375793457 | Test Loss: 1.3304400444030762\n",
      "Epoch 5396 | Train Loss: 1.1146719455718994 | Test Loss: 1.138026475906372\n",
      "Epoch 5397 | Train Loss: 1.2595175504684448 | Test Loss: 1.33046555519104\n",
      "Epoch 5398 | Train Loss: 1.114627480506897 | Test Loss: 1.138002872467041\n",
      "Epoch 5399 | Train Loss: 1.2594600915908813 | Test Loss: 1.3303906917572021\n",
      "Epoch 5400 | Train Loss: 1.1145864725112915 | Test Loss: 1.1379531621932983\n",
      "Epoch 5401 | Train Loss: 1.259420394897461 | Test Loss: 1.3303864002227783\n",
      "Epoch 5402 | Train Loss: 1.1145285367965698 | Test Loss: 1.1379330158233643\n",
      "Epoch 5403 | Train Loss: 1.2593605518341064 | Test Loss: 1.3303658962249756\n",
      "Epoch 5404 | Train Loss: 1.1144928932189941 | Test Loss: 1.1378782987594604\n",
      "Epoch 5405 | Train Loss: 1.2593104839324951 | Test Loss: 1.3303309679031372\n",
      "Epoch 5406 | Train Loss: 1.1144376993179321 | Test Loss: 1.1378545761108398\n",
      "Epoch 5407 | Train Loss: 1.2592612504959106 | Test Loss: 1.3303207159042358\n",
      "Epoch 5408 | Train Loss: 1.114393711090088 | Test Loss: 1.13780677318573\n",
      "Epoch 5409 | Train Loss: 1.2592096328735352 | Test Loss: 1.330265760421753\n",
      "Epoch 5410 | Train Loss: 1.1143497228622437 | Test Loss: 1.1377757787704468\n",
      "Epoch 5411 | Train Loss: 1.2591567039489746 | Test Loss: 1.3302652835845947\n",
      "Epoch 5412 | Train Loss: 1.1142981052398682 | Test Loss: 1.1377243995666504\n",
      "Epoch 5413 | Train Loss: 1.2591102123260498 | Test Loss: 1.330204963684082\n",
      "Epoch 5414 | Train Loss: 1.114251971244812 | Test Loss: 1.1376930475234985\n",
      "Epoch 5415 | Train Loss: 1.259061336517334 | Test Loss: 1.3301923274993896\n",
      "Epoch 5416 | Train Loss: 1.1142064332962036 | Test Loss: 1.1376686096191406\n",
      "Epoch 5417 | Train Loss: 1.259001612663269 | Test Loss: 1.3301618099212646\n",
      "Epoch 5418 | Train Loss: 1.1141629219055176 | Test Loss: 1.1376086473464966\n",
      "Epoch 5419 | Train Loss: 1.2589585781097412 | Test Loss: 1.330121397972107\n",
      "Epoch 5420 | Train Loss: 1.1141124963760376 | Test Loss: 1.1375885009765625\n",
      "Epoch 5421 | Train Loss: 1.2589040994644165 | Test Loss: 1.330136775970459\n",
      "Epoch 5422 | Train Loss: 1.1140685081481934 | Test Loss: 1.1375441551208496\n",
      "Epoch 5423 | Train Loss: 1.258853554725647 | Test Loss: 1.330051064491272\n",
      "Epoch 5424 | Train Loss: 1.1140216588974 | Test Loss: 1.1375014781951904\n",
      "Epoch 5425 | Train Loss: 1.2588082551956177 | Test Loss: 1.3300797939300537\n",
      "Epoch 5426 | Train Loss: 1.1139720678329468 | Test Loss: 1.1374735832214355\n",
      "Epoch 5427 | Train Loss: 1.2587538957595825 | Test Loss: 1.330032467842102\n",
      "Epoch 5428 | Train Loss: 1.1139261722564697 | Test Loss: 1.1374257802963257\n",
      "Epoch 5429 | Train Loss: 1.2587088346481323 | Test Loss: 1.329994797706604\n",
      "Epoch 5430 | Train Loss: 1.1138763427734375 | Test Loss: 1.1373916864395142\n",
      "Epoch 5431 | Train Loss: 1.2586556673049927 | Test Loss: 1.3300024271011353\n",
      "Epoch 5432 | Train Loss: 1.1138333082199097 | Test Loss: 1.1373528242111206\n",
      "Epoch 5433 | Train Loss: 1.2586098909378052 | Test Loss: 1.3299437761306763\n",
      "Epoch 5434 | Train Loss: 1.1137871742248535 | Test Loss: 1.137311577796936\n",
      "Epoch 5435 | Train Loss: 1.2585527896881104 | Test Loss: 1.329927921295166\n",
      "Epoch 5436 | Train Loss: 1.113747239112854 | Test Loss: 1.1372737884521484\n",
      "Epoch 5437 | Train Loss: 1.258505940437317 | Test Loss: 1.3298816680908203\n",
      "Epoch 5438 | Train Loss: 1.1136950254440308 | Test Loss: 1.1372469663619995\n",
      "Epoch 5439 | Train Loss: 1.2584539651870728 | Test Loss: 1.329879879951477\n",
      "Epoch 5440 | Train Loss: 1.113659381866455 | Test Loss: 1.1371989250183105\n",
      "Epoch 5441 | Train Loss: 1.2583991289138794 | Test Loss: 1.3298380374908447\n",
      "Epoch 5442 | Train Loss: 1.1136101484298706 | Test Loss: 1.1371712684631348\n",
      "Epoch 5443 | Train Loss: 1.2583576440811157 | Test Loss: 1.3298074007034302\n",
      "Epoch 5444 | Train Loss: 1.1135600805282593 | Test Loss: 1.137141466140747\n",
      "Epoch 5445 | Train Loss: 1.258302092552185 | Test Loss: 1.3297961950302124\n",
      "Epoch 5446 | Train Loss: 1.1135187149047852 | Test Loss: 1.1370925903320312\n",
      "Epoch 5447 | Train Loss: 1.258256435394287 | Test Loss: 1.329720139503479\n",
      "Epoch 5448 | Train Loss: 1.1134806871414185 | Test Loss: 1.1370785236358643\n",
      "Epoch 5449 | Train Loss: 1.2582093477249146 | Test Loss: 1.3297604322433472\n",
      "Epoch 5450 | Train Loss: 1.1134283542633057 | Test Loss: 1.1370092630386353\n",
      "Epoch 5451 | Train Loss: 1.2581642866134644 | Test Loss: 1.3296654224395752\n",
      "Epoch 5452 | Train Loss: 1.113390326499939 | Test Loss: 1.1369997262954712\n",
      "Epoch 5453 | Train Loss: 1.2581168413162231 | Test Loss: 1.3297113180160522\n",
      "Epoch 5454 | Train Loss: 1.1133363246917725 | Test Loss: 1.1369664669036865\n",
      "Epoch 5455 | Train Loss: 1.2580740451812744 | Test Loss: 1.3295960426330566\n",
      "Epoch 5456 | Train Loss: 1.1132962703704834 | Test Loss: 1.1369327306747437\n",
      "Epoch 5457 | Train Loss: 1.258023738861084 | Test Loss: 1.3296207189559937\n",
      "Epoch 5458 | Train Loss: 1.11324942111969 | Test Loss: 1.1368972063064575\n",
      "Epoch 5459 | Train Loss: 1.2579749822616577 | Test Loss: 1.3295749425888062\n",
      "Epoch 5460 | Train Loss: 1.113208293914795 | Test Loss: 1.1368651390075684\n",
      "Epoch 5461 | Train Loss: 1.2579176425933838 | Test Loss: 1.3295612335205078\n",
      "Epoch 5462 | Train Loss: 1.1131585836410522 | Test Loss: 1.1368321180343628\n",
      "Epoch 5463 | Train Loss: 1.2578767538070679 | Test Loss: 1.329530954360962\n",
      "Epoch 5464 | Train Loss: 1.1131106615066528 | Test Loss: 1.136784315109253\n",
      "Epoch 5465 | Train Loss: 1.257824420928955 | Test Loss: 1.3295342922210693\n",
      "Epoch 5466 | Train Loss: 1.1130632162094116 | Test Loss: 1.1367567777633667\n",
      "Epoch 5467 | Train Loss: 1.2577730417251587 | Test Loss: 1.329464316368103\n",
      "Epoch 5468 | Train Loss: 1.113018274307251 | Test Loss: 1.1367052793502808\n",
      "Epoch 5469 | Train Loss: 1.257714033126831 | Test Loss: 1.3294585943222046\n",
      "Epoch 5470 | Train Loss: 1.1129772663116455 | Test Loss: 1.1366873979568481\n",
      "Epoch 5471 | Train Loss: 1.2576719522476196 | Test Loss: 1.3294076919555664\n",
      "Epoch 5472 | Train Loss: 1.1129226684570312 | Test Loss: 1.1366382837295532\n",
      "Epoch 5473 | Train Loss: 1.257617712020874 | Test Loss: 1.3293818235397339\n",
      "Epoch 5474 | Train Loss: 1.11288583278656 | Test Loss: 1.1366145610809326\n",
      "Epoch 5475 | Train Loss: 1.2575629949569702 | Test Loss: 1.3293755054473877\n",
      "Epoch 5476 | Train Loss: 1.112837791442871 | Test Loss: 1.136570930480957\n",
      "Epoch 5477 | Train Loss: 1.2575143575668335 | Test Loss: 1.3293218612670898\n",
      "Epoch 5478 | Train Loss: 1.1127909421920776 | Test Loss: 1.1365379095077515\n",
      "Epoch 5479 | Train Loss: 1.2574554681777954 | Test Loss: 1.3293415307998657\n",
      "Epoch 5480 | Train Loss: 1.1127504110336304 | Test Loss: 1.1364829540252686\n",
      "Epoch 5481 | Train Loss: 1.257400631904602 | Test Loss: 1.3292354345321655\n",
      "Epoch 5482 | Train Loss: 1.1127101182937622 | Test Loss: 1.1364787817001343\n",
      "Epoch 5483 | Train Loss: 1.2573546171188354 | Test Loss: 1.329286813735962\n",
      "Epoch 5484 | Train Loss: 1.1126610040664673 | Test Loss: 1.136406421661377\n",
      "Epoch 5485 | Train Loss: 1.2573024034500122 | Test Loss: 1.3291645050048828\n",
      "Epoch 5486 | Train Loss: 1.1126245260238647 | Test Loss: 1.1363844871520996\n",
      "Epoch 5487 | Train Loss: 1.2572510242462158 | Test Loss: 1.3292351961135864\n",
      "Epoch 5488 | Train Loss: 1.1125768423080444 | Test Loss: 1.1363648176193237\n",
      "Epoch 5489 | Train Loss: 1.2571972608566284 | Test Loss: 1.329135537147522\n",
      "Epoch 5490 | Train Loss: 1.1125404834747314 | Test Loss: 1.136310338973999\n",
      "Epoch 5491 | Train Loss: 1.2571513652801514 | Test Loss: 1.3291525840759277\n",
      "Epoch 5492 | Train Loss: 1.1124874353408813 | Test Loss: 1.1362971067428589\n",
      "Epoch 5493 | Train Loss: 1.257101058959961 | Test Loss: 1.329107403755188\n",
      "Epoch 5494 | Train Loss: 1.1124491691589355 | Test Loss: 1.1362391710281372\n",
      "Epoch 5495 | Train Loss: 1.2570551633834839 | Test Loss: 1.329079508781433\n",
      "Epoch 5496 | Train Loss: 1.1123950481414795 | Test Loss: 1.136218786239624\n",
      "Epoch 5497 | Train Loss: 1.257009744644165 | Test Loss: 1.3290594816207886\n",
      "Epoch 5498 | Train Loss: 1.1123569011688232 | Test Loss: 1.1361794471740723\n",
      "Epoch 5499 | Train Loss: 1.2569537162780762 | Test Loss: 1.3290714025497437\n",
      "Epoch 5500 | Train Loss: 1.112310528755188 | Test Loss: 1.1361461877822876\n",
      "Epoch 5501 | Train Loss: 1.256915807723999 | Test Loss: 1.3289722204208374\n",
      "Epoch 5502 | Train Loss: 1.1122633218765259 | Test Loss: 1.1361098289489746\n",
      "Epoch 5503 | Train Loss: 1.2568639516830444 | Test Loss: 1.329014539718628\n",
      "Epoch 5504 | Train Loss: 1.1122276782989502 | Test Loss: 1.1360812187194824\n",
      "Epoch 5505 | Train Loss: 1.25680673122406 | Test Loss: 1.3289215564727783\n",
      "Epoch 5506 | Train Loss: 1.1121803522109985 | Test Loss: 1.1360549926757812\n",
      "Epoch 5507 | Train Loss: 1.256765604019165 | Test Loss: 1.3289066553115845\n",
      "Epoch 5508 | Train Loss: 1.1121330261230469 | Test Loss: 1.1360180377960205\n",
      "Epoch 5509 | Train Loss: 1.2567107677459717 | Test Loss: 1.3288800716400146\n",
      "Epoch 5510 | Train Loss: 1.112097978591919 | Test Loss: 1.1359745264053345\n",
      "Epoch 5511 | Train Loss: 1.256661057472229 | Test Loss: 1.3288596868515015\n",
      "Epoch 5512 | Train Loss: 1.112051248550415 | Test Loss: 1.1359704732894897\n",
      "Epoch 5513 | Train Loss: 1.2566136121749878 | Test Loss: 1.32884681224823\n",
      "Epoch 5514 | Train Loss: 1.1120134592056274 | Test Loss: 1.1358941793441772\n",
      "Epoch 5515 | Train Loss: 1.256563663482666 | Test Loss: 1.3287770748138428\n",
      "Epoch 5516 | Train Loss: 1.1119683980941772 | Test Loss: 1.1359103918075562\n",
      "Epoch 5517 | Train Loss: 1.256514072418213 | Test Loss: 1.32879638671875\n",
      "Epoch 5518 | Train Loss: 1.111928105354309 | Test Loss: 1.1358203887939453\n",
      "Epoch 5519 | Train Loss: 1.2564642429351807 | Test Loss: 1.3287172317504883\n",
      "Epoch 5520 | Train Loss: 1.1118830442428589 | Test Loss: 1.1358219385147095\n",
      "Epoch 5521 | Train Loss: 1.25641930103302 | Test Loss: 1.3287203311920166\n",
      "Epoch 5522 | Train Loss: 1.111831784248352 | Test Loss: 1.1357715129852295\n",
      "Epoch 5523 | Train Loss: 1.256371021270752 | Test Loss: 1.3286904096603394\n",
      "Epoch 5524 | Train Loss: 1.1117933988571167 | Test Loss: 1.1357370615005493\n",
      "Epoch 5525 | Train Loss: 1.2563239336013794 | Test Loss: 1.3286479711532593\n",
      "Epoch 5526 | Train Loss: 1.1117445230484009 | Test Loss: 1.1357150077819824\n",
      "Epoch 5527 | Train Loss: 1.2562800645828247 | Test Loss: 1.3286558389663696\n",
      "Epoch 5528 | Train Loss: 1.1117051839828491 | Test Loss: 1.1356761455535889\n",
      "Epoch 5529 | Train Loss: 1.2562278509140015 | Test Loss: 1.3285902738571167\n",
      "Epoch 5530 | Train Loss: 1.1116608381271362 | Test Loss: 1.1356348991394043\n",
      "Epoch 5531 | Train Loss: 1.2561826705932617 | Test Loss: 1.3285976648330688\n",
      "Epoch 5532 | Train Loss: 1.1116138696670532 | Test Loss: 1.1356053352355957\n",
      "Epoch 5533 | Train Loss: 1.256131649017334 | Test Loss: 1.3285164833068848\n",
      "Epoch 5534 | Train Loss: 1.1115731000900269 | Test Loss: 1.1355549097061157\n",
      "Epoch 5535 | Train Loss: 1.2560890913009644 | Test Loss: 1.3285518884658813\n",
      "Epoch 5536 | Train Loss: 1.1115241050720215 | Test Loss: 1.135533332824707\n",
      "Epoch 5537 | Train Loss: 1.2560391426086426 | Test Loss: 1.3284692764282227\n",
      "Epoch 5538 | Train Loss: 1.1114870309829712 | Test Loss: 1.1354928016662598\n",
      "Epoch 5539 | Train Loss: 1.2559876441955566 | Test Loss: 1.3284858465194702\n",
      "Epoch 5540 | Train Loss: 1.1114413738250732 | Test Loss: 1.1354732513427734\n",
      "Epoch 5541 | Train Loss: 1.2559410333633423 | Test Loss: 1.3284227848052979\n",
      "Epoch 5542 | Train Loss: 1.1114001274108887 | Test Loss: 1.1354230642318726\n",
      "Epoch 5543 | Train Loss: 1.2558977603912354 | Test Loss: 1.3283923864364624\n",
      "Epoch 5544 | Train Loss: 1.1113477945327759 | Test Loss: 1.1354058980941772\n",
      "Epoch 5545 | Train Loss: 1.2558529376983643 | Test Loss: 1.3284366130828857\n",
      "Epoch 5546 | Train Loss: 1.1113154888153076 | Test Loss: 1.1353533267974854\n",
      "Epoch 5547 | Train Loss: 1.2558072805404663 | Test Loss: 1.3283096551895142\n",
      "Epoch 5548 | Train Loss: 1.111266851425171 | Test Loss: 1.1353297233581543\n",
      "Epoch 5549 | Train Loss: 1.255765438079834 | Test Loss: 1.3284144401550293\n",
      "Epoch 5550 | Train Loss: 1.111234188079834 | Test Loss: 1.135270357131958\n",
      "Epoch 5551 | Train Loss: 1.2557052373886108 | Test Loss: 1.3282692432403564\n",
      "Epoch 5552 | Train Loss: 1.1111845970153809 | Test Loss: 1.1352530717849731\n",
      "Epoch 5553 | Train Loss: 1.255667805671692 | Test Loss: 1.32832932472229\n",
      "Epoch 5554 | Train Loss: 1.1111375093460083 | Test Loss: 1.135209321975708\n",
      "Epoch 5555 | Train Loss: 1.255614995956421 | Test Loss: 1.3282374143600464\n",
      "Epoch 5556 | Train Loss: 1.1110970973968506 | Test Loss: 1.1351759433746338\n",
      "Epoch 5557 | Train Loss: 1.2555763721466064 | Test Loss: 1.328274130821228\n",
      "Epoch 5558 | Train Loss: 1.1110517978668213 | Test Loss: 1.1351450681686401\n",
      "Epoch 5559 | Train Loss: 1.2555203437805176 | Test Loss: 1.3281893730163574\n",
      "Epoch 5560 | Train Loss: 1.111007571220398 | Test Loss: 1.1350994110107422\n",
      "Epoch 5561 | Train Loss: 1.2554858922958374 | Test Loss: 1.328215479850769\n",
      "Epoch 5562 | Train Loss: 1.110952615737915 | Test Loss: 1.1350843906402588\n",
      "Epoch 5563 | Train Loss: 1.2554351091384888 | Test Loss: 1.3281538486480713\n",
      "Epoch 5564 | Train Loss: 1.1109099388122559 | Test Loss: 1.1350224018096924\n",
      "Epoch 5565 | Train Loss: 1.2553905248641968 | Test Loss: 1.3281197547912598\n",
      "Epoch 5566 | Train Loss: 1.110862374305725 | Test Loss: 1.1350133419036865\n",
      "Epoch 5567 | Train Loss: 1.2553496360778809 | Test Loss: 1.3281344175338745\n",
      "Epoch 5568 | Train Loss: 1.1108158826828003 | Test Loss: 1.1349546909332275\n",
      "Epoch 5569 | Train Loss: 1.2552988529205322 | Test Loss: 1.3280683755874634\n",
      "Epoch 5570 | Train Loss: 1.110780119895935 | Test Loss: 1.1349438428878784\n",
      "Epoch 5571 | Train Loss: 1.2552495002746582 | Test Loss: 1.3280909061431885\n",
      "Epoch 5572 | Train Loss: 1.110732078552246 | Test Loss: 1.134901523590088\n",
      "Epoch 5573 | Train Loss: 1.2552087306976318 | Test Loss: 1.3280192613601685\n",
      "Epoch 5574 | Train Loss: 1.1106982231140137 | Test Loss: 1.1348806619644165\n",
      "Epoch 5575 | Train Loss: 1.2551549673080444 | Test Loss: 1.328060507774353\n",
      "Epoch 5576 | Train Loss: 1.110650658607483 | Test Loss: 1.134827971458435\n",
      "Epoch 5577 | Train Loss: 1.255116581916809 | Test Loss: 1.3279153108596802\n",
      "Epoch 5578 | Train Loss: 1.1106208562850952 | Test Loss: 1.1348174810409546\n",
      "Epoch 5579 | Train Loss: 1.2550690174102783 | Test Loss: 1.3280348777770996\n",
      "Epoch 5580 | Train Loss: 1.1105765104293823 | Test Loss: 1.1347627639770508\n",
      "Epoch 5581 | Train Loss: 1.2550318241119385 | Test Loss: 1.3278547525405884\n",
      "Epoch 5582 | Train Loss: 1.1105369329452515 | Test Loss: 1.1347501277923584\n",
      "Epoch 5583 | Train Loss: 1.2549850940704346 | Test Loss: 1.327958106994629\n",
      "Epoch 5584 | Train Loss: 1.1104859113693237 | Test Loss: 1.13471257686615\n",
      "Epoch 5585 | Train Loss: 1.2549424171447754 | Test Loss: 1.327854871749878\n",
      "Epoch 5586 | Train Loss: 1.1104427576065063 | Test Loss: 1.1346598863601685\n",
      "Epoch 5587 | Train Loss: 1.254899024963379 | Test Loss: 1.3278625011444092\n",
      "Epoch 5588 | Train Loss: 1.1103904247283936 | Test Loss: 1.1346663236618042\n",
      "Epoch 5589 | Train Loss: 1.2548586130142212 | Test Loss: 1.327840805053711\n",
      "Epoch 5590 | Train Loss: 1.1103521585464478 | Test Loss: 1.1345808506011963\n",
      "Epoch 5591 | Train Loss: 1.2548002004623413 | Test Loss: 1.327790379524231\n",
      "Epoch 5592 | Train Loss: 1.1103103160858154 | Test Loss: 1.134590744972229\n",
      "Epoch 5593 | Train Loss: 1.2547625303268433 | Test Loss: 1.3278112411499023\n",
      "Epoch 5594 | Train Loss: 1.1102650165557861 | Test Loss: 1.1345257759094238\n",
      "Epoch 5595 | Train Loss: 1.2547043561935425 | Test Loss: 1.3277307748794556\n",
      "Epoch 5596 | Train Loss: 1.1102267503738403 | Test Loss: 1.1344965696334839\n",
      "Epoch 5597 | Train Loss: 1.254664659500122 | Test Loss: 1.3277510404586792\n",
      "Epoch 5598 | Train Loss: 1.110176682472229 | Test Loss: 1.1344634294509888\n",
      "Epoch 5599 | Train Loss: 1.2546149492263794 | Test Loss: 1.327688217163086\n",
      "Epoch 5600 | Train Loss: 1.1101384162902832 | Test Loss: 1.1344112157821655\n",
      "Epoch 5601 | Train Loss: 1.254567265510559 | Test Loss: 1.3276951313018799\n",
      "Epoch 5602 | Train Loss: 1.110089659690857 | Test Loss: 1.1343904733657837\n",
      "Epoch 5603 | Train Loss: 1.2545199394226074 | Test Loss: 1.3276616334915161\n",
      "Epoch 5604 | Train Loss: 1.110055923461914 | Test Loss: 1.1343499422073364\n",
      "Epoch 5605 | Train Loss: 1.2544690370559692 | Test Loss: 1.3276596069335938\n",
      "Epoch 5606 | Train Loss: 1.1100085973739624 | Test Loss: 1.1343098878860474\n",
      "Epoch 5607 | Train Loss: 1.2544231414794922 | Test Loss: 1.327595829963684\n",
      "Epoch 5608 | Train Loss: 1.1099735498428345 | Test Loss: 1.1343015432357788\n",
      "Epoch 5609 | Train Loss: 1.2543739080429077 | Test Loss: 1.3276182413101196\n",
      "Epoch 5610 | Train Loss: 1.109928846359253 | Test Loss: 1.1342315673828125\n",
      "Epoch 5611 | Train Loss: 1.2543305158615112 | Test Loss: 1.3275244235992432\n",
      "Epoch 5612 | Train Loss: 1.1098953485488892 | Test Loss: 1.134250283241272\n",
      "Epoch 5613 | Train Loss: 1.254281759262085 | Test Loss: 1.3275600671768188\n",
      "Epoch 5614 | Train Loss: 1.1098477840423584 | Test Loss: 1.1341567039489746\n",
      "Epoch 5615 | Train Loss: 1.2542425394058228 | Test Loss: 1.327481746673584\n",
      "Epoch 5616 | Train Loss: 1.1098090410232544 | Test Loss: 1.1341789960861206\n",
      "Epoch 5617 | Train Loss: 1.254198431968689 | Test Loss: 1.3275110721588135\n",
      "Epoch 5618 | Train Loss: 1.1097575426101685 | Test Loss: 1.1341021060943604\n",
      "Epoch 5619 | Train Loss: 1.2541608810424805 | Test Loss: 1.3274120092391968\n",
      "Epoch 5620 | Train Loss: 1.1097307205200195 | Test Loss: 1.1341091394424438\n",
      "Epoch 5621 | Train Loss: 1.2541117668151855 | Test Loss: 1.3274692296981812\n",
      "Epoch 5622 | Train Loss: 1.1096750497817993 | Test Loss: 1.1340596675872803\n",
      "Epoch 5623 | Train Loss: 1.2540758848190308 | Test Loss: 1.3274043798446655\n",
      "Epoch 5624 | Train Loss: 1.1096463203430176 | Test Loss: 1.1340423822402954\n",
      "Epoch 5625 | Train Loss: 1.2540143728256226 | Test Loss: 1.327368974685669\n",
      "Epoch 5626 | Train Loss: 1.1095993518829346 | Test Loss: 1.1340129375457764\n",
      "Epoch 5627 | Train Loss: 1.2539902925491333 | Test Loss: 1.327376365661621\n",
      "Epoch 5628 | Train Loss: 1.1095554828643799 | Test Loss: 1.1339715719223022\n",
      "Epoch 5629 | Train Loss: 1.253924012184143 | Test Loss: 1.3273273706436157\n",
      "Epoch 5630 | Train Loss: 1.1095203161239624 | Test Loss: 1.1339064836502075\n",
      "Epoch 5631 | Train Loss: 1.2538914680480957 | Test Loss: 1.3273268938064575\n",
      "Epoch 5632 | Train Loss: 1.109468936920166 | Test Loss: 1.1339231729507446\n",
      "Epoch 5633 | Train Loss: 1.2538374662399292 | Test Loss: 1.3273013830184937\n",
      "Epoch 5634 | Train Loss: 1.1094378232955933 | Test Loss: 1.1338306665420532\n",
      "Epoch 5635 | Train Loss: 1.2537963390350342 | Test Loss: 1.3272761106491089\n",
      "Epoch 5636 | Train Loss: 1.109390377998352 | Test Loss: 1.133872151374817\n",
      "Epoch 5637 | Train Loss: 1.2537531852722168 | Test Loss: 1.3272600173950195\n",
      "Epoch 5638 | Train Loss: 1.1093586683273315 | Test Loss: 1.133785605430603\n",
      "Epoch 5639 | Train Loss: 1.2537016868591309 | Test Loss: 1.3272124528884888\n",
      "Epoch 5640 | Train Loss: 1.1093223094940186 | Test Loss: 1.133793830871582\n",
      "Epoch 5641 | Train Loss: 1.2536550760269165 | Test Loss: 1.327166199684143\n",
      "Epoch 5642 | Train Loss: 1.109276533126831 | Test Loss: 1.1337369680404663\n",
      "Epoch 5643 | Train Loss: 1.2536121606826782 | Test Loss: 1.3271923065185547\n",
      "Epoch 5644 | Train Loss: 1.1092396974563599 | Test Loss: 1.13370680809021\n",
      "Epoch 5645 | Train Loss: 1.2535651922225952 | Test Loss: 1.3271011114120483\n",
      "Epoch 5646 | Train Loss: 1.10919189453125 | Test Loss: 1.1336891651153564\n",
      "Epoch 5647 | Train Loss: 1.2535314559936523 | Test Loss: 1.3271404504776\n",
      "Epoch 5648 | Train Loss: 1.1091516017913818 | Test Loss: 1.1336448192596436\n",
      "Epoch 5649 | Train Loss: 1.2534793615341187 | Test Loss: 1.327075481414795\n",
      "Epoch 5650 | Train Loss: 1.1091196537017822 | Test Loss: 1.1336172819137573\n",
      "Epoch 5651 | Train Loss: 1.253442406654358 | Test Loss: 1.3270727396011353\n",
      "Epoch 5652 | Train Loss: 1.1090741157531738 | Test Loss: 1.1335783004760742\n",
      "Epoch 5653 | Train Loss: 1.253394603729248 | Test Loss: 1.3270289897918701\n",
      "Epoch 5654 | Train Loss: 1.1090412139892578 | Test Loss: 1.133540153503418\n",
      "Epoch 5655 | Train Loss: 1.2533589601516724 | Test Loss: 1.3270173072814941\n",
      "Epoch 5656 | Train Loss: 1.1089895963668823 | Test Loss: 1.133537769317627\n",
      "Epoch 5657 | Train Loss: 1.2533280849456787 | Test Loss: 1.32696533203125\n",
      "Epoch 5658 | Train Loss: 1.1089563369750977 | Test Loss: 1.1334859132766724\n",
      "Epoch 5659 | Train Loss: 1.2532825469970703 | Test Loss: 1.326995611190796\n",
      "Epoch 5660 | Train Loss: 1.1089123487472534 | Test Loss: 1.1334717273712158\n",
      "Epoch 5661 | Train Loss: 1.2532532215118408 | Test Loss: 1.3268846273422241\n",
      "Epoch 5662 | Train Loss: 1.1088712215423584 | Test Loss: 1.1334261894226074\n",
      "Epoch 5663 | Train Loss: 1.2532124519348145 | Test Loss: 1.3269679546356201\n",
      "Epoch 5664 | Train Loss: 1.1088327169418335 | Test Loss: 1.1334189176559448\n",
      "Epoch 5665 | Train Loss: 1.2531770467758179 | Test Loss: 1.3268455266952515\n",
      "Epoch 5666 | Train Loss: 1.1087912321090698 | Test Loss: 1.1333584785461426\n",
      "Epoch 5667 | Train Loss: 1.2531405687332153 | Test Loss: 1.326905608177185\n",
      "Epoch 5668 | Train Loss: 1.1087429523468018 | Test Loss: 1.1333647966384888\n",
      "Epoch 5669 | Train Loss: 1.25310480594635 | Test Loss: 1.3268272876739502\n",
      "Epoch 5670 | Train Loss: 1.1087044477462769 | Test Loss: 1.133293628692627\n",
      "Epoch 5671 | Train Loss: 1.253074288368225 | Test Loss: 1.3268412351608276\n",
      "Epoch 5672 | Train Loss: 1.1086573600769043 | Test Loss: 1.133320927619934\n",
      "Epoch 5673 | Train Loss: 1.2530343532562256 | Test Loss: 1.3267929553985596\n",
      "Epoch 5674 | Train Loss: 1.1086167097091675 | Test Loss: 1.1332290172576904\n",
      "Epoch 5675 | Train Loss: 1.252981185913086 | Test Loss: 1.3267539739608765\n",
      "Epoch 5676 | Train Loss: 1.10857093334198 | Test Loss: 1.1332286596298218\n",
      "Epoch 5677 | Train Loss: 1.252928614616394 | Test Loss: 1.3267568349838257\n",
      "Epoch 5678 | Train Loss: 1.108533263206482 | Test Loss: 1.1331536769866943\n",
      "Epoch 5679 | Train Loss: 1.252875804901123 | Test Loss: 1.326729416847229\n",
      "Epoch 5680 | Train Loss: 1.1084860563278198 | Test Loss: 1.1331521272659302\n",
      "Epoch 5681 | Train Loss: 1.252838373184204 | Test Loss: 1.3266994953155518\n",
      "Epoch 5682 | Train Loss: 1.1084494590759277 | Test Loss: 1.1330814361572266\n",
      "Epoch 5683 | Train Loss: 1.252781629562378 | Test Loss: 1.3266769647598267\n",
      "Epoch 5684 | Train Loss: 1.1084144115447998 | Test Loss: 1.1330759525299072\n",
      "Epoch 5685 | Train Loss: 1.2527390718460083 | Test Loss: 1.3267110586166382\n",
      "Epoch 5686 | Train Loss: 1.1083718538284302 | Test Loss: 1.1330186128616333\n",
      "Epoch 5687 | Train Loss: 1.2526851892471313 | Test Loss: 1.3266074657440186\n",
      "Epoch 5688 | Train Loss: 1.1083425283432007 | Test Loss: 1.1329890489578247\n",
      "Epoch 5689 | Train Loss: 1.2526373863220215 | Test Loss: 1.3266611099243164\n",
      "Epoch 5690 | Train Loss: 1.1082935333251953 | Test Loss: 1.1329739093780518\n",
      "Epoch 5691 | Train Loss: 1.2525925636291504 | Test Loss: 1.326605200767517\n",
      "Epoch 5692 | Train Loss: 1.1082526445388794 | Test Loss: 1.1329050064086914\n",
      "Epoch 5693 | Train Loss: 1.2525545358657837 | Test Loss: 1.3265777826309204\n",
      "Epoch 5694 | Train Loss: 1.1082053184509277 | Test Loss: 1.132907748222351\n",
      "Epoch 5695 | Train Loss: 1.2525111436843872 | Test Loss: 1.3265933990478516\n",
      "Epoch 5696 | Train Loss: 1.1081732511520386 | Test Loss: 1.1328579187393188\n",
      "Epoch 5697 | Train Loss: 1.2524642944335938 | Test Loss: 1.3265564441680908\n",
      "Epoch 5698 | Train Loss: 1.1081254482269287 | Test Loss: 1.132820963859558\n",
      "Epoch 5699 | Train Loss: 1.2524280548095703 | Test Loss: 1.3264989852905273\n",
      "Epoch 5700 | Train Loss: 1.1080811023712158 | Test Loss: 1.1327979564666748\n",
      "Epoch 5701 | Train Loss: 1.2523880004882812 | Test Loss: 1.3265712261199951\n",
      "Epoch 5702 | Train Loss: 1.108048915863037 | Test Loss: 1.1327476501464844\n",
      "Epoch 5703 | Train Loss: 1.2523455619812012 | Test Loss: 1.326412558555603\n",
      "Epoch 5704 | Train Loss: 1.1080150604248047 | Test Loss: 1.1327327489852905\n",
      "Epoch 5705 | Train Loss: 1.252310037612915 | Test Loss: 1.32651686668396\n",
      "Epoch 5706 | Train Loss: 1.1079745292663574 | Test Loss: 1.1327017545700073\n",
      "Epoch 5707 | Train Loss: 1.2522581815719604 | Test Loss: 1.3264025449752808\n",
      "Epoch 5708 | Train Loss: 1.1079362630844116 | Test Loss: 1.1326634883880615\n",
      "Epoch 5709 | Train Loss: 1.2522226572036743 | Test Loss: 1.326423168182373\n",
      "Epoch 5710 | Train Loss: 1.107881784439087 | Test Loss: 1.1326555013656616\n",
      "Epoch 5711 | Train Loss: 1.252181887626648 | Test Loss: 1.3263834714889526\n",
      "Epoch 5712 | Train Loss: 1.107850432395935 | Test Loss: 1.132588505744934\n",
      "Epoch 5713 | Train Loss: 1.2521361112594604 | Test Loss: 1.326340675354004\n",
      "Epoch 5714 | Train Loss: 1.1078033447265625 | Test Loss: 1.1326063871383667\n",
      "Epoch 5715 | Train Loss: 1.2521024942398071 | Test Loss: 1.3263829946517944\n",
      "Epoch 5716 | Train Loss: 1.107771396636963 | Test Loss: 1.1325172185897827\n",
      "Epoch 5717 | Train Loss: 1.2520520687103271 | Test Loss: 1.3262815475463867\n",
      "Epoch 5718 | Train Loss: 1.1077394485473633 | Test Loss: 1.132554054260254\n",
      "Epoch 5719 | Train Loss: 1.2520159482955933 | Test Loss: 1.3263109922409058\n",
      "Epoch 5720 | Train Loss: 1.1076878309249878 | Test Loss: 1.1324844360351562\n",
      "Epoch 5721 | Train Loss: 1.2519750595092773 | Test Loss: 1.3262628316879272\n",
      "Epoch 5722 | Train Loss: 1.1076525449752808 | Test Loss: 1.1324775218963623\n",
      "Epoch 5723 | Train Loss: 1.2519341707229614 | Test Loss: 1.3262395858764648\n",
      "Epoch 5724 | Train Loss: 1.1075986623764038 | Test Loss: 1.1324462890625\n",
      "Epoch 5725 | Train Loss: 1.2519038915634155 | Test Loss: 1.3262245655059814\n",
      "Epoch 5726 | Train Loss: 1.1075637340545654 | Test Loss: 1.132423996925354\n",
      "Epoch 5727 | Train Loss: 1.2518442869186401 | Test Loss: 1.3261853456497192\n",
      "Epoch 5728 | Train Loss: 1.1075254678726196 | Test Loss: 1.1323704719543457\n",
      "Epoch 5729 | Train Loss: 1.2518196105957031 | Test Loss: 1.326184630393982\n",
      "Epoch 5730 | Train Loss: 1.1074844598770142 | Test Loss: 1.1323835849761963\n",
      "Epoch 5731 | Train Loss: 1.2517582178115845 | Test Loss: 1.3261269330978394\n",
      "Epoch 5732 | Train Loss: 1.10745370388031 | Test Loss: 1.1322758197784424\n",
      "Epoch 5733 | Train Loss: 1.2517261505126953 | Test Loss: 1.3261165618896484\n",
      "Epoch 5734 | Train Loss: 1.1073987483978271 | Test Loss: 1.1323332786560059\n",
      "Epoch 5735 | Train Loss: 1.2516894340515137 | Test Loss: 1.3261505365371704\n",
      "Epoch 5736 | Train Loss: 1.1073700189590454 | Test Loss: 1.1322084665298462\n",
      "Epoch 5737 | Train Loss: 1.2516483068466187 | Test Loss: 1.3260442018508911\n",
      "Epoch 5738 | Train Loss: 1.1073482036590576 | Test Loss: 1.132263422012329\n",
      "Epoch 5739 | Train Loss: 1.2515971660614014 | Test Loss: 1.3261257410049438\n",
      "Epoch 5740 | Train Loss: 1.1072893142700195 | Test Loss: 1.132150411605835\n",
      "Epoch 5741 | Train Loss: 1.2515556812286377 | Test Loss: 1.326027750968933\n",
      "Epoch 5742 | Train Loss: 1.107243537902832 | Test Loss: 1.13215970993042\n",
      "Epoch 5743 | Train Loss: 1.2515114545822144 | Test Loss: 1.3260585069656372\n",
      "Epoch 5744 | Train Loss: 1.1071991920471191 | Test Loss: 1.1321229934692383\n",
      "Epoch 5745 | Train Loss: 1.2514761686325073 | Test Loss: 1.326010823249817\n",
      "Epoch 5746 | Train Loss: 1.1071587800979614 | Test Loss: 1.132070541381836\n",
      "Epoch 5747 | Train Loss: 1.2514348030090332 | Test Loss: 1.325940489768982\n",
      "Epoch 5748 | Train Loss: 1.1071118116378784 | Test Loss: 1.1320754289627075\n",
      "Epoch 5749 | Train Loss: 1.2514139413833618 | Test Loss: 1.3260283470153809\n",
      "Epoch 5750 | Train Loss: 1.1070754528045654 | Test Loss: 1.1320102214813232\n",
      "Epoch 5751 | Train Loss: 1.2513577938079834 | Test Loss: 1.325846552848816\n",
      "Epoch 5752 | Train Loss: 1.1070631742477417 | Test Loss: 1.1320183277130127\n",
      "Epoch 5753 | Train Loss: 1.2513136863708496 | Test Loss: 1.325998306274414\n",
      "Epoch 5754 | Train Loss: 1.1070020198822021 | Test Loss: 1.1319612264633179\n",
      "Epoch 5755 | Train Loss: 1.2512760162353516 | Test Loss: 1.325800895690918\n",
      "Epoch 5756 | Train Loss: 1.1069703102111816 | Test Loss: 1.1319379806518555\n",
      "Epoch 5757 | Train Loss: 1.2512320280075073 | Test Loss: 1.3259414434432983\n",
      "Epoch 5758 | Train Loss: 1.1069188117980957 | Test Loss: 1.131914496421814\n",
      "Epoch 5759 | Train Loss: 1.2512043714523315 | Test Loss: 1.3257654905319214\n",
      "Epoch 5760 | Train Loss: 1.1068925857543945 | Test Loss: 1.1318976879119873\n",
      "Epoch 5761 | Train Loss: 1.2511569261550903 | Test Loss: 1.325855016708374\n",
      "Epoch 5762 | Train Loss: 1.1068305969238281 | Test Loss: 1.1318522691726685\n",
      "Epoch 5763 | Train Loss: 1.2511361837387085 | Test Loss: 1.3257029056549072\n",
      "Epoch 5764 | Train Loss: 1.106795310974121 | Test Loss: 1.1318515539169312\n",
      "Epoch 5765 | Train Loss: 1.2510762214660645 | Test Loss: 1.3258095979690552\n",
      "Epoch 5766 | Train Loss: 1.1067595481872559 | Test Loss: 1.131785273551941\n",
      "Epoch 5767 | Train Loss: 1.251055359840393 | Test Loss: 1.325670599937439\n",
      "Epoch 5768 | Train Loss: 1.1067184209823608 | Test Loss: 1.1318161487579346\n",
      "Epoch 5769 | Train Loss: 1.2510098218917847 | Test Loss: 1.3257722854614258\n",
      "Epoch 5770 | Train Loss: 1.1066726446151733 | Test Loss: 1.1317176818847656\n",
      "Epoch 5771 | Train Loss: 1.250975489616394 | Test Loss: 1.3256540298461914\n",
      "Epoch 5772 | Train Loss: 1.106634497642517 | Test Loss: 1.1317566633224487\n",
      "Epoch 5773 | Train Loss: 1.250928282737732 | Test Loss: 1.325683355331421\n",
      "Epoch 5774 | Train Loss: 1.1065945625305176 | Test Loss: 1.131666660308838\n",
      "Epoch 5775 | Train Loss: 1.2508904933929443 | Test Loss: 1.3256444931030273\n",
      "Epoch 5776 | Train Loss: 1.1065607070922852 | Test Loss: 1.1316508054733276\n",
      "Epoch 5777 | Train Loss: 1.250833511352539 | Test Loss: 1.3255912065505981\n",
      "Epoch 5778 | Train Loss: 1.1065105199813843 | Test Loss: 1.1316317319869995\n",
      "Epoch 5779 | Train Loss: 1.2508039474487305 | Test Loss: 1.3256723880767822\n",
      "Epoch 5780 | Train Loss: 1.10647714138031 | Test Loss: 1.1315606832504272\n",
      "Epoch 5781 | Train Loss: 1.25075101852417 | Test Loss: 1.3255254030227661\n",
      "Epoch 5782 | Train Loss: 1.1064561605453491 | Test Loss: 1.1315869092941284\n",
      "Epoch 5783 | Train Loss: 1.2507065534591675 | Test Loss: 1.3257060050964355\n",
      "Epoch 5784 | Train Loss: 1.1064203977584839 | Test Loss: 1.131495714187622\n",
      "Epoch 5785 | Train Loss: 1.250649094581604 | Test Loss: 1.325472354888916\n",
      "Epoch 5786 | Train Loss: 1.1064015626907349 | Test Loss: 1.131505012512207\n",
      "Epoch 5787 | Train Loss: 1.2506152391433716 | Test Loss: 1.3256629705429077\n",
      "Epoch 5788 | Train Loss: 1.1063485145568848 | Test Loss: 1.1314460039138794\n",
      "Epoch 5789 | Train Loss: 1.2505714893341064 | Test Loss: 1.3254423141479492\n",
      "Epoch 5790 | Train Loss: 1.1063207387924194 | Test Loss: 1.1314351558685303\n",
      "Epoch 5791 | Train Loss: 1.2505290508270264 | Test Loss: 1.3255492448806763\n",
      "Epoch 5792 | Train Loss: 1.106241226196289 | Test Loss: 1.1313897371292114\n",
      "Epoch 5793 | Train Loss: 1.250514268875122 | Test Loss: 1.3254224061965942\n",
      "Epoch 5794 | Train Loss: 1.1062085628509521 | Test Loss: 1.1313958168029785\n",
      "Epoch 5795 | Train Loss: 1.2504512071609497 | Test Loss: 1.325477123260498\n",
      "Epoch 5796 | Train Loss: 1.1061625480651855 | Test Loss: 1.1313170194625854\n",
      "Epoch 5797 | Train Loss: 1.2504348754882812 | Test Loss: 1.325360655784607\n",
      "Epoch 5798 | Train Loss: 1.1061288118362427 | Test Loss: 1.131362795829773\n",
      "Epoch 5799 | Train Loss: 1.250382423400879 | Test Loss: 1.3254473209381104\n",
      "Epoch 5800 | Train Loss: 1.1060876846313477 | Test Loss: 1.131252408027649\n",
      "Epoch 5801 | Train Loss: 1.250351905822754 | Test Loss: 1.3253535032272339\n",
      "Epoch 5802 | Train Loss: 1.1060465574264526 | Test Loss: 1.131292462348938\n",
      "Epoch 5803 | Train Loss: 1.2502951622009277 | Test Loss: 1.325369119644165\n",
      "Epoch 5804 | Train Loss: 1.1060041189193726 | Test Loss: 1.1312077045440674\n",
      "Epoch 5805 | Train Loss: 1.250257968902588 | Test Loss: 1.3253474235534668\n",
      "Epoch 5806 | Train Loss: 1.1059684753417969 | Test Loss: 1.1312053203582764\n",
      "Epoch 5807 | Train Loss: 1.250206470489502 | Test Loss: 1.3253451585769653\n",
      "Epoch 5808 | Train Loss: 1.105930209159851 | Test Loss: 1.131144642829895\n",
      "Epoch 5809 | Train Loss: 1.2501696348190308 | Test Loss: 1.3252828121185303\n",
      "Epoch 5810 | Train Loss: 1.1058903932571411 | Test Loss: 1.1311509609222412\n",
      "Epoch 5811 | Train Loss: 1.2501252889633179 | Test Loss: 1.3252941370010376\n",
      "Epoch 5812 | Train Loss: 1.1058549880981445 | Test Loss: 1.131086826324463\n",
      "Epoch 5813 | Train Loss: 1.2500874996185303 | Test Loss: 1.3252520561218262\n",
      "Epoch 5814 | Train Loss: 1.10581636428833 | Test Loss: 1.1311018466949463\n",
      "Epoch 5815 | Train Loss: 1.2500439882278442 | Test Loss: 1.3252508640289307\n",
      "Epoch 5816 | Train Loss: 1.1057840585708618 | Test Loss: 1.1310359239578247\n",
      "Epoch 5817 | Train Loss: 1.2500029802322388 | Test Loss: 1.3251924514770508\n",
      "Epoch 5818 | Train Loss: 1.1057463884353638 | Test Loss: 1.1310356855392456\n",
      "Epoch 5819 | Train Loss: 1.2499593496322632 | Test Loss: 1.3251837491989136\n",
      "Epoch 5820 | Train Loss: 1.1057013273239136 | Test Loss: 1.1310064792633057\n",
      "Epoch 5821 | Train Loss: 1.2499289512634277 | Test Loss: 1.3252370357513428\n",
      "Epoch 5822 | Train Loss: 1.1056753396987915 | Test Loss: 1.130967617034912\n",
      "Epoch 5823 | Train Loss: 1.2498815059661865 | Test Loss: 1.3251157999038696\n",
      "Epoch 5824 | Train Loss: 1.1056526899337769 | Test Loss: 1.1309542655944824\n",
      "Epoch 5825 | Train Loss: 1.249845027923584 | Test Loss: 1.3252811431884766\n",
      "Epoch 5826 | Train Loss: 1.105627179145813 | Test Loss: 1.1309025287628174\n",
      "Epoch 5827 | Train Loss: 1.2497879266738892 | Test Loss: 1.325086236000061\n",
      "Epoch 5828 | Train Loss: 1.1055830717086792 | Test Loss: 1.1308759450912476\n",
      "Epoch 5829 | Train Loss: 1.2497538328170776 | Test Loss: 1.3251330852508545\n",
      "Epoch 5830 | Train Loss: 1.1055337190628052 | Test Loss: 1.1308636665344238\n",
      "Epoch 5831 | Train Loss: 1.2497085332870483 | Test Loss: 1.3250669240951538\n",
      "Epoch 5832 | Train Loss: 1.105493187904358 | Test Loss: 1.1307957172393799\n",
      "Epoch 5833 | Train Loss: 1.249678134918213 | Test Loss: 1.3250571489334106\n",
      "Epoch 5834 | Train Loss: 1.1054428815841675 | Test Loss: 1.130834698677063\n",
      "Epoch 5835 | Train Loss: 1.2496429681777954 | Test Loss: 1.32508385181427\n",
      "Epoch 5836 | Train Loss: 1.105407476425171 | Test Loss: 1.130746841430664\n",
      "Epoch 5837 | Train Loss: 1.2495996952056885 | Test Loss: 1.3249683380126953\n",
      "Epoch 5838 | Train Loss: 1.1053838729858398 | Test Loss: 1.130772352218628\n",
      "Epoch 5839 | Train Loss: 1.2495578527450562 | Test Loss: 1.3250441551208496\n",
      "Epoch 5840 | Train Loss: 1.1053462028503418 | Test Loss: 1.130698800086975\n",
      "Epoch 5841 | Train Loss: 1.2495144605636597 | Test Loss: 1.3249671459197998\n",
      "Epoch 5842 | Train Loss: 1.10530424118042 | Test Loss: 1.1306872367858887\n",
      "Epoch 5843 | Train Loss: 1.2494758367538452 | Test Loss: 1.324914574623108\n",
      "Epoch 5844 | Train Loss: 1.1052552461624146 | Test Loss: 1.1306443214416504\n",
      "Epoch 5845 | Train Loss: 1.2494431734085083 | Test Loss: 1.324973702430725\n",
      "Epoch 5846 | Train Loss: 1.1052312850952148 | Test Loss: 1.1306201219558716\n",
      "Epoch 5847 | Train Loss: 1.2493948936462402 | Test Loss: 1.3248505592346191\n",
      "Epoch 5848 | Train Loss: 1.105181097984314 | Test Loss: 1.130576729774475\n",
      "Epoch 5849 | Train Loss: 1.2493743896484375 | Test Loss: 1.3249439001083374\n",
      "Epoch 5850 | Train Loss: 1.1051579713821411 | Test Loss: 1.1305862665176392\n",
      "Epoch 5851 | Train Loss: 1.2493150234222412 | Test Loss: 1.324819564819336\n",
      "Epoch 5852 | Train Loss: 1.1051205396652222 | Test Loss: 1.1304819583892822\n",
      "Epoch 5853 | Train Loss: 1.2492965459823608 | Test Loss: 1.3248374462127686\n",
      "Epoch 5854 | Train Loss: 1.105067491531372 | Test Loss: 1.1305453777313232\n",
      "Epoch 5855 | Train Loss: 1.2492449283599854 | Test Loss: 1.3248330354690552\n",
      "Epoch 5856 | Train Loss: 1.1050337553024292 | Test Loss: 1.1304069757461548\n",
      "Epoch 5857 | Train Loss: 1.249208927154541 | Test Loss: 1.3247398138046265\n",
      "Epoch 5858 | Train Loss: 1.1049914360046387 | Test Loss: 1.1304805278778076\n",
      "Epoch 5859 | Train Loss: 1.2491663694381714 | Test Loss: 1.3248233795166016\n",
      "Epoch 5860 | Train Loss: 1.1049553155899048 | Test Loss: 1.1303415298461914\n",
      "Epoch 5861 | Train Loss: 1.2491204738616943 | Test Loss: 1.3247095346450806\n",
      "Epoch 5862 | Train Loss: 1.1049158573150635 | Test Loss: 1.1303774118423462\n",
      "Epoch 5863 | Train Loss: 1.2490782737731934 | Test Loss: 1.3247519731521606\n",
      "Epoch 5864 | Train Loss: 1.1048723459243774 | Test Loss: 1.1302903890609741\n",
      "Epoch 5865 | Train Loss: 1.2490416765213013 | Test Loss: 1.3247076272964478\n",
      "Epoch 5866 | Train Loss: 1.1048332452774048 | Test Loss: 1.1302844285964966\n",
      "Epoch 5867 | Train Loss: 1.2489988803863525 | Test Loss: 1.3246500492095947\n",
      "Epoch 5868 | Train Loss: 1.104785680770874 | Test Loss: 1.130233883857727\n",
      "Epoch 5869 | Train Loss: 1.2489653825759888 | Test Loss: 1.324712872505188\n",
      "Epoch 5870 | Train Loss: 1.1047557592391968 | Test Loss: 1.1301945447921753\n",
      "Epoch 5871 | Train Loss: 1.2489091157913208 | Test Loss: 1.324607014656067\n",
      "Epoch 5872 | Train Loss: 1.1047158241271973 | Test Loss: 1.1301530599594116\n",
      "Epoch 5873 | Train Loss: 1.2488737106323242 | Test Loss: 1.324652075767517\n",
      "Epoch 5874 | Train Loss: 1.104676365852356 | Test Loss: 1.1301472187042236\n",
      "Epoch 5875 | Train Loss: 1.248822808265686 | Test Loss: 1.3245803117752075\n",
      "Epoch 5876 | Train Loss: 1.1046358346939087 | Test Loss: 1.1300814151763916\n",
      "Epoch 5877 | Train Loss: 1.2487949132919312 | Test Loss: 1.3245710134506226\n",
      "Epoch 5878 | Train Loss: 1.104591965675354 | Test Loss: 1.1300928592681885\n",
      "Epoch 5879 | Train Loss: 1.2487423419952393 | Test Loss: 1.3246169090270996\n",
      "Epoch 5880 | Train Loss: 1.104565143585205 | Test Loss: 1.1300008296966553\n",
      "Epoch 5881 | Train Loss: 1.2487059831619263 | Test Loss: 1.3244946002960205\n",
      "Epoch 5882 | Train Loss: 1.1045299768447876 | Test Loss: 1.1300309896469116\n",
      "Epoch 5883 | Train Loss: 1.248660922050476 | Test Loss: 1.3246219158172607\n",
      "Epoch 5884 | Train Loss: 1.1045039892196655 | Test Loss: 1.1299386024475098\n",
      "Epoch 5885 | Train Loss: 1.248613715171814 | Test Loss: 1.3244438171386719\n",
      "Epoch 5886 | Train Loss: 1.1044600009918213 | Test Loss: 1.1299583911895752\n",
      "Epoch 5887 | Train Loss: 1.2485785484313965 | Test Loss: 1.3245446681976318\n",
      "Epoch 5888 | Train Loss: 1.1044132709503174 | Test Loss: 1.1299009323120117\n",
      "Epoch 5889 | Train Loss: 1.248533010482788 | Test Loss: 1.3244454860687256\n",
      "Epoch 5890 | Train Loss: 1.1043864488601685 | Test Loss: 1.1298965215682983\n",
      "Epoch 5891 | Train Loss: 1.2484910488128662 | Test Loss: 1.324439525604248\n",
      "Epoch 5892 | Train Loss: 1.1043330430984497 | Test Loss: 1.129858374595642\n",
      "Epoch 5893 | Train Loss: 1.2484549283981323 | Test Loss: 1.3244541883468628\n",
      "Epoch 5894 | Train Loss: 1.1043121814727783 | Test Loss: 1.1298296451568604\n",
      "Epoch 5895 | Train Loss: 1.2484060525894165 | Test Loss: 1.3243581056594849\n",
      "Epoch 5896 | Train Loss: 1.1042617559432983 | Test Loss: 1.1298191547393799\n",
      "Epoch 5897 | Train Loss: 1.2483795881271362 | Test Loss: 1.3243988752365112\n",
      "Epoch 5898 | Train Loss: 1.1042320728302002 | Test Loss: 1.1297922134399414\n",
      "Epoch 5899 | Train Loss: 1.2483277320861816 | Test Loss: 1.324349284172058\n",
      "Epoch 5900 | Train Loss: 1.1041886806488037 | Test Loss: 1.1297358274459839\n",
      "Epoch 5901 | Train Loss: 1.2482999563217163 | Test Loss: 1.3243125677108765\n",
      "Epoch 5902 | Train Loss: 1.1041496992111206 | Test Loss: 1.129744291305542\n",
      "Epoch 5903 | Train Loss: 1.2482469081878662 | Test Loss: 1.3243558406829834\n",
      "Epoch 5904 | Train Loss: 1.1041178703308105 | Test Loss: 1.129672646522522\n",
      "Epoch 5905 | Train Loss: 1.2482106685638428 | Test Loss: 1.3242945671081543\n",
      "Epoch 5906 | Train Loss: 1.104079246520996 | Test Loss: 1.1296905279159546\n",
      "Epoch 5907 | Train Loss: 1.248172640800476 | Test Loss: 1.3242988586425781\n",
      "Epoch 5908 | Train Loss: 1.1040352582931519 | Test Loss: 1.129604458808899\n",
      "Epoch 5909 | Train Loss: 1.2481348514556885 | Test Loss: 1.3242666721343994\n",
      "Epoch 5910 | Train Loss: 1.1040068864822388 | Test Loss: 1.1296398639678955\n",
      "Epoch 5911 | Train Loss: 1.2480952739715576 | Test Loss: 1.3242547512054443\n",
      "Epoch 5912 | Train Loss: 1.1039584875106812 | Test Loss: 1.129536509513855\n",
      "Epoch 5913 | Train Loss: 1.248069167137146 | Test Loss: 1.3242195844650269\n",
      "Epoch 5914 | Train Loss: 1.1039364337921143 | Test Loss: 1.1295802593231201\n",
      "Epoch 5915 | Train Loss: 1.2480239868164062 | Test Loss: 1.3242026567459106\n",
      "Epoch 5916 | Train Loss: 1.103881597518921 | Test Loss: 1.129491925239563\n",
      "Epoch 5917 | Train Loss: 1.248008370399475 | Test Loss: 1.3241957426071167\n",
      "Epoch 5918 | Train Loss: 1.1038483381271362 | Test Loss: 1.1295344829559326\n",
      "Epoch 5919 | Train Loss: 1.247955322265625 | Test Loss: 1.324178695678711\n",
      "Epoch 5920 | Train Loss: 1.1038063764572144 | Test Loss: 1.1294242143630981\n",
      "Epoch 5921 | Train Loss: 1.2479225397109985 | Test Loss: 1.324153184890747\n",
      "Epoch 5922 | Train Loss: 1.1037710905075073 | Test Loss: 1.1294702291488647\n",
      "Epoch 5923 | Train Loss: 1.247869849205017 | Test Loss: 1.324173092842102\n",
      "Epoch 5924 | Train Loss: 1.1037302017211914 | Test Loss: 1.129356026649475\n",
      "Epoch 5925 | Train Loss: 1.2478361129760742 | Test Loss: 1.324076533317566\n",
      "Epoch 5926 | Train Loss: 1.1036925315856934 | Test Loss: 1.1294057369232178\n",
      "Epoch 5927 | Train Loss: 1.2477904558181763 | Test Loss: 1.3241515159606934\n",
      "Epoch 5928 | Train Loss: 1.1036533117294312 | Test Loss: 1.1293151378631592\n",
      "Epoch 5929 | Train Loss: 1.2477515935897827 | Test Loss: 1.3240587711334229\n",
      "Epoch 5930 | Train Loss: 1.1036230325698853 | Test Loss: 1.1293329000473022\n",
      "Epoch 5931 | Train Loss: 1.2477095127105713 | Test Loss: 1.3240461349487305\n",
      "Epoch 5932 | Train Loss: 1.1035763025283813 | Test Loss: 1.129259705543518\n",
      "Epoch 5933 | Train Loss: 1.2476688623428345 | Test Loss: 1.3241039514541626\n",
      "Epoch 5934 | Train Loss: 1.103555679321289 | Test Loss: 1.129277229309082\n",
      "Epoch 5935 | Train Loss: 1.2476240396499634 | Test Loss: 1.3239877223968506\n",
      "Epoch 5936 | Train Loss: 1.1035109758377075 | Test Loss: 1.1292015314102173\n",
      "Epoch 5937 | Train Loss: 1.2475926876068115 | Test Loss: 1.324034571647644\n",
      "Epoch 5938 | Train Loss: 1.1034656763076782 | Test Loss: 1.1292020082473755\n",
      "Epoch 5939 | Train Loss: 1.2475451231002808 | Test Loss: 1.3240079879760742\n",
      "Epoch 5940 | Train Loss: 1.103432059288025 | Test Loss: 1.129149079322815\n",
      "Epoch 5941 | Train Loss: 1.24750554561615 | Test Loss: 1.3239713907241821\n",
      "Epoch 5942 | Train Loss: 1.1033923625946045 | Test Loss: 1.1291358470916748\n",
      "Epoch 5943 | Train Loss: 1.247459888458252 | Test Loss: 1.3239820003509521\n",
      "Epoch 5944 | Train Loss: 1.103364109992981 | Test Loss: 1.129090428352356\n",
      "Epoch 5945 | Train Loss: 1.2474160194396973 | Test Loss: 1.3239173889160156\n",
      "Epoch 5946 | Train Loss: 1.1033191680908203 | Test Loss: 1.129076600074768\n",
      "Epoch 5947 | Train Loss: 1.2473841905593872 | Test Loss: 1.3239564895629883\n",
      "Epoch 5948 | Train Loss: 1.103292465209961 | Test Loss: 1.129048466682434\n",
      "Epoch 5949 | Train Loss: 1.2473334074020386 | Test Loss: 1.3239063024520874\n",
      "Epoch 5950 | Train Loss: 1.1032509803771973 | Test Loss: 1.1290146112442017\n",
      "Epoch 5951 | Train Loss: 1.2473007440567017 | Test Loss: 1.3238693475723267\n",
      "Epoch 5952 | Train Loss: 1.1032177209854126 | Test Loss: 1.1289974451065063\n",
      "Epoch 5953 | Train Loss: 1.2472550868988037 | Test Loss: 1.3239589929580688\n",
      "Epoch 5954 | Train Loss: 1.103187084197998 | Test Loss: 1.1289479732513428\n",
      "Epoch 5955 | Train Loss: 1.2472143173217773 | Test Loss: 1.323801875114441\n",
      "Epoch 5956 | Train Loss: 1.1031455993652344 | Test Loss: 1.1289435625076294\n",
      "Epoch 5957 | Train Loss: 1.2471729516983032 | Test Loss: 1.323917031288147\n",
      "Epoch 5958 | Train Loss: 1.1031064987182617 | Test Loss: 1.128900408744812\n",
      "Epoch 5959 | Train Loss: 1.2471365928649902 | Test Loss: 1.3237590789794922\n",
      "Epoch 5960 | Train Loss: 1.103076696395874 | Test Loss: 1.1288923025131226\n",
      "Epoch 5961 | Train Loss: 1.2470977306365967 | Test Loss: 1.3238589763641357\n",
      "Epoch 5962 | Train Loss: 1.1030322313308716 | Test Loss: 1.1288251876831055\n",
      "Epoch 5963 | Train Loss: 1.2470592260360718 | Test Loss: 1.3237444162368774\n",
      "Epoch 5964 | Train Loss: 1.1030083894729614 | Test Loss: 1.1288601160049438\n",
      "Epoch 5965 | Train Loss: 1.2470200061798096 | Test Loss: 1.3238234519958496\n",
      "Epoch 5966 | Train Loss: 1.1029549837112427 | Test Loss: 1.128771185874939\n",
      "Epoch 5967 | Train Loss: 1.2469886541366577 | Test Loss: 1.323723316192627\n",
      "Epoch 5968 | Train Loss: 1.1029447317123413 | Test Loss: 1.1288120746612549\n",
      "Epoch 5969 | Train Loss: 1.2469419240951538 | Test Loss: 1.3237812519073486\n",
      "Epoch 5970 | Train Loss: 1.1028846502304077 | Test Loss: 1.1287270784378052\n",
      "Epoch 5971 | Train Loss: 1.2469227313995361 | Test Loss: 1.3236794471740723\n",
      "Epoch 5972 | Train Loss: 1.1028680801391602 | Test Loss: 1.1287693977355957\n",
      "Epoch 5973 | Train Loss: 1.2468664646148682 | Test Loss: 1.323738694190979\n",
      "Epoch 5974 | Train Loss: 1.1028132438659668 | Test Loss: 1.1286733150482178\n",
      "Epoch 5975 | Train Loss: 1.2468472719192505 | Test Loss: 1.3236593008041382\n",
      "Epoch 5976 | Train Loss: 1.1027864217758179 | Test Loss: 1.1287320852279663\n",
      "Epoch 5977 | Train Loss: 1.246790885925293 | Test Loss: 1.323665738105774\n",
      "Epoch 5978 | Train Loss: 1.1027354001998901 | Test Loss: 1.1286135911941528\n",
      "Epoch 5979 | Train Loss: 1.2467724084854126 | Test Loss: 1.3235968351364136\n",
      "Epoch 5980 | Train Loss: 1.1027030944824219 | Test Loss: 1.1286778450012207\n",
      "Epoch 5981 | Train Loss: 1.2467091083526611 | Test Loss: 1.3236571550369263\n",
      "Epoch 5982 | Train Loss: 1.1026688814163208 | Test Loss: 1.1285500526428223\n",
      "Epoch 5983 | Train Loss: 1.2466853857040405 | Test Loss: 1.3235098123550415\n",
      "Epoch 5984 | Train Loss: 1.102631688117981 | Test Loss: 1.1286171674728394\n",
      "Epoch 5985 | Train Loss: 1.2466298341751099 | Test Loss: 1.3236639499664307\n",
      "Epoch 5986 | Train Loss: 1.102608561515808 | Test Loss: 1.128521203994751\n",
      "Epoch 5987 | Train Loss: 1.2465970516204834 | Test Loss: 1.3235278129577637\n",
      "Epoch 5988 | Train Loss: 1.1025621891021729 | Test Loss: 1.128546118736267\n",
      "Epoch 5989 | Train Loss: 1.2465565204620361 | Test Loss: 1.3235540390014648\n",
      "Epoch 5990 | Train Loss: 1.102518081665039 | Test Loss: 1.1284980773925781\n",
      "Epoch 5991 | Train Loss: 1.2465192079544067 | Test Loss: 1.323584794998169\n",
      "Epoch 5992 | Train Loss: 1.1024938821792603 | Test Loss: 1.1284582614898682\n",
      "Epoch 5993 | Train Loss: 1.2464677095413208 | Test Loss: 1.323441743850708\n",
      "Epoch 5994 | Train Loss: 1.1024597883224487 | Test Loss: 1.1284428834915161\n",
      "Epoch 5995 | Train Loss: 1.2464311122894287 | Test Loss: 1.3236017227172852\n",
      "Epoch 5996 | Train Loss: 1.1024402379989624 | Test Loss: 1.128415584564209\n",
      "Epoch 5997 | Train Loss: 1.246382236480713 | Test Loss: 1.3234343528747559\n",
      "Epoch 5998 | Train Loss: 1.1023898124694824 | Test Loss: 1.128375768661499\n",
      "Epoch 5999 | Train Loss: 1.2463624477386475 | Test Loss: 1.3234573602676392\n",
      "Epoch 6000 | Train Loss: 1.1023366451263428 | Test Loss: 1.128372073173523\n",
      "Epoch 6001 | Train Loss: 1.2463122606277466 | Test Loss: 1.3234789371490479\n",
      "Epoch 6002 | Train Loss: 1.1023082733154297 | Test Loss: 1.1283345222473145\n",
      "Epoch 6003 | Train Loss: 1.246270775794983 | Test Loss: 1.3233729600906372\n",
      "Epoch 6004 | Train Loss: 1.1022683382034302 | Test Loss: 1.1283063888549805\n",
      "Epoch 6005 | Train Loss: 1.246233582496643 | Test Loss: 1.3234783411026\n",
      "Epoch 6006 | Train Loss: 1.1022324562072754 | Test Loss: 1.128301978111267\n",
      "Epoch 6007 | Train Loss: 1.2461878061294556 | Test Loss: 1.3233433961868286\n",
      "Epoch 6008 | Train Loss: 1.1021950244903564 | Test Loss: 1.1282458305358887\n",
      "Epoch 6009 | Train Loss: 1.2461621761322021 | Test Loss: 1.3234062194824219\n",
      "Epoch 6010 | Train Loss: 1.102152943611145 | Test Loss: 1.1282821893692017\n",
      "Epoch 6011 | Train Loss: 1.24611496925354 | Test Loss: 1.323398470878601\n",
      "Epoch 6012 | Train Loss: 1.1021281480789185 | Test Loss: 1.128191351890564\n",
      "Epoch 6013 | Train Loss: 1.2460784912109375 | Test Loss: 1.3232958316802979\n",
      "Epoch 6014 | Train Loss: 1.102096676826477 | Test Loss: 1.1282377243041992\n",
      "Epoch 6015 | Train Loss: 1.2460256814956665 | Test Loss: 1.3233766555786133\n",
      "Epoch 6016 | Train Loss: 1.1020681858062744 | Test Loss: 1.1281540393829346\n",
      "Epoch 6017 | Train Loss: 1.2459934949874878 | Test Loss: 1.3232938051223755\n",
      "Epoch 6018 | Train Loss: 1.1020101308822632 | Test Loss: 1.128182291984558\n",
      "Epoch 6019 | Train Loss: 1.2459527254104614 | Test Loss: 1.3232587575912476\n",
      "Epoch 6020 | Train Loss: 1.1019750833511353 | Test Loss: 1.1281176805496216\n",
      "Epoch 6021 | Train Loss: 1.2459136247634888 | Test Loss: 1.3233580589294434\n",
      "Epoch 6022 | Train Loss: 1.1019514799118042 | Test Loss: 1.1281191110610962\n",
      "Epoch 6023 | Train Loss: 1.2458631992340088 | Test Loss: 1.323178768157959\n",
      "Epoch 6024 | Train Loss: 1.1019216775894165 | Test Loss: 1.128055214881897\n",
      "Epoch 6025 | Train Loss: 1.2458299398422241 | Test Loss: 1.3233296871185303\n",
      "Epoch 6026 | Train Loss: 1.101883053779602 | Test Loss: 1.1281006336212158\n",
      "Epoch 6027 | Train Loss: 1.245792031288147 | Test Loss: 1.3232399225234985\n",
      "Epoch 6028 | Train Loss: 1.101832389831543 | Test Loss: 1.1279977560043335\n",
      "Epoch 6029 | Train Loss: 1.2457737922668457 | Test Loss: 1.3231946229934692\n",
      "Epoch 6030 | Train Loss: 1.1017990112304688 | Test Loss: 1.1280349493026733\n",
      "Epoch 6031 | Train Loss: 1.2457036972045898 | Test Loss: 1.3232754468917847\n",
      "Epoch 6032 | Train Loss: 1.1017767190933228 | Test Loss: 1.1279631853103638\n",
      "Epoch 6033 | Train Loss: 1.2456731796264648 | Test Loss: 1.3231617212295532\n",
      "Epoch 6034 | Train Loss: 1.1017178297042847 | Test Loss: 1.1279746294021606\n",
      "Epoch 6035 | Train Loss: 1.2456237077713013 | Test Loss: 1.3231803178787231\n",
      "Epoch 6036 | Train Loss: 1.101688027381897 | Test Loss: 1.1279213428497314\n",
      "Epoch 6037 | Train Loss: 1.2455873489379883 | Test Loss: 1.323149561882019\n",
      "Epoch 6038 | Train Loss: 1.1016466617584229 | Test Loss: 1.1279237270355225\n",
      "Epoch 6039 | Train Loss: 1.2455475330352783 | Test Loss: 1.3231620788574219\n",
      "Epoch 6040 | Train Loss: 1.101609706878662 | Test Loss: 1.127874732017517\n",
      "Epoch 6041 | Train Loss: 1.2455061674118042 | Test Loss: 1.323131799697876\n",
      "Epoch 6042 | Train Loss: 1.1015806198120117 | Test Loss: 1.1278833150863647\n",
      "Epoch 6043 | Train Loss: 1.24546217918396 | Test Loss: 1.3231154680252075\n",
      "Epoch 6044 | Train Loss: 1.101539969444275 | Test Loss: 1.1278318166732788\n",
      "Epoch 6045 | Train Loss: 1.245425820350647 | Test Loss: 1.3231321573257446\n",
      "Epoch 6046 | Train Loss: 1.1015095710754395 | Test Loss: 1.1278362274169922\n",
      "Epoch 6047 | Train Loss: 1.2453821897506714 | Test Loss: 1.323073148727417\n",
      "Epoch 6048 | Train Loss: 1.1014719009399414 | Test Loss: 1.1277741193771362\n",
      "Epoch 6049 | Train Loss: 1.2453439235687256 | Test Loss: 1.3230787515640259\n",
      "Epoch 6050 | Train Loss: 1.1014389991760254 | Test Loss: 1.1277761459350586\n",
      "Epoch 6051 | Train Loss: 1.2453031539916992 | Test Loss: 1.323046326637268\n",
      "Epoch 6052 | Train Loss: 1.1013988256454468 | Test Loss: 1.127732276916504\n",
      "Epoch 6053 | Train Loss: 1.2452725172042847 | Test Loss: 1.3230397701263428\n",
      "Epoch 6054 | Train Loss: 1.1013718843460083 | Test Loss: 1.127745509147644\n",
      "Epoch 6055 | Train Loss: 1.24522864818573 | Test Loss: 1.3230291604995728\n",
      "Epoch 6056 | Train Loss: 1.1013290882110596 | Test Loss: 1.1276684999465942\n",
      "Epoch 6057 | Train Loss: 1.2451965808868408 | Test Loss: 1.3229297399520874\n",
      "Epoch 6058 | Train Loss: 1.1013035774230957 | Test Loss: 1.1276953220367432\n",
      "Epoch 6059 | Train Loss: 1.2451562881469727 | Test Loss: 1.3230750560760498\n",
      "Epoch 6060 | Train Loss: 1.1012901067733765 | Test Loss: 1.1276311874389648\n",
      "Epoch 6061 | Train Loss: 1.2451128959655762 | Test Loss: 1.3228886127471924\n",
      "Epoch 6062 | Train Loss: 1.101242184638977 | Test Loss: 1.1276270151138306\n",
      "Epoch 6063 | Train Loss: 1.2450809478759766 | Test Loss: 1.3229515552520752\n",
      "Epoch 6064 | Train Loss: 1.1011918783187866 | Test Loss: 1.127589225769043\n",
      "Epoch 6065 | Train Loss: 1.245039463043213 | Test Loss: 1.3229432106018066\n",
      "Epoch 6066 | Train Loss: 1.1011686325073242 | Test Loss: 1.1275609731674194\n",
      "Epoch 6067 | Train Loss: 1.2449970245361328 | Test Loss: 1.3228169679641724\n",
      "Epoch 6068 | Train Loss: 1.1011326313018799 | Test Loss: 1.1275290250778198\n",
      "Epoch 6069 | Train Loss: 1.2449501752853394 | Test Loss: 1.3229453563690186\n",
      "Epoch 6070 | Train Loss: 1.1011048555374146 | Test Loss: 1.1275264024734497\n",
      "Epoch 6071 | Train Loss: 1.2449171543121338 | Test Loss: 1.3228375911712646\n",
      "Epoch 6072 | Train Loss: 1.1010515689849854 | Test Loss: 1.1274861097335815\n",
      "Epoch 6073 | Train Loss: 1.2448945045471191 | Test Loss: 1.3228377103805542\n",
      "Epoch 6074 | Train Loss: 1.1010167598724365 | Test Loss: 1.127488613128662\n",
      "Epoch 6075 | Train Loss: 1.2448447942733765 | Test Loss: 1.3228744268417358\n",
      "Epoch 6076 | Train Loss: 1.1010057926177979 | Test Loss: 1.1274406909942627\n",
      "Epoch 6077 | Train Loss: 1.2448068857192993 | Test Loss: 1.322758436203003\n",
      "Epoch 6078 | Train Loss: 1.100963830947876 | Test Loss: 1.1274384260177612\n",
      "Epoch 6079 | Train Loss: 1.244761347770691 | Test Loss: 1.3228347301483154\n",
      "Epoch 6080 | Train Loss: 1.1009396314620972 | Test Loss: 1.1273901462554932\n",
      "Epoch 6081 | Train Loss: 1.2447303533554077 | Test Loss: 1.3227853775024414\n",
      "Epoch 6082 | Train Loss: 1.1008777618408203 | Test Loss: 1.127384066581726\n",
      "Epoch 6083 | Train Loss: 1.244696855545044 | Test Loss: 1.3226855993270874\n",
      "Epoch 6084 | Train Loss: 1.1008472442626953 | Test Loss: 1.1273208856582642\n",
      "Epoch 6085 | Train Loss: 1.2446515560150146 | Test Loss: 1.3228063583374023\n",
      "Epoch 6086 | Train Loss: 1.1008188724517822 | Test Loss: 1.127346396446228\n",
      "Epoch 6087 | Train Loss: 1.2446092367172241 | Test Loss: 1.322628378868103\n",
      "Epoch 6088 | Train Loss: 1.1007758378982544 | Test Loss: 1.127259612083435\n",
      "Epoch 6089 | Train Loss: 1.2445886135101318 | Test Loss: 1.322723627090454\n",
      "Epoch 6090 | Train Loss: 1.100732684135437 | Test Loss: 1.1272990703582764\n",
      "Epoch 6091 | Train Loss: 1.2445342540740967 | Test Loss: 1.3226327896118164\n",
      "Epoch 6092 | Train Loss: 1.100702166557312 | Test Loss: 1.1272010803222656\n",
      "Epoch 6093 | Train Loss: 1.2445118427276611 | Test Loss: 1.3226351737976074\n",
      "Epoch 6094 | Train Loss: 1.1006609201431274 | Test Loss: 1.1272633075714111\n",
      "Epoch 6095 | Train Loss: 1.244460105895996 | Test Loss: 1.32264244556427\n",
      "Epoch 6096 | Train Loss: 1.100631594657898 | Test Loss: 1.127150058746338\n",
      "Epoch 6097 | Train Loss: 1.244431495666504 | Test Loss: 1.322550892829895\n",
      "Epoch 6098 | Train Loss: 1.1005991697311401 | Test Loss: 1.1272053718566895\n",
      "Epoch 6099 | Train Loss: 1.244378685951233 | Test Loss: 1.3225950002670288\n",
      "Epoch 6100 | Train Loss: 1.1005626916885376 | Test Loss: 1.1271024942398071\n",
      "Epoch 6101 | Train Loss: 1.2443432807922363 | Test Loss: 1.3225655555725098\n",
      "Epoch 6102 | Train Loss: 1.1005347967147827 | Test Loss: 1.1271374225616455\n",
      "Epoch 6103 | Train Loss: 1.2443022727966309 | Test Loss: 1.322532057762146\n",
      "Epoch 6104 | Train Loss: 1.1004936695098877 | Test Loss: 1.1270538568496704\n",
      "Epoch 6105 | Train Loss: 1.2442659139633179 | Test Loss: 1.3225213289260864\n",
      "Epoch 6106 | Train Loss: 1.1004596948623657 | Test Loss: 1.1270819902420044\n",
      "Epoch 6107 | Train Loss: 1.2442288398742676 | Test Loss: 1.3225032091140747\n",
      "Epoch 6108 | Train Loss: 1.1004352569580078 | Test Loss: 1.1270190477371216\n",
      "Epoch 6109 | Train Loss: 1.2441872358322144 | Test Loss: 1.3224538564682007\n",
      "Epoch 6110 | Train Loss: 1.10039484500885 | Test Loss: 1.1269935369491577\n",
      "Epoch 6111 | Train Loss: 1.2441412210464478 | Test Loss: 1.3224173784255981\n",
      "Epoch 6112 | Train Loss: 1.1003605127334595 | Test Loss: 1.1269761323928833\n",
      "Epoch 6113 | Train Loss: 1.2441166639328003 | Test Loss: 1.3225051164627075\n",
      "Epoch 6114 | Train Loss: 1.1003285646438599 | Test Loss: 1.1269458532333374\n",
      "Epoch 6115 | Train Loss: 1.24407160282135 | Test Loss: 1.3223809003829956\n",
      "Epoch 6116 | Train Loss: 1.1002981662750244 | Test Loss: 1.1269093751907349\n",
      "Epoch 6117 | Train Loss: 1.244036078453064 | Test Loss: 1.322442889213562\n",
      "Epoch 6118 | Train Loss: 1.100249171257019 | Test Loss: 1.1269030570983887\n",
      "Epoch 6119 | Train Loss: 1.2439967393875122 | Test Loss: 1.3224248886108398\n",
      "Epoch 6120 | Train Loss: 1.1002200841903687 | Test Loss: 1.126836895942688\n",
      "Epoch 6121 | Train Loss: 1.2439676523208618 | Test Loss: 1.3223490715026855\n",
      "Epoch 6122 | Train Loss: 1.1001907587051392 | Test Loss: 1.1268526315689087\n",
      "Epoch 6123 | Train Loss: 1.2439171075820923 | Test Loss: 1.322467565536499\n",
      "Epoch 6124 | Train Loss: 1.1001821756362915 | Test Loss: 1.1267991065979004\n",
      "Epoch 6125 | Train Loss: 1.2438912391662598 | Test Loss: 1.3223165273666382\n",
      "Epoch 6126 | Train Loss: 1.1001161336898804 | Test Loss: 1.1268106698989868\n",
      "Epoch 6127 | Train Loss: 1.2438559532165527 | Test Loss: 1.3223378658294678\n",
      "Epoch 6128 | Train Loss: 1.1000803709030151 | Test Loss: 1.126765489578247\n",
      "Epoch 6129 | Train Loss: 1.2438126802444458 | Test Loss: 1.3223909139633179\n",
      "Epoch 6130 | Train Loss: 1.1000605821609497 | Test Loss: 1.1267656087875366\n",
      "Epoch 6131 | Train Loss: 1.243776798248291 | Test Loss: 1.3222440481185913\n",
      "Epoch 6132 | Train Loss: 1.1000243425369263 | Test Loss: 1.126696228981018\n",
      "Epoch 6133 | Train Loss: 1.243739128112793 | Test Loss: 1.322376012802124\n",
      "Epoch 6134 | Train Loss: 1.0999929904937744 | Test Loss: 1.1267211437225342\n",
      "Epoch 6135 | Train Loss: 1.243696689605713 | Test Loss: 1.3222336769104004\n",
      "Epoch 6136 | Train Loss: 1.0999420881271362 | Test Loss: 1.1266504526138306\n",
      "Epoch 6137 | Train Loss: 1.2436749935150146 | Test Loss: 1.3222583532333374\n",
      "Epoch 6138 | Train Loss: 1.0999059677124023 | Test Loss: 1.1266934871673584\n",
      "Epoch 6139 | Train Loss: 1.243618130683899 | Test Loss: 1.322304368019104\n",
      "Epoch 6140 | Train Loss: 1.0998941659927368 | Test Loss: 1.1266071796417236\n",
      "Epoch 6141 | Train Loss: 1.2435935735702515 | Test Loss: 1.322206735610962\n",
      "Epoch 6142 | Train Loss: 1.099846601486206 | Test Loss: 1.1266502141952515\n",
      "Epoch 6143 | Train Loss: 1.2435530424118042 | Test Loss: 1.3222507238388062\n",
      "Epoch 6144 | Train Loss: 1.09981107711792 | Test Loss: 1.1265771389007568\n",
      "Epoch 6145 | Train Loss: 1.2435219287872314 | Test Loss: 1.322226881980896\n",
      "Epoch 6146 | Train Loss: 1.0997719764709473 | Test Loss: 1.1265870332717896\n",
      "Epoch 6147 | Train Loss: 1.2434781789779663 | Test Loss: 1.3221386671066284\n",
      "Epoch 6148 | Train Loss: 1.099747657775879 | Test Loss: 1.1265259981155396\n",
      "Epoch 6149 | Train Loss: 1.2434401512145996 | Test Loss: 1.3222743272781372\n",
      "Epoch 6150 | Train Loss: 1.0997215509414673 | Test Loss: 1.1265534162521362\n",
      "Epoch 6151 | Train Loss: 1.2434039115905762 | Test Loss: 1.3220797777175903\n",
      "Epoch 6152 | Train Loss: 1.0996739864349365 | Test Loss: 1.1264622211456299\n",
      "Epoch 6153 | Train Loss: 1.2433741092681885 | Test Loss: 1.3222031593322754\n",
      "Epoch 6154 | Train Loss: 1.099629282951355 | Test Loss: 1.1265078783035278\n",
      "Epoch 6155 | Train Loss: 1.2433265447616577 | Test Loss: 1.322155475616455\n",
      "Epoch 6156 | Train Loss: 1.0996004343032837 | Test Loss: 1.1263915300369263\n",
      "Epoch 6157 | Train Loss: 1.2433053255081177 | Test Loss: 1.3221055269241333\n",
      "Epoch 6158 | Train Loss: 1.0995641946792603 | Test Loss: 1.1264700889587402\n",
      "Epoch 6159 | Train Loss: 1.2432619333267212 | Test Loss: 1.3222135305404663\n",
      "Epoch 6160 | Train Loss: 1.0995548963546753 | Test Loss: 1.126348614692688\n",
      "Epoch 6161 | Train Loss: 1.243223786354065 | Test Loss: 1.322011113166809\n",
      "Epoch 6162 | Train Loss: 1.099510908126831 | Test Loss: 1.1264011859893799\n",
      "Epoch 6163 | Train Loss: 1.243184208869934 | Test Loss: 1.3220970630645752\n",
      "Epoch 6164 | Train Loss: 1.0994583368301392 | Test Loss: 1.1263322830200195\n",
      "Epoch 6165 | Train Loss: 1.243147850036621 | Test Loss: 1.3220816850662231\n",
      "Epoch 6166 | Train Loss: 1.0994287729263306 | Test Loss: 1.1263415813446045\n",
      "Epoch 6167 | Train Loss: 1.243105411529541 | Test Loss: 1.3219753503799438\n",
      "Epoch 6168 | Train Loss: 1.0993973016738892 | Test Loss: 1.1262978315353394\n",
      "Epoch 6169 | Train Loss: 1.2430716753005981 | Test Loss: 1.3220927715301514\n",
      "Epoch 6170 | Train Loss: 1.0993599891662598 | Test Loss: 1.1262861490249634\n",
      "Epoch 6171 | Train Loss: 1.2430297136306763 | Test Loss: 1.3219857215881348\n",
      "Epoch 6172 | Train Loss: 1.0993199348449707 | Test Loss: 1.1262280941009521\n",
      "Epoch 6173 | Train Loss: 1.2430064678192139 | Test Loss: 1.3219962120056152\n",
      "Epoch 6174 | Train Loss: 1.0992851257324219 | Test Loss: 1.1262682676315308\n",
      "Epoch 6175 | Train Loss: 1.2429533004760742 | Test Loss: 1.322074294090271\n",
      "Epoch 6176 | Train Loss: 1.0992659330368042 | Test Loss: 1.12617826461792\n",
      "Epoch 6177 | Train Loss: 1.2429240942001343 | Test Loss: 1.321943998336792\n",
      "Epoch 6178 | Train Loss: 1.09922456741333 | Test Loss: 1.126229166984558\n",
      "Epoch 6179 | Train Loss: 1.2428845167160034 | Test Loss: 1.3220365047454834\n",
      "Epoch 6180 | Train Loss: 1.0991934537887573 | Test Loss: 1.1261615753173828\n",
      "Epoch 6181 | Train Loss: 1.2428500652313232 | Test Loss: 1.3219190835952759\n",
      "Epoch 6182 | Train Loss: 1.0991538763046265 | Test Loss: 1.126171350479126\n",
      "Epoch 6183 | Train Loss: 1.2428127527236938 | Test Loss: 1.3219125270843506\n",
      "Epoch 6184 | Train Loss: 1.0991164445877075 | Test Loss: 1.126104712486267\n",
      "Epoch 6185 | Train Loss: 1.2427695989608765 | Test Loss: 1.3219809532165527\n",
      "Epoch 6186 | Train Loss: 1.0991014242172241 | Test Loss: 1.1261107921600342\n",
      "Epoch 6187 | Train Loss: 1.2427303791046143 | Test Loss: 1.321846842765808\n",
      "Epoch 6188 | Train Loss: 1.099050521850586 | Test Loss: 1.1260539293289185\n",
      "Epoch 6189 | Train Loss: 1.2427054643630981 | Test Loss: 1.321928858757019\n",
      "Epoch 6190 | Train Loss: 1.0990173816680908 | Test Loss: 1.126076340675354\n",
      "Epoch 6191 | Train Loss: 1.2426677942276 | Test Loss: 1.3218655586242676\n",
      "Epoch 6192 | Train Loss: 1.0989784002304077 | Test Loss: 1.1259961128234863\n",
      "Epoch 6193 | Train Loss: 1.2426353693008423 | Test Loss: 1.321826457977295\n",
      "Epoch 6194 | Train Loss: 1.0989537239074707 | Test Loss: 1.1260442733764648\n",
      "Epoch 6195 | Train Loss: 1.2425857782363892 | Test Loss: 1.3218990564346313\n",
      "Epoch 6196 | Train Loss: 1.098926067352295 | Test Loss: 1.125959038734436\n",
      "Epoch 6197 | Train Loss: 1.2425532341003418 | Test Loss: 1.321783423423767\n",
      "Epoch 6198 | Train Loss: 1.0988881587982178 | Test Loss: 1.1259968280792236\n",
      "Epoch 6199 | Train Loss: 1.2425169944763184 | Test Loss: 1.3218274116516113\n",
      "Epoch 6200 | Train Loss: 1.098853588104248 | Test Loss: 1.1259181499481201\n",
      "Epoch 6201 | Train Loss: 1.2424854040145874 | Test Loss: 1.321831464767456\n",
      "Epoch 6202 | Train Loss: 1.0988247394561768 | Test Loss: 1.1259232759475708\n",
      "Epoch 6203 | Train Loss: 1.2424461841583252 | Test Loss: 1.3217610120773315\n",
      "Epoch 6204 | Train Loss: 1.09878671169281 | Test Loss: 1.1258691549301147\n",
      "Epoch 6205 | Train Loss: 1.2424167394638062 | Test Loss: 1.3218646049499512\n",
      "Epoch 6206 | Train Loss: 1.0987608432769775 | Test Loss: 1.125901222229004\n",
      "Epoch 6207 | Train Loss: 1.242376685142517 | Test Loss: 1.3217105865478516\n",
      "Epoch 6208 | Train Loss: 1.0987186431884766 | Test Loss: 1.1258103847503662\n",
      "Epoch 6209 | Train Loss: 1.2423665523529053 | Test Loss: 1.3217836618423462\n",
      "Epoch 6210 | Train Loss: 1.098678469657898 | Test Loss: 1.125876545906067\n",
      "Epoch 6211 | Train Loss: 1.2423118352890015 | Test Loss: 1.321766972541809\n",
      "Epoch 6212 | Train Loss: 1.0986716747283936 | Test Loss: 1.1257579326629639\n",
      "Epoch 6213 | Train Loss: 1.2422879934310913 | Test Loss: 1.321694016456604\n",
      "Epoch 6214 | Train Loss: 1.0986255407333374 | Test Loss: 1.125830888748169\n",
      "Epoch 6215 | Train Loss: 1.2422382831573486 | Test Loss: 1.3217540979385376\n",
      "Epoch 6216 | Train Loss: 1.0985941886901855 | Test Loss: 1.1257201433181763\n",
      "Epoch 6217 | Train Loss: 1.242207646369934 | Test Loss: 1.3216742277145386\n",
      "Epoch 6218 | Train Loss: 1.098542332649231 | Test Loss: 1.1257591247558594\n",
      "Epoch 6219 | Train Loss: 1.24216628074646 | Test Loss: 1.3216158151626587\n",
      "Epoch 6220 | Train Loss: 1.0985125303268433 | Test Loss: 1.1256897449493408\n",
      "Epoch 6221 | Train Loss: 1.242138385772705 | Test Loss: 1.3217484951019287\n",
      "Epoch 6222 | Train Loss: 1.0984841585159302 | Test Loss: 1.1257052421569824\n",
      "Epoch 6223 | Train Loss: 1.24208664894104 | Test Loss: 1.3215268850326538\n",
      "Epoch 6224 | Train Loss: 1.0984715223312378 | Test Loss: 1.1256325244903564\n",
      "Epoch 6225 | Train Loss: 1.2420564889907837 | Test Loss: 1.321756362915039\n",
      "Epoch 6226 | Train Loss: 1.0984230041503906 | Test Loss: 1.1256701946258545\n",
      "Epoch 6227 | Train Loss: 1.2420185804367065 | Test Loss: 1.321567177772522\n",
      "Epoch 6228 | Train Loss: 1.0983949899673462 | Test Loss: 1.1255712509155273\n",
      "Epoch 6229 | Train Loss: 1.2419899702072144 | Test Loss: 1.321624994277954\n",
      "Epoch 6230 | Train Loss: 1.0983495712280273 | Test Loss: 1.1256290674209595\n",
      "Epoch 6231 | Train Loss: 1.2419425249099731 | Test Loss: 1.3216391801834106\n",
      "Epoch 6232 | Train Loss: 1.0983351469039917 | Test Loss: 1.1255367994308472\n",
      "Epoch 6233 | Train Loss: 1.241904854774475 | Test Loss: 1.3215581178665161\n",
      "Epoch 6234 | Train Loss: 1.0982797145843506 | Test Loss: 1.125546932220459\n",
      "Epoch 6235 | Train Loss: 1.241869330406189 | Test Loss: 1.3215413093566895\n",
      "Epoch 6236 | Train Loss: 1.0982537269592285 | Test Loss: 1.1255213022232056\n",
      "Epoch 6237 | Train Loss: 1.2418413162231445 | Test Loss: 1.3216415643692017\n",
      "Epoch 6238 | Train Loss: 1.0982154607772827 | Test Loss: 1.125500202178955\n",
      "Epoch 6239 | Train Loss: 1.2418090105056763 | Test Loss: 1.3214325904846191\n",
      "Epoch 6240 | Train Loss: 1.0982228517532349 | Test Loss: 1.125463843345642\n",
      "Epoch 6241 | Train Loss: 1.2417564392089844 | Test Loss: 1.3217271566390991\n",
      "Epoch 6242 | Train Loss: 1.0982062816619873 | Test Loss: 1.1254719495773315\n",
      "Epoch 6243 | Train Loss: 1.2417361736297607 | Test Loss: 1.3213969469070435\n",
      "Epoch 6244 | Train Loss: 1.0981537103652954 | Test Loss: 1.1254295110702515\n",
      "Epoch 6245 | Train Loss: 1.2417186498641968 | Test Loss: 1.321588397026062\n",
      "Epoch 6246 | Train Loss: 1.0980976819992065 | Test Loss: 1.125441074371338\n",
      "Epoch 6247 | Train Loss: 1.2416913509368896 | Test Loss: 1.32148277759552\n",
      "Epoch 6248 | Train Loss: 1.098111629486084 | Test Loss: 1.125434398651123\n",
      "Epoch 6249 | Train Loss: 1.2416609525680542 | Test Loss: 1.3215256929397583\n",
      "Epoch 6250 | Train Loss: 1.0980490446090698 | Test Loss: 1.1253546476364136\n",
      "Epoch 6251 | Train Loss: 1.2416541576385498 | Test Loss: 1.3214540481567383\n",
      "Epoch 6252 | Train Loss: 1.0980528593063354 | Test Loss: 1.1254669427871704\n",
      "Epoch 6253 | Train Loss: 1.241639256477356 | Test Loss: 1.321632981300354\n",
      "Epoch 6254 | Train Loss: 1.0979894399642944 | Test Loss: 1.1253162622451782\n",
      "Epoch 6255 | Train Loss: 1.2416629791259766 | Test Loss: 1.3213419914245605\n",
      "Epoch 6256 | Train Loss: 1.0979890823364258 | Test Loss: 1.125483751296997\n",
      "Epoch 6257 | Train Loss: 1.2416507005691528 | Test Loss: 1.3217060565948486\n",
      "Epoch 6258 | Train Loss: 1.0979361534118652 | Test Loss: 1.125309944152832\n",
      "Epoch 6259 | Train Loss: 1.2416683435440063 | Test Loss: 1.3213099241256714\n",
      "Epoch 6260 | Train Loss: 1.0979163646697998 | Test Loss: 1.125476598739624\n",
      "Epoch 6261 | Train Loss: 1.2416085004806519 | Test Loss: 1.3215887546539307\n",
      "Epoch 6262 | Train Loss: 1.0978299379348755 | Test Loss: 1.1252869367599487\n",
      "Epoch 6263 | Train Loss: 1.2416203022003174 | Test Loss: 1.3213545083999634\n",
      "Epoch 6264 | Train Loss: 1.097807765007019 | Test Loss: 1.125428318977356\n",
      "Epoch 6265 | Train Loss: 1.2415322065353394 | Test Loss: 1.3214412927627563\n",
      "Epoch 6266 | Train Loss: 1.0977497100830078 | Test Loss: 1.1252243518829346\n",
      "Epoch 6267 | Train Loss: 1.2415134906768799 | Test Loss: 1.3213768005371094\n",
      "Epoch 6268 | Train Loss: 1.0977189540863037 | Test Loss: 1.12532639503479\n",
      "Epoch 6269 | Train Loss: 1.241391897201538 | Test Loss: 1.3213239908218384\n",
      "Epoch 6270 | Train Loss: 1.0976955890655518 | Test Loss: 1.1251686811447144\n",
      "Epoch 6271 | Train Loss: 1.241345763206482 | Test Loss: 1.3214209079742432\n",
      "Epoch 6272 | Train Loss: 1.0976574420928955 | Test Loss: 1.1251546144485474\n",
      "Epoch 6273 | Train Loss: 1.2412344217300415 | Test Loss: 1.3212367296218872\n",
      "Epoch 6274 | Train Loss: 1.097651720046997 | Test Loss: 1.1250969171524048\n",
      "Epoch 6275 | Train Loss: 1.2411872148513794 | Test Loss: 1.321453332901001\n",
      "Epoch 6276 | Train Loss: 1.0976096391677856 | Test Loss: 1.125052571296692\n",
      "Epoch 6277 | Train Loss: 1.241121768951416 | Test Loss: 1.3212015628814697\n",
      "Epoch 6278 | Train Loss: 1.0975971221923828 | Test Loss: 1.1250280141830444\n",
      "Epoch 6279 | Train Loss: 1.2410829067230225 | Test Loss: 1.321416974067688\n",
      "Epoch 6280 | Train Loss: 1.0975382328033447 | Test Loss: 1.1250005960464478\n",
      "Epoch 6281 | Train Loss: 1.2410577535629272 | Test Loss: 1.3211861848831177\n",
      "Epoch 6282 | Train Loss: 1.0975133180618286 | Test Loss: 1.1250029802322388\n",
      "Epoch 6283 | Train Loss: 1.2410142421722412 | Test Loss: 1.3213170766830444\n",
      "Epoch 6284 | Train Loss: 1.0974633693695068 | Test Loss: 1.1249574422836304\n",
      "Epoch 6285 | Train Loss: 1.2410019636154175 | Test Loss: 1.3211641311645508\n",
      "Epoch 6286 | Train Loss: 1.0974340438842773 | Test Loss: 1.124989628791809\n",
      "Epoch 6287 | Train Loss: 1.2409530878067017 | Test Loss: 1.3212429285049438\n",
      "Epoch 6288 | Train Loss: 1.0973985195159912 | Test Loss: 1.1249046325683594\n",
      "Epoch 6289 | Train Loss: 1.2409343719482422 | Test Loss: 1.3211698532104492\n",
      "Epoch 6290 | Train Loss: 1.0973637104034424 | Test Loss: 1.1249566078186035\n",
      "Epoch 6291 | Train Loss: 1.2408909797668457 | Test Loss: 1.321202278137207\n",
      "Epoch 6292 | Train Loss: 1.0973379611968994 | Test Loss: 1.124861717224121\n",
      "Epoch 6293 | Train Loss: 1.2408658266067505 | Test Loss: 1.321204423904419\n",
      "Epoch 6294 | Train Loss: 1.0973039865493774 | Test Loss: 1.1248987913131714\n",
      "Epoch 6295 | Train Loss: 1.2408102750778198 | Test Loss: 1.3211157321929932\n",
      "Epoch 6296 | Train Loss: 1.0972737073898315 | Test Loss: 1.1248222589492798\n",
      "Epoch 6297 | Train Loss: 1.2407748699188232 | Test Loss: 1.3212337493896484\n",
      "Epoch 6298 | Train Loss: 1.0972532033920288 | Test Loss: 1.1248373985290527\n",
      "Epoch 6299 | Train Loss: 1.240727186203003 | Test Loss: 1.3210771083831787\n",
      "Epoch 6300 | Train Loss: 1.0972142219543457 | Test Loss: 1.1247875690460205\n",
      "Epoch 6301 | Train Loss: 1.24070405960083 | Test Loss: 1.3211915493011475\n",
      "Epoch 6302 | Train Loss: 1.0971800088882446 | Test Loss: 1.1247963905334473\n",
      "Epoch 6303 | Train Loss: 1.2406619787216187 | Test Loss: 1.3210995197296143\n",
      "Epoch 6304 | Train Loss: 1.0971473455429077 | Test Loss: 1.124735713005066\n",
      "Epoch 6305 | Train Loss: 1.240634560585022 | Test Loss: 1.3211008310317993\n",
      "Epoch 6306 | Train Loss: 1.0971095561981201 | Test Loss: 1.124751091003418\n",
      "Epoch 6307 | Train Loss: 1.2405904531478882 | Test Loss: 1.3211092948913574\n",
      "Epoch 6308 | Train Loss: 1.097090482711792 | Test Loss: 1.1246908903121948\n",
      "Epoch 6309 | Train Loss: 1.2405534982681274 | Test Loss: 1.3210289478302002\n",
      "Epoch 6310 | Train Loss: 1.0970470905303955 | Test Loss: 1.1247012615203857\n",
      "Epoch 6311 | Train Loss: 1.2405245304107666 | Test Loss: 1.321049690246582\n",
      "Epoch 6312 | Train Loss: 1.097015380859375 | Test Loss: 1.1246731281280518\n",
      "Epoch 6313 | Train Loss: 1.2404921054840088 | Test Loss: 1.3210755586624146\n",
      "Epoch 6314 | Train Loss: 1.0969901084899902 | Test Loss: 1.1246347427368164\n",
      "Epoch 6315 | Train Loss: 1.2404518127441406 | Test Loss: 1.3210015296936035\n",
      "Epoch 6316 | Train Loss: 1.0969659090042114 | Test Loss: 1.1246553659439087\n",
      "Epoch 6317 | Train Loss: 1.240416169166565 | Test Loss: 1.3211005926132202\n",
      "Epoch 6318 | Train Loss: 1.096934199333191 | Test Loss: 1.1246085166931152\n",
      "Epoch 6319 | Train Loss: 1.2403823137283325 | Test Loss: 1.3209694623947144\n",
      "Epoch 6320 | Train Loss: 1.0968942642211914 | Test Loss: 1.1246005296707153\n",
      "Epoch 6321 | Train Loss: 1.2403494119644165 | Test Loss: 1.3210253715515137\n",
      "Epoch 6322 | Train Loss: 1.0968605279922485 | Test Loss: 1.1245691776275635\n",
      "Epoch 6323 | Train Loss: 1.2403088808059692 | Test Loss: 1.32096266746521\n",
      "Epoch 6324 | Train Loss: 1.0968377590179443 | Test Loss: 1.1245580911636353\n",
      "Epoch 6325 | Train Loss: 1.2402796745300293 | Test Loss: 1.3209596872329712\n",
      "Epoch 6326 | Train Loss: 1.0967904329299927 | Test Loss: 1.1245301961898804\n",
      "Epoch 6327 | Train Loss: 1.24024498462677 | Test Loss: 1.3209590911865234\n",
      "Epoch 6328 | Train Loss: 1.0967717170715332 | Test Loss: 1.1245366334915161\n",
      "Epoch 6329 | Train Loss: 1.240203619003296 | Test Loss: 1.3209162950515747\n",
      "Epoch 6330 | Train Loss: 1.0967355966567993 | Test Loss: 1.1244860887527466\n",
      "Epoch 6331 | Train Loss: 1.2401745319366455 | Test Loss: 1.3209142684936523\n",
      "Epoch 6332 | Train Loss: 1.0967105627059937 | Test Loss: 1.124489188194275\n",
      "Epoch 6333 | Train Loss: 1.2401288747787476 | Test Loss: 1.3209350109100342\n",
      "Epoch 6334 | Train Loss: 1.096691370010376 | Test Loss: 1.1244690418243408\n",
      "Epoch 6335 | Train Loss: 1.2401065826416016 | Test Loss: 1.3208705186843872\n",
      "Epoch 6336 | Train Loss: 1.0966500043869019 | Test Loss: 1.1244451999664307\n",
      "Epoch 6337 | Train Loss: 1.2400727272033691 | Test Loss: 1.3208955526351929\n",
      "Epoch 6338 | Train Loss: 1.0966180562973022 | Test Loss: 1.1244300603866577\n",
      "Epoch 6339 | Train Loss: 1.240035891532898 | Test Loss: 1.3209103345870972\n",
      "Epoch 6340 | Train Loss: 1.096615195274353 | Test Loss: 1.1244232654571533\n",
      "Epoch 6341 | Train Loss: 1.239992380142212 | Test Loss: 1.3207995891571045\n",
      "Epoch 6342 | Train Loss: 1.096571445465088 | Test Loss: 1.1243810653686523\n",
      "Epoch 6343 | Train Loss: 1.2399673461914062 | Test Loss: 1.320871114730835\n",
      "Epoch 6344 | Train Loss: 1.0965344905853271 | Test Loss: 1.1243958473205566\n",
      "Epoch 6345 | Train Loss: 1.2399274110794067 | Test Loss: 1.3208402395248413\n",
      "Epoch 6346 | Train Loss: 1.0964964628219604 | Test Loss: 1.124324083328247\n",
      "Epoch 6347 | Train Loss: 1.2399051189422607 | Test Loss: 1.3207893371582031\n",
      "Epoch 6348 | Train Loss: 1.096472978591919 | Test Loss: 1.1243598461151123\n",
      "Epoch 6349 | Train Loss: 1.2398570775985718 | Test Loss: 1.3208816051483154\n",
      "Epoch 6350 | Train Loss: 1.0964568853378296 | Test Loss: 1.1242831945419312\n",
      "Epoch 6351 | Train Loss: 1.2398275136947632 | Test Loss: 1.3207579851150513\n",
      "Epoch 6352 | Train Loss: 1.0964140892028809 | Test Loss: 1.1243128776550293\n",
      "Epoch 6353 | Train Loss: 1.239795207977295 | Test Loss: 1.3208028078079224\n",
      "Epoch 6354 | Train Loss: 1.0963733196258545 | Test Loss: 1.1242585182189941\n",
      "Epoch 6355 | Train Loss: 1.2397637367248535 | Test Loss: 1.320778250694275\n",
      "Epoch 6356 | Train Loss: 1.096346378326416 | Test Loss: 1.1242913007736206\n",
      "Epoch 6357 | Train Loss: 1.2397266626358032 | Test Loss: 1.3207141160964966\n",
      "Epoch 6358 | Train Loss: 1.0963062047958374 | Test Loss: 1.124215006828308\n",
      "Epoch 6359 | Train Loss: 1.239702582359314 | Test Loss: 1.320797085762024\n",
      "Epoch 6360 | Train Loss: 1.0962849855422974 | Test Loss: 1.1242517232894897\n",
      "Epoch 6361 | Train Loss: 1.2396509647369385 | Test Loss: 1.3207199573516846\n",
      "Epoch 6362 | Train Loss: 1.0962506532669067 | Test Loss: 1.12416672706604\n",
      "Epoch 6363 | Train Loss: 1.2396371364593506 | Test Loss: 1.320737600326538\n",
      "Epoch 6364 | Train Loss: 1.096216082572937 | Test Loss: 1.124228596687317\n",
      "Epoch 6365 | Train Loss: 1.2395941019058228 | Test Loss: 1.3207674026489258\n",
      "Epoch 6366 | Train Loss: 1.0961960554122925 | Test Loss: 1.1241322755813599\n",
      "Epoch 6367 | Train Loss: 1.2395662069320679 | Test Loss: 1.3206831216812134\n",
      "Epoch 6368 | Train Loss: 1.0961551666259766 | Test Loss: 1.1241730451583862\n",
      "Epoch 6369 | Train Loss: 1.2395333051681519 | Test Loss: 1.3207178115844727\n",
      "Epoch 6370 | Train Loss: 1.0961240530014038 | Test Loss: 1.1241073608398438\n",
      "Epoch 6371 | Train Loss: 1.2394942045211792 | Test Loss: 1.3207567930221558\n",
      "Epoch 6372 | Train Loss: 1.0961229801177979 | Test Loss: 1.1241401433944702\n",
      "Epoch 6373 | Train Loss: 1.2394593954086304 | Test Loss: 1.3206619024276733\n",
      "Epoch 6374 | Train Loss: 1.0960766077041626 | Test Loss: 1.1240628957748413\n",
      "Epoch 6375 | Train Loss: 1.2394365072250366 | Test Loss: 1.3207330703735352\n",
      "Epoch 6376 | Train Loss: 1.096039891242981 | Test Loss: 1.124115228652954\n",
      "Epoch 6377 | Train Loss: 1.239397644996643 | Test Loss: 1.320669412612915\n",
      "Epoch 6378 | Train Loss: 1.0960040092468262 | Test Loss: 1.1239975690841675\n",
      "Epoch 6379 | Train Loss: 1.2393707036972046 | Test Loss: 1.3206210136413574\n",
      "Epoch 6380 | Train Loss: 1.095994472503662 | Test Loss: 1.124072790145874\n",
      "Epoch 6381 | Train Loss: 1.2393200397491455 | Test Loss: 1.3207262754440308\n",
      "Epoch 6382 | Train Loss: 1.0959720611572266 | Test Loss: 1.1239773035049438\n",
      "Epoch 6383 | Train Loss: 1.239298939704895 | Test Loss: 1.3205443620681763\n",
      "Epoch 6384 | Train Loss: 1.0959275960922241 | Test Loss: 1.1240109205245972\n",
      "Epoch 6385 | Train Loss: 1.2392643690109253 | Test Loss: 1.3206723928451538\n",
      "Epoch 6386 | Train Loss: 1.095877766609192 | Test Loss: 1.1239659786224365\n",
      "Epoch 6387 | Train Loss: 1.2392412424087524 | Test Loss: 1.3205976486206055\n",
      "Epoch 6388 | Train Loss: 1.0958555936813354 | Test Loss: 1.1239495277404785\n",
      "Epoch 6389 | Train Loss: 1.2391932010650635 | Test Loss: 1.3205395936965942\n",
      "Epoch 6390 | Train Loss: 1.0958354473114014 | Test Loss: 1.123889684677124\n",
      "Epoch 6391 | Train Loss: 1.2391600608825684 | Test Loss: 1.3206311464309692\n",
      "Epoch 6392 | Train Loss: 1.0958147048950195 | Test Loss: 1.1239384412765503\n",
      "Epoch 6393 | Train Loss: 1.2391220331192017 | Test Loss: 1.3205101490020752\n",
      "Epoch 6394 | Train Loss: 1.0957584381103516 | Test Loss: 1.123827338218689\n",
      "Epoch 6395 | Train Loss: 1.2391077280044556 | Test Loss: 1.320517897605896\n",
      "Epoch 6396 | Train Loss: 1.0957235097885132 | Test Loss: 1.1239135265350342\n",
      "Epoch 6397 | Train Loss: 1.2390563488006592 | Test Loss: 1.3205952644348145\n",
      "Epoch 6398 | Train Loss: 1.0957080125808716 | Test Loss: 1.1237940788269043\n",
      "Epoch 6399 | Train Loss: 1.2390249967575073 | Test Loss: 1.3204212188720703\n",
      "Epoch 6400 | Train Loss: 1.095677375793457 | Test Loss: 1.1238387823104858\n",
      "Epoch 6401 | Train Loss: 1.2389787435531616 | Test Loss: 1.320555567741394\n",
      "Epoch 6402 | Train Loss: 1.0956406593322754 | Test Loss: 1.1237895488739014\n",
      "Epoch 6403 | Train Loss: 1.2389473915100098 | Test Loss: 1.3204450607299805\n",
      "Epoch 6404 | Train Loss: 1.095608115196228 | Test Loss: 1.123774528503418\n",
      "Epoch 6405 | Train Loss: 1.2389086484909058 | Test Loss: 1.3204493522644043\n",
      "Epoch 6406 | Train Loss: 1.0955734252929688 | Test Loss: 1.1237441301345825\n",
      "Epoch 6407 | Train Loss: 1.2388758659362793 | Test Loss: 1.3205121755599976\n",
      "Epoch 6408 | Train Loss: 1.0955579280853271 | Test Loss: 1.1237510442733765\n",
      "Epoch 6409 | Train Loss: 1.2388420104980469 | Test Loss: 1.3204628229141235\n",
      "Epoch 6410 | Train Loss: 1.095501184463501 | Test Loss: 1.1236757040023804\n",
      "Epoch 6411 | Train Loss: 1.2388252019882202 | Test Loss: 1.3204383850097656\n",
      "Epoch 6412 | Train Loss: 1.0954911708831787 | Test Loss: 1.1237119436264038\n",
      "Epoch 6413 | Train Loss: 1.238770604133606 | Test Loss: 1.32054603099823\n",
      "Epoch 6414 | Train Loss: 1.0954771041870117 | Test Loss: 1.123642921447754\n",
      "Epoch 6415 | Train Loss: 1.238746166229248 | Test Loss: 1.320359706878662\n",
      "Epoch 6416 | Train Loss: 1.095429539680481 | Test Loss: 1.12364661693573\n",
      "Epoch 6417 | Train Loss: 1.2387104034423828 | Test Loss: 1.320448875427246\n",
      "Epoch 6418 | Train Loss: 1.0953890085220337 | Test Loss: 1.1236251592636108\n",
      "Epoch 6419 | Train Loss: 1.2386819124221802 | Test Loss: 1.3204258680343628\n",
      "Epoch 6420 | Train Loss: 1.0953799486160278 | Test Loss: 1.1236097812652588\n",
      "Epoch 6421 | Train Loss: 1.2386454343795776 | Test Loss: 1.3203585147857666\n",
      "Epoch 6422 | Train Loss: 1.0953607559204102 | Test Loss: 1.1235718727111816\n",
      "Epoch 6423 | Train Loss: 1.2386058568954468 | Test Loss: 1.3204541206359863\n",
      "Epoch 6424 | Train Loss: 1.0953327417373657 | Test Loss: 1.1235944032669067\n",
      "Epoch 6425 | Train Loss: 1.2385737895965576 | Test Loss: 1.320335865020752\n",
      "Epoch 6426 | Train Loss: 1.09526789188385 | Test Loss: 1.1234776973724365\n",
      "Epoch 6427 | Train Loss: 1.2385663986206055 | Test Loss: 1.3203091621398926\n",
      "Epoch 6428 | Train Loss: 1.0952434539794922 | Test Loss: 1.1235829591751099\n",
      "Epoch 6429 | Train Loss: 1.2385252714157104 | Test Loss: 1.3204892873764038\n",
      "Epoch 6430 | Train Loss: 1.095247745513916 | Test Loss: 1.1234463453292847\n",
      "Epoch 6431 | Train Loss: 1.238484263420105 | Test Loss: 1.320245385169983\n",
      "Epoch 6432 | Train Loss: 1.0952180624008179 | Test Loss: 1.1235214471817017\n",
      "Epoch 6433 | Train Loss: 1.238447904586792 | Test Loss: 1.320435881614685\n",
      "Epoch 6434 | Train Loss: 1.095160961151123 | Test Loss: 1.123436689376831\n",
      "Epoch 6435 | Train Loss: 1.2384196519851685 | Test Loss: 1.320243000984192\n",
      "Epoch 6436 | Train Loss: 1.0951290130615234 | Test Loss: 1.123460292816162\n",
      "Epoch 6437 | Train Loss: 1.238379955291748 | Test Loss: 1.320305585861206\n",
      "Epoch 6438 | Train Loss: 1.0950796604156494 | Test Loss: 1.1233906745910645\n",
      "Epoch 6439 | Train Loss: 1.2383564710617065 | Test Loss: 1.3203257322311401\n",
      "Epoch 6440 | Train Loss: 1.0950748920440674 | Test Loss: 1.12342369556427\n",
      "Epoch 6441 | Train Loss: 1.2383027076721191 | Test Loss: 1.3202768564224243\n",
      "Epoch 6442 | Train Loss: 1.0950262546539307 | Test Loss: 1.12332022190094\n",
      "Epoch 6443 | Train Loss: 1.238295316696167 | Test Loss: 1.320233941078186\n",
      "Epoch 6444 | Train Loss: 1.0949962139129639 | Test Loss: 1.1234146356582642\n",
      "Epoch 6445 | Train Loss: 1.2382575273513794 | Test Loss: 1.3203438520431519\n",
      "Epoch 6446 | Train Loss: 1.0949761867523193 | Test Loss: 1.1232787370681763\n",
      "Epoch 6447 | Train Loss: 1.2382346391677856 | Test Loss: 1.3201372623443604\n",
      "Epoch 6448 | Train Loss: 1.094960331916809 | Test Loss: 1.1233707666397095\n",
      "Epoch 6449 | Train Loss: 1.238186001777649 | Test Loss: 1.3203380107879639\n",
      "Epoch 6450 | Train Loss: 1.094916820526123 | Test Loss: 1.1232728958129883\n",
      "Epoch 6451 | Train Loss: 1.2381678819656372 | Test Loss: 1.3201271295547485\n",
      "Epoch 6452 | Train Loss: 1.0948747396469116 | Test Loss: 1.1233110427856445\n",
      "Epoch 6453 | Train Loss: 1.2381250858306885 | Test Loss: 1.3202303647994995\n",
      "Epoch 6454 | Train Loss: 1.0948412418365479 | Test Loss: 1.1232342720031738\n",
      "Epoch 6455 | Train Loss: 1.2380938529968262 | Test Loss: 1.3202595710754395\n",
      "Epoch 6456 | Train Loss: 1.0948326587677002 | Test Loss: 1.1232739686965942\n",
      "Epoch 6457 | Train Loss: 1.2380456924438477 | Test Loss: 1.3201557397842407\n",
      "Epoch 6458 | Train Loss: 1.0947970151901245 | Test Loss: 1.1231759786605835\n",
      "Epoch 6459 | Train Loss: 1.2380255460739136 | Test Loss: 1.3202052116394043\n",
      "Epoch 6460 | Train Loss: 1.0947563648223877 | Test Loss: 1.1232508420944214\n",
      "Epoch 6461 | Train Loss: 1.2379920482635498 | Test Loss: 1.320130705833435\n",
      "Epoch 6462 | Train Loss: 1.0947171449661255 | Test Loss: 1.123124122619629\n",
      "Epoch 6463 | Train Loss: 1.2379614114761353 | Test Loss: 1.3201103210449219\n",
      "Epoch 6464 | Train Loss: 1.0946944952011108 | Test Loss: 1.123202919960022\n",
      "Epoch 6465 | Train Loss: 1.2379130125045776 | Test Loss: 1.3201537132263184\n",
      "Epoch 6466 | Train Loss: 1.0946625471115112 | Test Loss: 1.1231073141098022\n",
      "Epoch 6467 | Train Loss: 1.2378867864608765 | Test Loss: 1.3200541734695435\n",
      "Epoch 6468 | Train Loss: 1.0946377515792847 | Test Loss: 1.1231225728988647\n",
      "Epoch 6469 | Train Loss: 1.23784339427948 | Test Loss: 1.320113182067871\n",
      "Epoch 6470 | Train Loss: 1.094596028327942 | Test Loss: 1.1230907440185547\n",
      "Epoch 6471 | Train Loss: 1.237817645072937 | Test Loss: 1.320067048072815\n",
      "Epoch 6472 | Train Loss: 1.0945727825164795 | Test Loss: 1.1230590343475342\n",
      "Epoch 6473 | Train Loss: 1.2377818822860718 | Test Loss: 1.320052981376648\n",
      "Epoch 6474 | Train Loss: 1.0945357084274292 | Test Loss: 1.1230442523956299\n",
      "Epoch 6475 | Train Loss: 1.2377599477767944 | Test Loss: 1.3200957775115967\n",
      "Epoch 6476 | Train Loss: 1.0945101976394653 | Test Loss: 1.1230425834655762\n",
      "Epoch 6477 | Train Loss: 1.2377172708511353 | Test Loss: 1.3200379610061646\n",
      "Epoch 6478 | Train Loss: 1.0944736003875732 | Test Loss: 1.122975468635559\n",
      "Epoch 6479 | Train Loss: 1.2376993894577026 | Test Loss: 1.3199946880340576\n",
      "Epoch 6480 | Train Loss: 1.0944476127624512 | Test Loss: 1.1230266094207764\n",
      "Epoch 6481 | Train Loss: 1.2376594543457031 | Test Loss: 1.3200880289077759\n",
      "Epoch 6482 | Train Loss: 1.0944483280181885 | Test Loss: 1.1229469776153564\n",
      "Epoch 6483 | Train Loss: 1.2376341819763184 | Test Loss: 1.319918155670166\n",
      "Epoch 6484 | Train Loss: 1.0944067239761353 | Test Loss: 1.1229968070983887\n",
      "Epoch 6485 | Train Loss: 1.2376059293746948 | Test Loss: 1.3200511932373047\n",
      "Epoch 6486 | Train Loss: 1.0943593978881836 | Test Loss: 1.1229486465454102\n",
      "Epoch 6487 | Train Loss: 1.237579345703125 | Test Loss: 1.3200085163116455\n",
      "Epoch 6488 | Train Loss: 1.0943478345870972 | Test Loss: 1.1229451894760132\n",
      "Epoch 6489 | Train Loss: 1.2375439405441284 | Test Loss: 1.3199560642242432\n",
      "Epoch 6490 | Train Loss: 1.0943477153778076 | Test Loss: 1.1228970289230347\n",
      "Epoch 6491 | Train Loss: 1.2375011444091797 | Test Loss: 1.3201076984405518\n",
      "Epoch 6492 | Train Loss: 1.094351887702942 | Test Loss: 1.1229020357131958\n",
      "Epoch 6493 | Train Loss: 1.2374660968780518 | Test Loss: 1.3198775053024292\n",
      "Epoch 6494 | Train Loss: 1.0942729711532593 | Test Loss: 1.122816801071167\n",
      "Epoch 6495 | Train Loss: 1.2374541759490967 | Test Loss: 1.3199784755706787\n",
      "Epoch 6496 | Train Loss: 1.0942206382751465 | Test Loss: 1.1228981018066406\n",
      "Epoch 6497 | Train Loss: 1.2374200820922852 | Test Loss: 1.3199310302734375\n",
      "Epoch 6498 | Train Loss: 1.0941777229309082 | Test Loss: 1.1227645874023438\n",
      "Epoch 6499 | Train Loss: 1.2374125719070435 | Test Loss: 1.319827914237976\n",
      "Epoch 6500 | Train Loss: 1.0941696166992188 | Test Loss: 1.1228581666946411\n",
      "Epoch 6501 | Train Loss: 1.2373607158660889 | Test Loss: 1.3200312852859497\n",
      "Epoch 6502 | Train Loss: 1.094163417816162 | Test Loss: 1.122735857963562\n",
      "Epoch 6503 | Train Loss: 1.2373285293579102 | Test Loss: 1.319787859916687\n",
      "Epoch 6504 | Train Loss: 1.094122290611267 | Test Loss: 1.1228127479553223\n",
      "Epoch 6505 | Train Loss: 1.2372877597808838 | Test Loss: 1.3199785947799683\n",
      "Epoch 6506 | Train Loss: 1.094082236289978 | Test Loss: 1.122717261314392\n",
      "Epoch 6507 | Train Loss: 1.2372633218765259 | Test Loss: 1.3197968006134033\n",
      "Epoch 6508 | Train Loss: 1.0940543413162231 | Test Loss: 1.1227912902832031\n",
      "Epoch 6509 | Train Loss: 1.2372264862060547 | Test Loss: 1.319884181022644\n",
      "Epoch 6510 | Train Loss: 1.0940093994140625 | Test Loss: 1.1226811408996582\n",
      "Epoch 6511 | Train Loss: 1.2372058629989624 | Test Loss: 1.3198120594024658\n",
      "Epoch 6512 | Train Loss: 1.093983769416809 | Test Loss: 1.1227627992630005\n",
      "Epoch 6513 | Train Loss: 1.2371646165847778 | Test Loss: 1.3197846412658691\n",
      "Epoch 6514 | Train Loss: 1.0939539670944214 | Test Loss: 1.12263023853302\n",
      "Epoch 6515 | Train Loss: 1.2371405363082886 | Test Loss: 1.3197908401489258\n",
      "Epoch 6516 | Train Loss: 1.0939253568649292 | Test Loss: 1.1227188110351562\n",
      "Epoch 6517 | Train Loss: 1.2370983362197876 | Test Loss: 1.3197733163833618\n",
      "Epoch 6518 | Train Loss: 1.0938904285430908 | Test Loss: 1.122596025466919\n",
      "Epoch 6519 | Train Loss: 1.2370737791061401 | Test Loss: 1.319727897644043\n",
      "Epoch 6520 | Train Loss: 1.093870759010315 | Test Loss: 1.1226726770401\n",
      "Epoch 6521 | Train Loss: 1.2370314598083496 | Test Loss: 1.319758653640747\n",
      "Epoch 6522 | Train Loss: 1.0938342809677124 | Test Loss: 1.122586965560913\n",
      "Epoch 6523 | Train Loss: 1.2370083332061768 | Test Loss: 1.3197119235992432\n",
      "Epoch 6524 | Train Loss: 1.0938053131103516 | Test Loss: 1.1226047277450562\n",
      "Epoch 6525 | Train Loss: 1.2369608879089355 | Test Loss: 1.3196794986724854\n",
      "Epoch 6526 | Train Loss: 1.0937801599502563 | Test Loss: 1.1225628852844238\n",
      "Epoch 6527 | Train Loss: 1.2369391918182373 | Test Loss: 1.3197225332260132\n",
      "Epoch 6528 | Train Loss: 1.0937451124191284 | Test Loss: 1.1225417852401733\n",
      "Epoch 6529 | Train Loss: 1.2369005680084229 | Test Loss: 1.3196604251861572\n",
      "Epoch 6530 | Train Loss: 1.0937191247940063 | Test Loss: 1.1225063800811768\n",
      "Epoch 6531 | Train Loss: 1.2368779182434082 | Test Loss: 1.3197165727615356\n",
      "Epoch 6532 | Train Loss: 1.0936843156814575 | Test Loss: 1.1225051879882812\n",
      "Epoch 6533 | Train Loss: 1.2368355989456177 | Test Loss: 1.3196909427642822\n",
      "Epoch 6534 | Train Loss: 1.093667984008789 | Test Loss: 1.1224387884140015\n",
      "Epoch 6535 | Train Loss: 1.2368186712265015 | Test Loss: 1.3196439743041992\n",
      "Epoch 6536 | Train Loss: 1.093633770942688 | Test Loss: 1.122493863105774\n",
      "Epoch 6537 | Train Loss: 1.2367862462997437 | Test Loss: 1.3196804523468018\n",
      "Epoch 6538 | Train Loss: 1.0936180353164673 | Test Loss: 1.1224106550216675\n",
      "Epoch 6539 | Train Loss: 1.2367583513259888 | Test Loss: 1.319600224494934\n",
      "Epoch 6540 | Train Loss: 1.0935726165771484 | Test Loss: 1.1224486827850342\n",
      "Epoch 6541 | Train Loss: 1.2367304563522339 | Test Loss: 1.319612979888916\n",
      "Epoch 6542 | Train Loss: 1.0935457944869995 | Test Loss: 1.1224029064178467\n",
      "Epoch 6543 | Train Loss: 1.2366929054260254 | Test Loss: 1.3196957111358643\n",
      "Epoch 6544 | Train Loss: 1.0935301780700684 | Test Loss: 1.1223887205123901\n",
      "Epoch 6545 | Train Loss: 1.2366650104522705 | Test Loss: 1.319516658782959\n",
      "Epoch 6546 | Train Loss: 1.0935163497924805 | Test Loss: 1.1223794221878052\n",
      "Epoch 6547 | Train Loss: 1.236634373664856 | Test Loss: 1.3197319507598877\n",
      "Epoch 6548 | Train Loss: 1.0934855937957764 | Test Loss: 1.1223492622375488\n",
      "Epoch 6549 | Train Loss: 1.2365989685058594 | Test Loss: 1.319510817527771\n",
      "Epoch 6550 | Train Loss: 1.0934377908706665 | Test Loss: 1.1223249435424805\n",
      "Epoch 6551 | Train Loss: 1.2365849018096924 | Test Loss: 1.3196130990982056\n",
      "Epoch 6552 | Train Loss: 1.0933994054794312 | Test Loss: 1.1223535537719727\n",
      "Epoch 6553 | Train Loss: 1.23654043674469 | Test Loss: 1.319596767425537\n",
      "Epoch 6554 | Train Loss: 1.0933855772018433 | Test Loss: 1.122253179550171\n",
      "Epoch 6555 | Train Loss: 1.236517310142517 | Test Loss: 1.3195011615753174\n",
      "Epoch 6556 | Train Loss: 1.093354344367981 | Test Loss: 1.1223152875900269\n",
      "Epoch 6557 | Train Loss: 1.2364848852157593 | Test Loss: 1.3196282386779785\n",
      "Epoch 6558 | Train Loss: 1.0933473110198975 | Test Loss: 1.1222161054611206\n",
      "Epoch 6559 | Train Loss: 1.2364490032196045 | Test Loss: 1.31948721408844\n",
      "Epoch 6560 | Train Loss: 1.0933014154434204 | Test Loss: 1.1222587823867798\n",
      "Epoch 6561 | Train Loss: 1.2364333868026733 | Test Loss: 1.3195303678512573\n",
      "Epoch 6562 | Train Loss: 1.0932505130767822 | Test Loss: 1.1222132444381714\n",
      "Epoch 6563 | Train Loss: 1.2364143133163452 | Test Loss: 1.319571614265442\n",
      "Epoch 6564 | Train Loss: 1.093228816986084 | Test Loss: 1.1222102642059326\n",
      "Epoch 6565 | Train Loss: 1.2363746166229248 | Test Loss: 1.3193964958190918\n",
      "Epoch 6566 | Train Loss: 1.0932095050811768 | Test Loss: 1.1221747398376465\n",
      "Epoch 6567 | Train Loss: 1.2363437414169312 | Test Loss: 1.3196183443069458\n",
      "Epoch 6568 | Train Loss: 1.093183159828186 | Test Loss: 1.1221685409545898\n",
      "Epoch 6569 | Train Loss: 1.236307144165039 | Test Loss: 1.3193989992141724\n",
      "Epoch 6570 | Train Loss: 1.0931453704833984 | Test Loss: 1.1221377849578857\n",
      "Epoch 6571 | Train Loss: 1.2362895011901855 | Test Loss: 1.3195148706436157\n",
      "Epoch 6572 | Train Loss: 1.0931041240692139 | Test Loss: 1.1221691370010376\n",
      "Epoch 6573 | Train Loss: 1.2362488508224487 | Test Loss: 1.3194389343261719\n",
      "Epoch 6574 | Train Loss: 1.0930835008621216 | Test Loss: 1.122068166732788\n",
      "Epoch 6575 | Train Loss: 1.236233115196228 | Test Loss: 1.3193811178207397\n",
      "Epoch 6576 | Train Loss: 1.0930564403533936 | Test Loss: 1.1221387386322021\n",
      "Epoch 6577 | Train Loss: 1.236196517944336 | Test Loss: 1.319509506225586\n",
      "Epoch 6578 | Train Loss: 1.0930671691894531 | Test Loss: 1.1220258474349976\n",
      "Epoch 6579 | Train Loss: 1.2361749410629272 | Test Loss: 1.3193671703338623\n",
      "Epoch 6580 | Train Loss: 1.0930099487304688 | Test Loss: 1.1221026182174683\n",
      "Epoch 6581 | Train Loss: 1.236152172088623 | Test Loss: 1.3194085359573364\n",
      "Epoch 6582 | Train Loss: 1.0929678678512573 | Test Loss: 1.1220219135284424\n",
      "Epoch 6583 | Train Loss: 1.236121416091919 | Test Loss: 1.3194596767425537\n",
      "Epoch 6584 | Train Loss: 1.0929385423660278 | Test Loss: 1.1220382452011108\n",
      "Epoch 6585 | Train Loss: 1.2360879182815552 | Test Loss: 1.3192412853240967\n",
      "Epoch 6586 | Train Loss: 1.0929266214370728 | Test Loss: 1.1219903230667114\n",
      "Epoch 6587 | Train Loss: 1.2360528707504272 | Test Loss: 1.3195149898529053\n",
      "Epoch 6588 | Train Loss: 1.0929043292999268 | Test Loss: 1.1219888925552368\n",
      "Epoch 6589 | Train Loss: 1.236014485359192 | Test Loss: 1.3192706108093262\n",
      "Epoch 6590 | Train Loss: 1.0928616523742676 | Test Loss: 1.1219514608383179\n",
      "Epoch 6591 | Train Loss: 1.236000657081604 | Test Loss: 1.3194266557693481\n",
      "Epoch 6592 | Train Loss: 1.0928168296813965 | Test Loss: 1.121972918510437\n",
      "Epoch 6593 | Train Loss: 1.2359548807144165 | Test Loss: 1.319305181503296\n",
      "Epoch 6594 | Train Loss: 1.0928092002868652 | Test Loss: 1.121903419494629\n",
      "Epoch 6595 | Train Loss: 1.2359275817871094 | Test Loss: 1.3193268775939941\n",
      "Epoch 6596 | Train Loss: 1.0927627086639404 | Test Loss: 1.1219308376312256\n",
      "Epoch 6597 | Train Loss: 1.2359018325805664 | Test Loss: 1.319329023361206\n",
      "Epoch 6598 | Train Loss: 1.09276282787323 | Test Loss: 1.121880054473877\n",
      "Epoch 6599 | Train Loss: 1.2358605861663818 | Test Loss: 1.3193124532699585\n",
      "Epoch 6600 | Train Loss: 1.0927093029022217 | Test Loss: 1.1218652725219727\n",
      "Epoch 6601 | Train Loss: 1.2358583211898804 | Test Loss: 1.3192096948623657\n",
      "Epoch 6602 | Train Loss: 1.0926902294158936 | Test Loss: 1.1218806505203247\n",
      "Epoch 6603 | Train Loss: 1.235827922821045 | Test Loss: 1.3194248676300049\n",
      "Epoch 6604 | Train Loss: 1.0926750898361206 | Test Loss: 1.1218054294586182\n",
      "Epoch 6605 | Train Loss: 1.2358161211013794 | Test Loss: 1.3191131353378296\n",
      "Epoch 6606 | Train Loss: 1.0926679372787476 | Test Loss: 1.121904730796814\n",
      "Epoch 6607 | Train Loss: 1.2358026504516602 | Test Loss: 1.3194615840911865\n",
      "Epoch 6608 | Train Loss: 1.0926263332366943 | Test Loss: 1.1217764616012573\n",
      "Epoch 6609 | Train Loss: 1.2357903718948364 | Test Loss: 1.319108247756958\n",
      "Epoch 6610 | Train Loss: 1.092624545097351 | Test Loss: 1.1218959093093872\n",
      "Epoch 6611 | Train Loss: 1.2357608079910278 | Test Loss: 1.3193888664245605\n",
      "Epoch 6612 | Train Loss: 1.0925602912902832 | Test Loss: 1.1217577457427979\n",
      "Epoch 6613 | Train Loss: 1.2357642650604248 | Test Loss: 1.3191202878952026\n",
      "Epoch 6614 | Train Loss: 1.0925545692443848 | Test Loss: 1.1218736171722412\n",
      "Epoch 6615 | Train Loss: 1.2357295751571655 | Test Loss: 1.319303274154663\n",
      "Epoch 6616 | Train Loss: 1.0924928188323975 | Test Loss: 1.121739149093628\n",
      "Epoch 6617 | Train Loss: 1.235734462738037 | Test Loss: 1.3191405534744263\n",
      "Epoch 6618 | Train Loss: 1.0924807786941528 | Test Loss: 1.1218596696853638\n",
      "Epoch 6619 | Train Loss: 1.2356961965560913 | Test Loss: 1.3192354440689087\n",
      "Epoch 6620 | Train Loss: 1.092419147491455 | Test Loss: 1.1217103004455566\n",
      "Epoch 6621 | Train Loss: 1.2357032299041748 | Test Loss: 1.319105625152588\n",
      "Epoch 6622 | Train Loss: 1.092406153678894 | Test Loss: 1.1218304634094238\n",
      "Epoch 6623 | Train Loss: 1.2356330156326294 | Test Loss: 1.3192070722579956\n",
      "Epoch 6624 | Train Loss: 1.0923535823822021 | Test Loss: 1.1216660737991333\n",
      "Epoch 6625 | Train Loss: 1.2356280088424683 | Test Loss: 1.3190765380859375\n",
      "Epoch 6626 | Train Loss: 1.0923336744308472 | Test Loss: 1.1217671632766724\n",
      "Epoch 6627 | Train Loss: 1.2355284690856934 | Test Loss: 1.3191672563552856\n",
      "Epoch 6628 | Train Loss: 1.092309832572937 | Test Loss: 1.121626615524292\n",
      "Epoch 6629 | Train Loss: 1.2355173826217651 | Test Loss: 1.3190652132034302\n",
      "Epoch 6630 | Train Loss: 1.0922757387161255 | Test Loss: 1.1216715574264526\n",
      "Epoch 6631 | Train Loss: 1.23543119430542 | Test Loss: 1.3190460205078125\n",
      "Epoch 6632 | Train Loss: 1.09226393699646 | Test Loss: 1.121586561203003\n",
      "Epoch 6633 | Train Loss: 1.2353943586349487 | Test Loss: 1.3191226720809937\n",
      "Epoch 6634 | Train Loss: 1.0922280550003052 | Test Loss: 1.12157142162323\n",
      "Epoch 6635 | Train Loss: 1.2353330850601196 | Test Loss: 1.3189351558685303\n",
      "Epoch 6636 | Train Loss: 1.0922218561172485 | Test Loss: 1.1215605735778809\n",
      "Epoch 6637 | Train Loss: 1.2352949380874634 | Test Loss: 1.319197416305542\n",
      "Epoch 6638 | Train Loss: 1.0921956300735474 | Test Loss: 1.1215060949325562\n",
      "Epoch 6639 | Train Loss: 1.2352474927902222 | Test Loss: 1.3189157247543335\n",
      "Epoch 6640 | Train Loss: 1.0921781063079834 | Test Loss: 1.121533989906311\n",
      "Epoch 6641 | Train Loss: 1.2352205514907837 | Test Loss: 1.3191051483154297\n",
      "Epoch 6642 | Train Loss: 1.092130184173584 | Test Loss: 1.1214709281921387\n",
      "Epoch 6643 | Train Loss: 1.2351889610290527 | Test Loss: 1.3189260959625244\n",
      "Epoch 6644 | Train Loss: 1.0921072959899902 | Test Loss: 1.1214969158172607\n",
      "Epoch 6645 | Train Loss: 1.2351608276367188 | Test Loss: 1.318999171257019\n",
      "Epoch 6646 | Train Loss: 1.0920599699020386 | Test Loss: 1.1214247941970825\n",
      "Epoch 6647 | Train Loss: 1.2351411581039429 | Test Loss: 1.3189016580581665\n",
      "Epoch 6648 | Train Loss: 1.0920504331588745 | Test Loss: 1.121462345123291\n",
      "Epoch 6649 | Train Loss: 1.2351019382476807 | Test Loss: 1.318993330001831\n",
      "Epoch 6650 | Train Loss: 1.0920127630233765 | Test Loss: 1.1213867664337158\n",
      "Epoch 6651 | Train Loss: 1.2350887060165405 | Test Loss: 1.3188879489898682\n",
      "Epoch 6652 | Train Loss: 1.0919946432113647 | Test Loss: 1.1214268207550049\n",
      "Epoch 6653 | Train Loss: 1.2350447177886963 | Test Loss: 1.318993330001831\n",
      "Epoch 6654 | Train Loss: 1.0919597148895264 | Test Loss: 1.1213854551315308\n",
      "Epoch 6655 | Train Loss: 1.235033631324768 | Test Loss: 1.318914771080017\n",
      "Epoch 6656 | Train Loss: 1.0919362306594849 | Test Loss: 1.1213833093643188\n",
      "Epoch 6657 | Train Loss: 1.2349885702133179 | Test Loss: 1.3189091682434082\n",
      "Epoch 6658 | Train Loss: 1.0919065475463867 | Test Loss: 1.1213765144348145\n",
      "Epoch 6659 | Train Loss: 1.2349704504013062 | Test Loss: 1.318971037864685\n",
      "Epoch 6660 | Train Loss: 1.0919020175933838 | Test Loss: 1.121336817741394\n",
      "Epoch 6661 | Train Loss: 1.234920859336853 | Test Loss: 1.318850040435791\n",
      "Epoch 6662 | Train Loss: 1.0918668508529663 | Test Loss: 1.1213161945343018\n",
      "Epoch 6663 | Train Loss: 1.2349097728729248 | Test Loss: 1.31890869140625\n",
      "Epoch 6664 | Train Loss: 1.0918307304382324 | Test Loss: 1.121310830116272\n",
      "Epoch 6665 | Train Loss: 1.2348692417144775 | Test Loss: 1.3188897371292114\n",
      "Epoch 6666 | Train Loss: 1.0918010473251343 | Test Loss: 1.1212612390518188\n",
      "Epoch 6667 | Train Loss: 1.2348573207855225 | Test Loss: 1.3187896013259888\n",
      "Epoch 6668 | Train Loss: 1.0917856693267822 | Test Loss: 1.1212964057922363\n",
      "Epoch 6669 | Train Loss: 1.2348113059997559 | Test Loss: 1.3189549446105957\n",
      "Epoch 6670 | Train Loss: 1.0917783975601196 | Test Loss: 1.1212315559387207\n",
      "Epoch 6671 | Train Loss: 1.2347936630249023 | Test Loss: 1.3187648057937622\n",
      "Epoch 6672 | Train Loss: 1.0917441844940186 | Test Loss: 1.1212459802627563\n",
      "Epoch 6673 | Train Loss: 1.2347631454467773 | Test Loss: 1.3188660144805908\n",
      "Epoch 6674 | Train Loss: 1.0917006731033325 | Test Loss: 1.12122642993927\n",
      "Epoch 6675 | Train Loss: 1.234731674194336 | Test Loss: 1.3188490867614746\n",
      "Epoch 6676 | Train Loss: 1.0916788578033447 | Test Loss: 1.1211804151535034\n",
      "Epoch 6677 | Train Loss: 1.2346980571746826 | Test Loss: 1.318748116493225\n",
      "Epoch 6678 | Train Loss: 1.0916768312454224 | Test Loss: 1.1212108135223389\n",
      "Epoch 6679 | Train Loss: 1.234660267829895 | Test Loss: 1.318942904472351\n",
      "Epoch 6680 | Train Loss: 1.0916799306869507 | Test Loss: 1.121132493019104\n",
      "Epoch 6681 | Train Loss: 1.2346221208572388 | Test Loss: 1.3187159299850464\n",
      "Epoch 6682 | Train Loss: 1.0916290283203125 | Test Loss: 1.1211698055267334\n",
      "Epoch 6683 | Train Loss: 1.234602451324463 | Test Loss: 1.3188899755477905\n",
      "Epoch 6684 | Train Loss: 1.0915944576263428 | Test Loss: 1.1211249828338623\n",
      "Epoch 6685 | Train Loss: 1.234567403793335 | Test Loss: 1.3187493085861206\n",
      "Epoch 6686 | Train Loss: 1.0915547609329224 | Test Loss: 1.121130347251892\n",
      "Epoch 6687 | Train Loss: 1.2345515489578247 | Test Loss: 1.3187255859375\n",
      "Epoch 6688 | Train Loss: 1.091513991355896 | Test Loss: 1.1211163997650146\n",
      "Epoch 6689 | Train Loss: 1.23451828956604 | Test Loss: 1.3188356161117554\n",
      "Epoch 6690 | Train Loss: 1.0915148258209229 | Test Loss: 1.1210609674453735\n",
      "Epoch 6691 | Train Loss: 1.234481930732727 | Test Loss: 1.3186537027359009\n",
      "Epoch 6692 | Train Loss: 1.0914926528930664 | Test Loss: 1.121091604232788\n",
      "Epoch 6693 | Train Loss: 1.2344574928283691 | Test Loss: 1.3188296556472778\n",
      "Epoch 6694 | Train Loss: 1.091459035873413 | Test Loss: 1.121039867401123\n",
      "Epoch 6695 | Train Loss: 1.2344255447387695 | Test Loss: 1.3186622858047485\n",
      "Epoch 6696 | Train Loss: 1.0914260149002075 | Test Loss: 1.1210383176803589\n",
      "Epoch 6697 | Train Loss: 1.2344011068344116 | Test Loss: 1.3187525272369385\n",
      "Epoch 6698 | Train Loss: 1.0913918018341064 | Test Loss: 1.1210333108901978\n",
      "Epoch 6699 | Train Loss: 1.2343690395355225 | Test Loss: 1.318697452545166\n",
      "Epoch 6700 | Train Loss: 1.0913708209991455 | Test Loss: 1.1209743022918701\n",
      "Epoch 6701 | Train Loss: 1.2343432903289795 | Test Loss: 1.318661093711853\n",
      "Epoch 6702 | Train Loss: 1.0913445949554443 | Test Loss: 1.1210047006607056\n",
      "Epoch 6703 | Train Loss: 1.2343140840530396 | Test Loss: 1.318756341934204\n",
      "Epoch 6704 | Train Loss: 1.0913312435150146 | Test Loss: 1.1209298372268677\n",
      "Epoch 6705 | Train Loss: 1.2342848777770996 | Test Loss: 1.3186603784561157\n",
      "Epoch 6706 | Train Loss: 1.0912888050079346 | Test Loss: 1.120970606803894\n",
      "Epoch 6707 | Train Loss: 1.234270691871643 | Test Loss: 1.318643569946289\n",
      "Epoch 6708 | Train Loss: 1.0912621021270752 | Test Loss: 1.120930790901184\n",
      "Epoch 6709 | Train Loss: 1.2342314720153809 | Test Loss: 1.318745493888855\n",
      "Epoch 6710 | Train Loss: 1.0912483930587769 | Test Loss: 1.1209027767181396\n",
      "Epoch 6711 | Train Loss: 1.2342129945755005 | Test Loss: 1.3185498714447021\n",
      "Epoch 6712 | Train Loss: 1.091226577758789 | Test Loss: 1.1209198236465454\n",
      "Epoch 6713 | Train Loss: 1.2341859340667725 | Test Loss: 1.3187865018844604\n",
      "Epoch 6714 | Train Loss: 1.0912102460861206 | Test Loss: 1.1208972930908203\n",
      "Epoch 6715 | Train Loss: 1.234151840209961 | Test Loss: 1.3185657262802124\n",
      "Epoch 6716 | Train Loss: 1.0911749601364136 | Test Loss: 1.1208715438842773\n",
      "Epoch 6717 | Train Loss: 1.2341336011886597 | Test Loss: 1.318689227104187\n",
      "Epoch 6718 | Train Loss: 1.0911356210708618 | Test Loss: 1.120904564857483\n",
      "Epoch 6719 | Train Loss: 1.2340974807739258 | Test Loss: 1.3186347484588623\n",
      "Epoch 6720 | Train Loss: 1.0911309719085693 | Test Loss: 1.1208124160766602\n",
      "Epoch 6721 | Train Loss: 1.2340651750564575 | Test Loss: 1.3185659646987915\n",
      "Epoch 6722 | Train Loss: 1.091103196144104 | Test Loss: 1.1208679676055908\n",
      "Epoch 6723 | Train Loss: 1.2340404987335205 | Test Loss: 1.3186579942703247\n",
      "Epoch 6724 | Train Loss: 1.0910831689834595 | Test Loss: 1.1207910776138306\n",
      "Epoch 6725 | Train Loss: 1.234010100364685 | Test Loss: 1.3185577392578125\n",
      "Epoch 6726 | Train Loss: 1.0910398960113525 | Test Loss: 1.120803713798523\n",
      "Epoch 6727 | Train Loss: 1.2339890003204346 | Test Loss: 1.3185735940933228\n",
      "Epoch 6728 | Train Loss: 1.0910078287124634 | Test Loss: 1.1207826137542725\n",
      "Epoch 6729 | Train Loss: 1.233960509300232 | Test Loss: 1.3186116218566895\n",
      "Epoch 6730 | Train Loss: 1.0909788608551025 | Test Loss: 1.1207510232925415\n",
      "Epoch 6731 | Train Loss: 1.233938455581665 | Test Loss: 1.3184731006622314\n",
      "Epoch 6732 | Train Loss: 1.090962290763855 | Test Loss: 1.1207572221755981\n",
      "Epoch 6733 | Train Loss: 1.2339059114456177 | Test Loss: 1.31864333152771\n",
      "Epoch 6734 | Train Loss: 1.0909444093704224 | Test Loss: 1.1207464933395386\n",
      "Epoch 6735 | Train Loss: 1.2338688373565674 | Test Loss: 1.3184623718261719\n",
      "Epoch 6736 | Train Loss: 1.0909159183502197 | Test Loss: 1.1206941604614258\n",
      "Epoch 6737 | Train Loss: 1.2338422536849976 | Test Loss: 1.3185428380966187\n",
      "Epoch 6738 | Train Loss: 1.0908716917037964 | Test Loss: 1.120736837387085\n",
      "Epoch 6739 | Train Loss: 1.2338099479675293 | Test Loss: 1.3185361623764038\n",
      "Epoch 6740 | Train Loss: 1.09086275100708 | Test Loss: 1.1206413507461548\n",
      "Epoch 6741 | Train Loss: 1.2337795495986938 | Test Loss: 1.3184655904769897\n",
      "Epoch 6742 | Train Loss: 1.0908297300338745 | Test Loss: 1.1206936836242676\n",
      "Epoch 6743 | Train Loss: 1.233747959136963 | Test Loss: 1.3185710906982422\n",
      "Epoch 6744 | Train Loss: 1.0908164978027344 | Test Loss: 1.1206283569335938\n",
      "Epoch 6745 | Train Loss: 1.233709454536438 | Test Loss: 1.318458914756775\n",
      "Epoch 6746 | Train Loss: 1.0907806158065796 | Test Loss: 1.120622992515564\n",
      "Epoch 6747 | Train Loss: 1.2336843013763428 | Test Loss: 1.3184731006622314\n",
      "Epoch 6748 | Train Loss: 1.0907484292984009 | Test Loss: 1.1206132173538208\n",
      "Epoch 6749 | Train Loss: 1.2336547374725342 | Test Loss: 1.3185168504714966\n",
      "Epoch 6750 | Train Loss: 1.090734839439392 | Test Loss: 1.1205723285675049\n",
      "Epoch 6751 | Train Loss: 1.2336281538009644 | Test Loss: 1.3183581829071045\n",
      "Epoch 6752 | Train Loss: 1.090720534324646 | Test Loss: 1.1205847263336182\n",
      "Epoch 6753 | Train Loss: 1.2335968017578125 | Test Loss: 1.3185473680496216\n",
      "Epoch 6754 | Train Loss: 1.0906949043273926 | Test Loss: 1.120539665222168\n",
      "Epoch 6755 | Train Loss: 1.2335643768310547 | Test Loss: 1.318375825881958\n",
      "Epoch 6756 | Train Loss: 1.0906580686569214 | Test Loss: 1.1205400228500366\n",
      "Epoch 6757 | Train Loss: 1.2335423231124878 | Test Loss: 1.318441390991211\n",
      "Epoch 6758 | Train Loss: 1.0906164646148682 | Test Loss: 1.1205438375473022\n",
      "Epoch 6759 | Train Loss: 1.23350989818573 | Test Loss: 1.318426489830017\n",
      "Epoch 6760 | Train Loss: 1.0906087160110474 | Test Loss: 1.1204755306243896\n",
      "Epoch 6761 | Train Loss: 1.2334787845611572 | Test Loss: 1.3183397054672241\n",
      "Epoch 6762 | Train Loss: 1.0905736684799194 | Test Loss: 1.1205326318740845\n",
      "Epoch 6763 | Train Loss: 1.233465552330017 | Test Loss: 1.3184831142425537\n",
      "Epoch 6764 | Train Loss: 1.0905691385269165 | Test Loss: 1.1204500198364258\n",
      "Epoch 6765 | Train Loss: 1.2334229946136475 | Test Loss: 1.3183659315109253\n",
      "Epoch 6766 | Train Loss: 1.0905306339263916 | Test Loss: 1.120490550994873\n",
      "Epoch 6767 | Train Loss: 1.2334071397781372 | Test Loss: 1.3184038400650024\n",
      "Epoch 6768 | Train Loss: 1.0904890298843384 | Test Loss: 1.120447039604187\n",
      "Epoch 6769 | Train Loss: 1.233376383781433 | Test Loss: 1.3184317350387573\n",
      "Epoch 6770 | Train Loss: 1.090474009513855 | Test Loss: 1.1204309463500977\n",
      "Epoch 6771 | Train Loss: 1.2333502769470215 | Test Loss: 1.318274974822998\n",
      "Epoch 6772 | Train Loss: 1.090452790260315 | Test Loss: 1.1204502582550049\n",
      "Epoch 6773 | Train Loss: 1.2333256006240845 | Test Loss: 1.3185219764709473\n",
      "Epoch 6774 | Train Loss: 1.0904511213302612 | Test Loss: 1.1203829050064087\n",
      "Epoch 6775 | Train Loss: 1.2333027124404907 | Test Loss: 1.3182659149169922\n",
      "Epoch 6776 | Train Loss: 1.0904227495193481 | Test Loss: 1.1204270124435425\n",
      "Epoch 6777 | Train Loss: 1.2332779169082642 | Test Loss: 1.3184514045715332\n",
      "Epoch 6778 | Train Loss: 1.0903818607330322 | Test Loss: 1.1203930377960205\n",
      "Epoch 6779 | Train Loss: 1.2332501411437988 | Test Loss: 1.3183016777038574\n",
      "Epoch 6780 | Train Loss: 1.0903452634811401 | Test Loss: 1.1203601360321045\n",
      "Epoch 6781 | Train Loss: 1.2332240343093872 | Test Loss: 1.3183387517929077\n",
      "Epoch 6782 | Train Loss: 1.0903077125549316 | Test Loss: 1.1203722953796387\n",
      "Epoch 6783 | Train Loss: 1.2331911325454712 | Test Loss: 1.3183377981185913\n",
      "Epoch 6784 | Train Loss: 1.0902915000915527 | Test Loss: 1.1203112602233887\n",
      "Epoch 6785 | Train Loss: 1.2331639528274536 | Test Loss: 1.3182843923568726\n",
      "Epoch 6786 | Train Loss: 1.0902588367462158 | Test Loss: 1.1203453540802002\n",
      "Epoch 6787 | Train Loss: 1.2331410646438599 | Test Loss: 1.3183619976043701\n",
      "Epoch 6788 | Train Loss: 1.090252161026001 | Test Loss: 1.1202987432479858\n",
      "Epoch 6789 | Train Loss: 1.2331113815307617 | Test Loss: 1.3183132410049438\n",
      "Epoch 6790 | Train Loss: 1.0902178287506104 | Test Loss: 1.1203157901763916\n",
      "Epoch 6791 | Train Loss: 1.2330937385559082 | Test Loss: 1.3182624578475952\n",
      "Epoch 6792 | Train Loss: 1.0901893377304077 | Test Loss: 1.120300054550171\n",
      "Epoch 6793 | Train Loss: 1.233068585395813 | Test Loss: 1.318387508392334\n",
      "Epoch 6794 | Train Loss: 1.0901650190353394 | Test Loss: 1.1202763319015503\n",
      "Epoch 6795 | Train Loss: 1.2330443859100342 | Test Loss: 1.3181533813476562\n",
      "Epoch 6796 | Train Loss: 1.0901561975479126 | Test Loss: 1.1202670335769653\n",
      "Epoch 6797 | Train Loss: 1.2330037355422974 | Test Loss: 1.318408489227295\n",
      "Epoch 6798 | Train Loss: 1.0901304483413696 | Test Loss: 1.1202491521835327\n",
      "Epoch 6799 | Train Loss: 1.232974886894226 | Test Loss: 1.3181798458099365\n",
      "Epoch 6800 | Train Loss: 1.0900943279266357 | Test Loss: 1.1202157735824585\n",
      "Epoch 6801 | Train Loss: 1.232957363128662 | Test Loss: 1.3183038234710693\n",
      "Epoch 6802 | Train Loss: 1.0900553464889526 | Test Loss: 1.1202552318572998\n",
      "Epoch 6803 | Train Loss: 1.2329211235046387 | Test Loss: 1.3182480335235596\n",
      "Epoch 6804 | Train Loss: 1.0900384187698364 | Test Loss: 1.1201605796813965\n",
      "Epoch 6805 | Train Loss: 1.2328870296478271 | Test Loss: 1.3181912899017334\n",
      "Epoch 6806 | Train Loss: 1.0900100469589233 | Test Loss: 1.1201988458633423\n",
      "Epoch 6807 | Train Loss: 1.232849359512329 | Test Loss: 1.3182774782180786\n",
      "Epoch 6808 | Train Loss: 1.0899949073791504 | Test Loss: 1.1201331615447998\n",
      "Epoch 6809 | Train Loss: 1.2328227758407593 | Test Loss: 1.3181779384613037\n",
      "Epoch 6810 | Train Loss: 1.0899457931518555 | Test Loss: 1.1201695203781128\n",
      "Epoch 6811 | Train Loss: 1.232804298400879 | Test Loss: 1.3182287216186523\n",
      "Epoch 6812 | Train Loss: 1.0899237394332886 | Test Loss: 1.1201128959655762\n",
      "Epoch 6813 | Train Loss: 1.2327651977539062 | Test Loss: 1.3182398080825806\n",
      "Epoch 6814 | Train Loss: 1.0898964405059814 | Test Loss: 1.1201281547546387\n",
      "Epoch 6815 | Train Loss: 1.2327399253845215 | Test Loss: 1.318139910697937\n",
      "Epoch 6816 | Train Loss: 1.089868187904358 | Test Loss: 1.1200810670852661\n",
      "Epoch 6817 | Train Loss: 1.2327044010162354 | Test Loss: 1.3182545900344849\n",
      "Epoch 6818 | Train Loss: 1.0898576974868774 | Test Loss: 1.120082974433899\n",
      "Epoch 6819 | Train Loss: 1.232672095298767 | Test Loss: 1.3180838823318481\n",
      "Epoch 6820 | Train Loss: 1.089830756187439 | Test Loss: 1.1200453042984009\n",
      "Epoch 6821 | Train Loss: 1.2326478958129883 | Test Loss: 1.318196415901184\n",
      "Epoch 6822 | Train Loss: 1.0898025035858154 | Test Loss: 1.120059847831726\n",
      "Epoch 6823 | Train Loss: 1.232611060142517 | Test Loss: 1.3181495666503906\n",
      "Epoch 6824 | Train Loss: 1.0897852182388306 | Test Loss: 1.1200217008590698\n",
      "Epoch 6825 | Train Loss: 1.2325818538665771 | Test Loss: 1.3181177377700806\n",
      "Epoch 6826 | Train Loss: 1.0897552967071533 | Test Loss: 1.1200181245803833\n",
      "Epoch 6827 | Train Loss: 1.2325525283813477 | Test Loss: 1.3181228637695312\n",
      "Epoch 6828 | Train Loss: 1.0897279977798462 | Test Loss: 1.1199994087219238\n",
      "Epoch 6829 | Train Loss: 1.2325279712677002 | Test Loss: 1.318135142326355\n",
      "Epoch 6830 | Train Loss: 1.0896997451782227 | Test Loss: 1.119971752166748\n",
      "Epoch 6831 | Train Loss: 1.232495665550232 | Test Loss: 1.3180992603302002\n",
      "Epoch 6832 | Train Loss: 1.0896798372268677 | Test Loss: 1.119964838027954\n",
      "Epoch 6833 | Train Loss: 1.2324647903442383 | Test Loss: 1.3181246519088745\n",
      "Epoch 6834 | Train Loss: 1.0896520614624023 | Test Loss: 1.1199567317962646\n",
      "Epoch 6835 | Train Loss: 1.232437252998352 | Test Loss: 1.3180855512619019\n",
      "Epoch 6836 | Train Loss: 1.0896245241165161 | Test Loss: 1.119929552078247\n",
      "Epoch 6837 | Train Loss: 1.2324146032333374 | Test Loss: 1.3180707693099976\n",
      "Epoch 6838 | Train Loss: 1.0895962715148926 | Test Loss: 1.1199427843093872\n",
      "Epoch 6839 | Train Loss: 1.2323824167251587 | Test Loss: 1.3181036710739136\n",
      "Epoch 6840 | Train Loss: 1.0895787477493286 | Test Loss: 1.1198766231536865\n",
      "Epoch 6841 | Train Loss: 1.2323614358901978 | Test Loss: 1.318037748336792\n",
      "Epoch 6842 | Train Loss: 1.0895508527755737 | Test Loss: 1.1199146509170532\n",
      "Epoch 6843 | Train Loss: 1.2323336601257324 | Test Loss: 1.3180845975875854\n",
      "Epoch 6844 | Train Loss: 1.0895285606384277 | Test Loss: 1.119844913482666\n",
      "Epoch 6845 | Train Loss: 1.2323054075241089 | Test Loss: 1.3180952072143555\n",
      "Epoch 6846 | Train Loss: 1.089503526687622 | Test Loss: 1.1198855638504028\n",
      "Epoch 6847 | Train Loss: 1.2322872877120972 | Test Loss: 1.3180091381072998\n",
      "Epoch 6848 | Train Loss: 1.0894867181777954 | Test Loss: 1.1198155879974365\n",
      "Epoch 6849 | Train Loss: 1.2322487831115723 | Test Loss: 1.3181155920028687\n",
      "Epoch 6850 | Train Loss: 1.089468002319336 | Test Loss: 1.1198595762252808\n",
      "Epoch 6851 | Train Loss: 1.2322337627410889 | Test Loss: 1.3180235624313354\n",
      "Epoch 6852 | Train Loss: 1.0894197225570679 | Test Loss: 1.1197863817214966\n",
      "Epoch 6853 | Train Loss: 1.2322083711624146 | Test Loss: 1.318040370941162\n",
      "Epoch 6854 | Train Loss: 1.0894129276275635 | Test Loss: 1.1198467016220093\n",
      "Epoch 6855 | Train Loss: 1.232176423072815 | Test Loss: 1.318082571029663\n",
      "Epoch 6856 | Train Loss: 1.089391827583313 | Test Loss: 1.119744896888733\n",
      "Epoch 6857 | Train Loss: 1.2321642637252808 | Test Loss: 1.317970871925354\n",
      "Epoch 6858 | Train Loss: 1.0893725156784058 | Test Loss: 1.1198267936706543\n",
      "Epoch 6859 | Train Loss: 1.2321348190307617 | Test Loss: 1.3180718421936035\n",
      "Epoch 6860 | Train Loss: 1.0893394947052002 | Test Loss: 1.1197236776351929\n",
      "Epoch 6861 | Train Loss: 1.2321159839630127 | Test Loss: 1.3179497718811035\n",
      "Epoch 6862 | Train Loss: 1.0893094539642334 | Test Loss: 1.1198067665100098\n",
      "Epoch 6863 | Train Loss: 1.2320916652679443 | Test Loss: 1.3179892301559448\n",
      "Epoch 6864 | Train Loss: 1.0892736911773682 | Test Loss: 1.1196932792663574\n",
      "Epoch 6865 | Train Loss: 1.2320619821548462 | Test Loss: 1.3179634809494019\n",
      "Epoch 6866 | Train Loss: 1.0892714262008667 | Test Loss: 1.119789481163025\n",
      "Epoch 6867 | Train Loss: 1.2320356369018555 | Test Loss: 1.317940592765808\n",
      "Epoch 6868 | Train Loss: 1.0892235040664673 | Test Loss: 1.1196459531784058\n",
      "Epoch 6869 | Train Loss: 1.2320218086242676 | Test Loss: 1.317945122718811\n",
      "Epoch 6870 | Train Loss: 1.0892072916030884 | Test Loss: 1.1197985410690308\n",
      "Epoch 6871 | Train Loss: 1.23198401927948 | Test Loss: 1.3180193901062012\n",
      "Epoch 6872 | Train Loss: 1.0891859531402588 | Test Loss: 1.119596004486084\n",
      "Epoch 6873 | Train Loss: 1.231974720954895 | Test Loss: 1.317855715751648\n",
      "Epoch 6874 | Train Loss: 1.0891859531402588 | Test Loss: 1.1197993755340576\n",
      "Epoch 6875 | Train Loss: 1.2319504022598267 | Test Loss: 1.3181178569793701\n",
      "Epoch 6876 | Train Loss: 1.0891858339309692 | Test Loss: 1.1195744276046753\n",
      "Epoch 6877 | Train Loss: 1.2319231033325195 | Test Loss: 1.3178329467773438\n",
      "Epoch 6878 | Train Loss: 1.0891472101211548 | Test Loss: 1.1197474002838135\n",
      "Epoch 6879 | Train Loss: 1.2318931818008423 | Test Loss: 1.317999005317688\n",
      "Epoch 6880 | Train Loss: 1.089087963104248 | Test Loss: 1.1195611953735352\n",
      "Epoch 6881 | Train Loss: 1.2318516969680786 | Test Loss: 1.3178898096084595\n",
      "Epoch 6882 | Train Loss: 1.089077115058899 | Test Loss: 1.1196560859680176\n",
      "Epoch 6883 | Train Loss: 1.2318111658096313 | Test Loss: 1.317882776260376\n",
      "Epoch 6884 | Train Loss: 1.089043378829956 | Test Loss: 1.1195591688156128\n",
      "Epoch 6885 | Train Loss: 1.2317875623703003 | Test Loss: 1.317967176437378\n",
      "Epoch 6886 | Train Loss: 1.0890458822250366 | Test Loss: 1.1195939779281616\n",
      "Epoch 6887 | Train Loss: 1.231726884841919 | Test Loss: 1.317876935005188\n",
      "Epoch 6888 | Train Loss: 1.0890125036239624 | Test Loss: 1.1195136308670044\n",
      "Epoch 6889 | Train Loss: 1.231717586517334 | Test Loss: 1.317916989326477\n",
      "Epoch 6890 | Train Loss: 1.0889803171157837 | Test Loss: 1.1195993423461914\n",
      "Epoch 6891 | Train Loss: 1.2316924333572388 | Test Loss: 1.3179564476013184\n",
      "Epoch 6892 | Train Loss: 1.0889426469802856 | Test Loss: 1.1194733381271362\n",
      "Epoch 6893 | Train Loss: 1.2316864728927612 | Test Loss: 1.3177566528320312\n",
      "Epoch 6894 | Train Loss: 1.0889441967010498 | Test Loss: 1.1195982694625854\n",
      "Epoch 6895 | Train Loss: 1.2316569089889526 | Test Loss: 1.318111777305603\n",
      "Epoch 6896 | Train Loss: 1.0889489650726318 | Test Loss: 1.1194465160369873\n",
      "Epoch 6897 | Train Loss: 1.2316356897354126 | Test Loss: 1.3177045583724976\n",
      "Epoch 6898 | Train Loss: 1.0889480113983154 | Test Loss: 1.1195604801177979\n",
      "Epoch 6899 | Train Loss: 1.231612205505371 | Test Loss: 1.3180869817733765\n",
      "Epoch 6900 | Train Loss: 1.0888938903808594 | Test Loss: 1.1194539070129395\n",
      "Epoch 6901 | Train Loss: 1.2316104173660278 | Test Loss: 1.3177096843719482\n",
      "Epoch 6902 | Train Loss: 1.0888569355010986 | Test Loss: 1.119554877281189\n",
      "Epoch 6903 | Train Loss: 1.2315865755081177 | Test Loss: 1.3178855180740356\n",
      "Epoch 6904 | Train Loss: 1.0888031721115112 | Test Loss: 1.119429349899292\n",
      "Epoch 6905 | Train Loss: 1.2315690517425537 | Test Loss: 1.3177465200424194\n",
      "Epoch 6906 | Train Loss: 1.088791847229004 | Test Loss: 1.1195592880249023\n",
      "Epoch 6907 | Train Loss: 1.2315356731414795 | Test Loss: 1.317827582359314\n",
      "Epoch 6908 | Train Loss: 1.0887585878372192 | Test Loss: 1.1193859577178955\n",
      "Epoch 6909 | Train Loss: 1.2315292358398438 | Test Loss: 1.3177516460418701\n",
      "Epoch 6910 | Train Loss: 1.0887460708618164 | Test Loss: 1.119553804397583\n",
      "Epoch 6911 | Train Loss: 1.2314934730529785 | Test Loss: 1.3178844451904297\n",
      "Epoch 6912 | Train Loss: 1.0887094736099243 | Test Loss: 1.1193921566009521\n",
      "Epoch 6913 | Train Loss: 1.2314797639846802 | Test Loss: 1.3177164793014526\n",
      "Epoch 6914 | Train Loss: 1.088693380355835 | Test Loss: 1.119479775428772\n",
      "Epoch 6915 | Train Loss: 1.2314084768295288 | Test Loss: 1.3178197145462036\n",
      "Epoch 6916 | Train Loss: 1.0886561870574951 | Test Loss: 1.1193808317184448\n",
      "Epoch 6917 | Train Loss: 1.2313976287841797 | Test Loss: 1.3176862001419067\n",
      "Epoch 6918 | Train Loss: 1.088637351989746 | Test Loss: 1.1194058656692505\n",
      "Epoch 6919 | Train Loss: 1.231331706047058 | Test Loss: 1.3177422285079956\n",
      "Epoch 6920 | Train Loss: 1.0886049270629883 | Test Loss: 1.1193584203720093\n",
      "Epoch 6921 | Train Loss: 1.231311559677124 | Test Loss: 1.3177438974380493\n",
      "Epoch 6922 | Train Loss: 1.088586449623108 | Test Loss: 1.1193761825561523\n",
      "Epoch 6923 | Train Loss: 1.2312493324279785 | Test Loss: 1.3177133798599243\n",
      "Epoch 6924 | Train Loss: 1.0885676145553589 | Test Loss: 1.1192877292633057\n",
      "Epoch 6925 | Train Loss: 1.2312358617782593 | Test Loss: 1.3177024126052856\n",
      "Epoch 6926 | Train Loss: 1.0885342359542847 | Test Loss: 1.1193370819091797\n",
      "Epoch 6927 | Train Loss: 1.2311912775039673 | Test Loss: 1.3177142143249512\n",
      "Epoch 6928 | Train Loss: 1.0885233879089355 | Test Loss: 1.1192610263824463\n",
      "Epoch 6929 | Train Loss: 1.2311691045761108 | Test Loss: 1.3176535367965698\n",
      "Epoch 6930 | Train Loss: 1.0884840488433838 | Test Loss: 1.11928391456604\n",
      "Epoch 6931 | Train Loss: 1.231131911277771 | Test Loss: 1.3176778554916382\n",
      "Epoch 6932 | Train Loss: 1.0884625911712646 | Test Loss: 1.1192508935928345\n",
      "Epoch 6933 | Train Loss: 1.2311025857925415 | Test Loss: 1.3176957368850708\n",
      "Epoch 6934 | Train Loss: 1.0884456634521484 | Test Loss: 1.1192317008972168\n",
      "Epoch 6935 | Train Loss: 1.2310655117034912 | Test Loss: 1.3176112174987793\n",
      "Epoch 6936 | Train Loss: 1.0884206295013428 | Test Loss: 1.1192302703857422\n",
      "Epoch 6937 | Train Loss: 1.231040596961975 | Test Loss: 1.3177160024642944\n",
      "Epoch 6938 | Train Loss: 1.0883996486663818 | Test Loss: 1.119209885597229\n",
      "Epoch 6939 | Train Loss: 1.231003999710083 | Test Loss: 1.317615032196045\n",
      "Epoch 6940 | Train Loss: 1.0883702039718628 | Test Loss: 1.1191813945770264\n",
      "Epoch 6941 | Train Loss: 1.230988621711731 | Test Loss: 1.3176497220993042\n",
      "Epoch 6942 | Train Loss: 1.0883517265319824 | Test Loss: 1.1192009449005127\n",
      "Epoch 6943 | Train Loss: 1.2309541702270508 | Test Loss: 1.317745327949524\n",
      "Epoch 6944 | Train Loss: 1.0883558988571167 | Test Loss: 1.1191343069076538\n",
      "Epoch 6945 | Train Loss: 1.2309342622756958 | Test Loss: 1.3175650835037231\n",
      "Epoch 6946 | Train Loss: 1.0883384943008423 | Test Loss: 1.119168758392334\n",
      "Epoch 6947 | Train Loss: 1.230907678604126 | Test Loss: 1.3177428245544434\n",
      "Epoch 6948 | Train Loss: 1.0883010625839233 | Test Loss: 1.1191456317901611\n",
      "Epoch 6949 | Train Loss: 1.2308850288391113 | Test Loss: 1.3176031112670898\n",
      "Epoch 6950 | Train Loss: 1.0882563591003418 | Test Loss: 1.1191117763519287\n",
      "Epoch 6951 | Train Loss: 1.2308731079101562 | Test Loss: 1.3175896406173706\n",
      "Epoch 6952 | Train Loss: 1.0882514715194702 | Test Loss: 1.1191482543945312\n",
      "Epoch 6953 | Train Loss: 1.2308305501937866 | Test Loss: 1.3177857398986816\n",
      "Epoch 6954 | Train Loss: 1.0882863998413086 | Test Loss: 1.119070291519165\n",
      "Epoch 6955 | Train Loss: 1.2307971715927124 | Test Loss: 1.3175307512283325\n",
      "Epoch 6956 | Train Loss: 1.0882505178451538 | Test Loss: 1.1191227436065674\n",
      "Epoch 6957 | Train Loss: 1.2307795286178589 | Test Loss: 1.3177803754806519\n",
      "Epoch 6958 | Train Loss: 1.0882174968719482 | Test Loss: 1.1190813779830933\n",
      "Epoch 6959 | Train Loss: 1.2307370901107788 | Test Loss: 1.3175160884857178\n",
      "Epoch 6960 | Train Loss: 1.0881597995758057 | Test Loss: 1.1190661191940308\n",
      "Epoch 6961 | Train Loss: 1.2307329177856445 | Test Loss: 1.3175514936447144\n",
      "Epoch 6962 | Train Loss: 1.088104248046875 | Test Loss: 1.119097113609314\n",
      "Epoch 6963 | Train Loss: 1.230717658996582 | Test Loss: 1.3176883459091187\n",
      "Epoch 6964 | Train Loss: 1.0881130695343018 | Test Loss: 1.1189892292022705\n",
      "Epoch 6965 | Train Loss: 1.2306855916976929 | Test Loss: 1.3174535036087036\n",
      "Epoch 6966 | Train Loss: 1.0881421566009521 | Test Loss: 1.1190667152404785\n",
      "Epoch 6967 | Train Loss: 1.2306498289108276 | Test Loss: 1.3178598880767822\n",
      "Epoch 6968 | Train Loss: 1.0881538391113281 | Test Loss: 1.1189619302749634\n",
      "Epoch 6969 | Train Loss: 1.2306132316589355 | Test Loss: 1.317430019378662\n",
      "Epoch 6970 | Train Loss: 1.0881112813949585 | Test Loss: 1.119049072265625\n",
      "Epoch 6971 | Train Loss: 1.2305939197540283 | Test Loss: 1.3177677392959595\n",
      "Epoch 6972 | Train Loss: 1.0880581140518188 | Test Loss: 1.1189606189727783\n",
      "Epoch 6973 | Train Loss: 1.2305763959884644 | Test Loss: 1.3174558877944946\n",
      "Epoch 6974 | Train Loss: 1.0880392789840698 | Test Loss: 1.1190283298492432\n",
      "Epoch 6975 | Train Loss: 1.2305519580841064 | Test Loss: 1.3176840543746948\n",
      "Epoch 6976 | Train Loss: 1.0879892110824585 | Test Loss: 1.1189225912094116\n",
      "Epoch 6977 | Train Loss: 1.2305352687835693 | Test Loss: 1.3174889087677002\n",
      "Epoch 6978 | Train Loss: 1.0879583358764648 | Test Loss: 1.1190263032913208\n",
      "Epoch 6979 | Train Loss: 1.2305183410644531 | Test Loss: 1.3176538944244385\n",
      "Epoch 6980 | Train Loss: 1.0879298448562622 | Test Loss: 1.1188856363296509\n",
      "Epoch 6981 | Train Loss: 1.230516791343689 | Test Loss: 1.317454218864441\n",
      "Epoch 6982 | Train Loss: 1.0879334211349487 | Test Loss: 1.1190526485443115\n",
      "Epoch 6983 | Train Loss: 1.230489730834961 | Test Loss: 1.3177286386489868\n",
      "Epoch 6984 | Train Loss: 1.087920904159546 | Test Loss: 1.1188839673995972\n",
      "Epoch 6985 | Train Loss: 1.2304919958114624 | Test Loss: 1.3174242973327637\n",
      "Epoch 6986 | Train Loss: 1.0878942012786865 | Test Loss: 1.1190471649169922\n",
      "Epoch 6987 | Train Loss: 1.230444073677063 | Test Loss: 1.3177307844161987\n",
      "Epoch 6988 | Train Loss: 1.087864637374878 | Test Loss: 1.1188591718673706\n",
      "Epoch 6989 | Train Loss: 1.2304412126541138 | Test Loss: 1.317388892173767\n",
      "Epoch 6990 | Train Loss: 1.0878405570983887 | Test Loss: 1.1189991235733032\n",
      "Epoch 6991 | Train Loss: 1.2303975820541382 | Test Loss: 1.3176804780960083\n",
      "Epoch 6992 | Train Loss: 1.0878163576126099 | Test Loss: 1.1188151836395264\n",
      "Epoch 6993 | Train Loss: 1.230376124382019 | Test Loss: 1.3174118995666504\n",
      "Epoch 6994 | Train Loss: 1.0877771377563477 | Test Loss: 1.1189874410629272\n",
      "Epoch 6995 | Train Loss: 1.2303422689437866 | Test Loss: 1.3176709413528442\n",
      "Epoch 6996 | Train Loss: 1.0877625942230225 | Test Loss: 1.1187834739685059\n",
      "Epoch 6997 | Train Loss: 1.2303352355957031 | Test Loss: 1.3174183368682861\n",
      "Epoch 6998 | Train Loss: 1.0877689123153687 | Test Loss: 1.1189426183700562\n",
      "Epoch 6999 | Train Loss: 1.2302756309509277 | Test Loss: 1.3177427053451538\n",
      "Epoch 7000 | Train Loss: 1.0877708196640015 | Test Loss: 1.1187840700149536\n",
      "Epoch 7001 | Train Loss: 1.2302560806274414 | Test Loss: 1.3174132108688354\n",
      "Epoch 7002 | Train Loss: 1.0877403020858765 | Test Loss: 1.1188554763793945\n",
      "Epoch 7003 | Train Loss: 1.2302061319351196 | Test Loss: 1.317686915397644\n",
      "Epoch 7004 | Train Loss: 1.087712049484253 | Test Loss: 1.1187989711761475\n",
      "Epoch 7005 | Train Loss: 1.2301850318908691 | Test Loss: 1.317423701286316\n",
      "Epoch 7006 | Train Loss: 1.087672233581543 | Test Loss: 1.1187806129455566\n",
      "Epoch 7007 | Train Loss: 1.2301521301269531 | Test Loss: 1.3176155090332031\n",
      "Epoch 7008 | Train Loss: 1.0876387357711792 | Test Loss: 1.1188219785690308\n",
      "Epoch 7009 | Train Loss: 1.2301452159881592 | Test Loss: 1.3173811435699463\n",
      "Epoch 7010 | Train Loss: 1.0876061916351318 | Test Loss: 1.1187572479248047\n",
      "Epoch 7011 | Train Loss: 1.2301093339920044 | Test Loss: 1.3176708221435547\n",
      "Epoch 7012 | Train Loss: 1.087607741355896 | Test Loss: 1.1187976598739624\n",
      "Epoch 7013 | Train Loss: 1.2301030158996582 | Test Loss: 1.3173770904541016\n",
      "Epoch 7014 | Train Loss: 1.0875859260559082 | Test Loss: 1.1187535524368286\n",
      "Epoch 7015 | Train Loss: 1.2300654649734497 | Test Loss: 1.317747950553894\n",
      "Epoch 7016 | Train Loss: 1.087594747543335 | Test Loss: 1.1187503337860107\n",
      "Epoch 7017 | Train Loss: 1.2300522327423096 | Test Loss: 1.3173768520355225\n",
      "Epoch 7018 | Train Loss: 1.0875777006149292 | Test Loss: 1.1187561750411987\n",
      "Epoch 7019 | Train Loss: 1.2300212383270264 | Test Loss: 1.31779146194458\n",
      "Epoch 7020 | Train Loss: 1.0875966548919678 | Test Loss: 1.1186903715133667\n",
      "Epoch 7021 | Train Loss: 1.2300057411193848 | Test Loss: 1.3174364566802979\n",
      "Epoch 7022 | Train Loss: 1.0876027345657349 | Test Loss: 1.1187645196914673\n",
      "Epoch 7023 | Train Loss: 1.229987621307373 | Test Loss: 1.3179891109466553\n",
      "Epoch 7024 | Train Loss: 1.0876718759536743 | Test Loss: 1.1186200380325317\n",
      "Epoch 7025 | Train Loss: 1.2300233840942383 | Test Loss: 1.3176348209381104\n",
      "Epoch 7026 | Train Loss: 1.0877606868743896 | Test Loss: 1.1188713312149048\n",
      "Epoch 7027 | Train Loss: 1.2300684452056885 | Test Loss: 1.3184919357299805\n",
      "Epoch 7028 | Train Loss: 1.087939977645874 | Test Loss: 1.1186269521713257\n",
      "Epoch 7029 | Train Loss: 1.2303149700164795 | Test Loss: 1.3179337978363037\n",
      "Epoch 7030 | Train Loss: 1.08808171749115 | Test Loss: 1.1192777156829834\n",
      "Epoch 7031 | Train Loss: 1.2306365966796875 | Test Loss: 1.3191779851913452\n",
      "Epoch 7032 | Train Loss: 1.0883691310882568 | Test Loss: 1.1190842390060425\n",
      "Epoch 7033 | Train Loss: 1.231451392173767 | Test Loss: 1.3180216550827026\n",
      "Epoch 7034 | Train Loss: 1.0881314277648926 | Test Loss: 1.1204638481140137\n",
      "Epoch 7035 | Train Loss: 1.2322371006011963 | Test Loss: 1.318851113319397\n",
      "Epoch 7036 | Train Loss: 1.0879898071289062 | Test Loss: 1.1201916933059692\n",
      "Epoch 7037 | Train Loss: 1.233641505241394 | Test Loss: 1.3176838159561157\n",
      "Epoch 7038 | Train Loss: 1.0873874425888062 | Test Loss: 1.1212817430496216\n",
      "Epoch 7039 | Train Loss: 1.23296058177948 | Test Loss: 1.3174084424972534\n",
      "Epoch 7040 | Train Loss: 1.0873879194259644 | Test Loss: 1.1195629835128784\n",
      "Epoch 7041 | Train Loss: 1.2319256067276 | Test Loss: 1.3187671899795532\n",
      "Epoch 7042 | Train Loss: 1.0877529382705688 | Test Loss: 1.118959665298462\n",
      "Epoch 7043 | Train Loss: 1.230116367340088 | Test Loss: 1.317847728729248\n",
      "Epoch 7044 | Train Loss: 1.08805251121521 | Test Loss: 1.118755578994751\n",
      "Epoch 7045 | Train Loss: 1.2297903299331665 | Test Loss: 1.3183470964431763\n",
      "Epoch 7046 | Train Loss: 1.0878067016601562 | Test Loss: 1.1187613010406494\n",
      "Epoch 7047 | Train Loss: 1.2304508686065674 | Test Loss: 1.3174620866775513\n",
      "Epoch 7048 | Train Loss: 1.087302327156067 | Test Loss: 1.1194329261779785\n",
      "Epoch 7049 | Train Loss: 1.230677843093872 | Test Loss: 1.317178726196289\n",
      "Epoch 7050 | Train Loss: 1.0872540473937988 | Test Loss: 1.1186996698379517\n",
      "Epoch 7051 | Train Loss: 1.2302219867706299 | Test Loss: 1.3179073333740234\n",
      "Epoch 7052 | Train Loss: 1.0873520374298096 | Test Loss: 1.1185555458068848\n",
      "Epoch 7053 | Train Loss: 1.229657769203186 | Test Loss: 1.317345142364502\n",
      "Epoch 7054 | Train Loss: 1.0874037742614746 | Test Loss: 1.118686556816101\n",
      "Epoch 7055 | Train Loss: 1.2296802997589111 | Test Loss: 1.3175876140594482\n",
      "Epoch 7056 | Train Loss: 1.087269902229309 | Test Loss: 1.118546962738037\n",
      "Epoch 7057 | Train Loss: 1.229920744895935 | Test Loss: 1.3173243999481201\n",
      "Epoch 7058 | Train Loss: 1.0871057510375977 | Test Loss: 1.1188284158706665\n",
      "Epoch 7059 | Train Loss: 1.2298115491867065 | Test Loss: 1.3171573877334595\n",
      "Epoch 7060 | Train Loss: 1.0871531963348389 | Test Loss: 1.118441104888916\n",
      "Epoch 7061 | Train Loss: 1.229547381401062 | Test Loss: 1.317544937133789\n",
      "Epoch 7062 | Train Loss: 1.0871248245239258 | Test Loss: 1.1184089183807373\n",
      "Epoch 7063 | Train Loss: 1.2294543981552124 | Test Loss: 1.3171250820159912\n",
      "Epoch 7064 | Train Loss: 1.0870866775512695 | Test Loss: 1.1185779571533203\n",
      "Epoch 7065 | Train Loss: 1.2295018434524536 | Test Loss: 1.317322850227356\n",
      "Epoch 7066 | Train Loss: 1.0870425701141357 | Test Loss: 1.1183629035949707\n",
      "Epoch 7067 | Train Loss: 1.2295079231262207 | Test Loss: 1.3172976970672607\n",
      "Epoch 7068 | Train Loss: 1.086990475654602 | Test Loss: 1.118525505065918\n",
      "Epoch 7069 | Train Loss: 1.2294270992279053 | Test Loss: 1.3170957565307617\n",
      "Epoch 7070 | Train Loss: 1.0869978666305542 | Test Loss: 1.118371844291687\n",
      "Epoch 7071 | Train Loss: 1.2293204069137573 | Test Loss: 1.3173776865005493\n",
      "Epoch 7072 | Train Loss: 1.0869553089141846 | Test Loss: 1.1183186769485474\n",
      "Epoch 7073 | Train Loss: 1.2293190956115723 | Test Loss: 1.3171441555023193\n",
      "Epoch 7074 | Train Loss: 1.0869215726852417 | Test Loss: 1.1184576749801636\n",
      "Epoch 7075 | Train Loss: 1.2293118238449097 | Test Loss: 1.317199945449829\n",
      "Epoch 7076 | Train Loss: 1.0868977308273315 | Test Loss: 1.1182737350463867\n",
      "Epoch 7077 | Train Loss: 1.2292896509170532 | Test Loss: 1.3172892332077026\n",
      "Epoch 7078 | Train Loss: 1.0868712663650513 | Test Loss: 1.1183580160140991\n",
      "Epoch 7079 | Train Loss: 1.2292269468307495 | Test Loss: 1.3171311616897583\n",
      "Epoch 7080 | Train Loss: 1.0868619680404663 | Test Loss: 1.1183069944381714\n",
      "Epoch 7081 | Train Loss: 1.2291791439056396 | Test Loss: 1.3173006772994995\n",
      "Epoch 7082 | Train Loss: 1.0868299007415771 | Test Loss: 1.1182467937469482\n",
      "Epoch 7083 | Train Loss: 1.229176640510559 | Test Loss: 1.3171945810317993\n",
      "Epoch 7084 | Train Loss: 1.0868043899536133 | Test Loss: 1.1183372735977173\n",
      "Epoch 7085 | Train Loss: 1.2291512489318848 | Test Loss: 1.3171792030334473\n",
      "Epoch 7086 | Train Loss: 1.086786150932312 | Test Loss: 1.1182398796081543\n",
      "Epoch 7087 | Train Loss: 1.229119896888733 | Test Loss: 1.3172850608825684\n",
      "Epoch 7088 | Train Loss: 1.086766004562378 | Test Loss: 1.118273138999939\n",
      "Epoch 7089 | Train Loss: 1.2290842533111572 | Test Loss: 1.3171929121017456\n",
      "Epoch 7090 | Train Loss: 1.08674955368042 | Test Loss: 1.1182526350021362\n",
      "Epoch 7091 | Train Loss: 1.2290557622909546 | Test Loss: 1.3172805309295654\n",
      "Epoch 7092 | Train Loss: 1.086720585823059 | Test Loss: 1.118219017982483\n",
      "Epoch 7093 | Train Loss: 1.2290394306182861 | Test Loss: 1.317237138748169\n",
      "Epoch 7094 | Train Loss: 1.086702823638916 | Test Loss: 1.1182461977005005\n",
      "Epoch 7095 | Train Loss: 1.2290112972259521 | Test Loss: 1.3172227144241333\n",
      "Epoch 7096 | Train Loss: 1.086679220199585 | Test Loss: 1.1181995868682861\n",
      "Epoch 7097 | Train Loss: 1.2289950847625732 | Test Loss: 1.3172489404678345\n",
      "Epoch 7098 | Train Loss: 1.0866578817367554 | Test Loss: 1.118213176727295\n",
      "Epoch 7099 | Train Loss: 1.2289589643478394 | Test Loss: 1.3172128200531006\n",
      "Epoch 7100 | Train Loss: 1.0866447687149048 | Test Loss: 1.1181780099868774\n",
      "Epoch 7101 | Train Loss: 1.2289376258850098 | Test Loss: 1.3172571659088135\n",
      "Epoch 7102 | Train Loss: 1.0866189002990723 | Test Loss: 1.1181864738464355\n",
      "Epoch 7103 | Train Loss: 1.2289124727249146 | Test Loss: 1.3172085285186768\n",
      "Epoch 7104 | Train Loss: 1.0866023302078247 | Test Loss: 1.1181641817092896\n",
      "Epoch 7105 | Train Loss: 1.2288906574249268 | Test Loss: 1.3172582387924194\n",
      "Epoch 7106 | Train Loss: 1.0865757465362549 | Test Loss: 1.1181658506393433\n",
      "Epoch 7107 | Train Loss: 1.2288717031478882 | Test Loss: 1.3172223567962646\n",
      "Epoch 7108 | Train Loss: 1.0865567922592163 | Test Loss: 1.1181480884552002\n",
      "Epoch 7109 | Train Loss: 1.2288451194763184 | Test Loss: 1.3172366619110107\n",
      "Epoch 7110 | Train Loss: 1.0865355730056763 | Test Loss: 1.1181325912475586\n",
      "Epoch 7111 | Train Loss: 1.2288250923156738 | Test Loss: 1.317238450050354\n",
      "Epoch 7112 | Train Loss: 1.0865168571472168 | Test Loss: 1.118141531944275\n",
      "Epoch 7113 | Train Loss: 1.2288014888763428 | Test Loss: 1.3172403573989868\n",
      "Epoch 7114 | Train Loss: 1.086495280265808 | Test Loss: 1.1180917024612427\n",
      "Epoch 7115 | Train Loss: 1.2287827730178833 | Test Loss: 1.3172316551208496\n",
      "Epoch 7116 | Train Loss: 1.0864759683609009 | Test Loss: 1.118120789527893\n",
      "Epoch 7117 | Train Loss: 1.2287570238113403 | Test Loss: 1.3172255754470825\n",
      "Epoch 7118 | Train Loss: 1.0864555835723877 | Test Loss: 1.1180672645568848\n",
      "Epoch 7119 | Train Loss: 1.2287379503250122 | Test Loss: 1.317204475402832\n",
      "Epoch 7120 | Train Loss: 1.0864337682724 | Test Loss: 1.1180860996246338\n",
      "Epoch 7121 | Train Loss: 1.228713870048523 | Test Loss: 1.3172352313995361\n",
      "Epoch 7122 | Train Loss: 1.0864135026931763 | Test Loss: 1.1180435419082642\n",
      "Epoch 7123 | Train Loss: 1.2286913394927979 | Test Loss: 1.3172205686569214\n",
      "Epoch 7124 | Train Loss: 1.086395263671875 | Test Loss: 1.1180721521377563\n",
      "Epoch 7125 | Train Loss: 1.2286701202392578 | Test Loss: 1.3172481060028076\n",
      "Epoch 7126 | Train Loss: 1.0863734483718872 | Test Loss: 1.1180261373519897\n",
      "Epoch 7127 | Train Loss: 1.2286490201950073 | Test Loss: 1.3172225952148438\n",
      "Epoch 7128 | Train Loss: 1.086356282234192 | Test Loss: 1.1180499792099\n",
      "Epoch 7129 | Train Loss: 1.2286261320114136 | Test Loss: 1.3172318935394287\n",
      "Epoch 7130 | Train Loss: 1.0863372087478638 | Test Loss: 1.1180088520050049\n",
      "Epoch 7131 | Train Loss: 1.2286056280136108 | Test Loss: 1.317202091217041\n",
      "Epoch 7132 | Train Loss: 1.0863174200057983 | Test Loss: 1.1180217266082764\n",
      "Epoch 7133 | Train Loss: 1.2285853624343872 | Test Loss: 1.3172667026519775\n",
      "Epoch 7134 | Train Loss: 1.0862981081008911 | Test Loss: 1.117987871170044\n",
      "Epoch 7135 | Train Loss: 1.2285630702972412 | Test Loss: 1.3171988725662231\n",
      "Epoch 7136 | Train Loss: 1.0862787961959839 | Test Loss: 1.118014931678772\n",
      "Epoch 7137 | Train Loss: 1.2285454273223877 | Test Loss: 1.3172576427459717\n",
      "Epoch 7138 | Train Loss: 1.0862562656402588 | Test Loss: 1.1179505586624146\n",
      "Epoch 7139 | Train Loss: 1.2285219430923462 | Test Loss: 1.31721031665802\n",
      "Epoch 7140 | Train Loss: 1.0862400531768799 | Test Loss: 1.1179888248443604\n",
      "Epoch 7141 | Train Loss: 1.2284997701644897 | Test Loss: 1.317225456237793\n",
      "Epoch 7142 | Train Loss: 1.086215853691101 | Test Loss: 1.1179356575012207\n",
      "Epoch 7143 | Train Loss: 1.2284770011901855 | Test Loss: 1.317226767539978\n",
      "Epoch 7144 | Train Loss: 1.0861961841583252 | Test Loss: 1.117945671081543\n",
      "Epoch 7145 | Train Loss: 1.2284547090530396 | Test Loss: 1.3172273635864258\n",
      "Epoch 7146 | Train Loss: 1.0861746072769165 | Test Loss: 1.1179341077804565\n",
      "Epoch 7147 | Train Loss: 1.2284282445907593 | Test Loss: 1.317234992980957\n",
      "Epoch 7148 | Train Loss: 1.0861608982086182 | Test Loss: 1.1179094314575195\n",
      "Epoch 7149 | Train Loss: 1.228402853012085 | Test Loss: 1.3172224760055542\n",
      "Epoch 7150 | Train Loss: 1.086138129234314 | Test Loss: 1.117928385734558\n",
      "Epoch 7151 | Train Loss: 1.2283860445022583 | Test Loss: 1.3172184228897095\n",
      "Epoch 7152 | Train Loss: 1.086121678352356 | Test Loss: 1.1178873777389526\n",
      "Epoch 7153 | Train Loss: 1.2283564805984497 | Test Loss: 1.3172320127487183\n",
      "Epoch 7154 | Train Loss: 1.0861010551452637 | Test Loss: 1.1179046630859375\n",
      "Epoch 7155 | Train Loss: 1.228340744972229 | Test Loss: 1.3172475099563599\n",
      "Epoch 7156 | Train Loss: 1.086084008216858 | Test Loss: 1.1178621053695679\n",
      "Epoch 7157 | Train Loss: 1.2283153533935547 | Test Loss: 1.317234992980957\n",
      "Epoch 7158 | Train Loss: 1.0860620737075806 | Test Loss: 1.1178920269012451\n",
      "Epoch 7159 | Train Loss: 1.2282971143722534 | Test Loss: 1.3172473907470703\n",
      "Epoch 7160 | Train Loss: 1.0860432386398315 | Test Loss: 1.1178381443023682\n",
      "Epoch 7161 | Train Loss: 1.2282748222351074 | Test Loss: 1.3172123432159424\n",
      "Epoch 7162 | Train Loss: 1.086021065711975 | Test Loss: 1.1178687810897827\n",
      "Epoch 7163 | Train Loss: 1.228255271911621 | Test Loss: 1.317242980003357\n",
      "Epoch 7164 | Train Loss: 1.0860068798065186 | Test Loss: 1.1177968978881836\n",
      "Epoch 7165 | Train Loss: 1.2282265424728394 | Test Loss: 1.3172210454940796\n",
      "Epoch 7166 | Train Loss: 1.085983395576477 | Test Loss: 1.1178690195083618\n",
      "Epoch 7167 | Train Loss: 1.2282114028930664 | Test Loss: 1.3172104358673096\n",
      "Epoch 7168 | Train Loss: 1.0859639644622803 | Test Loss: 1.1177839040756226\n",
      "Epoch 7169 | Train Loss: 1.2281841039657593 | Test Loss: 1.3172287940979004\n",
      "Epoch 7170 | Train Loss: 1.0859466791152954 | Test Loss: 1.1178416013717651\n",
      "Epoch 7171 | Train Loss: 1.2281626462936401 | Test Loss: 1.31719970703125\n",
      "Epoch 7172 | Train Loss: 1.0859267711639404 | Test Loss: 1.1177725791931152\n",
      "Epoch 7173 | Train Loss: 1.2281361818313599 | Test Loss: 1.3172515630722046\n",
      "Epoch 7174 | Train Loss: 1.0859090089797974 | Test Loss: 1.117797613143921\n",
      "Epoch 7175 | Train Loss: 1.2281142473220825 | Test Loss: 1.317221760749817\n",
      "Epoch 7176 | Train Loss: 1.0858898162841797 | Test Loss: 1.1177698373794556\n",
      "Epoch 7177 | Train Loss: 1.228089451789856 | Test Loss: 1.3172544240951538\n",
      "Epoch 7178 | Train Loss: 1.0858725309371948 | Test Loss: 1.1177728176116943\n",
      "Epoch 7179 | Train Loss: 1.2280668020248413 | Test Loss: 1.3172099590301514\n",
      "Epoch 7180 | Train Loss: 1.0858548879623413 | Test Loss: 1.1177641153335571\n",
      "Epoch 7181 | Train Loss: 1.2280471324920654 | Test Loss: 1.317276120185852\n",
      "Epoch 7182 | Train Loss: 1.08583664894104 | Test Loss: 1.1177494525909424\n",
      "Epoch 7183 | Train Loss: 1.2280229330062866 | Test Loss: 1.3172106742858887\n",
      "Epoch 7184 | Train Loss: 1.0858176946640015 | Test Loss: 1.1177475452423096\n",
      "Epoch 7185 | Train Loss: 1.2280027866363525 | Test Loss: 1.3172672986984253\n",
      "Epoch 7186 | Train Loss: 1.08579683303833 | Test Loss: 1.1177359819412231\n",
      "Epoch 7187 | Train Loss: 1.2279835939407349 | Test Loss: 1.317215919494629\n",
      "Epoch 7188 | Train Loss: 1.0857789516448975 | Test Loss: 1.1177334785461426\n",
      "Epoch 7189 | Train Loss: 1.2279679775238037 | Test Loss: 1.3172615766525269\n",
      "Epoch 7190 | Train Loss: 1.0857585668563843 | Test Loss: 1.1177119016647339\n",
      "Epoch 7191 | Train Loss: 1.227949857711792 | Test Loss: 1.3171944618225098\n",
      "Epoch 7192 | Train Loss: 1.085739016532898 | Test Loss: 1.1177266836166382\n",
      "Epoch 7193 | Train Loss: 1.2279356718063354 | Test Loss: 1.317291259765625\n",
      "Epoch 7194 | Train Loss: 1.0857168436050415 | Test Loss: 1.1176804304122925\n",
      "Epoch 7195 | Train Loss: 1.2279162406921387 | Test Loss: 1.3172136545181274\n",
      "Epoch 7196 | Train Loss: 1.0857009887695312 | Test Loss: 1.1177196502685547\n",
      "Epoch 7197 | Train Loss: 1.2278916835784912 | Test Loss: 1.3172687292099\n",
      "Epoch 7198 | Train Loss: 1.0856788158416748 | Test Loss: 1.1176623106002808\n",
      "Epoch 7199 | Train Loss: 1.227870225906372 | Test Loss: 1.3172417879104614\n",
      "Epoch 7200 | Train Loss: 1.0856603384017944 | Test Loss: 1.1176939010620117\n",
      "Epoch 7201 | Train Loss: 1.227845311164856 | Test Loss: 1.317226767539978\n",
      "Epoch 7202 | Train Loss: 1.0856406688690186 | Test Loss: 1.1176449060440063\n",
      "Epoch 7203 | Train Loss: 1.2278120517730713 | Test Loss: 1.3172650337219238\n",
      "Epoch 7204 | Train Loss: 1.085627794265747 | Test Loss: 1.1176567077636719\n",
      "Epoch 7205 | Train Loss: 1.2277837991714478 | Test Loss: 1.3171905279159546\n",
      "Epoch 7206 | Train Loss: 1.0856047868728638 | Test Loss: 1.1176276206970215\n",
      "Epoch 7207 | Train Loss: 1.227757453918457 | Test Loss: 1.3172428607940674\n",
      "Epoch 7208 | Train Loss: 1.0855896472930908 | Test Loss: 1.1176221370697021\n",
      "Epoch 7209 | Train Loss: 1.2277288436889648 | Test Loss: 1.3172028064727783\n",
      "Epoch 7210 | Train Loss: 1.0855681896209717 | Test Loss: 1.117615818977356\n",
      "Epoch 7211 | Train Loss: 1.2277098894119263 | Test Loss: 1.3172208070755005\n",
      "Epoch 7212 | Train Loss: 1.085547685623169 | Test Loss: 1.117596983909607\n",
      "Epoch 7213 | Train Loss: 1.2276849746704102 | Test Loss: 1.3172115087509155\n",
      "Epoch 7214 | Train Loss: 1.0855284929275513 | Test Loss: 1.1175994873046875\n",
      "Epoch 7215 | Train Loss: 1.2276678085327148 | Test Loss: 1.317221760749817\n",
      "Epoch 7216 | Train Loss: 1.0855093002319336 | Test Loss: 1.1175674200057983\n",
      "Epoch 7217 | Train Loss: 1.2276442050933838 | Test Loss: 1.3171859979629517\n",
      "Epoch 7218 | Train Loss: 1.0854871273040771 | Test Loss: 1.1175739765167236\n",
      "Epoch 7219 | Train Loss: 1.2276231050491333 | Test Loss: 1.317232608795166\n",
      "Epoch 7220 | Train Loss: 1.085472822189331 | Test Loss: 1.1175445318222046\n",
      "Epoch 7221 | Train Loss: 1.227600336074829 | Test Loss: 1.3171850442886353\n",
      "Epoch 7222 | Train Loss: 1.0854450464248657 | Test Loss: 1.117567777633667\n",
      "Epoch 7223 | Train Loss: 1.2275848388671875 | Test Loss: 1.317215085029602\n",
      "Epoch 7224 | Train Loss: 1.0854310989379883 | Test Loss: 1.117506504058838\n",
      "Epoch 7225 | Train Loss: 1.2275573015213013 | Test Loss: 1.317204236984253\n",
      "Epoch 7226 | Train Loss: 1.0854108333587646 | Test Loss: 1.1175674200057983\n",
      "Epoch 7227 | Train Loss: 1.2275444269180298 | Test Loss: 1.317206859588623\n",
      "Epoch 7228 | Train Loss: 1.0853909254074097 | Test Loss: 1.1174728870391846\n",
      "Epoch 7229 | Train Loss: 1.2275186777114868 | Test Loss: 1.3171924352645874\n",
      "Epoch 7230 | Train Loss: 1.0853767395019531 | Test Loss: 1.1175460815429688\n",
      "Epoch 7231 | Train Loss: 1.227501630783081 | Test Loss: 1.3172188997268677\n",
      "Epoch 7232 | Train Loss: 1.0853549242019653 | Test Loss: 1.1174622774124146\n",
      "Epoch 7233 | Train Loss: 1.2274826765060425 | Test Loss: 1.3171911239624023\n",
      "Epoch 7234 | Train Loss: 1.0853376388549805 | Test Loss: 1.1175177097320557\n",
      "Epoch 7235 | Train Loss: 1.2274562120437622 | Test Loss: 1.3172290325164795\n",
      "Epoch 7236 | Train Loss: 1.0853182077407837 | Test Loss: 1.1174509525299072\n",
      "Epoch 7237 | Train Loss: 1.227434754371643 | Test Loss: 1.3172186613082886\n",
      "Epoch 7238 | Train Loss: 1.0852974653244019 | Test Loss: 1.1175096035003662\n",
      "Epoch 7239 | Train Loss: 1.2274160385131836 | Test Loss: 1.3172153234481812\n",
      "Epoch 7240 | Train Loss: 1.0852808952331543 | Test Loss: 1.1174120903015137\n",
      "Epoch 7241 | Train Loss: 1.227393388748169 | Test Loss: 1.3172082901000977\n",
      "Epoch 7242 | Train Loss: 1.0852603912353516 | Test Loss: 1.117510199546814\n",
      "Epoch 7243 | Train Loss: 1.2273752689361572 | Test Loss: 1.3172062635421753\n",
      "Epoch 7244 | Train Loss: 1.0852422714233398 | Test Loss: 1.1173828840255737\n",
      "Epoch 7245 | Train Loss: 1.2273521423339844 | Test Loss: 1.3171900510787964\n",
      "Epoch 7246 | Train Loss: 1.0852210521697998 | Test Loss: 1.1174848079681396\n",
      "Epoch 7247 | Train Loss: 1.2273298501968384 | Test Loss: 1.3172045946121216\n",
      "Epoch 7248 | Train Loss: 1.085204839706421 | Test Loss: 1.117369532585144\n",
      "Epoch 7249 | Train Loss: 1.2273058891296387 | Test Loss: 1.3172290325164795\n",
      "Epoch 7250 | Train Loss: 1.0851796865463257 | Test Loss: 1.1174546480178833\n",
      "Epoch 7251 | Train Loss: 1.2272803783416748 | Test Loss: 1.3171647787094116\n",
      "Epoch 7252 | Train Loss: 1.0851627588272095 | Test Loss: 1.1173630952835083\n",
      "Epoch 7253 | Train Loss: 1.2272570133209229 | Test Loss: 1.3172645568847656\n",
      "Epoch 7254 | Train Loss: 1.0851420164108276 | Test Loss: 1.1174180507659912\n",
      "Epoch 7255 | Train Loss: 1.2272340059280396 | Test Loss: 1.3171509504318237\n",
      "Epoch 7256 | Train Loss: 1.0851234197616577 | Test Loss: 1.117342472076416\n",
      "Epoch 7257 | Train Loss: 1.2272095680236816 | Test Loss: 1.3172186613082886\n",
      "Epoch 7258 | Train Loss: 1.0850995779037476 | Test Loss: 1.11738121509552\n",
      "Epoch 7259 | Train Loss: 1.2271876335144043 | Test Loss: 1.3171751499176025\n",
      "Epoch 7260 | Train Loss: 1.0850857496261597 | Test Loss: 1.1173300743103027\n",
      "Epoch 7261 | Train Loss: 1.2271627187728882 | Test Loss: 1.3172253370285034\n",
      "Epoch 7262 | Train Loss: 1.0850622653961182 | Test Loss: 1.1173527240753174\n",
      "Epoch 7263 | Train Loss: 1.2271398305892944 | Test Loss: 1.3172154426574707\n",
      "Epoch 7264 | Train Loss: 1.0850473642349243 | Test Loss: 1.1173155307769775\n",
      "Epoch 7265 | Train Loss: 1.2271170616149902 | Test Loss: 1.3172513246536255\n",
      "Epoch 7266 | Train Loss: 1.0850223302841187 | Test Loss: 1.1173195838928223\n",
      "Epoch 7267 | Train Loss: 1.2271008491516113 | Test Loss: 1.3171706199645996\n",
      "Epoch 7268 | Train Loss: 1.0850106477737427 | Test Loss: 1.1172956228256226\n",
      "Epoch 7269 | Train Loss: 1.2270762920379639 | Test Loss: 1.3172696828842163\n",
      "Epoch 7270 | Train Loss: 1.0849862098693848 | Test Loss: 1.1172939538955688\n",
      "Epoch 7271 | Train Loss: 1.2270585298538208 | Test Loss: 1.3171424865722656\n",
      "Epoch 7272 | Train Loss: 1.0849738121032715 | Test Loss: 1.1172914505004883\n",
      "Epoch 7273 | Train Loss: 1.2270385026931763 | Test Loss: 1.3172955513000488\n",
      "Epoch 7274 | Train Loss: 1.0849523544311523 | Test Loss: 1.1172659397125244\n",
      "Epoch 7275 | Train Loss: 1.2270270586013794 | Test Loss: 1.3171510696411133\n",
      "Epoch 7276 | Train Loss: 1.0849394798278809 | Test Loss: 1.117302417755127\n",
      "Epoch 7277 | Train Loss: 1.2270023822784424 | Test Loss: 1.3172401189804077\n",
      "Epoch 7278 | Train Loss: 1.0849148035049438 | Test Loss: 1.1172447204589844\n",
      "Epoch 7279 | Train Loss: 1.226993441581726 | Test Loss: 1.31719172000885\n",
      "Epoch 7280 | Train Loss: 1.0848966836929321 | Test Loss: 1.117279291152954\n",
      "Epoch 7281 | Train Loss: 1.2269728183746338 | Test Loss: 1.3172560930252075\n",
      "Epoch 7282 | Train Loss: 1.0848768949508667 | Test Loss: 1.1172188520431519\n",
      "Epoch 7283 | Train Loss: 1.2269572019577026 | Test Loss: 1.3171792030334473\n",
      "Epoch 7284 | Train Loss: 1.0848588943481445 | Test Loss: 1.1172575950622559\n",
      "Epoch 7285 | Train Loss: 1.2269335985183716 | Test Loss: 1.3172850608825684\n",
      "Epoch 7286 | Train Loss: 1.0848395824432373 | Test Loss: 1.1172051429748535\n",
      "Epoch 7287 | Train Loss: 1.2269195318222046 | Test Loss: 1.3171825408935547\n",
      "Epoch 7288 | Train Loss: 1.0848214626312256 | Test Loss: 1.1172311305999756\n",
      "Epoch 7289 | Train Loss: 1.2268896102905273 | Test Loss: 1.3172671794891357\n",
      "Epoch 7290 | Train Loss: 1.0848032236099243 | Test Loss: 1.1171910762786865\n",
      "Epoch 7291 | Train Loss: 1.2268693447113037 | Test Loss: 1.3172235488891602\n",
      "Epoch 7292 | Train Loss: 1.0847773551940918 | Test Loss: 1.1172081232070923\n",
      "Epoch 7293 | Train Loss: 1.2268403768539429 | Test Loss: 1.317224144935608\n",
      "Epoch 7294 | Train Loss: 1.0847660303115845 | Test Loss: 1.1171661615371704\n",
      "Epoch 7295 | Train Loss: 1.2268195152282715 | Test Loss: 1.3172553777694702\n",
      "Epoch 7296 | Train Loss: 1.084739327430725 | Test Loss: 1.1171923875808716\n",
      "Epoch 7297 | Train Loss: 1.2267934083938599 | Test Loss: 1.3172054290771484\n",
      "Epoch 7298 | Train Loss: 1.0847258567810059 | Test Loss: 1.1171547174453735\n",
      "Epoch 7299 | Train Loss: 1.22676682472229 | Test Loss: 1.3172653913497925\n",
      "Epoch 7300 | Train Loss: 1.084699273109436 | Test Loss: 1.1171624660491943\n",
      "Epoch 7301 | Train Loss: 1.22674560546875 | Test Loss: 1.3172128200531006\n",
      "Epoch 7302 | Train Loss: 1.0846830606460571 | Test Loss: 1.117149829864502\n",
      "Epoch 7303 | Train Loss: 1.226717233657837 | Test Loss: 1.3172298669815063\n",
      "Epoch 7304 | Train Loss: 1.0846583843231201 | Test Loss: 1.1171321868896484\n",
      "Epoch 7305 | Train Loss: 1.2266945838928223 | Test Loss: 1.317213773727417\n",
      "Epoch 7306 | Train Loss: 1.0846434831619263 | Test Loss: 1.1171350479125977\n",
      "Epoch 7307 | Train Loss: 1.2266685962677002 | Test Loss: 1.3172383308410645\n",
      "Epoch 7308 | Train Loss: 1.0846184492111206 | Test Loss: 1.1171107292175293\n",
      "Epoch 7309 | Train Loss: 1.226643681526184 | Test Loss: 1.3172085285186768\n",
      "Epoch 7310 | Train Loss: 1.0846039056777954 | Test Loss: 1.1170953512191772\n",
      "Epoch 7311 | Train Loss: 1.22661554813385 | Test Loss: 1.317245602607727\n",
      "Epoch 7312 | Train Loss: 1.0845823287963867 | Test Loss: 1.1170974969863892\n",
      "Epoch 7313 | Train Loss: 1.2266037464141846 | Test Loss: 1.3172380924224854\n",
      "Epoch 7314 | Train Loss: 1.0845658779144287 | Test Loss: 1.1170495748519897\n",
      "Epoch 7315 | Train Loss: 1.2265729904174805 | Test Loss: 1.3172301054000854\n",
      "Epoch 7316 | Train Loss: 1.0845487117767334 | Test Loss: 1.1170837879180908\n",
      "Epoch 7317 | Train Loss: 1.2265551090240479 | Test Loss: 1.3172612190246582\n",
      "Epoch 7318 | Train Loss: 1.084532380104065 | Test Loss: 1.1170337200164795\n",
      "Epoch 7319 | Train Loss: 1.2265321016311646 | Test Loss: 1.317204236984253\n",
      "Epoch 7320 | Train Loss: 1.0845154523849487 | Test Loss: 1.1170647144317627\n",
      "Epoch 7321 | Train Loss: 1.2265146970748901 | Test Loss: 1.3172991275787354\n",
      "Epoch 7322 | Train Loss: 1.0844974517822266 | Test Loss: 1.1170088052749634\n",
      "Epoch 7323 | Train Loss: 1.2264952659606934 | Test Loss: 1.3171966075897217\n",
      "Epoch 7324 | Train Loss: 1.0844804048538208 | Test Loss: 1.1170772314071655\n",
      "Epoch 7325 | Train Loss: 1.2264811992645264 | Test Loss: 1.3173080682754517\n",
      "Epoch 7326 | Train Loss: 1.0844619274139404 | Test Loss: 1.1169699430465698\n",
      "Epoch 7327 | Train Loss: 1.2264716625213623 | Test Loss: 1.317205786705017\n",
      "Epoch 7328 | Train Loss: 1.0844444036483765 | Test Loss: 1.117110252380371\n",
      "Epoch 7329 | Train Loss: 1.2264673709869385 | Test Loss: 1.3172762393951416\n",
      "Epoch 7330 | Train Loss: 1.084421992301941 | Test Loss: 1.1169462203979492\n",
      "Epoch 7331 | Train Loss: 1.226453185081482 | Test Loss: 1.3172200918197632\n",
      "Epoch 7332 | Train Loss: 1.0844029188156128 | Test Loss: 1.1170837879180908\n",
      "Epoch 7333 | Train Loss: 1.2264224290847778 | Test Loss: 1.3172510862350464\n",
      "Epoch 7334 | Train Loss: 1.0843812227249146 | Test Loss: 1.116934061050415\n",
      "Epoch 7335 | Train Loss: 1.2264013290405273 | Test Loss: 1.3172600269317627\n",
      "Epoch 7336 | Train Loss: 1.0843603610992432 | Test Loss: 1.1170321702957153\n",
      "Epoch 7337 | Train Loss: 1.2263556718826294 | Test Loss: 1.3172013759613037\n",
      "Epoch 7338 | Train Loss: 1.0843422412872314 | Test Loss: 1.116930603981018\n",
      "Epoch 7339 | Train Loss: 1.2263249158859253 | Test Loss: 1.3173013925552368\n",
      "Epoch 7340 | Train Loss: 1.084324598312378 | Test Loss: 1.1169681549072266\n",
      "Epoch 7341 | Train Loss: 1.2262951135635376 | Test Loss: 1.3171799182891846\n",
      "Epoch 7342 | Train Loss: 1.084307074546814 | Test Loss: 1.1169092655181885\n",
      "Epoch 7343 | Train Loss: 1.2262639999389648 | Test Loss: 1.3173023462295532\n",
      "Epoch 7344 | Train Loss: 1.0842863321304321 | Test Loss: 1.1169359683990479\n",
      "Epoch 7345 | Train Loss: 1.226240634918213 | Test Loss: 1.317193865776062\n",
      "Epoch 7346 | Train Loss: 1.0842714309692383 | Test Loss: 1.116897702217102\n",
      "Epoch 7347 | Train Loss: 1.2262146472930908 | Test Loss: 1.3172811269760132\n",
      "Epoch 7348 | Train Loss: 1.0842467546463013 | Test Loss: 1.1169065237045288\n",
      "Epoch 7349 | Train Loss: 1.2261911630630493 | Test Loss: 1.3172162771224976\n",
      "Epoch 7350 | Train Loss: 1.0842328071594238 | Test Loss: 1.1168780326843262\n",
      "Epoch 7351 | Train Loss: 1.2261682748794556 | Test Loss: 1.317282795906067\n",
      "Epoch 7352 | Train Loss: 1.084207534790039 | Test Loss: 1.116889476776123\n",
      "Epoch 7353 | Train Loss: 1.2261539697647095 | Test Loss: 1.3172283172607422\n",
      "Epoch 7354 | Train Loss: 1.0841944217681885 | Test Loss: 1.1168794631958008\n",
      "Epoch 7355 | Train Loss: 1.2261276245117188 | Test Loss: 1.3172999620437622\n",
      "Epoch 7356 | Train Loss: 1.0841736793518066 | Test Loss: 1.1168475151062012\n",
      "Epoch 7357 | Train Loss: 1.2261141538619995 | Test Loss: 1.31721031665802\n",
      "Epoch 7358 | Train Loss: 1.0841560363769531 | Test Loss: 1.1168849468231201\n",
      "Epoch 7359 | Train Loss: 1.226094365119934 | Test Loss: 1.317339539527893\n",
      "Epoch 7360 | Train Loss: 1.0841426849365234 | Test Loss: 1.1168115139007568\n",
      "Epoch 7361 | Train Loss: 1.2260836362838745 | Test Loss: 1.3171920776367188\n",
      "Epoch 7362 | Train Loss: 1.0841249227523804 | Test Loss: 1.116886019706726\n",
      "Epoch 7363 | Train Loss: 1.2260576486587524 | Test Loss: 1.3173234462738037\n",
      "Epoch 7364 | Train Loss: 1.084104299545288 | Test Loss: 1.1167964935302734\n",
      "Epoch 7365 | Train Loss: 1.2260501384735107 | Test Loss: 1.3172006607055664\n",
      "Epoch 7366 | Train Loss: 1.0840864181518555 | Test Loss: 1.1168925762176514\n",
      "Epoch 7367 | Train Loss: 1.2260311841964722 | Test Loss: 1.3173068761825562\n",
      "Epoch 7368 | Train Loss: 1.0840635299682617 | Test Loss: 1.1167834997177124\n",
      "Epoch 7369 | Train Loss: 1.2260186672210693 | Test Loss: 1.3172780275344849\n",
      "Epoch 7370 | Train Loss: 1.0840458869934082 | Test Loss: 1.116883635520935\n",
      "Epoch 7371 | Train Loss: 1.225988745689392 | Test Loss: 1.3172850608825684\n",
      "Epoch 7372 | Train Loss: 1.0840257406234741 | Test Loss: 1.116774082183838\n",
      "Epoch 7373 | Train Loss: 1.2259678840637207 | Test Loss: 1.3172725439071655\n",
      "Epoch 7374 | Train Loss: 1.0840078592300415 | Test Loss: 1.1168153285980225\n",
      "Epoch 7375 | Train Loss: 1.2259294986724854 | Test Loss: 1.3172848224639893\n",
      "Epoch 7376 | Train Loss: 1.0839869976043701 | Test Loss: 1.1167752742767334\n",
      "Epoch 7377 | Train Loss: 1.2259025573730469 | Test Loss: 1.3172763586044312\n",
      "Epoch 7378 | Train Loss: 1.0839757919311523 | Test Loss: 1.1167480945587158\n",
      "Epoch 7379 | Train Loss: 1.2258696556091309 | Test Loss: 1.317304253578186\n",
      "Epoch 7380 | Train Loss: 1.0839530229568481 | Test Loss: 1.1167925596237183\n",
      "Epoch 7381 | Train Loss: 1.225857138633728 | Test Loss: 1.3172898292541504\n",
      "Epoch 7382 | Train Loss: 1.0839359760284424 | Test Loss: 1.1167187690734863\n",
      "Epoch 7383 | Train Loss: 1.225830078125 | Test Loss: 1.317301869392395\n",
      "Epoch 7384 | Train Loss: 1.083916187286377 | Test Loss: 1.1167916059494019\n",
      "Epoch 7385 | Train Loss: 1.2258195877075195 | Test Loss: 1.3172965049743652\n",
      "Epoch 7386 | Train Loss: 1.0839003324508667 | Test Loss: 1.1166996955871582\n",
      "Epoch 7387 | Train Loss: 1.2258038520812988 | Test Loss: 1.3172576427459717\n",
      "Epoch 7388 | Train Loss: 1.0838762521743774 | Test Loss: 1.1168087720870972\n",
      "Epoch 7389 | Train Loss: 1.225794792175293 | Test Loss: 1.3173375129699707\n",
      "Epoch 7390 | Train Loss: 1.0838661193847656 | Test Loss: 1.1166410446166992\n",
      "Epoch 7391 | Train Loss: 1.2257797718048096 | Test Loss: 1.3172584772109985\n",
      "Epoch 7392 | Train Loss: 1.0838388204574585 | Test Loss: 1.116821050643921\n",
      "Epoch 7393 | Train Loss: 1.225767731666565 | Test Loss: 1.3173189163208008\n",
      "Epoch 7394 | Train Loss: 1.0838202238082886 | Test Loss: 1.1166237592697144\n",
      "Epoch 7395 | Train Loss: 1.2257541418075562 | Test Loss: 1.3173052072525024\n",
      "Epoch 7396 | Train Loss: 1.0837961435317993 | Test Loss: 1.1168042421340942\n",
      "Epoch 7397 | Train Loss: 1.2257343530654907 | Test Loss: 1.317275881767273\n",
      "Epoch 7398 | Train Loss: 1.0837762355804443 | Test Loss: 1.116628646850586\n",
      "Epoch 7399 | Train Loss: 1.2257119417190552 | Test Loss: 1.3173073530197144\n",
      "Epoch 7400 | Train Loss: 1.0837547779083252 | Test Loss: 1.1167558431625366\n",
      "Epoch 7401 | Train Loss: 1.2256813049316406 | Test Loss: 1.3172317743301392\n",
      "Epoch 7402 | Train Loss: 1.0837382078170776 | Test Loss: 1.1166365146636963\n",
      "Epoch 7403 | Train Loss: 1.225641131401062 | Test Loss: 1.3173161745071411\n",
      "Epoch 7404 | Train Loss: 1.083717703819275 | Test Loss: 1.1166911125183105\n",
      "Epoch 7405 | Train Loss: 1.2256075143814087 | Test Loss: 1.3172833919525146\n",
      "Epoch 7406 | Train Loss: 1.0837098360061646 | Test Loss: 1.1166176795959473\n",
      "Epoch 7407 | Train Loss: 1.2255665063858032 | Test Loss: 1.3173325061798096\n",
      "Epoch 7408 | Train Loss: 1.083685040473938 | Test Loss: 1.1166576147079468\n",
      "Epoch 7409 | Train Loss: 1.2255520820617676 | Test Loss: 1.3172571659088135\n",
      "Epoch 7410 | Train Loss: 1.083670735359192 | Test Loss: 1.1166125535964966\n",
      "Epoch 7411 | Train Loss: 1.225515365600586 | Test Loss: 1.3173620700836182\n",
      "Epoch 7412 | Train Loss: 1.0836470127105713 | Test Loss: 1.1166142225265503\n",
      "Epoch 7413 | Train Loss: 1.2254996299743652 | Test Loss: 1.317254662513733\n",
      "Epoch 7414 | Train Loss: 1.0836281776428223 | Test Loss: 1.116624116897583\n",
      "Epoch 7415 | Train Loss: 1.225476861000061 | Test Loss: 1.3173527717590332\n",
      "Epoch 7416 | Train Loss: 1.0836060047149658 | Test Loss: 1.1165673732757568\n",
      "Epoch 7417 | Train Loss: 1.2254647016525269 | Test Loss: 1.3172345161437988\n",
      "Epoch 7418 | Train Loss: 1.0835920572280884 | Test Loss: 1.116632103919983\n",
      "Epoch 7419 | Train Loss: 1.2254358530044556 | Test Loss: 1.317339301109314\n",
      "Epoch 7420 | Train Loss: 1.0835703611373901 | Test Loss: 1.116550087928772\n",
      "Epoch 7421 | Train Loss: 1.2254198789596558 | Test Loss: 1.3172733783721924\n",
      "Epoch 7422 | Train Loss: 1.0835520029067993 | Test Loss: 1.116620659828186\n",
      "Epoch 7423 | Train Loss: 1.2253916263580322 | Test Loss: 1.3173104524612427\n",
      "Epoch 7424 | Train Loss: 1.0835323333740234 | Test Loss: 1.116531252861023\n",
      "Epoch 7425 | Train Loss: 1.225376009941101 | Test Loss: 1.3172658681869507\n",
      "Epoch 7426 | Train Loss: 1.0835145711898804 | Test Loss: 1.1165854930877686\n",
      "Epoch 7427 | Train Loss: 1.2253398895263672 | Test Loss: 1.3172770738601685\n",
      "Epoch 7428 | Train Loss: 1.0834941864013672 | Test Loss: 1.1165302991867065\n",
      "Epoch 7429 | Train Loss: 1.2253155708312988 | Test Loss: 1.3173182010650635\n",
      "Epoch 7430 | Train Loss: 1.08347749710083 | Test Loss: 1.1165390014648438\n",
      "Epoch 7431 | Train Loss: 1.2252880334854126 | Test Loss: 1.3172929286956787\n",
      "Epoch 7432 | Train Loss: 1.0834590196609497 | Test Loss: 1.1165242195129395\n",
      "Epoch 7433 | Train Loss: 1.2252694368362427 | Test Loss: 1.317291021347046\n",
      "Epoch 7434 | Train Loss: 1.083437204360962 | Test Loss: 1.1165094375610352\n",
      "Epoch 7435 | Train Loss: 1.2252355813980103 | Test Loss: 1.3172943592071533\n",
      "Epoch 7436 | Train Loss: 1.0834256410598755 | Test Loss: 1.1165038347244263\n",
      "Epoch 7437 | Train Loss: 1.2252130508422852 | Test Loss: 1.317286729812622\n",
      "Epoch 7438 | Train Loss: 1.0834029912948608 | Test Loss: 1.116491436958313\n",
      "Epoch 7439 | Train Loss: 1.2251900434494019 | Test Loss: 1.3172926902770996\n",
      "Epoch 7440 | Train Loss: 1.083389163017273 | Test Loss: 1.11648428440094\n",
      "Epoch 7441 | Train Loss: 1.2251635789871216 | Test Loss: 1.3172953128814697\n",
      "Epoch 7442 | Train Loss: 1.0833661556243896 | Test Loss: 1.1164748668670654\n",
      "Epoch 7443 | Train Loss: 1.2251455783843994 | Test Loss: 1.317275881767273\n",
      "Epoch 7444 | Train Loss: 1.083351492881775 | Test Loss: 1.1164731979370117\n",
      "Epoch 7445 | Train Loss: 1.225118637084961 | Test Loss: 1.3173056840896606\n",
      "Epoch 7446 | Train Loss: 1.08333420753479 | Test Loss: 1.1164461374282837\n",
      "Epoch 7447 | Train Loss: 1.225098967552185 | Test Loss: 1.31728196144104\n",
      "Epoch 7448 | Train Loss: 1.0833183526992798 | Test Loss: 1.1164710521697998\n",
      "Epoch 7449 | Train Loss: 1.2250760793685913 | Test Loss: 1.317322015762329\n",
      "Epoch 7450 | Train Loss: 1.0832982063293457 | Test Loss: 1.1164202690124512\n",
      "Epoch 7451 | Train Loss: 1.2250553369522095 | Test Loss: 1.317293643951416\n",
      "Epoch 7452 | Train Loss: 1.0832881927490234 | Test Loss: 1.1164722442626953\n",
      "Epoch 7453 | Train Loss: 1.2250287532806396 | Test Loss: 1.3173044919967651\n",
      "Epoch 7454 | Train Loss: 1.0832628011703491 | Test Loss: 1.1164039373397827\n",
      "Epoch 7455 | Train Loss: 1.2250148057937622 | Test Loss: 1.3172823190689087\n",
      "Epoch 7456 | Train Loss: 1.083250641822815 | Test Loss: 1.1164555549621582\n",
      "Epoch 7457 | Train Loss: 1.2249860763549805 | Test Loss: 1.3173013925552368\n",
      "Epoch 7458 | Train Loss: 1.083229422569275 | Test Loss: 1.1163878440856934\n",
      "Epoch 7459 | Train Loss: 1.224977731704712 | Test Loss: 1.3172916173934937\n",
      "Epoch 7460 | Train Loss: 1.0832139253616333 | Test Loss: 1.1164432764053345\n",
      "Epoch 7461 | Train Loss: 1.2249490022659302 | Test Loss: 1.3173437118530273\n",
      "Epoch 7462 | Train Loss: 1.0831913948059082 | Test Loss: 1.1163593530654907\n",
      "Epoch 7463 | Train Loss: 1.2249374389648438 | Test Loss: 1.3172900676727295\n",
      "Epoch 7464 | Train Loss: 1.0831769704818726 | Test Loss: 1.1164287328720093\n",
      "Epoch 7465 | Train Loss: 1.2249107360839844 | Test Loss: 1.3173285722732544\n",
      "Epoch 7466 | Train Loss: 1.083153247833252 | Test Loss: 1.1163471937179565\n",
      "Epoch 7467 | Train Loss: 1.2248972654342651 | Test Loss: 1.3172898292541504\n",
      "Epoch 7468 | Train Loss: 1.0831353664398193 | Test Loss: 1.1164220571517944\n",
      "Epoch 7469 | Train Loss: 1.2248705625534058 | Test Loss: 1.317335605621338\n",
      "Epoch 7470 | Train Loss: 1.083115577697754 | Test Loss: 1.1163147687911987\n",
      "Epoch 7471 | Train Loss: 1.2248585224151611 | Test Loss: 1.3172991275787354\n",
      "Epoch 7472 | Train Loss: 1.0830986499786377 | Test Loss: 1.116411805152893\n",
      "Epoch 7473 | Train Loss: 1.2248222827911377 | Test Loss: 1.3173073530197144\n",
      "Epoch 7474 | Train Loss: 1.0830801725387573 | Test Loss: 1.1163151264190674\n",
      "Epoch 7475 | Train Loss: 1.2248085737228394 | Test Loss: 1.317321538925171\n",
      "Epoch 7476 | Train Loss: 1.0830585956573486 | Test Loss: 1.1163755655288696\n",
      "Epoch 7477 | Train Loss: 1.2247790098190308 | Test Loss: 1.3173521757125854\n",
      "Epoch 7478 | Train Loss: 1.083044171333313 | Test Loss: 1.1162859201431274\n",
      "Epoch 7479 | Train Loss: 1.2247596979141235 | Test Loss: 1.3172907829284668\n",
      "Epoch 7480 | Train Loss: 1.08302903175354 | Test Loss: 1.1163562536239624\n",
      "Epoch 7481 | Train Loss: 1.2247302532196045 | Test Loss: 1.3173352479934692\n",
      "Epoch 7482 | Train Loss: 1.083011269569397 | Test Loss: 1.1162827014923096\n",
      "Epoch 7483 | Train Loss: 1.2247138023376465 | Test Loss: 1.3173083066940308\n",
      "Epoch 7484 | Train Loss: 1.0829907655715942 | Test Loss: 1.11634361743927\n",
      "Epoch 7485 | Train Loss: 1.2246862649917603 | Test Loss: 1.317362904548645\n",
      "Epoch 7486 | Train Loss: 1.0829812288284302 | Test Loss: 1.1162558794021606\n",
      "Epoch 7487 | Train Loss: 1.2246736288070679 | Test Loss: 1.3172944784164429\n",
      "Epoch 7488 | Train Loss: 1.0829633474349976 | Test Loss: 1.1163384914398193\n",
      "Epoch 7489 | Train Loss: 1.2246415615081787 | Test Loss: 1.317392349243164\n",
      "Epoch 7490 | Train Loss: 1.0829509496688843 | Test Loss: 1.116229772567749\n",
      "Epoch 7491 | Train Loss: 1.2246276140213013 | Test Loss: 1.3172955513000488\n",
      "Epoch 7492 | Train Loss: 1.0829260349273682 | Test Loss: 1.1163333654403687\n",
      "Epoch 7493 | Train Loss: 1.2246121168136597 | Test Loss: 1.3173776865005493\n",
      "Epoch 7494 | Train Loss: 1.0829167366027832 | Test Loss: 1.1162081956863403\n",
      "Epoch 7495 | Train Loss: 1.224593997001648 | Test Loss: 1.317260980606079\n",
      "Epoch 7496 | Train Loss: 1.082901120185852 | Test Loss: 1.1163157224655151\n",
      "Epoch 7497 | Train Loss: 1.224570631980896 | Test Loss: 1.3173656463623047\n",
      "Epoch 7498 | Train Loss: 1.0828794240951538 | Test Loss: 1.1162046194076538\n",
      "Epoch 7499 | Train Loss: 1.2245581150054932 | Test Loss: 1.317299246788025\n",
      "Epoch 7500 | Train Loss: 1.0828588008880615 | Test Loss: 1.1163032054901123\n",
      "Epoch 7501 | Train Loss: 1.2245458364486694 | Test Loss: 1.3173797130584717\n",
      "Epoch 7502 | Train Loss: 1.0828443765640259 | Test Loss: 1.116202712059021\n",
      "Epoch 7503 | Train Loss: 1.2245365381240845 | Test Loss: 1.3172775506973267\n",
      "Epoch 7504 | Train Loss: 1.0828218460083008 | Test Loss: 1.116294503211975\n",
      "Epoch 7505 | Train Loss: 1.2245118618011475 | Test Loss: 1.317407488822937\n",
      "Epoch 7506 | Train Loss: 1.082811713218689 | Test Loss: 1.1161837577819824\n",
      "Epoch 7507 | Train Loss: 1.2244980335235596 | Test Loss: 1.3172763586044312\n",
      "Epoch 7508 | Train Loss: 1.082783818244934 | Test Loss: 1.1163138151168823\n",
      "Epoch 7509 | Train Loss: 1.224483609199524 | Test Loss: 1.3174073696136475\n",
      "Epoch 7510 | Train Loss: 1.0827720165252686 | Test Loss: 1.116166114807129\n",
      "Epoch 7511 | Train Loss: 1.2244609594345093 | Test Loss: 1.3173049688339233\n",
      "Epoch 7512 | Train Loss: 1.0827510356903076 | Test Loss: 1.1162861585617065\n",
      "Epoch 7513 | Train Loss: 1.2244333028793335 | Test Loss: 1.3173307180404663\n",
      "Epoch 7514 | Train Loss: 1.082733392715454 | Test Loss: 1.1161843538284302\n",
      "Epoch 7515 | Train Loss: 1.2244007587432861 | Test Loss: 1.3173645734786987\n",
      "Epoch 7516 | Train Loss: 1.0827147960662842 | Test Loss: 1.1162281036376953\n",
      "Epoch 7517 | Train Loss: 1.224368691444397 | Test Loss: 1.3172903060913086\n",
      "Epoch 7518 | Train Loss: 1.0827003717422485 | Test Loss: 1.116197109222412\n",
      "Epoch 7519 | Train Loss: 1.2243337631225586 | Test Loss: 1.3173917531967163\n",
      "Epoch 7520 | Train Loss: 1.0826793909072876 | Test Loss: 1.116172432899475\n",
      "Epoch 7521 | Train Loss: 1.2243033647537231 | Test Loss: 1.3172752857208252\n",
      "Epoch 7522 | Train Loss: 1.0826674699783325 | Test Loss: 1.116179347038269\n",
      "Epoch 7523 | Train Loss: 1.2242658138275146 | Test Loss: 1.3173792362213135\n",
      "Epoch 7524 | Train Loss: 1.0826524496078491 | Test Loss: 1.1161388158798218\n",
      "Epoch 7525 | Train Loss: 1.2242408990859985 | Test Loss: 1.3172613382339478\n",
      "Epoch 7526 | Train Loss: 1.0826373100280762 | Test Loss: 1.1161563396453857\n",
      "Epoch 7527 | Train Loss: 1.22420334815979 | Test Loss: 1.3173527717590332\n",
      "Epoch 7528 | Train Loss: 1.082619547843933 | Test Loss: 1.116118311882019\n",
      "Epoch 7529 | Train Loss: 1.2241848707199097 | Test Loss: 1.317227840423584\n",
      "Epoch 7530 | Train Loss: 1.0826059579849243 | Test Loss: 1.1161285638809204\n",
      "Epoch 7531 | Train Loss: 1.2241536378860474 | Test Loss: 1.3173940181732178\n",
      "Epoch 7532 | Train Loss: 1.082586646080017 | Test Loss: 1.1160839796066284\n",
      "Epoch 7533 | Train Loss: 1.224142074584961 | Test Loss: 1.31719970703125\n",
      "Epoch 7534 | Train Loss: 1.0825697183609009 | Test Loss: 1.1161437034606934\n",
      "Epoch 7535 | Train Loss: 1.2241277694702148 | Test Loss: 1.3174681663513184\n",
      "Epoch 7536 | Train Loss: 1.0825574398040771 | Test Loss: 1.116034746170044\n",
      "Epoch 7537 | Train Loss: 1.2241266965866089 | Test Loss: 1.3172129392623901\n",
      "Epoch 7538 | Train Loss: 1.0825414657592773 | Test Loss: 1.1161754131317139\n",
      "Epoch 7539 | Train Loss: 1.224116563796997 | Test Loss: 1.317444920539856\n",
      "Epoch 7540 | Train Loss: 1.0825227499008179 | Test Loss: 1.1160149574279785\n",
      "Epoch 7541 | Train Loss: 1.2241203784942627 | Test Loss: 1.3172506093978882\n",
      "Epoch 7542 | Train Loss: 1.0825042724609375 | Test Loss: 1.1161954402923584\n",
      "Epoch 7543 | Train Loss: 1.2241125106811523 | Test Loss: 1.3174151182174683\n",
      "Epoch 7544 | Train Loss: 1.0824825763702393 | Test Loss: 1.116017460823059\n",
      "Epoch 7545 | Train Loss: 1.224117636680603 | Test Loss: 1.3173085451126099\n",
      "Epoch 7546 | Train Loss: 1.0824638605117798 | Test Loss: 1.1161844730377197\n",
      "Epoch 7547 | Train Loss: 1.2240773439407349 | Test Loss: 1.317380428314209\n",
      "Epoch 7548 | Train Loss: 1.0824402570724487 | Test Loss: 1.1160191297531128\n",
      "Epoch 7549 | Train Loss: 1.2240720987319946 | Test Loss: 1.3173136711120605\n",
      "Epoch 7550 | Train Loss: 1.0824192762374878 | Test Loss: 1.1161761283874512\n",
      "Epoch 7551 | Train Loss: 1.2240428924560547 | Test Loss: 1.317380666732788\n",
      "Epoch 7552 | Train Loss: 1.0824024677276611 | Test Loss: 1.1159943342208862\n",
      "Epoch 7553 | Train Loss: 1.2240240573883057 | Test Loss: 1.3173317909240723\n",
      "Epoch 7554 | Train Loss: 1.0823805332183838 | Test Loss: 1.116152048110962\n",
      "Epoch 7555 | Train Loss: 1.2239683866500854 | Test Loss: 1.3173938989639282\n",
      "Epoch 7556 | Train Loss: 1.0823652744293213 | Test Loss: 1.1159614324569702\n",
      "Epoch 7557 | Train Loss: 1.223945140838623 | Test Loss: 1.3173831701278687\n",
      "Epoch 7558 | Train Loss: 1.0823450088500977 | Test Loss: 1.1160914897918701\n",
      "Epoch 7559 | Train Loss: 1.2238905429840088 | Test Loss: 1.3173412084579468\n",
      "Epoch 7560 | Train Loss: 1.0823302268981934 | Test Loss: 1.1159734725952148\n",
      "Epoch 7561 | Train Loss: 1.2238492965698242 | Test Loss: 1.3174409866333008\n",
      "Epoch 7562 | Train Loss: 1.0823177099227905 | Test Loss: 1.116002082824707\n",
      "Epoch 7563 | Train Loss: 1.2238126993179321 | Test Loss: 1.3173213005065918\n",
      "Epoch 7564 | Train Loss: 1.0822988748550415 | Test Loss: 1.1159900426864624\n",
      "Epoch 7565 | Train Loss: 1.2237848043441772 | Test Loss: 1.3174192905426025\n",
      "Epoch 7566 | Train Loss: 1.082283616065979 | Test Loss: 1.1159451007843018\n",
      "Epoch 7567 | Train Loss: 1.2237629890441895 | Test Loss: 1.3173412084579468\n",
      "Epoch 7568 | Train Loss: 1.0822683572769165 | Test Loss: 1.1160006523132324\n",
      "Epoch 7569 | Train Loss: 1.2237417697906494 | Test Loss: 1.3173778057098389\n",
      "Epoch 7570 | Train Loss: 1.0822519063949585 | Test Loss: 1.1159107685089111\n",
      "Epoch 7571 | Train Loss: 1.2237303256988525 | Test Loss: 1.3173773288726807\n",
      "Epoch 7572 | Train Loss: 1.082234263420105 | Test Loss: 1.1160095930099487\n",
      "Epoch 7573 | Train Loss: 1.2237119674682617 | Test Loss: 1.3173725605010986\n",
      "Epoch 7574 | Train Loss: 1.082220435142517 | Test Loss: 1.1158785820007324\n",
      "Epoch 7575 | Train Loss: 1.2237122058868408 | Test Loss: 1.3173668384552002\n",
      "Epoch 7576 | Train Loss: 1.0822018384933472 | Test Loss: 1.1160427331924438\n",
      "Epoch 7577 | Train Loss: 1.2237000465393066 | Test Loss: 1.3174210786819458\n",
      "Epoch 7578 | Train Loss: 1.0821925401687622 | Test Loss: 1.1158472299575806\n",
      "Epoch 7579 | Train Loss: 1.223702311515808 | Test Loss: 1.3173611164093018\n",
      "Epoch 7580 | Train Loss: 1.0821644067764282 | Test Loss: 1.1160707473754883\n",
      "Epoch 7581 | Train Loss: 1.2236851453781128 | Test Loss: 1.31741464138031\n",
      "Epoch 7582 | Train Loss: 1.082155466079712 | Test Loss: 1.1158195734024048\n",
      "Epoch 7583 | Train Loss: 1.2236865758895874 | Test Loss: 1.31740140914917\n",
      "Epoch 7584 | Train Loss: 1.0821311473846436 | Test Loss: 1.1160743236541748\n",
      "Epoch 7585 | Train Loss: 1.2236613035202026 | Test Loss: 1.317365288734436\n",
      "Epoch 7586 | Train Loss: 1.082118272781372 | Test Loss: 1.115813136100769\n",
      "Epoch 7587 | Train Loss: 1.2236554622650146 | Test Loss: 1.3174406290054321\n",
      "Epoch 7588 | Train Loss: 1.0820984840393066 | Test Loss: 1.1160380840301514\n",
      "Epoch 7589 | Train Loss: 1.223620891571045 | Test Loss: 1.3173662424087524\n",
      "Epoch 7590 | Train Loss: 1.082088828086853 | Test Loss: 1.1158288717269897\n",
      "Epoch 7591 | Train Loss: 1.2235981225967407 | Test Loss: 1.3174456357955933\n",
      "Epoch 7592 | Train Loss: 1.082061767578125 | Test Loss: 1.1159993410110474\n",
      "Epoch 7593 | Train Loss: 1.2235767841339111 | Test Loss: 1.3173747062683105\n",
      "Epoch 7594 | Train Loss: 1.0820661783218384 | Test Loss: 1.1158421039581299\n",
      "Epoch 7595 | Train Loss: 1.2235370874404907 | Test Loss: 1.317468285560608\n",
      "Epoch 7596 | Train Loss: 1.082031011581421 | Test Loss: 1.1159589290618896\n",
      "Epoch 7597 | Train Loss: 1.2235255241394043 | Test Loss: 1.317347764968872\n",
      "Epoch 7598 | Train Loss: 1.0820283889770508 | Test Loss: 1.115862250328064\n",
      "Epoch 7599 | Train Loss: 1.22348952293396 | Test Loss: 1.3175092935562134\n",
      "Epoch 7600 | Train Loss: 1.08200204372406 | Test Loss: 1.1159257888793945\n",
      "Epoch 7601 | Train Loss: 1.2234883308410645 | Test Loss: 1.31734037399292\n",
      "Epoch 7602 | Train Loss: 1.0819885730743408 | Test Loss: 1.1158913373947144\n",
      "Epoch 7603 | Train Loss: 1.2234666347503662 | Test Loss: 1.3175170421600342\n",
      "Epoch 7604 | Train Loss: 1.0819634199142456 | Test Loss: 1.11589515209198\n",
      "Epoch 7605 | Train Loss: 1.2234587669372559 | Test Loss: 1.3173421621322632\n",
      "Epoch 7606 | Train Loss: 1.0819437503814697 | Test Loss: 1.1159062385559082\n",
      "Epoch 7607 | Train Loss: 1.223426342010498 | Test Loss: 1.3174889087677002\n",
      "Epoch 7608 | Train Loss: 1.0819205045700073 | Test Loss: 1.115853190422058\n",
      "Epoch 7609 | Train Loss: 1.223408579826355 | Test Loss: 1.3173257112503052\n",
      "Epoch 7610 | Train Loss: 1.0819001197814941 | Test Loss: 1.1158968210220337\n",
      "Epoch 7611 | Train Loss: 1.2233680486679077 | Test Loss: 1.3174492120742798\n",
      "Epoch 7612 | Train Loss: 1.0818802118301392 | Test Loss: 1.115822434425354\n",
      "Epoch 7613 | Train Loss: 1.223339319229126 | Test Loss: 1.3173725605010986\n",
      "Epoch 7614 | Train Loss: 1.0818593502044678 | Test Loss: 1.115858793258667\n",
      "Epoch 7615 | Train Loss: 1.2232991456985474 | Test Loss: 1.3174214363098145\n",
      "Epoch 7616 | Train Loss: 1.081841230392456 | Test Loss: 1.1157968044281006\n",
      "Epoch 7617 | Train Loss: 1.2232673168182373 | Test Loss: 1.31745183467865\n",
      "Epoch 7618 | Train Loss: 1.0818251371383667 | Test Loss: 1.1158167123794556\n",
      "Epoch 7619 | Train Loss: 1.2232275009155273 | Test Loss: 1.3174090385437012\n",
      "Epoch 7620 | Train Loss: 1.081810712814331 | Test Loss: 1.1157718896865845\n",
      "Epoch 7621 | Train Loss: 1.2232028245925903 | Test Loss: 1.317480444908142\n",
      "Epoch 7622 | Train Loss: 1.0817922353744507 | Test Loss: 1.1157652139663696\n",
      "Epoch 7623 | Train Loss: 1.2231619358062744 | Test Loss: 1.3174118995666504\n",
      "Epoch 7624 | Train Loss: 1.0817831754684448 | Test Loss: 1.1157580614089966\n",
      "Epoch 7625 | Train Loss: 1.223141074180603 | Test Loss: 1.3174421787261963\n",
      "Epoch 7626 | Train Loss: 1.0817623138427734 | Test Loss: 1.1157352924346924\n",
      "Epoch 7627 | Train Loss: 1.2231059074401855 | Test Loss: 1.317413568496704\n",
      "Epoch 7628 | Train Loss: 1.0817499160766602 | Test Loss: 1.1157501935958862\n",
      "Epoch 7629 | Train Loss: 1.2230796813964844 | Test Loss: 1.3174591064453125\n",
      "Epoch 7630 | Train Loss: 1.0817304849624634 | Test Loss: 1.1157230138778687\n",
      "Epoch 7631 | Train Loss: 1.2230525016784668 | Test Loss: 1.3173816204071045\n",
      "Epoch 7632 | Train Loss: 1.0817172527313232 | Test Loss: 1.1157394647598267\n",
      "Epoch 7633 | Train Loss: 1.2230361700057983 | Test Loss: 1.3175064325332642\n",
      "Epoch 7634 | Train Loss: 1.0816997289657593 | Test Loss: 1.115688681602478\n",
      "Epoch 7635 | Train Loss: 1.2230091094970703 | Test Loss: 1.3173621892929077\n",
      "Epoch 7636 | Train Loss: 1.08168625831604 | Test Loss: 1.1157395839691162\n",
      "Epoch 7637 | Train Loss: 1.2229951620101929 | Test Loss: 1.3175480365753174\n",
      "Epoch 7638 | Train Loss: 1.0816730260849 | Test Loss: 1.1156460046768188\n",
      "Epoch 7639 | Train Loss: 1.2229763269424438 | Test Loss: 1.3173893690109253\n",
      "Epoch 7640 | Train Loss: 1.0816551446914673 | Test Loss: 1.11574387550354\n",
      "Epoch 7641 | Train Loss: 1.2229607105255127 | Test Loss: 1.3175495862960815\n",
      "Epoch 7642 | Train Loss: 1.0816396474838257 | Test Loss: 1.1156169176101685\n",
      "Epoch 7643 | Train Loss: 1.2229582071304321 | Test Loss: 1.3174179792404175\n",
      "Epoch 7644 | Train Loss: 1.0816158056259155 | Test Loss: 1.1157612800598145\n",
      "Epoch 7645 | Train Loss: 1.222949504852295 | Test Loss: 1.3175127506256104\n",
      "Epoch 7646 | Train Loss: 1.0816007852554321 | Test Loss: 1.1155966520309448\n",
      "Epoch 7647 | Train Loss: 1.2229440212249756 | Test Loss: 1.3174374103546143\n",
      "Epoch 7648 | Train Loss: 1.0815781354904175 | Test Loss: 1.1157664060592651\n",
      "Epoch 7649 | Train Loss: 1.2229307889938354 | Test Loss: 1.3174813985824585\n",
      "Epoch 7650 | Train Loss: 1.0815606117248535 | Test Loss: 1.1155911684036255\n",
      "Epoch 7651 | Train Loss: 1.2229249477386475 | Test Loss: 1.3174604177474976\n",
      "Epoch 7652 | Train Loss: 1.0815410614013672 | Test Loss: 1.1157808303833008\n",
      "Epoch 7653 | Train Loss: 1.2229114770889282 | Test Loss: 1.3175119161605835\n",
      "Epoch 7654 | Train Loss: 1.0815266370773315 | Test Loss: 1.1155738830566406\n",
      "Epoch 7655 | Train Loss: 1.2229127883911133 | Test Loss: 1.317449927330017\n",
      "Epoch 7656 | Train Loss: 1.0815058946609497 | Test Loss: 1.1157739162445068\n",
      "Epoch 7657 | Train Loss: 1.2228684425354004 | Test Loss: 1.3175157308578491\n",
      "Epoch 7658 | Train Loss: 1.0814934968948364 | Test Loss: 1.1155428886413574\n",
      "Epoch 7659 | Train Loss: 1.2228528261184692 | Test Loss: 1.3174586296081543\n",
      "Epoch 7660 | Train Loss: 1.0814710855484009 | Test Loss: 1.1157419681549072\n",
      "Epoch 7661 | Train Loss: 1.2228103876113892 | Test Loss: 1.317488431930542\n",
      "Epoch 7662 | Train Loss: 1.0814558267593384 | Test Loss: 1.115541934967041\n",
      "Epoch 7663 | Train Loss: 1.2227814197540283 | Test Loss: 1.3174996376037598\n",
      "Epoch 7664 | Train Loss: 1.0814385414123535 | Test Loss: 1.1156853437423706\n",
      "Epoch 7665 | Train Loss: 1.2227309942245483 | Test Loss: 1.3174716234207153\n",
      "Epoch 7666 | Train Loss: 1.0814257860183716 | Test Loss: 1.115545630455017\n",
      "Epoch 7667 | Train Loss: 1.2226954698562622 | Test Loss: 1.3175212144851685\n",
      "Epoch 7668 | Train Loss: 1.0814085006713867 | Test Loss: 1.1156113147735596\n",
      "Epoch 7669 | Train Loss: 1.2226605415344238 | Test Loss: 1.317435622215271\n",
      "Epoch 7670 | Train Loss: 1.0813941955566406 | Test Loss: 1.1155669689178467\n",
      "Epoch 7671 | Train Loss: 1.2226314544677734 | Test Loss: 1.3174991607666016\n",
      "Epoch 7672 | Train Loss: 1.0813771486282349 | Test Loss: 1.115553379058838\n",
      "Epoch 7673 | Train Loss: 1.2226067781448364 | Test Loss: 1.3174494504928589\n",
      "Epoch 7674 | Train Loss: 1.0813627243041992 | Test Loss: 1.1155861616134644\n",
      "Epoch 7675 | Train Loss: 1.22258460521698 | Test Loss: 1.317497730255127\n",
      "Epoch 7676 | Train Loss: 1.0813430547714233 | Test Loss: 1.1155037879943848\n",
      "Epoch 7677 | Train Loss: 1.2225650548934937 | Test Loss: 1.317501425743103\n",
      "Epoch 7678 | Train Loss: 1.0813324451446533 | Test Loss: 1.1155812740325928\n",
      "Epoch 7679 | Train Loss: 1.222544550895691 | Test Loss: 1.3175092935562134\n",
      "Epoch 7680 | Train Loss: 1.0813158750534058 | Test Loss: 1.1154850721359253\n",
      "Epoch 7681 | Train Loss: 1.2225329875946045 | Test Loss: 1.3174684047698975\n",
      "Epoch 7682 | Train Loss: 1.0813024044036865 | Test Loss: 1.1155809164047241\n",
      "Epoch 7683 | Train Loss: 1.2225149869918823 | Test Loss: 1.317504644393921\n",
      "Epoch 7684 | Train Loss: 1.0812898874282837 | Test Loss: 1.1154541969299316\n",
      "Epoch 7685 | Train Loss: 1.2225110530853271 | Test Loss: 1.3174539804458618\n",
      "Epoch 7686 | Train Loss: 1.0812697410583496 | Test Loss: 1.115595817565918\n",
      "Epoch 7687 | Train Loss: 1.222488522529602 | Test Loss: 1.3175073862075806\n",
      "Epoch 7688 | Train Loss: 1.081254005432129 | Test Loss: 1.1154234409332275\n",
      "Epoch 7689 | Train Loss: 1.2224836349487305 | Test Loss: 1.317484974861145\n",
      "Epoch 7690 | Train Loss: 1.0812351703643799 | Test Loss: 1.1156182289123535\n",
      "Epoch 7691 | Train Loss: 1.2224653959274292 | Test Loss: 1.3175160884857178\n",
      "Epoch 7692 | Train Loss: 1.0812163352966309 | Test Loss: 1.1153991222381592\n",
      "Epoch 7693 | Train Loss: 1.2224504947662354 | Test Loss: 1.3175220489501953\n",
      "Epoch 7694 | Train Loss: 1.0812044143676758 | Test Loss: 1.115593433380127\n",
      "Epoch 7695 | Train Loss: 1.222416639328003 | Test Loss: 1.317488193511963\n",
      "Epoch 7696 | Train Loss: 1.0811878442764282 | Test Loss: 1.1154030561447144\n",
      "Epoch 7697 | Train Loss: 1.222394347190857 | Test Loss: 1.3175345659255981\n",
      "Epoch 7698 | Train Loss: 1.0811705589294434 | Test Loss: 1.115540862083435\n",
      "Epoch 7699 | Train Loss: 1.2223541736602783 | Test Loss: 1.3175092935562134\n",
      "Epoch 7700 | Train Loss: 1.0811625719070435 | Test Loss: 1.115403652191162\n",
      "Epoch 7701 | Train Loss: 1.2223398685455322 | Test Loss: 1.3175522089004517\n",
      "Epoch 7702 | Train Loss: 1.0811399221420288 | Test Loss: 1.1155040264129639\n",
      "Epoch 7703 | Train Loss: 1.2223080396652222 | Test Loss: 1.3174872398376465\n",
      "Epoch 7704 | Train Loss: 1.0811328887939453 | Test Loss: 1.1153984069824219\n",
      "Epoch 7705 | Train Loss: 1.2222917079925537 | Test Loss: 1.3175584077835083\n",
      "Epoch 7706 | Train Loss: 1.081117033958435 | Test Loss: 1.115497350692749\n",
      "Epoch 7707 | Train Loss: 1.2222756147384644 | Test Loss: 1.3174917697906494\n",
      "Epoch 7708 | Train Loss: 1.0811092853546143 | Test Loss: 1.1154050827026367\n",
      "Epoch 7709 | Train Loss: 1.222265362739563 | Test Loss: 1.3175865411758423\n",
      "Epoch 7710 | Train Loss: 1.0810905694961548 | Test Loss: 1.1154859066009521\n",
      "Epoch 7711 | Train Loss: 1.2222416400909424 | Test Loss: 1.3175156116485596\n",
      "Epoch 7712 | Train Loss: 1.0810939073562622 | Test Loss: 1.1153932809829712\n",
      "Epoch 7713 | Train Loss: 1.2222257852554321 | Test Loss: 1.3176003694534302\n",
      "Epoch 7714 | Train Loss: 1.0810643434524536 | Test Loss: 1.1154779195785522\n",
      "Epoch 7715 | Train Loss: 1.2222189903259277 | Test Loss: 1.3174924850463867\n",
      "Epoch 7716 | Train Loss: 1.0810643434524536 | Test Loss: 1.1153972148895264\n",
      "Epoch 7717 | Train Loss: 1.2222087383270264 | Test Loss: 1.3176140785217285\n",
      "Epoch 7718 | Train Loss: 1.0810348987579346 | Test Loss: 1.1154699325561523\n",
      "Epoch 7719 | Train Loss: 1.2222092151641846 | Test Loss: 1.3175246715545654\n",
      "Epoch 7720 | Train Loss: 1.0810283422470093 | Test Loss: 1.115403175354004\n",
      "Epoch 7721 | Train Loss: 1.2222000360488892 | Test Loss: 1.3175952434539795\n",
      "Epoch 7722 | Train Loss: 1.0809999704360962 | Test Loss: 1.1154768466949463\n",
      "Epoch 7723 | Train Loss: 1.2221968173980713 | Test Loss: 1.3176112174987793\n",
      "Epoch 7724 | Train Loss: 1.0810023546218872 | Test Loss: 1.1153943538665771\n",
      "Epoch 7725 | Train Loss: 1.2221852540969849 | Test Loss: 1.3175580501556396\n",
      "Epoch 7726 | Train Loss: 1.0809732675552368 | Test Loss: 1.1155149936676025\n",
      "Epoch 7727 | Train Loss: 1.2221906185150146 | Test Loss: 1.3176639080047607\n",
      "Epoch 7728 | Train Loss: 1.0809749364852905 | Test Loss: 1.1153758764266968\n",
      "Epoch 7729 | Train Loss: 1.222174048423767 | Test Loss: 1.3175443410873413\n",
      "Epoch 7730 | Train Loss: 1.080946683883667 | Test Loss: 1.1155191659927368\n",
      "Epoch 7731 | Train Loss: 1.2221797704696655 | Test Loss: 1.317686915397644\n",
      "Epoch 7732 | Train Loss: 1.080942153930664 | Test Loss: 1.1153737306594849\n",
      "Epoch 7733 | Train Loss: 1.2221710681915283 | Test Loss: 1.3175225257873535\n",
      "Epoch 7734 | Train Loss: 1.0809112787246704 | Test Loss: 1.115505337715149\n",
      "Epoch 7735 | Train Loss: 1.2221710681915283 | Test Loss: 1.3177376985549927\n",
      "Epoch 7736 | Train Loss: 1.0809065103530884 | Test Loss: 1.115372896194458\n",
      "Epoch 7737 | Train Loss: 1.2221581935882568 | Test Loss: 1.317534327507019\n",
      "Epoch 7738 | Train Loss: 1.080867886543274 | Test Loss: 1.115493893623352\n",
      "Epoch 7739 | Train Loss: 1.22214937210083 | Test Loss: 1.3177504539489746\n",
      "Epoch 7740 | Train Loss: 1.0808541774749756 | Test Loss: 1.115374207496643\n",
      "Epoch 7741 | Train Loss: 1.2221206426620483 | Test Loss: 1.3175708055496216\n",
      "Epoch 7742 | Train Loss: 1.0808225870132446 | Test Loss: 1.1154805421829224\n",
      "Epoch 7743 | Train Loss: 1.2220993041992188 | Test Loss: 1.317694902420044\n",
      "Epoch 7744 | Train Loss: 1.0808063745498657 | Test Loss: 1.115334391593933\n",
      "Epoch 7745 | Train Loss: 1.2220667600631714 | Test Loss: 1.3176127672195435\n",
      "Epoch 7746 | Train Loss: 1.080782175064087 | Test Loss: 1.1154314279556274\n",
      "Epoch 7747 | Train Loss: 1.222010612487793 | Test Loss: 1.3176013231277466\n",
      "Epoch 7748 | Train Loss: 1.0807677507400513 | Test Loss: 1.115294337272644\n",
      "Epoch 7749 | Train Loss: 1.221948504447937 | Test Loss: 1.3176466226577759\n",
      "Epoch 7750 | Train Loss: 1.0807468891143799 | Test Loss: 1.1153589487075806\n",
      "Epoch 7751 | Train Loss: 1.2219041585922241 | Test Loss: 1.3175650835037231\n",
      "Epoch 7752 | Train Loss: 1.0807284116744995 | Test Loss: 1.1152790784835815\n",
      "Epoch 7753 | Train Loss: 1.221840262413025 | Test Loss: 1.317598819732666\n",
      "Epoch 7754 | Train Loss: 1.0807170867919922 | Test Loss: 1.1152942180633545\n",
      "Epoch 7755 | Train Loss: 1.2218049764633179 | Test Loss: 1.3175554275512695\n",
      "Epoch 7756 | Train Loss: 1.0806984901428223 | Test Loss: 1.115251064300537\n",
      "Epoch 7757 | Train Loss: 1.2217539548873901 | Test Loss: 1.3175870180130005\n",
      "Epoch 7758 | Train Loss: 1.0806890726089478 | Test Loss: 1.1152489185333252\n",
      "Epoch 7759 | Train Loss: 1.2217262983322144 | Test Loss: 1.3175731897354126\n",
      "Epoch 7760 | Train Loss: 1.0806714296340942 | Test Loss: 1.1152502298355103\n",
      "Epoch 7761 | Train Loss: 1.2216862440109253 | Test Loss: 1.3175894021987915\n",
      "Epoch 7762 | Train Loss: 1.080661416053772 | Test Loss: 1.1152093410491943\n",
      "Epoch 7763 | Train Loss: 1.2216697931289673 | Test Loss: 1.3175501823425293\n",
      "Epoch 7764 | Train Loss: 1.080645203590393 | Test Loss: 1.1152476072311401\n",
      "Epoch 7765 | Train Loss: 1.2216359376907349 | Test Loss: 1.3175747394561768\n",
      "Epoch 7766 | Train Loss: 1.0806344747543335 | Test Loss: 1.1151659488677979\n",
      "Epoch 7767 | Train Loss: 1.2216235399246216 | Test Loss: 1.3175562620162964\n",
      "Epoch 7768 | Train Loss: 1.080613374710083 | Test Loss: 1.1152487993240356\n",
      "Epoch 7769 | Train Loss: 1.2215994596481323 | Test Loss: 1.3175904750823975\n",
      "Epoch 7770 | Train Loss: 1.0806021690368652 | Test Loss: 1.1151384115219116\n",
      "Epoch 7771 | Train Loss: 1.2215845584869385 | Test Loss: 1.3175859451293945\n",
      "Epoch 7772 | Train Loss: 1.0805801153182983 | Test Loss: 1.115247368812561\n",
      "Epoch 7773 | Train Loss: 1.2215667963027954 | Test Loss: 1.3175972700119019\n",
      "Epoch 7774 | Train Loss: 1.0805690288543701 | Test Loss: 1.1151067018508911\n",
      "Epoch 7775 | Train Loss: 1.2215518951416016 | Test Loss: 1.3175830841064453\n",
      "Epoch 7776 | Train Loss: 1.0805503129959106 | Test Loss: 1.1152431964874268\n",
      "Epoch 7777 | Train Loss: 1.2215343713760376 | Test Loss: 1.31760573387146\n",
      "Epoch 7778 | Train Loss: 1.080539345741272 | Test Loss: 1.1150932312011719\n",
      "Epoch 7779 | Train Loss: 1.2215197086334229 | Test Loss: 1.3175748586654663\n",
      "Epoch 7780 | Train Loss: 1.0805211067199707 | Test Loss: 1.1152347326278687\n",
      "Epoch 7781 | Train Loss: 1.2214998006820679 | Test Loss: 1.3176285028457642\n",
      "Epoch 7782 | Train Loss: 1.0805093050003052 | Test Loss: 1.1150991916656494\n",
      "Epoch 7783 | Train Loss: 1.2214868068695068 | Test Loss: 1.3175610303878784\n",
      "Epoch 7784 | Train Loss: 1.080488920211792 | Test Loss: 1.115220069885254\n",
      "Epoch 7785 | Train Loss: 1.2214711904525757 | Test Loss: 1.317631483078003\n",
      "Epoch 7786 | Train Loss: 1.0804795026779175 | Test Loss: 1.1151044368743896\n",
      "Epoch 7787 | Train Loss: 1.221454381942749 | Test Loss: 1.3175417184829712\n",
      "Epoch 7788 | Train Loss: 1.0804566144943237 | Test Loss: 1.1152070760726929\n",
      "Epoch 7789 | Train Loss: 1.2214438915252686 | Test Loss: 1.3176456689834595\n",
      "Epoch 7790 | Train Loss: 1.0804482698440552 | Test Loss: 1.1150864362716675\n",
      "Epoch 7791 | Train Loss: 1.2214341163635254 | Test Loss: 1.3175593614578247\n",
      "Epoch 7792 | Train Loss: 1.0804245471954346 | Test Loss: 1.115211844444275\n",
      "Epoch 7793 | Train Loss: 1.2214322090148926 | Test Loss: 1.3176379203796387\n",
      "Epoch 7794 | Train Loss: 1.080417275428772 | Test Loss: 1.115079402923584\n",
      "Epoch 7795 | Train Loss: 1.2214136123657227 | Test Loss: 1.3175761699676514\n",
      "Epoch 7796 | Train Loss: 1.080397129058838 | Test Loss: 1.115200161933899\n",
      "Epoch 7797 | Train Loss: 1.221400499343872 | Test Loss: 1.3176343441009521\n",
      "Epoch 7798 | Train Loss: 1.080389380455017 | Test Loss: 1.115081548690796\n",
      "Epoch 7799 | Train Loss: 1.221378207206726 | Test Loss: 1.3176287412643433\n",
      "Epoch 7800 | Train Loss: 1.0803669691085815 | Test Loss: 1.115189790725708\n",
      "Epoch 7801 | Train Loss: 1.2213592529296875 | Test Loss: 1.3175971508026123\n",
      "Epoch 7802 | Train Loss: 1.080354928970337 | Test Loss: 1.1150734424591064\n",
      "Epoch 7803 | Train Loss: 1.221335530281067 | Test Loss: 1.3176403045654297\n",
      "Epoch 7804 | Train Loss: 1.0803312063217163 | Test Loss: 1.1151748895645142\n",
      "Epoch 7805 | Train Loss: 1.2213191986083984 | Test Loss: 1.317562460899353\n",
      "Epoch 7806 | Train Loss: 1.080322027206421 | Test Loss: 1.1150323152542114\n",
      "Epoch 7807 | Train Loss: 1.2212904691696167 | Test Loss: 1.3176220655441284\n",
      "Epoch 7808 | Train Loss: 1.0802956819534302 | Test Loss: 1.1151599884033203\n",
      "Epoch 7809 | Train Loss: 1.2212769985198975 | Test Loss: 1.3175851106643677\n",
      "Epoch 7810 | Train Loss: 1.080281376838684 | Test Loss: 1.114989161491394\n",
      "Epoch 7811 | Train Loss: 1.2212544679641724 | Test Loss: 1.317592978477478\n",
      "Epoch 7812 | Train Loss: 1.0802592039108276 | Test Loss: 1.115142583847046\n",
      "Epoch 7813 | Train Loss: 1.2212238311767578 | Test Loss: 1.3176279067993164\n",
      "Epoch 7814 | Train Loss: 1.0802407264709473 | Test Loss: 1.1149802207946777\n",
      "Epoch 7815 | Train Loss: 1.2212027311325073 | Test Loss: 1.3176020383834839\n",
      "Epoch 7816 | Train Loss: 1.0802226066589355 | Test Loss: 1.1151047945022583\n",
      "Epoch 7817 | Train Loss: 1.2211650609970093 | Test Loss: 1.317598819732666\n",
      "Epoch 7818 | Train Loss: 1.0802075862884521 | Test Loss: 1.1149930953979492\n",
      "Epoch 7819 | Train Loss: 1.2211298942565918 | Test Loss: 1.317611813545227\n",
      "Epoch 7820 | Train Loss: 1.0801924467086792 | Test Loss: 1.1150509119033813\n",
      "Epoch 7821 | Train Loss: 1.2211010456085205 | Test Loss: 1.3175771236419678\n",
      "Epoch 7822 | Train Loss: 1.0801827907562256 | Test Loss: 1.1149812936782837\n",
      "Epoch 7823 | Train Loss: 1.2210644483566284 | Test Loss: 1.3175922632217407\n",
      "Epoch 7824 | Train Loss: 1.0801641941070557 | Test Loss: 1.1150078773498535\n",
      "Epoch 7825 | Train Loss: 1.2210513353347778 | Test Loss: 1.317620873451233\n",
      "Epoch 7826 | Train Loss: 1.0801492929458618 | Test Loss: 1.1149715185165405\n",
      "Epoch 7827 | Train Loss: 1.2210172414779663 | Test Loss: 1.317582368850708\n",
      "Epoch 7828 | Train Loss: 1.0801334381103516 | Test Loss: 1.1149743795394897\n",
      "Epoch 7829 | Train Loss: 1.2210050821304321 | Test Loss: 1.3176476955413818\n",
      "Epoch 7830 | Train Loss: 1.0801180601119995 | Test Loss: 1.1149721145629883\n",
      "Epoch 7831 | Train Loss: 1.2209796905517578 | Test Loss: 1.3175766468048096\n",
      "Epoch 7832 | Train Loss: 1.0800994634628296 | Test Loss: 1.1149590015411377\n",
      "Epoch 7833 | Train Loss: 1.220963954925537 | Test Loss: 1.317619800567627\n",
      "Epoch 7834 | Train Loss: 1.0800856351852417 | Test Loss: 1.114962100982666\n",
      "Epoch 7835 | Train Loss: 1.2209349870681763 | Test Loss: 1.3176043033599854\n",
      "Epoch 7836 | Train Loss: 1.0800673961639404 | Test Loss: 1.1149327754974365\n",
      "Epoch 7837 | Train Loss: 1.2209197282791138 | Test Loss: 1.3175915479660034\n",
      "Epoch 7838 | Train Loss: 1.0800541639328003 | Test Loss: 1.1149506568908691\n",
      "Epoch 7839 | Train Loss: 1.2208900451660156 | Test Loss: 1.3176449537277222\n",
      "Epoch 7840 | Train Loss: 1.0800405740737915 | Test Loss: 1.114922046661377\n",
      "Epoch 7841 | Train Loss: 1.220876693725586 | Test Loss: 1.3175894021987915\n",
      "Epoch 7842 | Train Loss: 1.0800249576568604 | Test Loss: 1.1149548292160034\n",
      "Epoch 7843 | Train Loss: 1.220848560333252 | Test Loss: 1.317649006843567\n",
      "Epoch 7844 | Train Loss: 1.0800096988677979 | Test Loss: 1.1149063110351562\n",
      "Epoch 7845 | Train Loss: 1.2208319902420044 | Test Loss: 1.3176125288009644\n",
      "Epoch 7846 | Train Loss: 1.0799963474273682 | Test Loss: 1.1149556636810303\n",
      "Epoch 7847 | Train Loss: 1.2208082675933838 | Test Loss: 1.317641019821167\n",
      "Epoch 7848 | Train Loss: 1.0799802541732788 | Test Loss: 1.1148773431777954\n",
      "Epoch 7849 | Train Loss: 1.2207896709442139 | Test Loss: 1.317643642425537\n",
      "Epoch 7850 | Train Loss: 1.079969882965088 | Test Loss: 1.1149548292160034\n",
      "Epoch 7851 | Train Loss: 1.2207645177841187 | Test Loss: 1.317645788192749\n",
      "Epoch 7852 | Train Loss: 1.0799543857574463 | Test Loss: 1.1148567199707031\n",
      "Epoch 7853 | Train Loss: 1.2207509279251099 | Test Loss: 1.3176653385162354\n",
      "Epoch 7854 | Train Loss: 1.079939842224121 | Test Loss: 1.114949345588684\n",
      "Epoch 7855 | Train Loss: 1.2207309007644653 | Test Loss: 1.3176259994506836\n",
      "Epoch 7856 | Train Loss: 1.0799272060394287 | Test Loss: 1.1148658990859985\n",
      "Epoch 7857 | Train Loss: 1.2207194566726685 | Test Loss: 1.3176196813583374\n",
      "Epoch 7858 | Train Loss: 1.0799124240875244 | Test Loss: 1.1149312257766724\n",
      "Epoch 7859 | Train Loss: 1.2206978797912598 | Test Loss: 1.317656397819519\n",
      "Epoch 7860 | Train Loss: 1.0798975229263306 | Test Loss: 1.114850640296936\n",
      "Epoch 7861 | Train Loss: 1.2206918001174927 | Test Loss: 1.3176270723342896\n",
      "Epoch 7862 | Train Loss: 1.0798816680908203 | Test Loss: 1.1149548292160034\n",
      "Epoch 7863 | Train Loss: 1.2206740379333496 | Test Loss: 1.3176920413970947\n",
      "Epoch 7864 | Train Loss: 1.0798763036727905 | Test Loss: 1.1148056983947754\n",
      "Epoch 7865 | Train Loss: 1.2206758260726929 | Test Loss: 1.3176368474960327\n",
      "Epoch 7866 | Train Loss: 1.0798547267913818 | Test Loss: 1.1150169372558594\n",
      "Epoch 7867 | Train Loss: 1.2206741571426392 | Test Loss: 1.3177138566970825\n",
      "Epoch 7868 | Train Loss: 1.079850673675537 | Test Loss: 1.1147631406784058\n",
      "Epoch 7869 | Train Loss: 1.220679521560669 | Test Loss: 1.3176348209381104\n",
      "Epoch 7870 | Train Loss: 1.0798300504684448 | Test Loss: 1.115045428276062\n",
      "Epoch 7871 | Train Loss: 1.2206679582595825 | Test Loss: 1.3176956176757812\n",
      "Epoch 7872 | Train Loss: 1.0798190832138062 | Test Loss: 1.1147650480270386\n",
      "Epoch 7873 | Train Loss: 1.220657467842102 | Test Loss: 1.3176501989364624\n",
      "Epoch 7874 | Train Loss: 1.0798038244247437 | Test Loss: 1.1150102615356445\n",
      "Epoch 7875 | Train Loss: 1.2206425666809082 | Test Loss: 1.3177382946014404\n",
      "Epoch 7876 | Train Loss: 1.079793095588684 | Test Loss: 1.11477530002594\n",
      "Epoch 7877 | Train Loss: 1.2206370830535889 | Test Loss: 1.3176155090332031\n",
      "Epoch 7878 | Train Loss: 1.0797749757766724 | Test Loss: 1.1149811744689941\n",
      "Epoch 7879 | Train Loss: 1.2206066846847534 | Test Loss: 1.3177963495254517\n",
      "Epoch 7880 | Train Loss: 1.0797746181488037 | Test Loss: 1.1147665977478027\n",
      "Epoch 7881 | Train Loss: 1.220587134361267 | Test Loss: 1.3175857067108154\n",
      "Epoch 7882 | Train Loss: 1.0797452926635742 | Test Loss: 1.1149613857269287\n",
      "Epoch 7883 | Train Loss: 1.2205750942230225 | Test Loss: 1.3178461790084839\n",
      "Epoch 7884 | Train Loss: 1.079749584197998 | Test Loss: 1.114742398262024\n",
      "Epoch 7885 | Train Loss: 1.2205588817596436 | Test Loss: 1.3176077604293823\n",
      "Epoch 7886 | Train Loss: 1.0797199010849 | Test Loss: 1.1149224042892456\n",
      "Epoch 7887 | Train Loss: 1.2205355167388916 | Test Loss: 1.3178263902664185\n",
      "Epoch 7888 | Train Loss: 1.079716682434082 | Test Loss: 1.1147617101669312\n",
      "Epoch 7889 | Train Loss: 1.2205132246017456 | Test Loss: 1.3176357746124268\n",
      "Epoch 7890 | Train Loss: 1.0796844959259033 | Test Loss: 1.1148978471755981\n",
      "Epoch 7891 | Train Loss: 1.2205168008804321 | Test Loss: 1.3177847862243652\n",
      "Epoch 7892 | Train Loss: 1.0796796083450317 | Test Loss: 1.1148011684417725\n",
      "Epoch 7893 | Train Loss: 1.2205090522766113 | Test Loss: 1.317663550376892\n",
      "Epoch 7894 | Train Loss: 1.0796533823013306 | Test Loss: 1.1148761510849\n",
      "Epoch 7895 | Train Loss: 1.2204980850219727 | Test Loss: 1.317771315574646\n",
      "Epoch 7896 | Train Loss: 1.0796400308609009 | Test Loss: 1.1148344278335571\n",
      "Epoch 7897 | Train Loss: 1.2204782962799072 | Test Loss: 1.3176155090332031\n",
      "Epoch 7898 | Train Loss: 1.079604983329773 | Test Loss: 1.114865779876709\n",
      "Epoch 7899 | Train Loss: 1.2204735279083252 | Test Loss: 1.3177963495254517\n",
      "Epoch 7900 | Train Loss: 1.0795955657958984 | Test Loss: 1.1148109436035156\n",
      "Epoch 7901 | Train Loss: 1.2204450368881226 | Test Loss: 1.3176140785217285\n",
      "Epoch 7902 | Train Loss: 1.0795730352401733 | Test Loss: 1.1148594617843628\n",
      "Epoch 7903 | Train Loss: 1.2204147577285767 | Test Loss: 1.3177530765533447\n",
      "Epoch 7904 | Train Loss: 1.0795537233352661 | Test Loss: 1.1147840023040771\n",
      "Epoch 7905 | Train Loss: 1.2203716039657593 | Test Loss: 1.3176398277282715\n",
      "Epoch 7906 | Train Loss: 1.0795350074768066 | Test Loss: 1.1148269176483154\n",
      "Epoch 7907 | Train Loss: 1.2203445434570312 | Test Loss: 1.3177192211151123\n",
      "Epoch 7908 | Train Loss: 1.079521656036377 | Test Loss: 1.1147526502609253\n",
      "Epoch 7909 | Train Loss: 1.2203048467636108 | Test Loss: 1.3176662921905518\n",
      "Epoch 7910 | Train Loss: 1.0795108079910278 | Test Loss: 1.1147977113723755\n",
      "Epoch 7911 | Train Loss: 1.2202754020690918 | Test Loss: 1.3176920413970947\n",
      "Epoch 7912 | Train Loss: 1.0794942378997803 | Test Loss: 1.1147172451019287\n",
      "Epoch 7913 | Train Loss: 1.220221757888794 | Test Loss: 1.3176672458648682\n",
      "Epoch 7914 | Train Loss: 1.0794872045516968 | Test Loss: 1.1147669553756714\n",
      "Epoch 7915 | Train Loss: 1.2202022075653076 | Test Loss: 1.3176987171173096\n",
      "Epoch 7916 | Train Loss: 1.0794671773910522 | Test Loss: 1.1146906614303589\n",
      "Epoch 7917 | Train Loss: 1.2201591730117798 | Test Loss: 1.3176891803741455\n",
      "Epoch 7918 | Train Loss: 1.079460620880127 | Test Loss: 1.1147371530532837\n",
      "Epoch 7919 | Train Loss: 1.220131516456604 | Test Loss: 1.3177196979522705\n",
      "Epoch 7920 | Train Loss: 1.0794421434402466 | Test Loss: 1.1146525144577026\n",
      "Epoch 7921 | Train Loss: 1.2200978994369507 | Test Loss: 1.3176600933074951\n",
      "Epoch 7922 | Train Loss: 1.0794293880462646 | Test Loss: 1.1147388219833374\n",
      "Epoch 7923 | Train Loss: 1.220075249671936 | Test Loss: 1.3177088499069214\n",
      "Epoch 7924 | Train Loss: 1.0794117450714111 | Test Loss: 1.1146163940429688\n",
      "Epoch 7925 | Train Loss: 1.2200514078140259 | Test Loss: 1.3176865577697754\n",
      "Epoch 7926 | Train Loss: 1.0794001817703247 | Test Loss: 1.1147438287734985\n",
      "Epoch 7927 | Train Loss: 1.2200320959091187 | Test Loss: 1.3177207708358765\n",
      "Epoch 7928 | Train Loss: 1.0793808698654175 | Test Loss: 1.1145923137664795\n",
      "Epoch 7929 | Train Loss: 1.2200098037719727 | Test Loss: 1.317702293395996\n",
      "Epoch 7930 | Train Loss: 1.0793709754943848 | Test Loss: 1.1147371530532837\n",
      "Epoch 7931 | Train Loss: 1.2199883460998535 | Test Loss: 1.3177008628845215\n",
      "Epoch 7932 | Train Loss: 1.079345703125 | Test Loss: 1.1145893335342407\n",
      "Epoch 7933 | Train Loss: 1.2199740409851074 | Test Loss: 1.317740797996521\n",
      "Epoch 7934 | Train Loss: 1.0793355703353882 | Test Loss: 1.1147044897079468\n",
      "Epoch 7935 | Train Loss: 1.2199491262435913 | Test Loss: 1.3176771402359009\n",
      "Epoch 7936 | Train Loss: 1.0793131589889526 | Test Loss: 1.1145765781402588\n",
      "Epoch 7937 | Train Loss: 1.2199307680130005 | Test Loss: 1.3177493810653687\n",
      "Epoch 7938 | Train Loss: 1.0793014764785767 | Test Loss: 1.114670991897583\n",
      "Epoch 7939 | Train Loss: 1.2199090719223022 | Test Loss: 1.3176896572113037\n",
      "Epoch 7940 | Train Loss: 1.0792840719223022 | Test Loss: 1.114578366279602\n",
      "Epoch 7941 | Train Loss: 1.2198889255523682 | Test Loss: 1.317718744277954\n",
      "Epoch 7942 | Train Loss: 1.079269528388977 | Test Loss: 1.1146607398986816\n",
      "Epoch 7943 | Train Loss: 1.219866156578064 | Test Loss: 1.3177324533462524\n",
      "Epoch 7944 | Train Loss: 1.0792564153671265 | Test Loss: 1.1145617961883545\n",
      "Epoch 7945 | Train Loss: 1.2198450565338135 | Test Loss: 1.3177067041397095\n",
      "Epoch 7946 | Train Loss: 1.079240083694458 | Test Loss: 1.1146469116210938\n",
      "Epoch 7947 | Train Loss: 1.219822645187378 | Test Loss: 1.3177353143692017\n",
      "Epoch 7948 | Train Loss: 1.0792272090911865 | Test Loss: 1.1145439147949219\n",
      "Epoch 7949 | Train Loss: 1.2198047637939453 | Test Loss: 1.3176841735839844\n",
      "Epoch 7950 | Train Loss: 1.079210638999939 | Test Loss: 1.1146228313446045\n",
      "Epoch 7951 | Train Loss: 1.2197790145874023 | Test Loss: 1.3177268505096436\n",
      "Epoch 7952 | Train Loss: 1.079201102256775 | Test Loss: 1.1145416498184204\n",
      "Epoch 7953 | Train Loss: 1.2197595834732056 | Test Loss: 1.3176772594451904\n",
      "Epoch 7954 | Train Loss: 1.0791815519332886 | Test Loss: 1.1146024465560913\n",
      "Epoch 7955 | Train Loss: 1.2197340726852417 | Test Loss: 1.3177438974380493\n",
      "Epoch 7956 | Train Loss: 1.0791711807250977 | Test Loss: 1.1145364046096802\n",
      "Epoch 7957 | Train Loss: 1.2197129726409912 | Test Loss: 1.317692518234253\n",
      "Epoch 7958 | Train Loss: 1.0791535377502441 | Test Loss: 1.114583134651184\n",
      "Epoch 7959 | Train Loss: 1.2196930646896362 | Test Loss: 1.317723274230957\n",
      "Epoch 7960 | Train Loss: 1.0791393518447876 | Test Loss: 1.114522099494934\n",
      "Epoch 7961 | Train Loss: 1.21967613697052 | Test Loss: 1.3177285194396973\n",
      "Epoch 7962 | Train Loss: 1.079125165939331 | Test Loss: 1.1145522594451904\n",
      "Epoch 7963 | Train Loss: 1.2196531295776367 | Test Loss: 1.3177059888839722\n",
      "Epoch 7964 | Train Loss: 1.0791101455688477 | Test Loss: 1.114517331123352\n",
      "Epoch 7965 | Train Loss: 1.219641089439392 | Test Loss: 1.3177490234375\n",
      "Epoch 7966 | Train Loss: 1.0790966749191284 | Test Loss: 1.1145292520523071\n",
      "Epoch 7967 | Train Loss: 1.2196139097213745 | Test Loss: 1.31769859790802\n",
      "Epoch 7968 | Train Loss: 1.0790846347808838 | Test Loss: 1.1145079135894775\n",
      "Epoch 7969 | Train Loss: 1.2195931673049927 | Test Loss: 1.317754864692688\n",
      "Epoch 7970 | Train Loss: 1.079071283340454 | Test Loss: 1.1145232915878296\n",
      "Epoch 7971 | Train Loss: 1.2195767164230347 | Test Loss: 1.3177423477172852\n",
      "Epoch 7972 | Train Loss: 1.0790537595748901 | Test Loss: 1.1144945621490479\n",
      "Epoch 7973 | Train Loss: 1.2195576429367065 | Test Loss: 1.3177495002746582\n",
      "Epoch 7974 | Train Loss: 1.0790430307388306 | Test Loss: 1.1145222187042236\n",
      "Epoch 7975 | Train Loss: 1.2195420265197754 | Test Loss: 1.317751169204712\n",
      "Epoch 7976 | Train Loss: 1.0790261030197144 | Test Loss: 1.1144769191741943\n",
      "Epoch 7977 | Train Loss: 1.2195229530334473 | Test Loss: 1.3177648782730103\n",
      "Epoch 7978 | Train Loss: 1.0790154933929443 | Test Loss: 1.1145057678222656\n",
      "Epoch 7979 | Train Loss: 1.2195008993148804 | Test Loss: 1.3177258968353271\n",
      "Epoch 7980 | Train Loss: 1.0789979696273804 | Test Loss: 1.114467740058899\n",
      "Epoch 7981 | Train Loss: 1.2194942235946655 | Test Loss: 1.3177772760391235\n",
      "Epoch 7982 | Train Loss: 1.0789865255355835 | Test Loss: 1.1144939661026\n",
      "Epoch 7983 | Train Loss: 1.2194688320159912 | Test Loss: 1.317745327949524\n",
      "Epoch 7984 | Train Loss: 1.0789738893508911 | Test Loss: 1.1144506931304932\n",
      "Epoch 7985 | Train Loss: 1.2194654941558838 | Test Loss: 1.3177722692489624\n",
      "Epoch 7986 | Train Loss: 1.078957438468933 | Test Loss: 1.1145081520080566\n",
      "Epoch 7987 | Train Loss: 1.219449758529663 | Test Loss: 1.317786455154419\n",
      "Epoch 7988 | Train Loss: 1.078953504562378 | Test Loss: 1.1144227981567383\n",
      "Epoch 7989 | Train Loss: 1.2194403409957886 | Test Loss: 1.3177204132080078\n",
      "Epoch 7990 | Train Loss: 1.07893967628479 | Test Loss: 1.114545464515686\n",
      "Epoch 7991 | Train Loss: 1.2194321155548096 | Test Loss: 1.3178492784500122\n",
      "Epoch 7992 | Train Loss: 1.0789324045181274 | Test Loss: 1.1143890619277954\n",
      "Epoch 7993 | Train Loss: 1.219428300857544 | Test Loss: 1.3176913261413574\n",
      "Epoch 7994 | Train Loss: 1.078926920890808 | Test Loss: 1.1145857572555542\n",
      "Epoch 7995 | Train Loss: 1.2194169759750366 | Test Loss: 1.317897915840149\n",
      "Epoch 7996 | Train Loss: 1.078924298286438 | Test Loss: 1.1143674850463867\n",
      "Epoch 7997 | Train Loss: 1.219425082206726 | Test Loss: 1.3176636695861816\n",
      "Epoch 7998 | Train Loss: 1.0789152383804321 | Test Loss: 1.1146106719970703\n",
      "Epoch 7999 | Train Loss: 1.2194056510925293 | Test Loss: 1.3179413080215454\n",
      "Epoch 8000 | Train Loss: 1.0789110660552979 | Test Loss: 1.1143654584884644\n",
      "Epoch 8001 | Train Loss: 1.2194218635559082 | Test Loss: 1.317677617073059\n",
      "Epoch 8002 | Train Loss: 1.078892469406128 | Test Loss: 1.1146340370178223\n",
      "Epoch 8003 | Train Loss: 1.2194154262542725 | Test Loss: 1.3179570436477661\n",
      "Epoch 8004 | Train Loss: 1.0788891315460205 | Test Loss: 1.1143683195114136\n",
      "Epoch 8005 | Train Loss: 1.2194148302078247 | Test Loss: 1.3176947832107544\n",
      "Epoch 8006 | Train Loss: 1.078866958618164 | Test Loss: 1.1146241426467896\n",
      "Epoch 8007 | Train Loss: 1.2194195985794067 | Test Loss: 1.3179923295974731\n",
      "Epoch 8008 | Train Loss: 1.0788664817810059 | Test Loss: 1.114370584487915\n",
      "Epoch 8009 | Train Loss: 1.2194126844406128 | Test Loss: 1.3176754713058472\n",
      "Epoch 8010 | Train Loss: 1.0788452625274658 | Test Loss: 1.1146113872528076\n",
      "Epoch 8011 | Train Loss: 1.2194089889526367 | Test Loss: 1.317955493927002\n",
      "Epoch 8012 | Train Loss: 1.0788495540618896 | Test Loss: 1.1143938302993774\n",
      "Epoch 8013 | Train Loss: 1.2193866968154907 | Test Loss: 1.3176651000976562\n",
      "Epoch 8014 | Train Loss: 1.0788209438323975 | Test Loss: 1.1145925521850586\n",
      "Epoch 8015 | Train Loss: 1.2193968296051025 | Test Loss: 1.3178977966308594\n",
      "Epoch 8016 | Train Loss: 1.0788192749023438 | Test Loss: 1.1144169569015503\n",
      "Epoch 8017 | Train Loss: 1.21937894821167 | Test Loss: 1.3177287578582764\n",
      "Epoch 8018 | Train Loss: 1.078791618347168 | Test Loss: 1.1145681142807007\n",
      "Epoch 8019 | Train Loss: 1.2193771600723267 | Test Loss: 1.3178685903549194\n",
      "Epoch 8020 | Train Loss: 1.0787842273712158 | Test Loss: 1.1144505739212036\n",
      "Epoch 8021 | Train Loss: 1.2193464040756226 | Test Loss: 1.3177651166915894\n",
      "Epoch 8022 | Train Loss: 1.0787460803985596 | Test Loss: 1.1145548820495605\n",
      "Epoch 8023 | Train Loss: 1.2193529605865479 | Test Loss: 1.3178218603134155\n",
      "Epoch 8024 | Train Loss: 1.0787323713302612 | Test Loss: 1.1144596338272095\n",
      "Epoch 8025 | Train Loss: 1.219317078590393 | Test Loss: 1.3177564144134521\n",
      "Epoch 8026 | Train Loss: 1.0787036418914795 | Test Loss: 1.1145153045654297\n",
      "Epoch 8027 | Train Loss: 1.219292402267456 | Test Loss: 1.317762017250061\n",
      "Epoch 8028 | Train Loss: 1.078684687614441 | Test Loss: 1.114452600479126\n",
      "Epoch 8029 | Train Loss: 1.2192323207855225 | Test Loss: 1.3177434206008911\n",
      "Epoch 8030 | Train Loss: 1.0786617994308472 | Test Loss: 1.1144492626190186\n",
      "Epoch 8031 | Train Loss: 1.2192039489746094 | Test Loss: 1.317724347114563\n",
      "Epoch 8032 | Train Loss: 1.078643798828125 | Test Loss: 1.1144204139709473\n",
      "Epoch 8033 | Train Loss: 1.2191441059112549 | Test Loss: 1.317748785018921\n",
      "Epoch 8034 | Train Loss: 1.0786279439926147 | Test Loss: 1.1143978834152222\n",
      "Epoch 8035 | Train Loss: 1.2191048860549927 | Test Loss: 1.3177028894424438\n",
      "Epoch 8036 | Train Loss: 1.0786155462265015 | Test Loss: 1.114373803138733\n",
      "Epoch 8037 | Train Loss: 1.2190463542938232 | Test Loss: 1.3177731037139893\n",
      "Epoch 8038 | Train Loss: 1.0786079168319702 | Test Loss: 1.1143616437911987\n",
      "Epoch 8039 | Train Loss: 1.219011664390564 | Test Loss: 1.3177076578140259\n",
      "Epoch 8040 | Train Loss: 1.0785911083221436 | Test Loss: 1.1143211126327515\n",
      "Epoch 8041 | Train Loss: 1.218967318534851 | Test Loss: 1.3177764415740967\n",
      "Epoch 8042 | Train Loss: 1.0785835981369019 | Test Loss: 1.1143324375152588\n",
      "Epoch 8043 | Train Loss: 1.2189273834228516 | Test Loss: 1.3177130222320557\n",
      "Epoch 8044 | Train Loss: 1.078566074371338 | Test Loss: 1.114297866821289\n",
      "Epoch 8045 | Train Loss: 1.2188947200775146 | Test Loss: 1.317781925201416\n",
      "Epoch 8046 | Train Loss: 1.0785586833953857 | Test Loss: 1.1143126487731934\n",
      "Epoch 8047 | Train Loss: 1.2188717126846313 | Test Loss: 1.317745566368103\n",
      "Epoch 8048 | Train Loss: 1.078537940979004 | Test Loss: 1.1142807006835938\n",
      "Epoch 8049 | Train Loss: 1.2188475131988525 | Test Loss: 1.3177813291549683\n",
      "Epoch 8050 | Train Loss: 1.0785272121429443 | Test Loss: 1.1143063306808472\n",
      "Epoch 8051 | Train Loss: 1.2188260555267334 | Test Loss: 1.3177564144134521\n",
      "Epoch 8052 | Train Loss: 1.0785080194473267 | Test Loss: 1.1142715215682983\n",
      "Epoch 8053 | Train Loss: 1.2188085317611694 | Test Loss: 1.3177603483200073\n",
      "Epoch 8054 | Train Loss: 1.0784993171691895 | Test Loss: 1.1143152713775635\n",
      "Epoch 8055 | Train Loss: 1.218788743019104 | Test Loss: 1.3177881240844727\n",
      "Epoch 8056 | Train Loss: 1.078480839729309 | Test Loss: 1.1142641305923462\n",
      "Epoch 8057 | Train Loss: 1.2187767028808594 | Test Loss: 1.3177589178085327\n",
      "Epoch 8058 | Train Loss: 1.0784701108932495 | Test Loss: 1.1143312454223633\n",
      "Epoch 8059 | Train Loss: 1.218757152557373 | Test Loss: 1.3177917003631592\n",
      "Epoch 8060 | Train Loss: 1.0784549713134766 | Test Loss: 1.1142338514328003\n",
      "Epoch 8061 | Train Loss: 1.2187498807907104 | Test Loss: 1.317749261856079\n",
      "Epoch 8062 | Train Loss: 1.0784375667572021 | Test Loss: 1.1143298149108887\n",
      "Epoch 8063 | Train Loss: 1.218731164932251 | Test Loss: 1.3177858591079712\n",
      "Epoch 8064 | Train Loss: 1.078426718711853 | Test Loss: 1.114211082458496\n",
      "Epoch 8065 | Train Loss: 1.218711256980896 | Test Loss: 1.3177490234375\n",
      "Epoch 8066 | Train Loss: 1.0784095525741577 | Test Loss: 1.114309310913086\n",
      "Epoch 8067 | Train Loss: 1.2186939716339111 | Test Loss: 1.3177913427352905\n",
      "Epoch 8068 | Train Loss: 1.078395962715149 | Test Loss: 1.1142041683197021\n",
      "Epoch 8069 | Train Loss: 1.2186741828918457 | Test Loss: 1.3177821636199951\n",
      "Epoch 8070 | Train Loss: 1.0783798694610596 | Test Loss: 1.1142804622650146\n",
      "Epoch 8071 | Train Loss: 1.218650460243225 | Test Loss: 1.317787528038025\n",
      "Epoch 8072 | Train Loss: 1.0783636569976807 | Test Loss: 1.114212989807129\n",
      "Epoch 8073 | Train Loss: 1.2186355590820312 | Test Loss: 1.31779944896698\n",
      "Epoch 8074 | Train Loss: 1.0783483982086182 | Test Loss: 1.114254117012024\n",
      "Epoch 8075 | Train Loss: 1.2186086177825928 | Test Loss: 1.3177968263626099\n",
      "Epoch 8076 | Train Loss: 1.0783331394195557 | Test Loss: 1.114217758178711\n",
      "Epoch 8077 | Train Loss: 1.2185949087142944 | Test Loss: 1.3178027868270874\n",
      "Epoch 8078 | Train Loss: 1.0783171653747559 | Test Loss: 1.114230990409851\n",
      "Epoch 8079 | Train Loss: 1.218570351600647 | Test Loss: 1.3178032636642456\n",
      "Epoch 8080 | Train Loss: 1.078304409980774 | Test Loss: 1.1142067909240723\n",
      "Epoch 8081 | Train Loss: 1.2185542583465576 | Test Loss: 1.3178174495697021\n",
      "Epoch 8082 | Train Loss: 1.0782904624938965 | Test Loss: 1.1142135858535767\n",
      "Epoch 8083 | Train Loss: 1.218529224395752 | Test Loss: 1.3178226947784424\n",
      "Epoch 8084 | Train Loss: 1.0782777070999146 | Test Loss: 1.1141846179962158\n",
      "Epoch 8085 | Train Loss: 1.2185121774673462 | Test Loss: 1.317785620689392\n",
      "Epoch 8086 | Train Loss: 1.0782654285430908 | Test Loss: 1.1142207384109497\n",
      "Epoch 8087 | Train Loss: 1.2184897661209106 | Test Loss: 1.3178484439849854\n",
      "Epoch 8088 | Train Loss: 1.0782562494277954 | Test Loss: 1.1141711473464966\n",
      "Epoch 8089 | Train Loss: 1.2184747457504272 | Test Loss: 1.317758321762085\n",
      "Epoch 8090 | Train Loss: 1.078238606452942 | Test Loss: 1.1142234802246094\n",
      "Epoch 8091 | Train Loss: 1.218453288078308 | Test Loss: 1.3178528547286987\n",
      "Epoch 8092 | Train Loss: 1.0782315731048584 | Test Loss: 1.1141383647918701\n",
      "Epoch 8093 | Train Loss: 1.218436360359192 | Test Loss: 1.3177752494812012\n",
      "Epoch 8094 | Train Loss: 1.078211784362793 | Test Loss: 1.1142200231552124\n",
      "Epoch 8095 | Train Loss: 1.2184224128723145 | Test Loss: 1.317867636680603\n",
      "Epoch 8096 | Train Loss: 1.0782091617584229 | Test Loss: 1.1141369342803955\n",
      "Epoch 8097 | Train Loss: 1.2184009552001953 | Test Loss: 1.3177884817123413\n",
      "Epoch 8098 | Train Loss: 1.0781961679458618 | Test Loss: 1.1142171621322632\n",
      "Epoch 8099 | Train Loss: 1.2183865308761597 | Test Loss: 1.317830204963684\n",
      "Epoch 8100 | Train Loss: 1.0781763792037964 | Test Loss: 1.1141369342803955\n",
      "Epoch 8101 | Train Loss: 1.2183759212493896 | Test Loss: 1.3178272247314453\n",
      "Epoch 8102 | Train Loss: 1.0781633853912354 | Test Loss: 1.1142020225524902\n",
      "Epoch 8103 | Train Loss: 1.2183600664138794 | Test Loss: 1.3178333044052124\n",
      "Epoch 8104 | Train Loss: 1.0781488418579102 | Test Loss: 1.1141211986541748\n",
      "Epoch 8105 | Train Loss: 1.218356728553772 | Test Loss: 1.3178467750549316\n",
      "Epoch 8106 | Train Loss: 1.0781368017196655 | Test Loss: 1.1142209768295288\n",
      "Epoch 8107 | Train Loss: 1.2183459997177124 | Test Loss: 1.3179138898849487\n",
      "Epoch 8108 | Train Loss: 1.0781387090682983 | Test Loss: 1.1141002178192139\n",
      "Epoch 8109 | Train Loss: 1.218348741531372 | Test Loss: 1.3177664279937744\n",
      "Epoch 8110 | Train Loss: 1.0781265497207642 | Test Loss: 1.1142823696136475\n",
      "Epoch 8111 | Train Loss: 1.2183458805084229 | Test Loss: 1.3180240392684937\n",
      "Epoch 8112 | Train Loss: 1.078129529953003 | Test Loss: 1.114065408706665\n",
      "Epoch 8113 | Train Loss: 1.2183544635772705 | Test Loss: 1.3177359104156494\n",
      "Epoch 8114 | Train Loss: 1.0781289339065552 | Test Loss: 1.114323377609253\n",
      "Epoch 8115 | Train Loss: 1.2183575630187988 | Test Loss: 1.318052887916565\n",
      "Epoch 8116 | Train Loss: 1.078122854232788 | Test Loss: 1.1140636205673218\n",
      "Epoch 8117 | Train Loss: 1.2183634042739868 | Test Loss: 1.3177238702774048\n",
      "Epoch 8118 | Train Loss: 1.0781004428863525 | Test Loss: 1.114322304725647\n",
      "Epoch 8119 | Train Loss: 1.2183653116226196 | Test Loss: 1.3180240392684937\n",
      "Epoch 8120 | Train Loss: 1.0780904293060303 | Test Loss: 1.1140711307525635\n",
      "Epoch 8121 | Train Loss: 1.2183676958084106 | Test Loss: 1.317751407623291\n",
      "Epoch 8122 | Train Loss: 1.078066349029541 | Test Loss: 1.114316463470459\n",
      "Epoch 8123 | Train Loss: 1.218370795249939 | Test Loss: 1.3179702758789062\n",
      "Epoch 8124 | Train Loss: 1.078051209449768 | Test Loss: 1.1141055822372437\n",
      "Epoch 8125 | Train Loss: 1.2183600664138794 | Test Loss: 1.3177741765975952\n",
      "Epoch 8126 | Train Loss: 1.07802414894104 | Test Loss: 1.1143065690994263\n",
      "Epoch 8127 | Train Loss: 1.2183642387390137 | Test Loss: 1.3179501295089722\n",
      "Epoch 8128 | Train Loss: 1.0780127048492432 | Test Loss: 1.1141365766525269\n",
      "Epoch 8129 | Train Loss: 1.2183505296707153 | Test Loss: 1.3177833557128906\n",
      "Epoch 8130 | Train Loss: 1.0779882669448853 | Test Loss: 1.1142832040786743\n",
      "Epoch 8131 | Train Loss: 1.2183222770690918 | Test Loss: 1.3179107904434204\n",
      "Epoch 8132 | Train Loss: 1.0779833793640137 | Test Loss: 1.1141387224197388\n",
      "Epoch 8133 | Train Loss: 1.2182871103286743 | Test Loss: 1.3178459405899048\n",
      "Epoch 8134 | Train Loss: 1.077951192855835 | Test Loss: 1.1142408847808838\n",
      "Epoch 8135 | Train Loss: 1.2182661294937134 | Test Loss: 1.3178550004959106\n",
      "Epoch 8136 | Train Loss: 1.0779438018798828 | Test Loss: 1.114118218421936\n",
      "Epoch 8137 | Train Loss: 1.2182070016860962 | Test Loss: 1.3178985118865967\n",
      "Epoch 8138 | Train Loss: 1.0779204368591309 | Test Loss: 1.1141916513442993\n",
      "Epoch 8139 | Train Loss: 1.2181901931762695 | Test Loss: 1.3178049325942993\n",
      "Epoch 8140 | Train Loss: 1.0779080390930176 | Test Loss: 1.1141103506088257\n",
      "Epoch 8141 | Train Loss: 1.2181318998336792 | Test Loss: 1.317882776260376\n",
      "Epoch 8142 | Train Loss: 1.077885389328003 | Test Loss: 1.1141350269317627\n",
      "Epoch 8143 | Train Loss: 1.2181086540222168 | Test Loss: 1.317771315574646\n",
      "Epoch 8144 | Train Loss: 1.0778725147247314 | Test Loss: 1.114108920097351\n",
      "Epoch 8145 | Train Loss: 1.218050241470337 | Test Loss: 1.3178640604019165\n",
      "Epoch 8146 | Train Loss: 1.0778545141220093 | Test Loss: 1.1140843629837036\n",
      "Epoch 8147 | Train Loss: 1.2180325984954834 | Test Loss: 1.3178088665008545\n",
      "Epoch 8148 | Train Loss: 1.0778404474258423 | Test Loss: 1.1140953302383423\n",
      "Epoch 8149 | Train Loss: 1.2179834842681885 | Test Loss: 1.317868947982788\n",
      "Epoch 8150 | Train Loss: 1.0778286457061768 | Test Loss: 1.1140735149383545\n",
      "Epoch 8151 | Train Loss: 1.2179683446884155 | Test Loss: 1.317818284034729\n",
      "Epoch 8152 | Train Loss: 1.0778110027313232 | Test Loss: 1.1140682697296143\n",
      "Epoch 8153 | Train Loss: 1.217926263809204 | Test Loss: 1.3178907632827759\n",
      "Epoch 8154 | Train Loss: 1.0778053998947144 | Test Loss: 1.114056944847107\n",
      "Epoch 8155 | Train Loss: 1.2179075479507446 | Test Loss: 1.3178268671035767\n",
      "Epoch 8156 | Train Loss: 1.077786922454834 | Test Loss: 1.1140518188476562\n",
      "Epoch 8157 | Train Loss: 1.2178730964660645 | Test Loss: 1.317886471748352\n",
      "Epoch 8158 | Train Loss: 1.0777744054794312 | Test Loss: 1.1140328645706177\n",
      "Epoch 8159 | Train Loss: 1.2178514003753662 | Test Loss: 1.3178383111953735\n",
      "Epoch 8160 | Train Loss: 1.0777587890625 | Test Loss: 1.114046573638916\n",
      "Epoch 8161 | Train Loss: 1.2178242206573486 | Test Loss: 1.3178777694702148\n",
      "Epoch 8162 | Train Loss: 1.0777486562728882 | Test Loss: 1.114020586013794\n",
      "Epoch 8163 | Train Loss: 1.2178081274032593 | Test Loss: 1.3178577423095703\n",
      "Epoch 8164 | Train Loss: 1.0777301788330078 | Test Loss: 1.1140437126159668\n",
      "Epoch 8165 | Train Loss: 1.2177848815917969 | Test Loss: 1.3178855180740356\n",
      "Epoch 8166 | Train Loss: 1.0777190923690796 | Test Loss: 1.1140022277832031\n",
      "Epoch 8167 | Train Loss: 1.217768907546997 | Test Loss: 1.3178720474243164\n",
      "Epoch 8168 | Train Loss: 1.0777024030685425 | Test Loss: 1.1140416860580444\n",
      "Epoch 8169 | Train Loss: 1.217748761177063 | Test Loss: 1.3178985118865967\n",
      "Epoch 8170 | Train Loss: 1.0776902437210083 | Test Loss: 1.113979697227478\n",
      "Epoch 8171 | Train Loss: 1.2177319526672363 | Test Loss: 1.3178977966308594\n",
      "Epoch 8172 | Train Loss: 1.0776747465133667 | Test Loss: 1.114045262336731\n",
      "Epoch 8173 | Train Loss: 1.2177162170410156 | Test Loss: 1.317948579788208\n",
      "Epoch 8174 | Train Loss: 1.0776636600494385 | Test Loss: 1.1139761209487915\n",
      "Epoch 8175 | Train Loss: 1.2176992893218994 | Test Loss: 1.3179020881652832\n",
      "Epoch 8176 | Train Loss: 1.077648639678955 | Test Loss: 1.1140378713607788\n",
      "Epoch 8177 | Train Loss: 1.2176809310913086 | Test Loss: 1.3179609775543213\n",
      "Epoch 8178 | Train Loss: 1.0776444673538208 | Test Loss: 1.1139699220657349\n",
      "Epoch 8179 | Train Loss: 1.2176676988601685 | Test Loss: 1.317889928817749\n",
      "Epoch 8180 | Train Loss: 1.077625036239624 | Test Loss: 1.1140238046646118\n",
      "Epoch 8181 | Train Loss: 1.217650294303894 | Test Loss: 1.3180006742477417\n",
      "Epoch 8182 | Train Loss: 1.077623724937439 | Test Loss: 1.1139525175094604\n",
      "Epoch 8183 | Train Loss: 1.2176448106765747 | Test Loss: 1.3178949356079102\n",
      "Epoch 8184 | Train Loss: 1.077606201171875 | Test Loss: 1.1140329837799072\n",
      "Epoch 8185 | Train Loss: 1.2176423072814941 | Test Loss: 1.3180664777755737\n",
      "Epoch 8186 | Train Loss: 1.0776084661483765 | Test Loss: 1.1139155626296997\n",
      "Epoch 8187 | Train Loss: 1.2176434993743896 | Test Loss: 1.3178908824920654\n",
      "Epoch 8188 | Train Loss: 1.0775941610336304 | Test Loss: 1.1140756607055664\n",
      "Epoch 8189 | Train Loss: 1.2176549434661865 | Test Loss: 1.3181078433990479\n",
      "Epoch 8190 | Train Loss: 1.077589750289917 | Test Loss: 1.113895058631897\n",
      "Epoch 8191 | Train Loss: 1.2176730632781982 | Test Loss: 1.31788969039917\n",
      "Epoch 8192 | Train Loss: 1.0775729417800903 | Test Loss: 1.1141200065612793\n",
      "Epoch 8193 | Train Loss: 1.2176835536956787 | Test Loss: 1.3180930614471436\n",
      "Epoch 8194 | Train Loss: 1.0775648355484009 | Test Loss: 1.1139053106307983\n",
      "Epoch 8195 | Train Loss: 1.217693567276001 | Test Loss: 1.3178646564483643\n",
      "Epoch 8196 | Train Loss: 1.077539086341858 | Test Loss: 1.1141449213027954\n",
      "Epoch 8197 | Train Loss: 1.2176975011825562 | Test Loss: 1.3180861473083496\n",
      "Epoch 8198 | Train Loss: 1.07752525806427 | Test Loss: 1.1139274835586548\n",
      "Epoch 8199 | Train Loss: 1.217704176902771 | Test Loss: 1.317876935005188\n",
      "Epoch 8200 | Train Loss: 1.0775014162063599 | Test Loss: 1.1141340732574463\n",
      "Epoch 8201 | Train Loss: 1.2176833152770996 | Test Loss: 1.318057894706726\n",
      "Epoch 8202 | Train Loss: 1.077484369277954 | Test Loss: 1.1139404773712158\n",
      "Epoch 8203 | Train Loss: 1.2176594734191895 | Test Loss: 1.3178989887237549\n",
      "Epoch 8204 | Train Loss: 1.0774577856063843 | Test Loss: 1.1141016483306885\n",
      "Epoch 8205 | Train Loss: 1.217634677886963 | Test Loss: 1.3179998397827148\n",
      "Epoch 8206 | Train Loss: 1.0774462223052979 | Test Loss: 1.1139321327209473\n",
      "Epoch 8207 | Train Loss: 1.217586874961853 | Test Loss: 1.3179583549499512\n",
      "Epoch 8208 | Train Loss: 1.077426791191101 | Test Loss: 1.1140307188034058\n",
      "Epoch 8209 | Train Loss: 1.2175461053848267 | Test Loss: 1.3179233074188232\n",
      "Epoch 8210 | Train Loss: 1.0774179697036743 | Test Loss: 1.1139154434204102\n",
      "Epoch 8211 | Train Loss: 1.217482566833496 | Test Loss: 1.3179956674575806\n",
      "Epoch 8212 | Train Loss: 1.077406883239746 | Test Loss: 1.113961935043335\n",
      "Epoch 8213 | Train Loss: 1.2174503803253174 | Test Loss: 1.3178694248199463\n",
      "Epoch 8214 | Train Loss: 1.0773975849151611 | Test Loss: 1.1139096021652222\n",
      "Epoch 8215 | Train Loss: 1.2173972129821777 | Test Loss: 1.3180005550384521\n",
      "Epoch 8216 | Train Loss: 1.0773831605911255 | Test Loss: 1.1138920783996582\n",
      "Epoch 8217 | Train Loss: 1.2173749208450317 | Test Loss: 1.3178727626800537\n",
      "Epoch 8218 | Train Loss: 1.077366590499878 | Test Loss: 1.1138966083526611\n",
      "Epoch 8219 | Train Loss: 1.2173351049423218 | Test Loss: 1.3179742097854614\n",
      "Epoch 8220 | Train Loss: 1.0773531198501587 | Test Loss: 1.1138561964035034\n",
      "Epoch 8221 | Train Loss: 1.2173255681991577 | Test Loss: 1.3179031610488892\n",
      "Epoch 8222 | Train Loss: 1.0773358345031738 | Test Loss: 1.1138931512832642\n",
      "Epoch 8223 | Train Loss: 1.2172975540161133 | Test Loss: 1.317962884902954\n",
      "Epoch 8224 | Train Loss: 1.077317476272583 | Test Loss: 1.1138458251953125\n",
      "Epoch 8225 | Train Loss: 1.2172945737838745 | Test Loss: 1.3179354667663574\n",
      "Epoch 8226 | Train Loss: 1.077301025390625 | Test Loss: 1.1138997077941895\n",
      "Epoch 8227 | Train Loss: 1.2172701358795166 | Test Loss: 1.3179516792297363\n",
      "Epoch 8228 | Train Loss: 1.0772836208343506 | Test Loss: 1.1138231754302979\n",
      "Epoch 8229 | Train Loss: 1.2172541618347168 | Test Loss: 1.317949652671814\n",
      "Epoch 8230 | Train Loss: 1.0772689580917358 | Test Loss: 1.1138967275619507\n",
      "Epoch 8231 | Train Loss: 1.217228651046753 | Test Loss: 1.317973256111145\n",
      "Epoch 8232 | Train Loss: 1.0772591829299927 | Test Loss: 1.1138020753860474\n",
      "Epoch 8233 | Train Loss: 1.2172150611877441 | Test Loss: 1.3179255723953247\n",
      "Epoch 8234 | Train Loss: 1.077243447303772 | Test Loss: 1.1139106750488281\n",
      "Epoch 8235 | Train Loss: 1.2171902656555176 | Test Loss: 1.318007469177246\n",
      "Epoch 8236 | Train Loss: 1.077240228652954 | Test Loss: 1.1137856245040894\n",
      "Epoch 8237 | Train Loss: 1.2171732187271118 | Test Loss: 1.3179124593734741\n",
      "Epoch 8238 | Train Loss: 1.0772230625152588 | Test Loss: 1.1139212846755981\n",
      "Epoch 8239 | Train Loss: 1.2171603441238403 | Test Loss: 1.3180233240127563\n",
      "Epoch 8240 | Train Loss: 1.0772162675857544 | Test Loss: 1.113761067390442\n",
      "Epoch 8241 | Train Loss: 1.217144250869751 | Test Loss: 1.3179750442504883\n",
      "Epoch 8242 | Train Loss: 1.0772002935409546 | Test Loss: 1.113898754119873\n",
      "Epoch 8243 | Train Loss: 1.2171272039413452 | Test Loss: 1.318016529083252\n",
      "Epoch 8244 | Train Loss: 1.0771902799606323 | Test Loss: 1.1137490272521973\n",
      "Epoch 8245 | Train Loss: 1.2171181440353394 | Test Loss: 1.3180224895477295\n",
      "Epoch 8246 | Train Loss: 1.0771753787994385 | Test Loss: 1.1138817071914673\n",
      "Epoch 8247 | Train Loss: 1.2170995473861694 | Test Loss: 1.3180509805679321\n",
      "Epoch 8248 | Train Loss: 1.0771708488464355 | Test Loss: 1.113755226135254\n",
      "Epoch 8249 | Train Loss: 1.2170928716659546 | Test Loss: 1.3179774284362793\n",
      "Epoch 8250 | Train Loss: 1.0771493911743164 | Test Loss: 1.113871693611145\n",
      "Epoch 8251 | Train Loss: 1.2170771360397339 | Test Loss: 1.3181235790252686\n",
      "Epoch 8252 | Train Loss: 1.0771470069885254 | Test Loss: 1.1137439012527466\n",
      "Epoch 8253 | Train Loss: 1.2170662879943848 | Test Loss: 1.3179618120193481\n",
      "Epoch 8254 | Train Loss: 1.0771315097808838 | Test Loss: 1.1138880252838135\n",
      "Epoch 8255 | Train Loss: 1.2170629501342773 | Test Loss: 1.3181623220443726\n",
      "Epoch 8256 | Train Loss: 1.077134370803833 | Test Loss: 1.1137375831604004\n",
      "Epoch 8257 | Train Loss: 1.2170495986938477 | Test Loss: 1.3179564476013184\n",
      "Epoch 8258 | Train Loss: 1.0771101713180542 | Test Loss: 1.113878846168518\n",
      "Epoch 8259 | Train Loss: 1.2170408964157104 | Test Loss: 1.3181627988815308\n",
      "Epoch 8260 | Train Loss: 1.077107310295105 | Test Loss: 1.1137382984161377\n",
      "Epoch 8261 | Train Loss: 1.2170323133468628 | Test Loss: 1.3179841041564941\n",
      "Epoch 8262 | Train Loss: 1.077083706855774 | Test Loss: 1.1138743162155151\n",
      "Epoch 8263 | Train Loss: 1.2170376777648926 | Test Loss: 1.3181543350219727\n",
      "Epoch 8264 | Train Loss: 1.07707941532135 | Test Loss: 1.113765001296997\n",
      "Epoch 8265 | Train Loss: 1.2170348167419434 | Test Loss: 1.3180088996887207\n",
      "Epoch 8266 | Train Loss: 1.077053189277649 | Test Loss: 1.113876223564148\n",
      "Epoch 8267 | Train Loss: 1.2170432806015015 | Test Loss: 1.3180674314498901\n",
      "Epoch 8268 | Train Loss: 1.0770375728607178 | Test Loss: 1.1137819290161133\n",
      "Epoch 8269 | Train Loss: 1.2170361280441284 | Test Loss: 1.3179727792739868\n",
      "Epoch 8270 | Train Loss: 1.077012300491333 | Test Loss: 1.1138728857040405\n",
      "Epoch 8271 | Train Loss: 1.2170265913009644 | Test Loss: 1.3180382251739502\n",
      "Epoch 8272 | Train Loss: 1.0770026445388794 | Test Loss: 1.1137912273406982\n",
      "Epoch 8273 | Train Loss: 1.2169861793518066 | Test Loss: 1.3179543018341064\n",
      "Epoch 8274 | Train Loss: 1.0769810676574707 | Test Loss: 1.1138503551483154\n",
      "Epoch 8275 | Train Loss: 1.216957688331604 | Test Loss: 1.3180288076400757\n",
      "Epoch 8276 | Train Loss: 1.0769691467285156 | Test Loss: 1.1137617826461792\n",
      "Epoch 8277 | Train Loss: 1.2169040441513062 | Test Loss: 1.3179864883422852\n",
      "Epoch 8278 | Train Loss: 1.07695472240448 | Test Loss: 1.1137984991073608\n",
      "Epoch 8279 | Train Loss: 1.2168740034103394 | Test Loss: 1.3179703950881958\n",
      "Epoch 8280 | Train Loss: 1.0769401788711548 | Test Loss: 1.1137332916259766\n",
      "Epoch 8281 | Train Loss: 1.2168171405792236 | Test Loss: 1.3180382251739502\n",
      "Epoch 8282 | Train Loss: 1.0769344568252563 | Test Loss: 1.1137336492538452\n",
      "Epoch 8283 | Train Loss: 1.2167888879776 | Test Loss: 1.3179129362106323\n",
      "Epoch 8284 | Train Loss: 1.0769199132919312 | Test Loss: 1.1137200593948364\n",
      "Epoch 8285 | Train Loss: 1.2167493104934692 | Test Loss: 1.3180540800094604\n",
      "Epoch 8286 | Train Loss: 1.0769091844558716 | Test Loss: 1.1136986017227173\n",
      "Epoch 8287 | Train Loss: 1.2167325019836426 | Test Loss: 1.3179374933242798\n",
      "Epoch 8288 | Train Loss: 1.0768905878067017 | Test Loss: 1.1137211322784424\n",
      "Epoch 8289 | Train Loss: 1.216700792312622 | Test Loss: 1.3180429935455322\n",
      "Epoch 8290 | Train Loss: 1.0768805742263794 | Test Loss: 1.1136611700057983\n",
      "Epoch 8291 | Train Loss: 1.216687560081482 | Test Loss: 1.3179693222045898\n",
      "Epoch 8292 | Train Loss: 1.0768598318099976 | Test Loss: 1.1137298345565796\n",
      "Epoch 8293 | Train Loss: 1.2166682481765747 | Test Loss: 1.3180556297302246\n",
      "Epoch 8294 | Train Loss: 1.0768524408340454 | Test Loss: 1.1136388778686523\n",
      "Epoch 8295 | Train Loss: 1.2166508436203003 | Test Loss: 1.3179949522018433\n",
      "Epoch 8296 | Train Loss: 1.0768299102783203 | Test Loss: 1.1137298345565796\n",
      "Epoch 8297 | Train Loss: 1.216640591621399 | Test Loss: 1.3180593252182007\n",
      "Epoch 8298 | Train Loss: 1.0768203735351562 | Test Loss: 1.1136350631713867\n",
      "Epoch 8299 | Train Loss: 1.2166156768798828 | Test Loss: 1.3179961442947388\n",
      "Epoch 8300 | Train Loss: 1.0768043994903564 | Test Loss: 1.113724708557129\n",
      "Epoch 8301 | Train Loss: 1.2166036367416382 | Test Loss: 1.3180774450302124\n",
      "Epoch 8302 | Train Loss: 1.0767934322357178 | Test Loss: 1.1136341094970703\n",
      "Epoch 8303 | Train Loss: 1.2165874242782593 | Test Loss: 1.318037748336792\n",
      "Epoch 8304 | Train Loss: 1.0767797231674194 | Test Loss: 1.1137281656265259\n",
      "Epoch 8305 | Train Loss: 1.2165664434432983 | Test Loss: 1.3180673122406006\n",
      "Epoch 8306 | Train Loss: 1.0767706632614136 | Test Loss: 1.113625168800354\n",
      "Epoch 8307 | Train Loss: 1.216559886932373 | Test Loss: 1.3180512189865112\n",
      "Epoch 8308 | Train Loss: 1.0767515897750854 | Test Loss: 1.1137323379516602\n",
      "Epoch 8309 | Train Loss: 1.2165447473526 | Test Loss: 1.3180897235870361\n",
      "Epoch 8310 | Train Loss: 1.0767449140548706 | Test Loss: 1.1136016845703125\n",
      "Epoch 8311 | Train Loss: 1.2165296077728271 | Test Loss: 1.3180720806121826\n",
      "Epoch 8312 | Train Loss: 1.0767288208007812 | Test Loss: 1.1137410402297974\n",
      "Epoch 8313 | Train Loss: 1.2165191173553467 | Test Loss: 1.318122386932373\n",
      "Epoch 8314 | Train Loss: 1.076718807220459 | Test Loss: 1.1135960817337036\n",
      "Epoch 8315 | Train Loss: 1.2165089845657349 | Test Loss: 1.3180726766586304\n",
      "Epoch 8316 | Train Loss: 1.0767090320587158 | Test Loss: 1.1137651205062866\n",
      "Epoch 8317 | Train Loss: 1.2165050506591797 | Test Loss: 1.318132758140564\n",
      "Epoch 8318 | Train Loss: 1.076699137687683 | Test Loss: 1.1135852336883545\n",
      "Epoch 8319 | Train Loss: 1.216494083404541 | Test Loss: 1.3180937767028809\n",
      "Epoch 8320 | Train Loss: 1.076684832572937 | Test Loss: 1.1137632131576538\n",
      "Epoch 8321 | Train Loss: 1.2164808511734009 | Test Loss: 1.3181567192077637\n",
      "Epoch 8322 | Train Loss: 1.076677680015564 | Test Loss: 1.1135891675949097\n",
      "Epoch 8323 | Train Loss: 1.2164736986160278 | Test Loss: 1.3180872201919556\n",
      "Epoch 8324 | Train Loss: 1.076658844947815 | Test Loss: 1.1137429475784302\n",
      "Epoch 8325 | Train Loss: 1.2164393663406372 | Test Loss: 1.3181507587432861\n",
      "Epoch 8326 | Train Loss: 1.076651692390442 | Test Loss: 1.1135905981063843\n",
      "Epoch 8327 | Train Loss: 1.2164247035980225 | Test Loss: 1.3180716037750244\n",
      "Epoch 8328 | Train Loss: 1.0766282081604004 | Test Loss: 1.1137187480926514\n",
      "Epoch 8329 | Train Loss: 1.216411828994751 | Test Loss: 1.3181957006454468\n",
      "Epoch 8330 | Train Loss: 1.0766234397888184 | Test Loss: 1.1135814189910889\n",
      "Epoch 8331 | Train Loss: 1.2163842916488647 | Test Loss: 1.3180670738220215\n",
      "Epoch 8332 | Train Loss: 1.0766032934188843 | Test Loss: 1.113695740699768\n",
      "Epoch 8333 | Train Loss: 1.2163634300231934 | Test Loss: 1.318165898323059\n",
      "Epoch 8334 | Train Loss: 1.0765929222106934 | Test Loss: 1.1135826110839844\n",
      "Epoch 8335 | Train Loss: 1.2163399457931519 | Test Loss: 1.3180776834487915\n",
      "Epoch 8336 | Train Loss: 1.0765782594680786 | Test Loss: 1.1136776208877563\n",
      "Epoch 8337 | Train Loss: 1.2163335084915161 | Test Loss: 1.3181438446044922\n",
      "Epoch 8338 | Train Loss: 1.076568841934204 | Test Loss: 1.1135969161987305\n",
      "Epoch 8339 | Train Loss: 1.2163037061691284 | Test Loss: 1.3181345462799072\n",
      "Epoch 8340 | Train Loss: 1.0765485763549805 | Test Loss: 1.1136233806610107\n",
      "Epoch 8341 | Train Loss: 1.2162810564041138 | Test Loss: 1.318149209022522\n",
      "Epoch 8342 | Train Loss: 1.0765388011932373 | Test Loss: 1.1136173009872437\n",
      "Epoch 8343 | Train Loss: 1.2162529230117798 | Test Loss: 1.318142056465149\n",
      "Epoch 8344 | Train Loss: 1.0765175819396973 | Test Loss: 1.1135804653167725\n",
      "Epoch 8345 | Train Loss: 1.2162402868270874 | Test Loss: 1.3181140422821045\n",
      "Epoch 8346 | Train Loss: 1.0765037536621094 | Test Loss: 1.113627552986145\n",
      "Epoch 8347 | Train Loss: 1.216202735900879 | Test Loss: 1.3181681632995605\n",
      "Epoch 8348 | Train Loss: 1.0764951705932617 | Test Loss: 1.1135435104370117\n",
      "Epoch 8349 | Train Loss: 1.216181993484497 | Test Loss: 1.3180923461914062\n",
      "Epoch 8350 | Train Loss: 1.0764802694320679 | Test Loss: 1.1136161088943481\n",
      "Epoch 8351 | Train Loss: 1.21615731716156 | Test Loss: 1.318181037902832\n",
      "Epoch 8352 | Train Loss: 1.0764728784561157 | Test Loss: 1.1135246753692627\n",
      "Epoch 8353 | Train Loss: 1.2161319255828857 | Test Loss: 1.3180564641952515\n",
      "Epoch 8354 | Train Loss: 1.0764579772949219 | Test Loss: 1.1135807037353516\n",
      "Epoch 8355 | Train Loss: 1.2161098718643188 | Test Loss: 1.318187952041626\n",
      "Epoch 8356 | Train Loss: 1.0764455795288086 | Test Loss: 1.113523244857788\n",
      "Epoch 8357 | Train Loss: 1.216098427772522 | Test Loss: 1.3180968761444092\n",
      "Epoch 8358 | Train Loss: 1.076423168182373 | Test Loss: 1.1135550737380981\n",
      "Epoch 8359 | Train Loss: 1.2160817384719849 | Test Loss: 1.3181804418563843\n",
      "Epoch 8360 | Train Loss: 1.0764093399047852 | Test Loss: 1.1135227680206299\n",
      "Epoch 8361 | Train Loss: 1.2160670757293701 | Test Loss: 1.3181496858596802\n",
      "Epoch 8362 | Train Loss: 1.0763907432556152 | Test Loss: 1.1135587692260742\n",
      "Epoch 8363 | Train Loss: 1.2160522937774658 | Test Loss: 1.3181577920913696\n",
      "Epoch 8364 | Train Loss: 1.0763777494430542 | Test Loss: 1.1134912967681885\n",
      "Epoch 8365 | Train Loss: 1.2160358428955078 | Test Loss: 1.3181620836257935\n",
      "Epoch 8366 | Train Loss: 1.0763593912124634 | Test Loss: 1.1135669946670532\n",
      "Epoch 8367 | Train Loss: 1.2160239219665527 | Test Loss: 1.3181445598602295\n",
      "Epoch 8368 | Train Loss: 1.0763497352600098 | Test Loss: 1.1134756803512573\n",
      "Epoch 8369 | Train Loss: 1.2159993648529053 | Test Loss: 1.3181711435317993\n",
      "Epoch 8370 | Train Loss: 1.0763354301452637 | Test Loss: 1.113551139831543\n",
      "Epoch 8371 | Train Loss: 1.2159779071807861 | Test Loss: 1.3181476593017578\n",
      "Epoch 8372 | Train Loss: 1.0763229131698608 | Test Loss: 1.1134659051895142\n",
      "Epoch 8373 | Train Loss: 1.215958833694458 | Test Loss: 1.318163514137268\n",
      "Epoch 8374 | Train Loss: 1.0763126611709595 | Test Loss: 1.113527774810791\n",
      "Epoch 8375 | Train Loss: 1.2159401178359985 | Test Loss: 1.3181483745574951\n",
      "Epoch 8376 | Train Loss: 1.0763037204742432 | Test Loss: 1.1134529113769531\n",
      "Epoch 8377 | Train Loss: 1.215918779373169 | Test Loss: 1.3181777000427246\n",
      "Epoch 8378 | Train Loss: 1.0762875080108643 | Test Loss: 1.1135234832763672\n",
      "Epoch 8379 | Train Loss: 1.2159082889556885 | Test Loss: 1.3181564807891846\n",
      "Epoch 8380 | Train Loss: 1.0762723684310913 | Test Loss: 1.1134371757507324\n",
      "Epoch 8381 | Train Loss: 1.215897560119629 | Test Loss: 1.318172574043274\n",
      "Epoch 8382 | Train Loss: 1.0762617588043213 | Test Loss: 1.113546371459961\n",
      "Epoch 8383 | Train Loss: 1.2158923149108887 | Test Loss: 1.3181753158569336\n",
      "Epoch 8384 | Train Loss: 1.0762462615966797 | Test Loss: 1.113409161567688\n",
      "Epoch 8385 | Train Loss: 1.2158857583999634 | Test Loss: 1.3182035684585571\n",
      "Epoch 8386 | Train Loss: 1.076233983039856 | Test Loss: 1.1135631799697876\n",
      "Epoch 8387 | Train Loss: 1.2158666849136353 | Test Loss: 1.3182235956192017\n",
      "Epoch 8388 | Train Loss: 1.076222538948059 | Test Loss: 1.1133923530578613\n",
      "Epoch 8389 | Train Loss: 1.215857982635498 | Test Loss: 1.318143606185913\n",
      "Epoch 8390 | Train Loss: 1.0762064456939697 | Test Loss: 1.1135542392730713\n",
      "Epoch 8391 | Train Loss: 1.2158383131027222 | Test Loss: 1.318200707435608\n",
      "Epoch 8392 | Train Loss: 1.0761938095092773 | Test Loss: 1.1133874654769897\n",
      "Epoch 8393 | Train Loss: 1.2158207893371582 | Test Loss: 1.3181300163269043\n",
      "Epoch 8394 | Train Loss: 1.0761773586273193 | Test Loss: 1.113524079322815\n",
      "Epoch 8395 | Train Loss: 1.2157961130142212 | Test Loss: 1.318233847618103\n",
      "Epoch 8396 | Train Loss: 1.0761685371398926 | Test Loss: 1.1133884191513062\n",
      "Epoch 8397 | Train Loss: 1.2157700061798096 | Test Loss: 1.318147897720337\n",
      "Epoch 8398 | Train Loss: 1.0761505365371704 | Test Loss: 1.1134828329086304\n",
      "Epoch 8399 | Train Loss: 1.2157444953918457 | Test Loss: 1.3182573318481445\n",
      "Epoch 8400 | Train Loss: 1.076140284538269 | Test Loss: 1.1133856773376465\n",
      "Epoch 8401 | Train Loss: 1.2157233953475952 | Test Loss: 1.3181846141815186\n",
      "Epoch 8402 | Train Loss: 1.0761220455169678 | Test Loss: 1.113455891609192\n",
      "Epoch 8403 | Train Loss: 1.2157036066055298 | Test Loss: 1.3182756900787354\n",
      "Epoch 8404 | Train Loss: 1.0761171579360962 | Test Loss: 1.1133806705474854\n",
      "Epoch 8405 | Train Loss: 1.2156838178634644 | Test Loss: 1.3181432485580444\n",
      "Epoch 8406 | Train Loss: 1.0760979652404785 | Test Loss: 1.1134508848190308\n",
      "Epoch 8407 | Train Loss: 1.2156751155853271 | Test Loss: 1.3183060884475708\n",
      "Epoch 8408 | Train Loss: 1.0761003494262695 | Test Loss: 1.113369107246399\n",
      "Epoch 8409 | Train Loss: 1.2156579494476318 | Test Loss: 1.318143606185913\n",
      "Epoch 8410 | Train Loss: 1.076085090637207 | Test Loss: 1.1134380102157593\n",
      "Epoch 8411 | Train Loss: 1.2156527042388916 | Test Loss: 1.3183066844940186\n",
      "Epoch 8412 | Train Loss: 1.0760834217071533 | Test Loss: 1.1133767366409302\n",
      "Epoch 8413 | Train Loss: 1.2156306505203247 | Test Loss: 1.3181569576263428\n",
      "Epoch 8414 | Train Loss: 1.0760599374771118 | Test Loss: 1.113423466682434\n",
      "Epoch 8415 | Train Loss: 1.2156436443328857 | Test Loss: 1.3183006048202515\n",
      "Epoch 8416 | Train Loss: 1.0760612487792969 | Test Loss: 1.113389015197754\n",
      "Epoch 8417 | Train Loss: 1.2156263589859009 | Test Loss: 1.3181843757629395\n",
      "Epoch 8418 | Train Loss: 1.0760509967803955 | Test Loss: 1.113431453704834\n",
      "Epoch 8419 | Train Loss: 1.2156277894973755 | Test Loss: 1.3183326721191406\n",
      "Epoch 8420 | Train Loss: 1.0760420560836792 | Test Loss: 1.113391399383545\n",
      "Epoch 8421 | Train Loss: 1.215614676475525 | Test Loss: 1.31821608543396\n",
      "Epoch 8422 | Train Loss: 1.0760244131088257 | Test Loss: 1.1134494543075562\n",
      "Epoch 8423 | Train Loss: 1.2156283855438232 | Test Loss: 1.318339228630066\n",
      "Epoch 8424 | Train Loss: 1.0760250091552734 | Test Loss: 1.1133792400360107\n",
      "Epoch 8425 | Train Loss: 1.2156200408935547 | Test Loss: 1.3182117938995361\n",
      "Epoch 8426 | Train Loss: 1.0760080814361572 | Test Loss: 1.1134963035583496\n",
      "Epoch 8427 | Train Loss: 1.2156305313110352 | Test Loss: 1.3183338642120361\n",
      "Epoch 8428 | Train Loss: 1.0759936571121216 | Test Loss: 1.1133582592010498\n",
      "Epoch 8429 | Train Loss: 1.2156461477279663 | Test Loss: 1.3182231187820435\n",
      "Epoch 8430 | Train Loss: 1.075971245765686 | Test Loss: 1.1135388612747192\n",
      "Epoch 8431 | Train Loss: 1.2156647443771362 | Test Loss: 1.3183317184448242\n",
      "Epoch 8432 | Train Loss: 1.0759618282318115 | Test Loss: 1.113345742225647\n",
      "Epoch 8433 | Train Loss: 1.215672492980957 | Test Loss: 1.3182415962219238\n",
      "Epoch 8434 | Train Loss: 1.0759278535842896 | Test Loss: 1.1135560274124146\n",
      "Epoch 8435 | Train Loss: 1.2156490087509155 | Test Loss: 1.3182891607284546\n",
      "Epoch 8436 | Train Loss: 1.0759031772613525 | Test Loss: 1.1133335828781128\n",
      "Epoch 8437 | Train Loss: 1.2156591415405273 | Test Loss: 1.3182322978973389\n",
      "Epoch 8438 | Train Loss: 1.0758821964263916 | Test Loss: 1.113523006439209\n",
      "Epoch 8439 | Train Loss: 1.2156054973602295 | Test Loss: 1.3182218074798584\n",
      "Epoch 8440 | Train Loss: 1.0758615732192993 | Test Loss: 1.1133283376693726\n",
      "Epoch 8441 | Train Loss: 1.21556556224823 | Test Loss: 1.3182471990585327\n",
      "Epoch 8442 | Train Loss: 1.0758445262908936 | Test Loss: 1.1134666204452515\n",
      "Epoch 8443 | Train Loss: 1.215501070022583 | Test Loss: 1.318231225013733\n",
      "Epoch 8444 | Train Loss: 1.0758342742919922 | Test Loss: 1.1132988929748535\n",
      "Epoch 8445 | Train Loss: 1.2154520750045776 | Test Loss: 1.3182321786880493\n",
      "Epoch 8446 | Train Loss: 1.075821042060852 | Test Loss: 1.1133959293365479\n",
      "Epoch 8447 | Train Loss: 1.2154074907302856 | Test Loss: 1.3182344436645508\n",
      "Epoch 8448 | Train Loss: 1.0758100748062134 | Test Loss: 1.1132715940475464\n",
      "Epoch 8449 | Train Loss: 1.2153546810150146 | Test Loss: 1.3182963132858276\n",
      "Epoch 8450 | Train Loss: 1.075808048248291 | Test Loss: 1.1132994890213013\n",
      "Epoch 8451 | Train Loss: 1.2153234481811523 | Test Loss: 1.318246841430664\n",
      "Epoch 8452 | Train Loss: 1.0757899284362793 | Test Loss: 1.1132856607437134\n",
      "Epoch 8453 | Train Loss: 1.2152907848358154 | Test Loss: 1.318311333656311\n",
      "Epoch 8454 | Train Loss: 1.0757811069488525 | Test Loss: 1.1132394075393677\n",
      "Epoch 8455 | Train Loss: 1.2152754068374634 | Test Loss: 1.3182481527328491\n",
      "Epoch 8456 | Train Loss: 1.0757628679275513 | Test Loss: 1.113314151763916\n",
      "Epoch 8457 | Train Loss: 1.2152589559555054 | Test Loss: 1.318272352218628\n",
      "Epoch 8458 | Train Loss: 1.075748324394226 | Test Loss: 1.113217830657959\n",
      "Epoch 8459 | Train Loss: 1.2152513265609741 | Test Loss: 1.3182579278945923\n",
      "Epoch 8460 | Train Loss: 1.0757355690002441 | Test Loss: 1.1133339405059814\n",
      "Epoch 8461 | Train Loss: 1.2152351140975952 | Test Loss: 1.318280816078186\n",
      "Epoch 8462 | Train Loss: 1.0757197141647339 | Test Loss: 1.1132011413574219\n",
      "Epoch 8463 | Train Loss: 1.21523118019104 | Test Loss: 1.3182724714279175\n",
      "Epoch 8464 | Train Loss: 1.0757064819335938 | Test Loss: 1.1133352518081665\n",
      "Epoch 8465 | Train Loss: 1.215218424797058 | Test Loss: 1.3183149099349976\n",
      "Epoch 8466 | Train Loss: 1.0756996870040894 | Test Loss: 1.1131829023361206\n",
      "Epoch 8467 | Train Loss: 1.2152138948440552 | Test Loss: 1.318250060081482\n",
      "Epoch 8468 | Train Loss: 1.0756794214248657 | Test Loss: 1.113348126411438\n",
      "Epoch 8469 | Train Loss: 1.2151983976364136 | Test Loss: 1.3183406591415405\n",
      "Epoch 8470 | Train Loss: 1.075674057006836 | Test Loss: 1.1131585836410522\n",
      "Epoch 8471 | Train Loss: 1.2151819467544556 | Test Loss: 1.3182382583618164\n",
      "Epoch 8472 | Train Loss: 1.0756555795669556 | Test Loss: 1.1133514642715454\n",
      "Epoch 8473 | Train Loss: 1.2151660919189453 | Test Loss: 1.3183501958847046\n",
      "Epoch 8474 | Train Loss: 1.0756452083587646 | Test Loss: 1.1131569147109985\n",
      "Epoch 8475 | Train Loss: 1.2151590585708618 | Test Loss: 1.3182692527770996\n",
      "Epoch 8476 | Train Loss: 1.0756281614303589 | Test Loss: 1.1133489608764648\n",
      "Epoch 8477 | Train Loss: 1.2151410579681396 | Test Loss: 1.3183565139770508\n",
      "Epoch 8478 | Train Loss: 1.0756185054779053 | Test Loss: 1.1131621599197388\n",
      "Epoch 8479 | Train Loss: 1.2151294946670532 | Test Loss: 1.3183069229125977\n",
      "Epoch 8480 | Train Loss: 1.075600028038025 | Test Loss: 1.1133140325546265\n",
      "Epoch 8481 | Train Loss: 1.2151098251342773 | Test Loss: 1.3183521032333374\n",
      "Epoch 8482 | Train Loss: 1.0755860805511475 | Test Loss: 1.1131702661514282\n",
      "Epoch 8483 | Train Loss: 1.2150884866714478 | Test Loss: 1.3183560371398926\n",
      "Epoch 8484 | Train Loss: 1.0755723714828491 | Test Loss: 1.1132816076278687\n",
      "Epoch 8485 | Train Loss: 1.2150747776031494 | Test Loss: 1.3183401823043823\n",
      "Epoch 8486 | Train Loss: 1.075556755065918 | Test Loss: 1.1131819486618042\n",
      "Epoch 8487 | Train Loss: 1.2150522470474243 | Test Loss: 1.3183751106262207\n",
      "Epoch 8488 | Train Loss: 1.07554292678833 | Test Loss: 1.1132711172103882\n",
      "Epoch 8489 | Train Loss: 1.21503484249115 | Test Loss: 1.3183212280273438\n",
      "Epoch 8490 | Train Loss: 1.07552969455719 | Test Loss: 1.1131683588027954\n",
      "Epoch 8491 | Train Loss: 1.2150070667266846 | Test Loss: 1.318363070487976\n",
      "Epoch 8492 | Train Loss: 1.075516700744629 | Test Loss: 1.1132367849349976\n",
      "Epoch 8493 | Train Loss: 1.214982271194458 | Test Loss: 1.3183040618896484\n",
      "Epoch 8494 | Train Loss: 1.0755044221878052 | Test Loss: 1.1131446361541748\n",
      "Epoch 8495 | Train Loss: 1.2149525880813599 | Test Loss: 1.3183526992797852\n",
      "Epoch 8496 | Train Loss: 1.0754896402359009 | Test Loss: 1.1132054328918457\n",
      "Epoch 8497 | Train Loss: 1.2149324417114258 | Test Loss: 1.3183221817016602\n",
      "Epoch 8498 | Train Loss: 1.0754762887954712 | Test Loss: 1.1131292581558228\n",
      "Epoch 8499 | Train Loss: 1.2149031162261963 | Test Loss: 1.3183470964431763\n",
      "Epoch 8500 | Train Loss: 1.0754663944244385 | Test Loss: 1.113170862197876\n",
      "Epoch 8501 | Train Loss: 1.2148863077163696 | Test Loss: 1.3183598518371582\n",
      "Epoch 8502 | Train Loss: 1.0754518508911133 | Test Loss: 1.1131110191345215\n",
      "Epoch 8503 | Train Loss: 1.2148686647415161 | Test Loss: 1.318354845046997\n",
      "Epoch 8504 | Train Loss: 1.0754410028457642 | Test Loss: 1.1131552457809448\n",
      "Epoch 8505 | Train Loss: 1.2148470878601074 | Test Loss: 1.3183645009994507\n",
      "Epoch 8506 | Train Loss: 1.0754294395446777 | Test Loss: 1.1131190061569214\n",
      "Epoch 8507 | Train Loss: 1.2148315906524658 | Test Loss: 1.3183828592300415\n",
      "Epoch 8508 | Train Loss: 1.0754112005233765 | Test Loss: 1.113139271736145\n",
      "Epoch 8509 | Train Loss: 1.214812159538269 | Test Loss: 1.3183588981628418\n",
      "Epoch 8510 | Train Loss: 1.0753982067108154 | Test Loss: 1.1131153106689453\n",
      "Epoch 8511 | Train Loss: 1.214798927307129 | Test Loss: 1.3183608055114746\n",
      "Epoch 8512 | Train Loss: 1.0753846168518066 | Test Loss: 1.1131505966186523\n",
      "Epoch 8513 | Train Loss: 1.2147823572158813 | Test Loss: 1.318359375\n",
      "Epoch 8514 | Train Loss: 1.0753722190856934 | Test Loss: 1.1131043434143066\n",
      "Epoch 8515 | Train Loss: 1.2147653102874756 | Test Loss: 1.3183695077896118\n",
      "Epoch 8516 | Train Loss: 1.0753588676452637 | Test Loss: 1.1131304502487183\n",
      "Epoch 8517 | Train Loss: 1.2147458791732788 | Test Loss: 1.3183633089065552\n",
      "Epoch 8518 | Train Loss: 1.0753438472747803 | Test Loss: 1.1130932569503784\n",
      "Epoch 8519 | Train Loss: 1.214734673500061 | Test Loss: 1.3183753490447998\n",
      "Epoch 8520 | Train Loss: 1.075331211090088 | Test Loss: 1.1131155490875244\n",
      "Epoch 8521 | Train Loss: 1.2147167921066284 | Test Loss: 1.318394660949707\n",
      "Epoch 8522 | Train Loss: 1.0753169059753418 | Test Loss: 1.1130931377410889\n",
      "Epoch 8523 | Train Loss: 1.214703917503357 | Test Loss: 1.3184231519699097\n",
      "Epoch 8524 | Train Loss: 1.0753052234649658 | Test Loss: 1.1130969524383545\n",
      "Epoch 8525 | Train Loss: 1.2146893739700317 | Test Loss: 1.318406581878662\n",
      "Epoch 8526 | Train Loss: 1.0752935409545898 | Test Loss: 1.1130791902542114\n",
      "Epoch 8527 | Train Loss: 1.2146707773208618 | Test Loss: 1.3184356689453125\n",
      "Epoch 8528 | Train Loss: 1.0752851963043213 | Test Loss: 1.113114356994629\n",
      "Epoch 8529 | Train Loss: 1.2146553993225098 | Test Loss: 1.3184268474578857\n",
      "Epoch 8530 | Train Loss: 1.0752710103988647 | Test Loss: 1.1130568981170654\n",
      "Epoch 8531 | Train Loss: 1.2146393060684204 | Test Loss: 1.3184477090835571\n",
      "Epoch 8532 | Train Loss: 1.0752655267715454 | Test Loss: 1.1131078004837036\n",
      "Epoch 8533 | Train Loss: 1.2146141529083252 | Test Loss: 1.3184367418289185\n",
      "Epoch 8534 | Train Loss: 1.075252890586853 | Test Loss: 1.1130326986312866\n",
      "Epoch 8535 | Train Loss: 1.2146053314208984 | Test Loss: 1.3184571266174316\n",
      "Epoch 8536 | Train Loss: 1.075232744216919 | Test Loss: 1.113102674484253\n",
      "Epoch 8537 | Train Loss: 1.2145856618881226 | Test Loss: 1.3184269666671753\n",
      "Epoch 8538 | Train Loss: 1.0752230882644653 | Test Loss: 1.11301589012146\n",
      "Epoch 8539 | Train Loss: 1.2145723104476929 | Test Loss: 1.3184236288070679\n",
      "Epoch 8540 | Train Loss: 1.075208067893982 | Test Loss: 1.1130861043930054\n",
      "Epoch 8541 | Train Loss: 1.214555025100708 | Test Loss: 1.3184914588928223\n",
      "Epoch 8542 | Train Loss: 1.0752098560333252 | Test Loss: 1.113012671470642\n",
      "Epoch 8543 | Train Loss: 1.2145371437072754 | Test Loss: 1.3184096813201904\n",
      "Epoch 8544 | Train Loss: 1.0752016305923462 | Test Loss: 1.1130789518356323\n",
      "Epoch 8545 | Train Loss: 1.214516520500183 | Test Loss: 1.3185293674468994\n",
      "Epoch 8546 | Train Loss: 1.075194001197815 | Test Loss: 1.113012433052063\n",
      "Epoch 8547 | Train Loss: 1.214516043663025 | Test Loss: 1.3183989524841309\n",
      "Epoch 8548 | Train Loss: 1.0751796960830688 | Test Loss: 1.1130810976028442\n",
      "Epoch 8549 | Train Loss: 1.214499831199646 | Test Loss: 1.3185250759124756\n",
      "Epoch 8550 | Train Loss: 1.075181484222412 | Test Loss: 1.1129804849624634\n",
      "Epoch 8551 | Train Loss: 1.2145020961761475 | Test Loss: 1.318415880203247\n",
      "Epoch 8552 | Train Loss: 1.0751711130142212 | Test Loss: 1.1131267547607422\n",
      "Epoch 8553 | Train Loss: 1.2145005464553833 | Test Loss: 1.3185750246047974\n",
      "Epoch 8554 | Train Loss: 1.075176477432251 | Test Loss: 1.112922191619873\n",
      "Epoch 8555 | Train Loss: 1.2145133018493652 | Test Loss: 1.318445086479187\n",
      "Epoch 8556 | Train Loss: 1.0751540660858154 | Test Loss: 1.1131943464279175\n",
      "Epoch 8557 | Train Loss: 1.2145376205444336 | Test Loss: 1.318584680557251\n",
      "Epoch 8558 | Train Loss: 1.0751516819000244 | Test Loss: 1.1129120588302612\n",
      "Epoch 8559 | Train Loss: 1.2145636081695557 | Test Loss: 1.3184782266616821\n",
      "Epoch 8560 | Train Loss: 1.075127363204956 | Test Loss: 1.11324143409729\n",
      "Epoch 8561 | Train Loss: 1.214575171470642 | Test Loss: 1.3185962438583374\n",
      "Epoch 8562 | Train Loss: 1.0751149654388428 | Test Loss: 1.1129430532455444\n",
      "Epoch 8563 | Train Loss: 1.2146022319793701 | Test Loss: 1.3184709548950195\n",
      "Epoch 8564 | Train Loss: 1.0750837326049805 | Test Loss: 1.1132413148880005\n",
      "Epoch 8565 | Train Loss: 1.2145966291427612 | Test Loss: 1.3185832500457764\n",
      "Epoch 8566 | Train Loss: 1.0750786066055298 | Test Loss: 1.1129589080810547\n",
      "Epoch 8567 | Train Loss: 1.2145711183547974 | Test Loss: 1.318481683731079\n",
      "Epoch 8568 | Train Loss: 1.0750492811203003 | Test Loss: 1.1132044792175293\n",
      "Epoch 8569 | Train Loss: 1.2145332098007202 | Test Loss: 1.318566918373108\n",
      "Epoch 8570 | Train Loss: 1.0750452280044556 | Test Loss: 1.112927794456482\n",
      "Epoch 8571 | Train Loss: 1.2145036458969116 | Test Loss: 1.3185234069824219\n",
      "Epoch 8572 | Train Loss: 1.0750353336334229 | Test Loss: 1.113184928894043\n",
      "Epoch 8573 | Train Loss: 1.2144756317138672 | Test Loss: 1.3185253143310547\n",
      "Epoch 8574 | Train Loss: 1.0750302076339722 | Test Loss: 1.1128950119018555\n",
      "Epoch 8575 | Train Loss: 1.2144255638122559 | Test Loss: 1.3185601234436035\n",
      "Epoch 8576 | Train Loss: 1.0750178098678589 | Test Loss: 1.113120198249817\n",
      "Epoch 8577 | Train Loss: 1.2143769264221191 | Test Loss: 1.3185012340545654\n",
      "Epoch 8578 | Train Loss: 1.075014352798462 | Test Loss: 1.1129080057144165\n",
      "Epoch 8579 | Train Loss: 1.21433687210083 | Test Loss: 1.318593144416809\n",
      "Epoch 8580 | Train Loss: 1.0749973058700562 | Test Loss: 1.113023042678833\n",
      "Epoch 8581 | Train Loss: 1.2143033742904663 | Test Loss: 1.3184973001480103\n",
      "Epoch 8582 | Train Loss: 1.0749884843826294 | Test Loss: 1.1129553318023682\n",
      "Epoch 8583 | Train Loss: 1.2142572402954102 | Test Loss: 1.318602204322815\n",
      "Epoch 8584 | Train Loss: 1.074973464012146 | Test Loss: 1.1129392385482788\n",
      "Epoch 8585 | Train Loss: 1.2142541408538818 | Test Loss: 1.3185359239578247\n",
      "Epoch 8586 | Train Loss: 1.0749598741531372 | Test Loss: 1.112978219985962\n",
      "Epoch 8587 | Train Loss: 1.2142289876937866 | Test Loss: 1.3185818195343018\n",
      "Epoch 8588 | Train Loss: 1.0749491453170776 | Test Loss: 1.112911343574524\n",
      "Epoch 8589 | Train Loss: 1.2142258882522583 | Test Loss: 1.3185421228408813\n",
      "Epoch 8590 | Train Loss: 1.0749393701553345 | Test Loss: 1.1129930019378662\n",
      "Epoch 8591 | Train Loss: 1.2142006158828735 | Test Loss: 1.3185651302337646\n",
      "Epoch 8592 | Train Loss: 1.0749199390411377 | Test Loss: 1.1129056215286255\n",
      "Epoch 8593 | Train Loss: 1.214205026626587 | Test Loss: 1.3185486793518066\n",
      "Epoch 8594 | Train Loss: 1.0749105215072632 | Test Loss: 1.1130075454711914\n",
      "Epoch 8595 | Train Loss: 1.2141917943954468 | Test Loss: 1.3185697793960571\n",
      "Epoch 8596 | Train Loss: 1.0748867988586426 | Test Loss: 1.1128796339035034\n",
      "Epoch 8597 | Train Loss: 1.214186429977417 | Test Loss: 1.3185278177261353\n",
      "Epoch 8598 | Train Loss: 1.0748761892318726 | Test Loss: 1.1130015850067139\n",
      "Epoch 8599 | Train Loss: 1.2141599655151367 | Test Loss: 1.318579912185669\n",
      "Epoch 8600 | Train Loss: 1.074854850769043 | Test Loss: 1.1128543615341187\n",
      "Epoch 8601 | Train Loss: 1.214139461517334 | Test Loss: 1.3185217380523682\n",
      "Epoch 8602 | Train Loss: 1.074842095375061 | Test Loss: 1.1129653453826904\n",
      "Epoch 8603 | Train Loss: 1.2141140699386597 | Test Loss: 1.318572998046875\n",
      "Epoch 8604 | Train Loss: 1.0748302936553955 | Test Loss: 1.1128443479537964\n",
      "Epoch 8605 | Train Loss: 1.21408212184906 | Test Loss: 1.3185460567474365\n",
      "Epoch 8606 | Train Loss: 1.074824333190918 | Test Loss: 1.112902045249939\n",
      "Epoch 8607 | Train Loss: 1.2140629291534424 | Test Loss: 1.318549394607544\n",
      "Epoch 8608 | Train Loss: 1.0748043060302734 | Test Loss: 1.1128653287887573\n",
      "Epoch 8609 | Train Loss: 1.2140475511550903 | Test Loss: 1.3186061382293701\n",
      "Epoch 8610 | Train Loss: 1.074812889099121 | Test Loss: 1.1128307580947876\n",
      "Epoch 8611 | Train Loss: 1.2140424251556396 | Test Loss: 1.3185515403747559\n",
      "Epoch 8612 | Train Loss: 1.0747870206832886 | Test Loss: 1.1129299402236938\n",
      "Epoch 8613 | Train Loss: 1.2140532732009888 | Test Loss: 1.31864333152771\n",
      "Epoch 8614 | Train Loss: 1.0747911930084229 | Test Loss: 1.1127872467041016\n",
      "Epoch 8615 | Train Loss: 1.2140672206878662 | Test Loss: 1.3185982704162598\n",
      "Epoch 8616 | Train Loss: 1.0747662782669067 | Test Loss: 1.112982988357544\n",
      "Epoch 8617 | Train Loss: 1.2140846252441406 | Test Loss: 1.3186733722686768\n",
      "Epoch 8618 | Train Loss: 1.074764609336853 | Test Loss: 1.1127756834030151\n",
      "Epoch 8619 | Train Loss: 1.214117407798767 | Test Loss: 1.318626046180725\n",
      "Epoch 8620 | Train Loss: 1.0747432708740234 | Test Loss: 1.113023281097412\n",
      "Epoch 8621 | Train Loss: 1.2141293287277222 | Test Loss: 1.3186867237091064\n",
      "Epoch 8622 | Train Loss: 1.0747427940368652 | Test Loss: 1.1128026247024536\n",
      "Epoch 8623 | Train Loss: 1.2141462564468384 | Test Loss: 1.3186334371566772\n",
      "Epoch 8624 | Train Loss: 1.074712872505188 | Test Loss: 1.1130619049072266\n",
      "Epoch 8625 | Train Loss: 1.2141523361206055 | Test Loss: 1.3186485767364502\n",
      "Epoch 8626 | Train Loss: 1.0747069120407104 | Test Loss: 1.112813115119934\n",
      "Epoch 8627 | Train Loss: 1.2141532897949219 | Test Loss: 1.3186662197113037\n",
      "Epoch 8628 | Train Loss: 1.0746909379959106 | Test Loss: 1.113040804862976\n",
      "Epoch 8629 | Train Loss: 1.2141268253326416 | Test Loss: 1.3185975551605225\n",
      "Epoch 8630 | Train Loss: 1.0746886730194092 | Test Loss: 1.1128041744232178\n",
      "Epoch 8631 | Train Loss: 1.2140648365020752 | Test Loss: 1.3187243938446045\n",
      "Epoch 8632 | Train Loss: 1.0746759176254272 | Test Loss: 1.112943172454834\n",
      "Epoch 8633 | Train Loss: 1.2140092849731445 | Test Loss: 1.3185666799545288\n",
      "Epoch 8634 | Train Loss: 1.074679970741272 | Test Loss: 1.1128122806549072\n",
      "Epoch 8635 | Train Loss: 1.213935375213623 | Test Loss: 1.3188000917434692\n",
      "Epoch 8636 | Train Loss: 1.0746734142303467 | Test Loss: 1.1128308773040771\n",
      "Epoch 8637 | Train Loss: 1.2138938903808594 | Test Loss: 1.318529486656189\n",
      "Epoch 8638 | Train Loss: 1.0746772289276123 | Test Loss: 1.112835168838501\n",
      "Epoch 8639 | Train Loss: 1.213860273361206 | Test Loss: 1.3188669681549072\n",
      "Epoch 8640 | Train Loss: 1.074684500694275 | Test Loss: 1.1127547025680542\n",
      "Epoch 8641 | Train Loss: 1.2138477563858032 | Test Loss: 1.318528175354004\n",
      "Epoch 8642 | Train Loss: 1.0746750831604004 | Test Loss: 1.1128710508346558\n",
      "Epoch 8643 | Train Loss: 1.2138539552688599 | Test Loss: 1.3188575506210327\n",
      "Epoch 8644 | Train Loss: 1.0746692419052124 | Test Loss: 1.1127634048461914\n",
      "Epoch 8645 | Train Loss: 1.2138651609420776 | Test Loss: 1.3185625076293945\n",
      "Epoch 8646 | Train Loss: 1.074649453163147 | Test Loss: 1.1129158735275269\n",
      "Epoch 8647 | Train Loss: 1.2138952016830444 | Test Loss: 1.3188462257385254\n",
      "Epoch 8648 | Train Loss: 1.074639916419983 | Test Loss: 1.1127941608428955\n",
      "Epoch 8649 | Train Loss: 1.213942289352417 | Test Loss: 1.318610668182373\n",
      "Epoch 8650 | Train Loss: 1.0746220350265503 | Test Loss: 1.1129716634750366\n",
      "Epoch 8651 | Train Loss: 1.2139809131622314 | Test Loss: 1.3188427686691284\n",
      "Epoch 8652 | Train Loss: 1.0745960474014282 | Test Loss: 1.1128560304641724\n",
      "Epoch 8653 | Train Loss: 1.2140223979949951 | Test Loss: 1.3185863494873047\n",
      "Epoch 8654 | Train Loss: 1.074562668800354 | Test Loss: 1.1130229234695435\n",
      "Epoch 8655 | Train Loss: 1.2140530347824097 | Test Loss: 1.3187681436538696\n",
      "Epoch 8656 | Train Loss: 1.0745337009429932 | Test Loss: 1.1129034757614136\n",
      "Epoch 8657 | Train Loss: 1.21405029296875 | Test Loss: 1.318602204322815\n",
      "Epoch 8658 | Train Loss: 1.0744988918304443 | Test Loss: 1.1130048036575317\n",
      "Epoch 8659 | Train Loss: 1.2140213251113892 | Test Loss: 1.3186473846435547\n",
      "Epoch 8660 | Train Loss: 1.0744701623916626 | Test Loss: 1.1129034757614136\n",
      "Epoch 8661 | Train Loss: 1.2139580249786377 | Test Loss: 1.3186575174331665\n",
      "Epoch 8662 | Train Loss: 1.0744527578353882 | Test Loss: 1.112876534461975\n",
      "Epoch 8663 | Train Loss: 1.2138715982437134 | Test Loss: 1.3186180591583252\n",
      "Epoch 8664 | Train Loss: 1.0744423866271973 | Test Loss: 1.1128263473510742\n",
      "Epoch 8665 | Train Loss: 1.2137724161148071 | Test Loss: 1.3187496662139893\n",
      "Epoch 8666 | Train Loss: 1.0744417905807495 | Test Loss: 1.1127182245254517\n",
      "Epoch 8667 | Train Loss: 1.2136716842651367 | Test Loss: 1.318629503250122\n",
      "Epoch 8668 | Train Loss: 1.0744317770004272 | Test Loss: 1.112724781036377\n",
      "Epoch 8669 | Train Loss: 1.2135958671569824 | Test Loss: 1.3187469244003296\n",
      "Epoch 8670 | Train Loss: 1.0744227170944214 | Test Loss: 1.1126211881637573\n",
      "Epoch 8671 | Train Loss: 1.2135441303253174 | Test Loss: 1.3186357021331787\n",
      "Epoch 8672 | Train Loss: 1.0744081735610962 | Test Loss: 1.112671136856079\n",
      "Epoch 8673 | Train Loss: 1.2135071754455566 | Test Loss: 1.3187652826309204\n",
      "Epoch 8674 | Train Loss: 1.0743917226791382 | Test Loss: 1.1125985383987427\n",
      "Epoch 8675 | Train Loss: 1.213488221168518 | Test Loss: 1.3186017274856567\n",
      "Epoch 8676 | Train Loss: 1.0743685960769653 | Test Loss: 1.1126447916030884\n",
      "Epoch 8677 | Train Loss: 1.213475227355957 | Test Loss: 1.3187479972839355\n",
      "Epoch 8678 | Train Loss: 1.074355125427246 | Test Loss: 1.1126004457473755\n",
      "Epoch 8679 | Train Loss: 1.2134605646133423 | Test Loss: 1.3186097145080566\n",
      "Epoch 8680 | Train Loss: 1.074336051940918 | Test Loss: 1.1126377582550049\n",
      "Epoch 8681 | Train Loss: 1.2134531736373901 | Test Loss: 1.3187178373336792\n",
      "Epoch 8682 | Train Loss: 1.074321985244751 | Test Loss: 1.1126060485839844\n",
      "Epoch 8683 | Train Loss: 1.2134429216384888 | Test Loss: 1.3186249732971191\n",
      "Epoch 8684 | Train Loss: 1.0743074417114258 | Test Loss: 1.112620234489441\n",
      "Epoch 8685 | Train Loss: 1.2134305238723755 | Test Loss: 1.3186806440353394\n",
      "Epoch 8686 | Train Loss: 1.074296474456787 | Test Loss: 1.1126116514205933\n",
      "Epoch 8687 | Train Loss: 1.2134109735488892 | Test Loss: 1.318656086921692\n",
      "Epoch 8688 | Train Loss: 1.0742828845977783 | Test Loss: 1.1125954389572144\n",
      "Epoch 8689 | Train Loss: 1.213396668434143 | Test Loss: 1.3187015056610107\n",
      "Epoch 8690 | Train Loss: 1.0742720365524292 | Test Loss: 1.112610101699829\n",
      "Epoch 8691 | Train Loss: 1.2133785486221313 | Test Loss: 1.3187074661254883\n",
      "Epoch 8692 | Train Loss: 1.0742610692977905 | Test Loss: 1.1125738620758057\n",
      "Epoch 8693 | Train Loss: 1.2133558988571167 | Test Loss: 1.3187146186828613\n",
      "Epoch 8694 | Train Loss: 1.0742472410202026 | Test Loss: 1.1126035451889038\n",
      "Epoch 8695 | Train Loss: 1.21333909034729 | Test Loss: 1.3187094926834106\n",
      "Epoch 8696 | Train Loss: 1.074234962463379 | Test Loss: 1.1125611066818237\n",
      "Epoch 8697 | Train Loss: 1.2133188247680664 | Test Loss: 1.3187068700790405\n",
      "Epoch 8698 | Train Loss: 1.0742230415344238 | Test Loss: 1.1125831604003906\n",
      "Epoch 8699 | Train Loss: 1.2132989168167114 | Test Loss: 1.31871497631073\n",
      "Epoch 8700 | Train Loss: 1.0742100477218628 | Test Loss: 1.1125576496124268\n",
      "Epoch 8701 | Train Loss: 1.213281512260437 | Test Loss: 1.3187317848205566\n",
      "Epoch 8702 | Train Loss: 1.0741972923278809 | Test Loss: 1.1125634908676147\n",
      "Epoch 8703 | Train Loss: 1.2132622003555298 | Test Loss: 1.3187479972839355\n",
      "Epoch 8704 | Train Loss: 1.0741832256317139 | Test Loss: 1.112550973892212\n",
      "Epoch 8705 | Train Loss: 1.2132446765899658 | Test Loss: 1.3187341690063477\n",
      "Epoch 8706 | Train Loss: 1.0741710662841797 | Test Loss: 1.1125353574752808\n",
      "Epoch 8707 | Train Loss: 1.213229775428772 | Test Loss: 1.3187366724014282\n",
      "Epoch 8708 | Train Loss: 1.0741586685180664 | Test Loss: 1.112533688545227\n",
      "Epoch 8709 | Train Loss: 1.2132148742675781 | Test Loss: 1.318727970123291\n",
      "Epoch 8710 | Train Loss: 1.0741472244262695 | Test Loss: 1.1125092506408691\n",
      "Epoch 8711 | Train Loss: 1.2131967544555664 | Test Loss: 1.3187471628189087\n",
      "Epoch 8712 | Train Loss: 1.0741355419158936 | Test Loss: 1.1125249862670898\n",
      "Epoch 8713 | Train Loss: 1.2131810188293457 | Test Loss: 1.3187528848648071\n",
      "Epoch 8714 | Train Loss: 1.0741229057312012 | Test Loss: 1.112507939338684\n",
      "Epoch 8715 | Train Loss: 1.2131648063659668 | Test Loss: 1.3187199831008911\n",
      "Epoch 8716 | Train Loss: 1.0741134881973267 | Test Loss: 1.1125143766403198\n",
      "Epoch 8717 | Train Loss: 1.2131459712982178 | Test Loss: 1.3187776803970337\n",
      "Epoch 8718 | Train Loss: 1.07410728931427 | Test Loss: 1.1125035285949707\n",
      "Epoch 8719 | Train Loss: 1.213131308555603 | Test Loss: 1.318703293800354\n",
      "Epoch 8720 | Train Loss: 1.0740877389907837 | Test Loss: 1.112493872642517\n",
      "Epoch 8721 | Train Loss: 1.2131160497665405 | Test Loss: 1.3187849521636963\n",
      "Epoch 8722 | Train Loss: 1.0740801095962524 | Test Loss: 1.1125071048736572\n",
      "Epoch 8723 | Train Loss: 1.2130986452102661 | Test Loss: 1.3187580108642578\n",
      "Epoch 8724 | Train Loss: 1.074066400527954 | Test Loss: 1.1124815940856934\n",
      "Epoch 8725 | Train Loss: 1.2130861282348633 | Test Loss: 1.3187956809997559\n",
      "Epoch 8726 | Train Loss: 1.07405424118042 | Test Loss: 1.1125023365020752\n",
      "Epoch 8727 | Train Loss: 1.2130697965621948 | Test Loss: 1.3187676668167114\n",
      "Epoch 8728 | Train Loss: 1.0740445852279663 | Test Loss: 1.11246919631958\n",
      "Epoch 8729 | Train Loss: 1.2130565643310547 | Test Loss: 1.3188029527664185\n",
      "Epoch 8730 | Train Loss: 1.0740351676940918 | Test Loss: 1.1124836206436157\n",
      "Epoch 8731 | Train Loss: 1.2130407094955444 | Test Loss: 1.3187659978866577\n",
      "Epoch 8732 | Train Loss: 1.0740240812301636 | Test Loss: 1.1124634742736816\n",
      "Epoch 8733 | Train Loss: 1.2130310535430908 | Test Loss: 1.3188209533691406\n",
      "Epoch 8734 | Train Loss: 1.0740188360214233 | Test Loss: 1.11247980594635\n",
      "Epoch 8735 | Train Loss: 1.213014841079712 | Test Loss: 1.318782925605774\n",
      "Epoch 8736 | Train Loss: 1.0740067958831787 | Test Loss: 1.1124556064605713\n",
      "Epoch 8737 | Train Loss: 1.213009238243103 | Test Loss: 1.31884765625\n",
      "Epoch 8738 | Train Loss: 1.0739983320236206 | Test Loss: 1.112493634223938\n",
      "Epoch 8739 | Train Loss: 1.2129900455474854 | Test Loss: 1.318784236907959\n",
      "Epoch 8740 | Train Loss: 1.0739864110946655 | Test Loss: 1.1124248504638672\n",
      "Epoch 8741 | Train Loss: 1.212977409362793 | Test Loss: 1.318856954574585\n",
      "Epoch 8742 | Train Loss: 1.0739747285842896 | Test Loss: 1.1125049591064453\n",
      "Epoch 8743 | Train Loss: 1.21296226978302 | Test Loss: 1.318825364112854\n",
      "Epoch 8744 | Train Loss: 1.0739619731903076 | Test Loss: 1.1124049425125122\n",
      "Epoch 8745 | Train Loss: 1.2129533290863037 | Test Loss: 1.3188554048538208\n",
      "Epoch 8746 | Train Loss: 1.0739498138427734 | Test Loss: 1.112504243850708\n",
      "Epoch 8747 | Train Loss: 1.2129440307617188 | Test Loss: 1.3188246488571167\n",
      "Epoch 8748 | Train Loss: 1.0739381313323975 | Test Loss: 1.1124109029769897\n",
      "Epoch 8749 | Train Loss: 1.2129307985305786 | Test Loss: 1.3188426494598389\n",
      "Epoch 8750 | Train Loss: 1.0739268064498901 | Test Loss: 1.1124850511550903\n",
      "Epoch 8751 | Train Loss: 1.2129210233688354 | Test Loss: 1.3188320398330688\n",
      "Epoch 8752 | Train Loss: 1.0739144086837769 | Test Loss: 1.112412691116333\n",
      "Epoch 8753 | Train Loss: 1.2129148244857788 | Test Loss: 1.3188369274139404\n",
      "Epoch 8754 | Train Loss: 1.0738979578018188 | Test Loss: 1.1124802827835083\n",
      "Epoch 8755 | Train Loss: 1.2129050493240356 | Test Loss: 1.318832516670227\n",
      "Epoch 8756 | Train Loss: 1.0738909244537354 | Test Loss: 1.112390160560608\n",
      "Epoch 8757 | Train Loss: 1.2128872871398926 | Test Loss: 1.3188798427581787\n",
      "Epoch 8758 | Train Loss: 1.0738784074783325 | Test Loss: 1.1124831438064575\n",
      "Epoch 8759 | Train Loss: 1.212878942489624 | Test Loss: 1.3188530206680298\n",
      "Epoch 8760 | Train Loss: 1.0738749504089355 | Test Loss: 1.1123703718185425\n",
      "Epoch 8761 | Train Loss: 1.212868571281433 | Test Loss: 1.3188774585723877\n",
      "Epoch 8762 | Train Loss: 1.0738623142242432 | Test Loss: 1.1124672889709473\n",
      "Epoch 8763 | Train Loss: 1.2128528356552124 | Test Loss: 1.3188958168029785\n",
      "Epoch 8764 | Train Loss: 1.0738650560379028 | Test Loss: 1.1123592853546143\n",
      "Epoch 8765 | Train Loss: 1.212835431098938 | Test Loss: 1.3189133405685425\n",
      "Epoch 8766 | Train Loss: 1.0738493204116821 | Test Loss: 1.1124347448349\n",
      "Epoch 8767 | Train Loss: 1.2128275632858276 | Test Loss: 1.3189178705215454\n",
      "Epoch 8768 | Train Loss: 1.0738497972488403 | Test Loss: 1.1123666763305664\n",
      "Epoch 8769 | Train Loss: 1.2128137350082397 | Test Loss: 1.3189060688018799\n",
      "Epoch 8770 | Train Loss: 1.0738329887390137 | Test Loss: 1.1124286651611328\n",
      "Epoch 8771 | Train Loss: 1.2128078937530518 | Test Loss: 1.3189358711242676\n",
      "Epoch 8772 | Train Loss: 1.0738437175750732 | Test Loss: 1.1123429536819458\n",
      "Epoch 8773 | Train Loss: 1.2128063440322876 | Test Loss: 1.318945050239563\n",
      "Epoch 8774 | Train Loss: 1.073826789855957 | Test Loss: 1.112442970275879\n",
      "Epoch 8775 | Train Loss: 1.2128111124038696 | Test Loss: 1.318984031677246\n",
      "Epoch 8776 | Train Loss: 1.0738294124603271 | Test Loss: 1.112351417541504\n",
      "Epoch 8777 | Train Loss: 1.2128218412399292 | Test Loss: 1.3189178705215454\n",
      "Epoch 8778 | Train Loss: 1.0738054513931274 | Test Loss: 1.1124591827392578\n",
      "Epoch 8779 | Train Loss: 1.2128450870513916 | Test Loss: 1.3190337419509888\n",
      "Epoch 8780 | Train Loss: 1.07381272315979 | Test Loss: 1.112391471862793\n",
      "Epoch 8781 | Train Loss: 1.2128567695617676 | Test Loss: 1.3188828229904175\n",
      "Epoch 8782 | Train Loss: 1.073779821395874 | Test Loss: 1.1124712228775024\n",
      "Epoch 8783 | Train Loss: 1.2128797769546509 | Test Loss: 1.3190221786499023\n",
      "Epoch 8784 | Train Loss: 1.0737731456756592 | Test Loss: 1.1124292612075806\n",
      "Epoch 8785 | Train Loss: 1.2129037380218506 | Test Loss: 1.3188852071762085\n",
      "Epoch 8786 | Train Loss: 1.0737367868423462 | Test Loss: 1.112500548362732\n",
      "Epoch 8787 | Train Loss: 1.2129240036010742 | Test Loss: 1.3190345764160156\n",
      "Epoch 8788 | Train Loss: 1.0737296342849731 | Test Loss: 1.1124162673950195\n",
      "Epoch 8789 | Train Loss: 1.2129303216934204 | Test Loss: 1.3189457654953003\n",
      "Epoch 8790 | Train Loss: 1.0736958980560303 | Test Loss: 1.1125404834747314\n",
      "Epoch 8791 | Train Loss: 1.2129299640655518 | Test Loss: 1.318975806236267\n",
      "Epoch 8792 | Train Loss: 1.0736818313598633 | Test Loss: 1.1124094724655151\n",
      "Epoch 8793 | Train Loss: 1.212917685508728 | Test Loss: 1.3189492225646973\n",
      "Epoch 8794 | Train Loss: 1.0736631155014038 | Test Loss: 1.1125043630599976\n",
      "Epoch 8795 | Train Loss: 1.2128849029541016 | Test Loss: 1.3189715147018433\n",
      "Epoch 8796 | Train Loss: 1.073654294013977 | Test Loss: 1.1123803853988647\n",
      "Epoch 8797 | Train Loss: 1.2128108739852905 | Test Loss: 1.318946361541748\n",
      "Epoch 8798 | Train Loss: 1.0736385583877563 | Test Loss: 1.1124475002288818\n",
      "Epoch 8799 | Train Loss: 1.212751865386963 | Test Loss: 1.318976879119873\n",
      "Epoch 8800 | Train Loss: 1.0736299753189087 | Test Loss: 1.11232590675354\n",
      "Epoch 8801 | Train Loss: 1.2126768827438354 | Test Loss: 1.318934679031372\n",
      "Epoch 8802 | Train Loss: 1.0736255645751953 | Test Loss: 1.1123831272125244\n",
      "Epoch 8803 | Train Loss: 1.2126260995864868 | Test Loss: 1.3189582824707031\n",
      "Epoch 8804 | Train Loss: 1.0736174583435059 | Test Loss: 1.1122527122497559\n",
      "Epoch 8805 | Train Loss: 1.212555170059204 | Test Loss: 1.3189353942871094\n",
      "Epoch 8806 | Train Loss: 1.0736197233200073 | Test Loss: 1.1123077869415283\n",
      "Epoch 8807 | Train Loss: 1.2125093936920166 | Test Loss: 1.3189584016799927\n",
      "Epoch 8808 | Train Loss: 1.0736075639724731 | Test Loss: 1.1122163534164429\n",
      "Epoch 8809 | Train Loss: 1.2124742269515991 | Test Loss: 1.3189724683761597\n",
      "Epoch 8810 | Train Loss: 1.0736005306243896 | Test Loss: 1.1122514009475708\n",
      "Epoch 8811 | Train Loss: 1.212449073791504 | Test Loss: 1.3189685344696045\n",
      "Epoch 8812 | Train Loss: 1.073581337928772 | Test Loss: 1.112233281135559\n",
      "Epoch 8813 | Train Loss: 1.212428331375122 | Test Loss: 1.3189609050750732\n",
      "Epoch 8814 | Train Loss: 1.0735667943954468 | Test Loss: 1.1122310161590576\n",
      "Epoch 8815 | Train Loss: 1.2124183177947998 | Test Loss: 1.3189632892608643\n",
      "Epoch 8816 | Train Loss: 1.0735408067703247 | Test Loss: 1.1122503280639648\n",
      "Epoch 8817 | Train Loss: 1.2124097347259521 | Test Loss: 1.3189247846603394\n",
      "Epoch 8818 | Train Loss: 1.0735324621200562 | Test Loss: 1.1122208833694458\n",
      "Epoch 8819 | Train Loss: 1.2123939990997314 | Test Loss: 1.318970799446106\n",
      "Epoch 8820 | Train Loss: 1.0735163688659668 | Test Loss: 1.1122468709945679\n",
      "Epoch 8821 | Train Loss: 1.2123849391937256 | Test Loss: 1.3189316987991333\n",
      "Epoch 8822 | Train Loss: 1.0735054016113281 | Test Loss: 1.1122232675552368\n",
      "Epoch 8823 | Train Loss: 1.2123711109161377 | Test Loss: 1.3189772367477417\n",
      "Epoch 8824 | Train Loss: 1.0734935998916626 | Test Loss: 1.1122448444366455\n",
      "Epoch 8825 | Train Loss: 1.21236252784729 | Test Loss: 1.3189525604248047\n",
      "Epoch 8826 | Train Loss: 1.0734846591949463 | Test Loss: 1.1122360229492188\n",
      "Epoch 8827 | Train Loss: 1.2123465538024902 | Test Loss: 1.3189713954925537\n",
      "Epoch 8828 | Train Loss: 1.0734719038009644 | Test Loss: 1.1122366189956665\n",
      "Epoch 8829 | Train Loss: 1.212337851524353 | Test Loss: 1.318962574005127\n",
      "Epoch 8830 | Train Loss: 1.0734663009643555 | Test Loss: 1.112229824066162\n",
      "Epoch 8831 | Train Loss: 1.2123218774795532 | Test Loss: 1.3190056085586548\n",
      "Epoch 8832 | Train Loss: 1.0734539031982422 | Test Loss: 1.1122421026229858\n",
      "Epoch 8833 | Train Loss: 1.212320327758789 | Test Loss: 1.3189769983291626\n",
      "Epoch 8834 | Train Loss: 1.0734400749206543 | Test Loss: 1.112215280532837\n",
      "Epoch 8835 | Train Loss: 1.2123085260391235 | Test Loss: 1.319068193435669\n",
      "Epoch 8836 | Train Loss: 1.0734304189682007 | Test Loss: 1.1122503280639648\n",
      "Epoch 8837 | Train Loss: 1.2123053073883057 | Test Loss: 1.318968415260315\n",
      "Epoch 8838 | Train Loss: 1.073417067527771 | Test Loss: 1.1122044324874878\n",
      "Epoch 8839 | Train Loss: 1.2122912406921387 | Test Loss: 1.3190906047821045\n",
      "Epoch 8840 | Train Loss: 1.0734081268310547 | Test Loss: 1.112249493598938\n",
      "Epoch 8841 | Train Loss: 1.212285041809082 | Test Loss: 1.3189723491668701\n",
      "Epoch 8842 | Train Loss: 1.0733909606933594 | Test Loss: 1.1121894121170044\n",
      "Epoch 8843 | Train Loss: 1.2122743129730225 | Test Loss: 1.3191028833389282\n",
      "Epoch 8844 | Train Loss: 1.0733749866485596 | Test Loss: 1.1122417449951172\n",
      "Epoch 8845 | Train Loss: 1.212267518043518 | Test Loss: 1.3189975023269653\n",
      "Epoch 8846 | Train Loss: 1.0733582973480225 | Test Loss: 1.112170696258545\n",
      "Epoch 8847 | Train Loss: 1.2122441530227661 | Test Loss: 1.3190884590148926\n",
      "Epoch 8848 | Train Loss: 1.0733473300933838 | Test Loss: 1.112230896949768\n",
      "Epoch 8849 | Train Loss: 1.2122249603271484 | Test Loss: 1.3190288543701172\n",
      "Epoch 8850 | Train Loss: 1.0733311176300049 | Test Loss: 1.112152099609375\n",
      "Epoch 8851 | Train Loss: 1.2122057676315308 | Test Loss: 1.3190933465957642\n",
      "Epoch 8852 | Train Loss: 1.0733193159103394 | Test Loss: 1.1122149229049683\n",
      "Epoch 8853 | Train Loss: 1.2121835947036743 | Test Loss: 1.31903874874115\n",
      "Epoch 8854 | Train Loss: 1.0733039379119873 | Test Loss: 1.112130880355835\n",
      "Epoch 8855 | Train Loss: 1.2121607065200806 | Test Loss: 1.3190850019454956\n",
      "Epoch 8856 | Train Loss: 1.0732935667037964 | Test Loss: 1.1121846437454224\n",
      "Epoch 8857 | Train Loss: 1.2121411561965942 | Test Loss: 1.3190507888793945\n",
      "Epoch 8858 | Train Loss: 1.0732802152633667 | Test Loss: 1.112117052078247\n",
      "Epoch 8859 | Train Loss: 1.2121164798736572 | Test Loss: 1.3191072940826416\n",
      "Epoch 8860 | Train Loss: 1.073274850845337 | Test Loss: 1.1121363639831543\n",
      "Epoch 8861 | Train Loss: 1.2120972871780396 | Test Loss: 1.3190613985061646\n",
      "Epoch 8862 | Train Loss: 1.073261022567749 | Test Loss: 1.1121257543563843\n",
      "Epoch 8863 | Train Loss: 1.2120792865753174 | Test Loss: 1.319138526916504\n",
      "Epoch 8864 | Train Loss: 1.0732554197311401 | Test Loss: 1.1121017932891846\n",
      "Epoch 8865 | Train Loss: 1.2120637893676758 | Test Loss: 1.3190817832946777\n",
      "Epoch 8866 | Train Loss: 1.073237657546997 | Test Loss: 1.1121333837509155\n",
      "Epoch 8867 | Train Loss: 1.2120492458343506 | Test Loss: 1.3191661834716797\n",
      "Epoch 8868 | Train Loss: 1.0732311010360718 | Test Loss: 1.1120679378509521\n",
      "Epoch 8869 | Train Loss: 1.2120370864868164 | Test Loss: 1.3190938234329224\n",
      "Epoch 8870 | Train Loss: 1.0732253789901733 | Test Loss: 1.1121339797973633\n",
      "Epoch 8871 | Train Loss: 1.2120269536972046 | Test Loss: 1.3192119598388672\n",
      "Epoch 8872 | Train Loss: 1.0732288360595703 | Test Loss: 1.1120368242263794\n",
      "Epoch 8873 | Train Loss: 1.2120165824890137 | Test Loss: 1.3190772533416748\n",
      "Epoch 8874 | Train Loss: 1.0732085704803467 | Test Loss: 1.1121653318405151\n",
      "Epoch 8875 | Train Loss: 1.2120263576507568 | Test Loss: 1.3192591667175293\n",
      "Epoch 8876 | Train Loss: 1.0732182264328003 | Test Loss: 1.1120270490646362\n",
      "Epoch 8877 | Train Loss: 1.212043046951294 | Test Loss: 1.3190926313400269\n",
      "Epoch 8878 | Train Loss: 1.0731983184814453 | Test Loss: 1.112221598625183\n",
      "Epoch 8879 | Train Loss: 1.2120646238327026 | Test Loss: 1.3193047046661377\n",
      "Epoch 8880 | Train Loss: 1.073203444480896 | Test Loss: 1.1120266914367676\n",
      "Epoch 8881 | Train Loss: 1.2120931148529053 | Test Loss: 1.3190847635269165\n",
      "Epoch 8882 | Train Loss: 1.0731711387634277 | Test Loss: 1.1122926473617554\n",
      "Epoch 8883 | Train Loss: 1.2121350765228271 | Test Loss: 1.3193455934524536\n",
      "Epoch 8884 | Train Loss: 1.0731828212738037 | Test Loss: 1.1120270490646362\n",
      "Epoch 8885 | Train Loss: 1.212178111076355 | Test Loss: 1.3191150426864624\n",
      "Epoch 8886 | Train Loss: 1.0731537342071533 | Test Loss: 1.1123548746109009\n",
      "Epoch 8887 | Train Loss: 1.212188959121704 | Test Loss: 1.3193012475967407\n",
      "Epoch 8888 | Train Loss: 1.0731480121612549 | Test Loss: 1.1120351552963257\n",
      "Epoch 8889 | Train Loss: 1.2122156620025635 | Test Loss: 1.3191181421279907\n",
      "Epoch 8890 | Train Loss: 1.0731192827224731 | Test Loss: 1.1123812198638916\n",
      "Epoch 8891 | Train Loss: 1.2122143507003784 | Test Loss: 1.3192495107650757\n",
      "Epoch 8892 | Train Loss: 1.0731114149093628 | Test Loss: 1.1120448112487793\n",
      "Epoch 8893 | Train Loss: 1.2122037410736084 | Test Loss: 1.3191537857055664\n",
      "Epoch 8894 | Train Loss: 1.0730946063995361 | Test Loss: 1.1123268604278564\n",
      "Epoch 8895 | Train Loss: 1.2121089696884155 | Test Loss: 1.3191841840744019\n",
      "Epoch 8896 | Train Loss: 1.073083758354187 | Test Loss: 1.112048864364624\n",
      "Epoch 8897 | Train Loss: 1.2120603322982788 | Test Loss: 1.3191217184066772\n",
      "Epoch 8898 | Train Loss: 1.0730689764022827 | Test Loss: 1.112213134765625\n",
      "Epoch 8899 | Train Loss: 1.2119817733764648 | Test Loss: 1.3191328048706055\n",
      "Epoch 8900 | Train Loss: 1.0730587244033813 | Test Loss: 1.1120414733886719\n",
      "Epoch 8901 | Train Loss: 1.2119256258010864 | Test Loss: 1.3191461563110352\n",
      "Epoch 8902 | Train Loss: 1.0730501413345337 | Test Loss: 1.112113118171692\n",
      "Epoch 8903 | Train Loss: 1.211867332458496 | Test Loss: 1.3191497325897217\n",
      "Epoch 8904 | Train Loss: 1.0730364322662354 | Test Loss: 1.1120516061782837\n",
      "Epoch 8905 | Train Loss: 1.2118306159973145 | Test Loss: 1.3191900253295898\n",
      "Epoch 8906 | Train Loss: 1.073024034500122 | Test Loss: 1.1120390892028809\n",
      "Epoch 8907 | Train Loss: 1.2118127346038818 | Test Loss: 1.319158911705017\n",
      "Epoch 8908 | Train Loss: 1.0730029344558716 | Test Loss: 1.1120744943618774\n",
      "Epoch 8909 | Train Loss: 1.21178138256073 | Test Loss: 1.3192310333251953\n",
      "Epoch 8910 | Train Loss: 1.072995901107788 | Test Loss: 1.111984372138977\n",
      "Epoch 8911 | Train Loss: 1.2117609977722168 | Test Loss: 1.3191512823104858\n",
      "Epoch 8912 | Train Loss: 1.072983980178833 | Test Loss: 1.11207115650177\n",
      "Epoch 8913 | Train Loss: 1.2117363214492798 | Test Loss: 1.319242238998413\n",
      "Epoch 8914 | Train Loss: 1.0729817152023315 | Test Loss: 1.1119463443756104\n",
      "Epoch 8915 | Train Loss: 1.2117177248001099 | Test Loss: 1.319130778312683\n",
      "Epoch 8916 | Train Loss: 1.0729646682739258 | Test Loss: 1.112057089805603\n",
      "Epoch 8917 | Train Loss: 1.2116994857788086 | Test Loss: 1.3192384243011475\n",
      "Epoch 8918 | Train Loss: 1.0729615688323975 | Test Loss: 1.1119296550750732\n",
      "Epoch 8919 | Train Loss: 1.2116791009902954 | Test Loss: 1.319136381149292\n",
      "Epoch 8920 | Train Loss: 1.072949767112732 | Test Loss: 1.1120270490646362\n",
      "Epoch 8921 | Train Loss: 1.2116633653640747 | Test Loss: 1.31922447681427\n",
      "Epoch 8922 | Train Loss: 1.0729389190673828 | Test Loss: 1.1119379997253418\n",
      "Epoch 8923 | Train Loss: 1.2116444110870361 | Test Loss: 1.3191479444503784\n",
      "Epoch 8924 | Train Loss: 1.0729210376739502 | Test Loss: 1.1120048761367798\n",
      "Epoch 8925 | Train Loss: 1.2116445302963257 | Test Loss: 1.3192429542541504\n",
      "Epoch 8926 | Train Loss: 1.0729091167449951 | Test Loss: 1.111948847770691\n",
      "Epoch 8927 | Train Loss: 1.21163010597229 | Test Loss: 1.3192155361175537\n",
      "Epoch 8928 | Train Loss: 1.072896957397461 | Test Loss: 1.1119946241378784\n",
      "Epoch 8929 | Train Loss: 1.2116239070892334 | Test Loss: 1.319277048110962\n",
      "Epoch 8930 | Train Loss: 1.0728884935379028 | Test Loss: 1.1119368076324463\n",
      "Epoch 8931 | Train Loss: 1.2116035223007202 | Test Loss: 1.3192222118377686\n",
      "Epoch 8932 | Train Loss: 1.0728718042373657 | Test Loss: 1.1120100021362305\n",
      "Epoch 8933 | Train Loss: 1.2116085290908813 | Test Loss: 1.319274663925171\n",
      "Epoch 8934 | Train Loss: 1.0728672742843628 | Test Loss: 1.1119272708892822\n",
      "Epoch 8935 | Train Loss: 1.2115954160690308 | Test Loss: 1.3192464113235474\n",
      "Epoch 8936 | Train Loss: 1.0728583335876465 | Test Loss: 1.1120175123214722\n",
      "Epoch 8937 | Train Loss: 1.2115815877914429 | Test Loss: 1.3192719221115112\n",
      "Epoch 8938 | Train Loss: 1.0728508234024048 | Test Loss: 1.1119294166564941\n",
      "Epoch 8939 | Train Loss: 1.2115598917007446 | Test Loss: 1.3192569017410278\n",
      "Epoch 8940 | Train Loss: 1.0728342533111572 | Test Loss: 1.1120271682739258\n",
      "Epoch 8941 | Train Loss: 1.2115695476531982 | Test Loss: 1.319281816482544\n",
      "Epoch 8942 | Train Loss: 1.0728280544281006 | Test Loss: 1.111926555633545\n",
      "Epoch 8943 | Train Loss: 1.211553931236267 | Test Loss: 1.3192484378814697\n",
      "Epoch 8944 | Train Loss: 1.0728142261505127 | Test Loss: 1.1120158433914185\n",
      "Epoch 8945 | Train Loss: 1.2115411758422852 | Test Loss: 1.3192660808563232\n",
      "Epoch 8946 | Train Loss: 1.072803020477295 | Test Loss: 1.111922264099121\n",
      "Epoch 8947 | Train Loss: 1.2115263938903809 | Test Loss: 1.3192607164382935\n",
      "Epoch 8948 | Train Loss: 1.0727827548980713 | Test Loss: 1.1120232343673706\n",
      "Epoch 8949 | Train Loss: 1.211532473564148 | Test Loss: 1.3193434476852417\n",
      "Epoch 8950 | Train Loss: 1.0727753639221191 | Test Loss: 1.111891746520996\n",
      "Epoch 8951 | Train Loss: 1.211521029472351 | Test Loss: 1.3192895650863647\n",
      "Epoch 8952 | Train Loss: 1.0727615356445312 | Test Loss: 1.1120264530181885\n",
      "Epoch 8953 | Train Loss: 1.2114975452423096 | Test Loss: 1.3193211555480957\n",
      "Epoch 8954 | Train Loss: 1.0727486610412598 | Test Loss: 1.1118718385696411\n",
      "Epoch 8955 | Train Loss: 1.2114689350128174 | Test Loss: 1.3192763328552246\n",
      "Epoch 8956 | Train Loss: 1.0727320909500122 | Test Loss: 1.111985683441162\n",
      "Epoch 8957 | Train Loss: 1.2114605903625488 | Test Loss: 1.3193280696868896\n",
      "Epoch 8958 | Train Loss: 1.0727237462997437 | Test Loss: 1.1118764877319336\n",
      "Epoch 8959 | Train Loss: 1.2114325761795044 | Test Loss: 1.319305419921875\n",
      "Epoch 8960 | Train Loss: 1.0727105140686035 | Test Loss: 1.111956238746643\n",
      "Epoch 8961 | Train Loss: 1.2114169597625732 | Test Loss: 1.3193058967590332\n",
      "Epoch 8962 | Train Loss: 1.0727006196975708 | Test Loss: 1.1118724346160889\n",
      "Epoch 8963 | Train Loss: 1.2113871574401855 | Test Loss: 1.319326400756836\n",
      "Epoch 8964 | Train Loss: 1.0726916790008545 | Test Loss: 1.1119375228881836\n",
      "Epoch 8965 | Train Loss: 1.2113810777664185 | Test Loss: 1.3192921876907349\n",
      "Epoch 8966 | Train Loss: 1.0726842880249023 | Test Loss: 1.1118676662445068\n",
      "Epoch 8967 | Train Loss: 1.2113620042800903 | Test Loss: 1.3193881511688232\n",
      "Epoch 8968 | Train Loss: 1.0726794004440308 | Test Loss: 1.1119229793548584\n",
      "Epoch 8969 | Train Loss: 1.211349368095398 | Test Loss: 1.3192912340164185\n",
      "Epoch 8970 | Train Loss: 1.0726696252822876 | Test Loss: 1.1118690967559814\n",
      "Epoch 8971 | Train Loss: 1.2113316059112549 | Test Loss: 1.3194077014923096\n",
      "Epoch 8972 | Train Loss: 1.072668433189392 | Test Loss: 1.1118909120559692\n",
      "Epoch 8973 | Train Loss: 1.2113239765167236 | Test Loss: 1.3193097114562988\n",
      "Epoch 8974 | Train Loss: 1.0726613998413086 | Test Loss: 1.111899971961975\n",
      "Epoch 8975 | Train Loss: 1.2113116979599 | Test Loss: 1.3194667100906372\n",
      "Epoch 8976 | Train Loss: 1.0726567506790161 | Test Loss: 1.1118643283843994\n",
      "Epoch 8977 | Train Loss: 1.2113142013549805 | Test Loss: 1.3193196058273315\n",
      "Epoch 8978 | Train Loss: 1.0726540088653564 | Test Loss: 1.1119292974472046\n",
      "Epoch 8979 | Train Loss: 1.2113182544708252 | Test Loss: 1.3194894790649414\n",
      "Epoch 8980 | Train Loss: 1.0726457834243774 | Test Loss: 1.1118701696395874\n",
      "Epoch 8981 | Train Loss: 1.2113227844238281 | Test Loss: 1.319331169128418\n",
      "Epoch 8982 | Train Loss: 1.0726317167282104 | Test Loss: 1.1119577884674072\n",
      "Epoch 8983 | Train Loss: 1.2113384008407593 | Test Loss: 1.3194533586502075\n",
      "Epoch 8984 | Train Loss: 1.072614312171936 | Test Loss: 1.1118783950805664\n",
      "Epoch 8985 | Train Loss: 1.2113490104675293 | Test Loss: 1.3193544149398804\n",
      "Epoch 8986 | Train Loss: 1.0726017951965332 | Test Loss: 1.111994743347168\n",
      "Epoch 8987 | Train Loss: 1.21135413646698 | Test Loss: 1.3194248676300049\n",
      "Epoch 8988 | Train Loss: 1.0725771188735962 | Test Loss: 1.1118718385696411\n",
      "Epoch 8989 | Train Loss: 1.211368203163147 | Test Loss: 1.319366693496704\n",
      "Epoch 8990 | Train Loss: 1.0725642442703247 | Test Loss: 1.1120363473892212\n",
      "Epoch 8991 | Train Loss: 1.2113730907440186 | Test Loss: 1.319398283958435\n",
      "Epoch 8992 | Train Loss: 1.0725477933883667 | Test Loss: 1.111867904663086\n",
      "Epoch 8993 | Train Loss: 1.2113585472106934 | Test Loss: 1.3193703889846802\n",
      "Epoch 8994 | Train Loss: 1.0725308656692505 | Test Loss: 1.1120296716690063\n",
      "Epoch 8995 | Train Loss: 1.2113360166549683 | Test Loss: 1.319366216659546\n",
      "Epoch 8996 | Train Loss: 1.0725146532058716 | Test Loss: 1.111842155456543\n",
      "Epoch 8997 | Train Loss: 1.2113007307052612 | Test Loss: 1.3193550109863281\n",
      "Epoch 8998 | Train Loss: 1.0725033283233643 | Test Loss: 1.1119917631149292\n",
      "Epoch 8999 | Train Loss: 1.211262822151184 | Test Loss: 1.3193714618682861\n",
      "Epoch 9000 | Train Loss: 1.0724925994873047 | Test Loss: 1.1118065118789673\n",
      "Epoch 9001 | Train Loss: 1.211214303970337 | Test Loss: 1.3194139003753662\n",
      "Epoch 9002 | Train Loss: 1.072482943534851 | Test Loss: 1.1119147539138794\n",
      "Epoch 9003 | Train Loss: 1.2111762762069702 | Test Loss: 1.3193891048431396\n",
      "Epoch 9004 | Train Loss: 1.0724694728851318 | Test Loss: 1.1117753982543945\n",
      "Epoch 9005 | Train Loss: 1.211132287979126 | Test Loss: 1.3194482326507568\n",
      "Epoch 9006 | Train Loss: 1.0724600553512573 | Test Loss: 1.1118508577346802\n",
      "Epoch 9007 | Train Loss: 1.2110952138900757 | Test Loss: 1.319408893585205\n",
      "Epoch 9008 | Train Loss: 1.0724472999572754 | Test Loss: 1.1117682456970215\n",
      "Epoch 9009 | Train Loss: 1.211068034172058 | Test Loss: 1.3194711208343506\n",
      "Epoch 9010 | Train Loss: 1.0724329948425293 | Test Loss: 1.1118015050888062\n",
      "Epoch 9011 | Train Loss: 1.2110410928726196 | Test Loss: 1.319398045539856\n",
      "Epoch 9012 | Train Loss: 1.0724232196807861 | Test Loss: 1.111761450767517\n",
      "Epoch 9013 | Train Loss: 1.2110204696655273 | Test Loss: 1.3194730281829834\n",
      "Epoch 9014 | Train Loss: 1.0724148750305176 | Test Loss: 1.1117688417434692\n",
      "Epoch 9015 | Train Loss: 1.2110050916671753 | Test Loss: 1.319415807723999\n",
      "Epoch 9016 | Train Loss: 1.0723998546600342 | Test Loss: 1.1117589473724365\n",
      "Epoch 9017 | Train Loss: 1.210986614227295 | Test Loss: 1.319474220275879\n",
      "Epoch 9018 | Train Loss: 1.072391390800476 | Test Loss: 1.1117514371871948\n",
      "Epoch 9019 | Train Loss: 1.2109712362289429 | Test Loss: 1.319400429725647\n",
      "Epoch 9020 | Train Loss: 1.0723755359649658 | Test Loss: 1.1117545366287231\n",
      "Epoch 9021 | Train Loss: 1.2109577655792236 | Test Loss: 1.3194955587387085\n",
      "Epoch 9022 | Train Loss: 1.0723705291748047 | Test Loss: 1.1117395162582397\n",
      "Epoch 9023 | Train Loss: 1.2109391689300537 | Test Loss: 1.319394826889038\n",
      "Epoch 9024 | Train Loss: 1.0723567008972168 | Test Loss: 1.111751675605774\n",
      "Epoch 9025 | Train Loss: 1.2109277248382568 | Test Loss: 1.3195198774337769\n",
      "Epoch 9026 | Train Loss: 1.0723483562469482 | Test Loss: 1.1117196083068848\n",
      "Epoch 9027 | Train Loss: 1.210911750793457 | Test Loss: 1.3193800449371338\n",
      "Epoch 9028 | Train Loss: 1.0723388195037842 | Test Loss: 1.1117562055587769\n",
      "Epoch 9029 | Train Loss: 1.2109062671661377 | Test Loss: 1.3195395469665527\n",
      "Epoch 9030 | Train Loss: 1.0723332166671753 | Test Loss: 1.1117017269134521\n",
      "Epoch 9031 | Train Loss: 1.2108910083770752 | Test Loss: 1.3194080591201782\n",
      "Epoch 9032 | Train Loss: 1.0723191499710083 | Test Loss: 1.1117632389068604\n",
      "Epoch 9033 | Train Loss: 1.2108875513076782 | Test Loss: 1.3195496797561646\n",
      "Epoch 9034 | Train Loss: 1.0723206996917725 | Test Loss: 1.1116925477981567\n",
      "Epoch 9035 | Train Loss: 1.210870385169983 | Test Loss: 1.3194200992584229\n",
      "Epoch 9036 | Train Loss: 1.072301983833313 | Test Loss: 1.1117815971374512\n",
      "Epoch 9037 | Train Loss: 1.2108755111694336 | Test Loss: 1.3195600509643555\n",
      "Epoch 9038 | Train Loss: 1.0723005533218384 | Test Loss: 1.111676812171936\n",
      "Epoch 9039 | Train Loss: 1.2108705043792725 | Test Loss: 1.3194578886032104\n",
      "Epoch 9040 | Train Loss: 1.0722936391830444 | Test Loss: 1.111796259880066\n",
      "Epoch 9041 | Train Loss: 1.2108821868896484 | Test Loss: 1.3195875883102417\n",
      "Epoch 9042 | Train Loss: 1.0722957849502563 | Test Loss: 1.1116713285446167\n",
      "Epoch 9043 | Train Loss: 1.2108960151672363 | Test Loss: 1.3194557428359985\n",
      "Epoch 9044 | Train Loss: 1.0722754001617432 | Test Loss: 1.1118489503860474\n",
      "Epoch 9045 | Train Loss: 1.2109217643737793 | Test Loss: 1.319620966911316\n",
      "Epoch 9046 | Train Loss: 1.0722798109054565 | Test Loss: 1.1116663217544556\n",
      "Epoch 9047 | Train Loss: 1.2109678983688354 | Test Loss: 1.319465160369873\n",
      "Epoch 9048 | Train Loss: 1.0722565650939941 | Test Loss: 1.1119059324264526\n",
      "Epoch 9049 | Train Loss: 1.2109730243682861 | Test Loss: 1.3196156024932861\n",
      "Epoch 9050 | Train Loss: 1.072256326675415 | Test Loss: 1.1116639375686646\n",
      "Epoch 9051 | Train Loss: 1.2109942436218262 | Test Loss: 1.319478988647461\n",
      "Epoch 9052 | Train Loss: 1.072229027748108 | Test Loss: 1.1119351387023926\n",
      "Epoch 9053 | Train Loss: 1.2109835147857666 | Test Loss: 1.3195947408676147\n",
      "Epoch 9054 | Train Loss: 1.0722248554229736 | Test Loss: 1.1116673946380615\n",
      "Epoch 9055 | Train Loss: 1.2110230922698975 | Test Loss: 1.3195340633392334\n",
      "Epoch 9056 | Train Loss: 1.072218418121338 | Test Loss: 1.1119158267974854\n",
      "Epoch 9057 | Train Loss: 1.210965633392334 | Test Loss: 1.3195672035217285\n",
      "Epoch 9058 | Train Loss: 1.072218418121338 | Test Loss: 1.1116920709609985\n",
      "Epoch 9059 | Train Loss: 1.2109432220458984 | Test Loss: 1.3195472955703735\n",
      "Epoch 9060 | Train Loss: 1.0722110271453857 | Test Loss: 1.1118652820587158\n",
      "Epoch 9061 | Train Loss: 1.2109142541885376 | Test Loss: 1.3195704221725464\n",
      "Epoch 9062 | Train Loss: 1.0722121000289917 | Test Loss: 1.1117242574691772\n",
      "Epoch 9063 | Train Loss: 1.2109183073043823 | Test Loss: 1.3195983171463013\n",
      "Epoch 9064 | Train Loss: 1.072205662727356 | Test Loss: 1.1118086576461792\n",
      "Epoch 9065 | Train Loss: 1.2108960151672363 | Test Loss: 1.3195515871047974\n",
      "Epoch 9066 | Train Loss: 1.072209358215332 | Test Loss: 1.1117976903915405\n",
      "Epoch 9067 | Train Loss: 1.2108933925628662 | Test Loss: 1.3196444511413574\n",
      "Epoch 9068 | Train Loss: 1.0721989870071411 | Test Loss: 1.1117750406265259\n",
      "Epoch 9069 | Train Loss: 1.2109450101852417 | Test Loss: 1.3195701837539673\n",
      "Epoch 9070 | Train Loss: 1.0721940994262695 | Test Loss: 1.1119154691696167\n",
      "Epoch 9071 | Train Loss: 1.2109776735305786 | Test Loss: 1.3196752071380615\n",
      "Epoch 9072 | Train Loss: 1.0721700191497803 | Test Loss: 1.111794114112854\n",
      "Epoch 9073 | Train Loss: 1.2110093832015991 | Test Loss: 1.3195583820343018\n",
      "Epoch 9074 | Train Loss: 1.0721471309661865 | Test Loss: 1.1119967699050903\n",
      "Epoch 9075 | Train Loss: 1.211019515991211 | Test Loss: 1.3196269273757935\n",
      "Epoch 9076 | Train Loss: 1.0721157789230347 | Test Loss: 1.1118052005767822\n",
      "Epoch 9077 | Train Loss: 1.2110122442245483 | Test Loss: 1.3195571899414062\n",
      "Epoch 9078 | Train Loss: 1.0720890760421753 | Test Loss: 1.1119749546051025\n",
      "Epoch 9079 | Train Loss: 1.2109625339508057 | Test Loss: 1.3196079730987549\n",
      "Epoch 9080 | Train Loss: 1.0720638036727905 | Test Loss: 1.1117712259292603\n",
      "Epoch 9081 | Train Loss: 1.2108945846557617 | Test Loss: 1.3195792436599731\n",
      "Epoch 9082 | Train Loss: 1.0720475912094116 | Test Loss: 1.1118718385696411\n",
      "Epoch 9083 | Train Loss: 1.2108087539672852 | Test Loss: 1.3195563554763794\n",
      "Epoch 9084 | Train Loss: 1.0720387697219849 | Test Loss: 1.1117067337036133\n",
      "Epoch 9085 | Train Loss: 1.2107242345809937 | Test Loss: 1.3196319341659546\n",
      "Epoch 9086 | Train Loss: 1.0720363855361938 | Test Loss: 1.1117359399795532\n",
      "Epoch 9087 | Train Loss: 1.210636854171753 | Test Loss: 1.3195383548736572\n",
      "Epoch 9088 | Train Loss: 1.0720276832580566 | Test Loss: 1.111649990081787\n",
      "Epoch 9089 | Train Loss: 1.2105807065963745 | Test Loss: 1.319648027420044\n",
      "Epoch 9090 | Train Loss: 1.0720263719558716 | Test Loss: 1.1116387844085693\n",
      "Epoch 9091 | Train Loss: 1.2105263471603394 | Test Loss: 1.3195439577102661\n",
      "Epoch 9092 | Train Loss: 1.0720129013061523 | Test Loss: 1.1116219758987427\n",
      "Epoch 9093 | Train Loss: 1.2105010747909546 | Test Loss: 1.3196557760238647\n",
      "Epoch 9094 | Train Loss: 1.0720003843307495 | Test Loss: 1.1115792989730835\n",
      "Epoch 9095 | Train Loss: 1.2104657888412476 | Test Loss: 1.3195737600326538\n",
      "Epoch 9096 | Train Loss: 1.0719852447509766 | Test Loss: 1.1116222143173218\n",
      "Epoch 9097 | Train Loss: 1.2104589939117432 | Test Loss: 1.3196346759796143\n",
      "Epoch 9098 | Train Loss: 1.0719691514968872 | Test Loss: 1.1115349531173706\n",
      "Epoch 9099 | Train Loss: 1.2104473114013672 | Test Loss: 1.3195797204971313\n",
      "Epoch 9100 | Train Loss: 1.0719544887542725 | Test Loss: 1.1116364002227783\n",
      "Epoch 9101 | Train Loss: 1.2104437351226807 | Test Loss: 1.319628119468689\n",
      "Epoch 9102 | Train Loss: 1.0719470977783203 | Test Loss: 1.1115200519561768\n",
      "Epoch 9103 | Train Loss: 1.2104285955429077 | Test Loss: 1.3195639848709106\n",
      "Epoch 9104 | Train Loss: 1.0719294548034668 | Test Loss: 1.1116420030593872\n",
      "Epoch 9105 | Train Loss: 1.2104254961013794 | Test Loss: 1.3196442127227783\n",
      "Epoch 9106 | Train Loss: 1.0719200372695923 | Test Loss: 1.1115128993988037\n",
      "Epoch 9107 | Train Loss: 1.2104120254516602 | Test Loss: 1.3196206092834473\n",
      "Epoch 9108 | Train Loss: 1.0719059705734253 | Test Loss: 1.1116316318511963\n",
      "Epoch 9109 | Train Loss: 1.2104021310806274 | Test Loss: 1.3196749687194824\n",
      "Epoch 9110 | Train Loss: 1.071895956993103 | Test Loss: 1.1115092039108276\n",
      "Epoch 9111 | Train Loss: 1.2103893756866455 | Test Loss: 1.319631576538086\n",
      "Epoch 9112 | Train Loss: 1.0718834400177002 | Test Loss: 1.1116409301757812\n",
      "Epoch 9113 | Train Loss: 1.210378646850586 | Test Loss: 1.3196409940719604\n",
      "Epoch 9114 | Train Loss: 1.071873426437378 | Test Loss: 1.111509084701538\n",
      "Epoch 9115 | Train Loss: 1.2103623151779175 | Test Loss: 1.3196169137954712\n",
      "Epoch 9116 | Train Loss: 1.0718607902526855 | Test Loss: 1.1116387844085693\n",
      "Epoch 9117 | Train Loss: 1.210348129272461 | Test Loss: 1.3196508884429932\n",
      "Epoch 9118 | Train Loss: 1.0718506574630737 | Test Loss: 1.1114989519119263\n",
      "Epoch 9119 | Train Loss: 1.2103307247161865 | Test Loss: 1.3196243047714233\n",
      "Epoch 9120 | Train Loss: 1.0718380212783813 | Test Loss: 1.1116032600402832\n",
      "Epoch 9121 | Train Loss: 1.2103115320205688 | Test Loss: 1.3197040557861328\n",
      "Epoch 9122 | Train Loss: 1.0718294382095337 | Test Loss: 1.111495018005371\n",
      "Epoch 9123 | Train Loss: 1.2102813720703125 | Test Loss: 1.3196595907211304\n",
      "Epoch 9124 | Train Loss: 1.0718166828155518 | Test Loss: 1.1115498542785645\n",
      "Epoch 9125 | Train Loss: 1.2102655172348022 | Test Loss: 1.3197311162948608\n",
      "Epoch 9126 | Train Loss: 1.0718048810958862 | Test Loss: 1.1115041971206665\n",
      "Epoch 9127 | Train Loss: 1.2102504968643188 | Test Loss: 1.3197078704833984\n",
      "Epoch 9128 | Train Loss: 1.071793794631958 | Test Loss: 1.1115138530731201\n",
      "Epoch 9129 | Train Loss: 1.2102376222610474 | Test Loss: 1.3197076320648193\n",
      "Epoch 9130 | Train Loss: 1.0717824697494507 | Test Loss: 1.1115074157714844\n",
      "Epoch 9131 | Train Loss: 1.2102210521697998 | Test Loss: 1.3197295665740967\n",
      "Epoch 9132 | Train Loss: 1.0717740058898926 | Test Loss: 1.1115031242370605\n",
      "Epoch 9133 | Train Loss: 1.2102093696594238 | Test Loss: 1.3197213411331177\n",
      "Epoch 9134 | Train Loss: 1.071763515472412 | Test Loss: 1.11150062084198\n",
      "Epoch 9135 | Train Loss: 1.2101986408233643 | Test Loss: 1.3197300434112549\n",
      "Epoch 9136 | Train Loss: 1.0717551708221436 | Test Loss: 1.1114901304244995\n",
      "Epoch 9137 | Train Loss: 1.2101855278015137 | Test Loss: 1.319727897644043\n",
      "Epoch 9138 | Train Loss: 1.0717440843582153 | Test Loss: 1.111498236656189\n",
      "Epoch 9139 | Train Loss: 1.2101696729660034 | Test Loss: 1.3197343349456787\n",
      "Epoch 9140 | Train Loss: 1.0717376470565796 | Test Loss: 1.111484169960022\n",
      "Epoch 9141 | Train Loss: 1.2101571559906006 | Test Loss: 1.3197557926177979\n",
      "Epoch 9142 | Train Loss: 1.0717251300811768 | Test Loss: 1.1115105152130127\n",
      "Epoch 9143 | Train Loss: 1.2101428508758545 | Test Loss: 1.3197731971740723\n",
      "Epoch 9144 | Train Loss: 1.0717219114303589 | Test Loss: 1.1114627122879028\n",
      "Epoch 9145 | Train Loss: 1.2101322412490845 | Test Loss: 1.319743275642395\n",
      "Epoch 9146 | Train Loss: 1.0717074871063232 | Test Loss: 1.1115232706069946\n",
      "Epoch 9147 | Train Loss: 1.2101266384124756 | Test Loss: 1.3198057413101196\n",
      "Epoch 9148 | Train Loss: 1.0717015266418457 | Test Loss: 1.1114449501037598\n",
      "Epoch 9149 | Train Loss: 1.2101142406463623 | Test Loss: 1.319742202758789\n",
      "Epoch 9150 | Train Loss: 1.0716885328292847 | Test Loss: 1.1115278005599976\n",
      "Epoch 9151 | Train Loss: 1.2101110219955444 | Test Loss: 1.3198037147521973\n",
      "Epoch 9152 | Train Loss: 1.071681022644043 | Test Loss: 1.1114459037780762\n",
      "Epoch 9153 | Train Loss: 1.2101019620895386 | Test Loss: 1.3197704553604126\n",
      "Epoch 9154 | Train Loss: 1.0716677904129028 | Test Loss: 1.1115232706069946\n",
      "Epoch 9155 | Train Loss: 1.2100974321365356 | Test Loss: 1.3197859525680542\n",
      "Epoch 9156 | Train Loss: 1.071657657623291 | Test Loss: 1.111456036567688\n",
      "Epoch 9157 | Train Loss: 1.2100919485092163 | Test Loss: 1.3198225498199463\n",
      "Epoch 9158 | Train Loss: 1.0716462135314941 | Test Loss: 1.1115281581878662\n",
      "Epoch 9159 | Train Loss: 1.2100812196731567 | Test Loss: 1.3198002576828003\n",
      "Epoch 9160 | Train Loss: 1.071638584136963 | Test Loss: 1.111433506011963\n",
      "Epoch 9161 | Train Loss: 1.210066318511963 | Test Loss: 1.3198457956314087\n",
      "Epoch 9162 | Train Loss: 1.071629285812378 | Test Loss: 1.111547589302063\n",
      "Epoch 9163 | Train Loss: 1.2100722789764404 | Test Loss: 1.319852352142334\n",
      "Epoch 9164 | Train Loss: 1.0716294050216675 | Test Loss: 1.111401915550232\n",
      "Epoch 9165 | Train Loss: 1.2100601196289062 | Test Loss: 1.319806456565857\n",
      "Epoch 9166 | Train Loss: 1.0716148614883423 | Test Loss: 1.111584186553955\n",
      "Epoch 9167 | Train Loss: 1.210065484046936 | Test Loss: 1.3199083805084229\n",
      "Epoch 9168 | Train Loss: 1.0716137886047363 | Test Loss: 1.1113977432250977\n",
      "Epoch 9169 | Train Loss: 1.210071325302124 | Test Loss: 1.3198233842849731\n",
      "Epoch 9170 | Train Loss: 1.071604609489441 | Test Loss: 1.1116026639938354\n",
      "Epoch 9171 | Train Loss: 1.210076093673706 | Test Loss: 1.3199270963668823\n",
      "Epoch 9172 | Train Loss: 1.0716004371643066 | Test Loss: 1.1114022731781006\n",
      "Epoch 9173 | Train Loss: 1.2100739479064941 | Test Loss: 1.3198373317718506\n",
      "Epoch 9174 | Train Loss: 1.071582317352295 | Test Loss: 1.1115976572036743\n",
      "Epoch 9175 | Train Loss: 1.210081696510315 | Test Loss: 1.3199487924575806\n",
      "Epoch 9176 | Train Loss: 1.0715819597244263 | Test Loss: 1.1114208698272705\n",
      "Epoch 9177 | Train Loss: 1.2100948095321655 | Test Loss: 1.319845199584961\n",
      "Epoch 9178 | Train Loss: 1.0715628862380981 | Test Loss: 1.1115875244140625\n",
      "Epoch 9179 | Train Loss: 1.2100976705551147 | Test Loss: 1.3199899196624756\n",
      "Epoch 9180 | Train Loss: 1.0715656280517578 | Test Loss: 1.111429214477539\n",
      "Epoch 9181 | Train Loss: 1.210085153579712 | Test Loss: 1.3198344707489014\n",
      "Epoch 9182 | Train Loss: 1.071545124053955 | Test Loss: 1.111575722694397\n",
      "Epoch 9183 | Train Loss: 1.210086464881897 | Test Loss: 1.3199907541275024\n",
      "Epoch 9184 | Train Loss: 1.071532964706421 | Test Loss: 1.111438512802124\n",
      "Epoch 9185 | Train Loss: 1.2100764513015747 | Test Loss: 1.3198708295822144\n",
      "Epoch 9186 | Train Loss: 1.0715199708938599 | Test Loss: 1.1115528345108032\n",
      "Epoch 9187 | Train Loss: 1.210038661956787 | Test Loss: 1.3199470043182373\n",
      "Epoch 9188 | Train Loss: 1.0715066194534302 | Test Loss: 1.1114518642425537\n",
      "Epoch 9189 | Train Loss: 1.2100157737731934 | Test Loss: 1.3199032545089722\n",
      "Epoch 9190 | Train Loss: 1.071494698524475 | Test Loss: 1.1115238666534424\n",
      "Epoch 9191 | Train Loss: 1.209985613822937 | Test Loss: 1.3199161291122437\n",
      "Epoch 9192 | Train Loss: 1.071481466293335 | Test Loss: 1.1114522218704224\n",
      "Epoch 9193 | Train Loss: 1.2099618911743164 | Test Loss: 1.3199125528335571\n",
      "Epoch 9194 | Train Loss: 1.0714718103408813 | Test Loss: 1.1114859580993652\n",
      "Epoch 9195 | Train Loss: 1.2099263668060303 | Test Loss: 1.319886565208435\n",
      "Epoch 9196 | Train Loss: 1.071456789970398 | Test Loss: 1.111453890800476\n",
      "Epoch 9197 | Train Loss: 1.2098942995071411 | Test Loss: 1.3199145793914795\n",
      "Epoch 9198 | Train Loss: 1.0714507102966309 | Test Loss: 1.1114345788955688\n",
      "Epoch 9199 | Train Loss: 1.2098736763000488 | Test Loss: 1.3199036121368408\n",
      "Epoch 9200 | Train Loss: 1.0714360475540161 | Test Loss: 1.111470341682434\n",
      "Epoch 9201 | Train Loss: 1.209848403930664 | Test Loss: 1.3199278116226196\n",
      "Epoch 9202 | Train Loss: 1.0714294910430908 | Test Loss: 1.1113927364349365\n",
      "Epoch 9203 | Train Loss: 1.2098273038864136 | Test Loss: 1.3199067115783691\n",
      "Epoch 9204 | Train Loss: 1.0714138746261597 | Test Loss: 1.1114777326583862\n",
      "Epoch 9205 | Train Loss: 1.2098116874694824 | Test Loss: 1.3199307918548584\n",
      "Epoch 9206 | Train Loss: 1.071407437324524 | Test Loss: 1.1113702058792114\n",
      "Epoch 9207 | Train Loss: 1.2097971439361572 | Test Loss: 1.319908857345581\n",
      "Epoch 9208 | Train Loss: 1.0713938474655151 | Test Loss: 1.1114728450775146\n",
      "Epoch 9209 | Train Loss: 1.2097787857055664 | Test Loss: 1.3199505805969238\n",
      "Epoch 9210 | Train Loss: 1.071385383605957 | Test Loss: 1.111348271369934\n",
      "Epoch 9211 | Train Loss: 1.2097687721252441 | Test Loss: 1.3198961019515991\n",
      "Epoch 9212 | Train Loss: 1.0713701248168945 | Test Loss: 1.1114610433578491\n",
      "Epoch 9213 | Train Loss: 1.2097464799880981 | Test Loss: 1.3199665546417236\n",
      "Epoch 9214 | Train Loss: 1.0713633298873901 | Test Loss: 1.1113191843032837\n",
      "Epoch 9215 | Train Loss: 1.209730863571167 | Test Loss: 1.3199021816253662\n",
      "Epoch 9216 | Train Loss: 1.0713526010513306 | Test Loss: 1.111446499824524\n",
      "Epoch 9217 | Train Loss: 1.2097171545028687 | Test Loss: 1.3200091123580933\n",
      "Epoch 9218 | Train Loss: 1.0713437795639038 | Test Loss: 1.1113054752349854\n",
      "Epoch 9219 | Train Loss: 1.209696650505066 | Test Loss: 1.3199303150177002\n",
      "Epoch 9220 | Train Loss: 1.0713293552398682 | Test Loss: 1.1114323139190674\n",
      "Epoch 9221 | Train Loss: 1.2096822261810303 | Test Loss: 1.3200324773788452\n",
      "Epoch 9222 | Train Loss: 1.0713191032409668 | Test Loss: 1.111309289932251\n",
      "Epoch 9223 | Train Loss: 1.2096638679504395 | Test Loss: 1.3199349641799927\n",
      "Epoch 9224 | Train Loss: 1.071313500404358 | Test Loss: 1.1114094257354736\n",
      "Epoch 9225 | Train Loss: 1.2096461057662964 | Test Loss: 1.3200428485870361\n",
      "Epoch 9226 | Train Loss: 1.0713061094284058 | Test Loss: 1.1113073825836182\n",
      "Epoch 9227 | Train Loss: 1.2096240520477295 | Test Loss: 1.3199552297592163\n",
      "Epoch 9228 | Train Loss: 1.0712894201278687 | Test Loss: 1.1113886833190918\n",
      "Epoch 9229 | Train Loss: 1.2096132040023804 | Test Loss: 1.320054292678833\n",
      "Epoch 9230 | Train Loss: 1.0712788105010986 | Test Loss: 1.1112984418869019\n",
      "Epoch 9231 | Train Loss: 1.209592580795288 | Test Loss: 1.3199653625488281\n",
      "Epoch 9232 | Train Loss: 1.0712735652923584 | Test Loss: 1.1113536357879639\n",
      "Epoch 9233 | Train Loss: 1.2095810174942017 | Test Loss: 1.3200656175613403\n",
      "Epoch 9234 | Train Loss: 1.0712722539901733 | Test Loss: 1.1112996339797974\n",
      "Epoch 9235 | Train Loss: 1.2095646858215332 | Test Loss: 1.3199542760849\n",
      "Epoch 9236 | Train Loss: 1.0712614059448242 | Test Loss: 1.1113682985305786\n",
      "Epoch 9237 | Train Loss: 1.209561824798584 | Test Loss: 1.3200949430465698\n",
      "Epoch 9238 | Train Loss: 1.0712623596191406 | Test Loss: 1.111293911933899\n",
      "Epoch 9239 | Train Loss: 1.20955491065979 | Test Loss: 1.3199851512908936\n",
      "Epoch 9240 | Train Loss: 1.0712568759918213 | Test Loss: 1.1114020347595215\n",
      "Epoch 9241 | Train Loss: 1.209564447402954 | Test Loss: 1.3201353549957275\n",
      "Epoch 9242 | Train Loss: 1.0712593793869019 | Test Loss: 1.1112829446792603\n",
      "Epoch 9243 | Train Loss: 1.2095752954483032 | Test Loss: 1.320013165473938\n",
      "Epoch 9244 | Train Loss: 1.071244239807129 | Test Loss: 1.111445426940918\n",
      "Epoch 9245 | Train Loss: 1.2095907926559448 | Test Loss: 1.3201420307159424\n",
      "Epoch 9246 | Train Loss: 1.0712391138076782 | Test Loss: 1.1112890243530273\n",
      "Epoch 9247 | Train Loss: 1.2096197605133057 | Test Loss: 1.3200007677078247\n",
      "Epoch 9248 | Train Loss: 1.0712249279022217 | Test Loss: 1.1114628314971924\n",
      "Epoch 9249 | Train Loss: 1.2096205949783325 | Test Loss: 1.3201651573181152\n",
      "Epoch 9250 | Train Loss: 1.0712189674377441 | Test Loss: 1.11130952835083\n",
      "Epoch 9251 | Train Loss: 1.2096378803253174 | Test Loss: 1.319987177848816\n",
      "Epoch 9252 | Train Loss: 1.071197271347046 | Test Loss: 1.1114917993545532\n",
      "Epoch 9253 | Train Loss: 1.2096315622329712 | Test Loss: 1.3201788663864136\n",
      "Epoch 9254 | Train Loss: 1.0711842775344849 | Test Loss: 1.1113221645355225\n",
      "Epoch 9255 | Train Loss: 1.209670901298523 | Test Loss: 1.3200074434280396\n",
      "Epoch 9256 | Train Loss: 1.0711727142333984 | Test Loss: 1.111519455909729\n",
      "Epoch 9257 | Train Loss: 1.209628701210022 | Test Loss: 1.3201414346694946\n",
      "Epoch 9258 | Train Loss: 1.0711582899093628 | Test Loss: 1.1113125085830688\n",
      "Epoch 9259 | Train Loss: 1.209633469581604 | Test Loss: 1.3200173377990723\n",
      "Epoch 9260 | Train Loss: 1.071138858795166 | Test Loss: 1.1115182638168335\n",
      "Epoch 9261 | Train Loss: 1.209581971168518 | Test Loss: 1.3200774192810059\n",
      "Epoch 9262 | Train Loss: 1.0711288452148438 | Test Loss: 1.1113042831420898\n",
      "Epoch 9263 | Train Loss: 1.2095789909362793 | Test Loss: 1.320052146911621\n",
      "Epoch 9264 | Train Loss: 1.0711162090301514 | Test Loss: 1.1114498376846313\n",
      "Epoch 9265 | Train Loss: 1.209490180015564 | Test Loss: 1.320020079612732\n",
      "Epoch 9266 | Train Loss: 1.071107268333435 | Test Loss: 1.1113052368164062\n",
      "Epoch 9267 | Train Loss: 1.2094396352767944 | Test Loss: 1.3201080560684204\n",
      "Epoch 9268 | Train Loss: 1.0711045265197754 | Test Loss: 1.1113615036010742\n",
      "Epoch 9269 | Train Loss: 1.2094030380249023 | Test Loss: 1.3200502395629883\n",
      "Epoch 9270 | Train Loss: 1.071095585823059 | Test Loss: 1.1112945079803467\n",
      "Epoch 9271 | Train Loss: 1.2093595266342163 | Test Loss: 1.320153832435608\n",
      "Epoch 9272 | Train Loss: 1.0710912942886353 | Test Loss: 1.1112903356552124\n",
      "Epoch 9273 | Train Loss: 1.2093292474746704 | Test Loss: 1.320054054260254\n",
      "Epoch 9274 | Train Loss: 1.071080207824707 | Test Loss: 1.111272931098938\n",
      "Epoch 9275 | Train Loss: 1.2092969417572021 | Test Loss: 1.3201558589935303\n",
      "Epoch 9276 | Train Loss: 1.0710774660110474 | Test Loss: 1.1112604141235352\n",
      "Epoch 9277 | Train Loss: 1.2092844247817993 | Test Loss: 1.3200788497924805\n",
      "Epoch 9278 | Train Loss: 1.0710577964782715 | Test Loss: 1.1112641096115112\n",
      "Epoch 9279 | Train Loss: 1.2092677354812622 | Test Loss: 1.3201791048049927\n",
      "Epoch 9280 | Train Loss: 1.0710495710372925 | Test Loss: 1.111229658126831\n",
      "Epoch 9281 | Train Loss: 1.2092605829238892 | Test Loss: 1.32009756565094\n",
      "Epoch 9282 | Train Loss: 1.0710351467132568 | Test Loss: 1.1112717390060425\n",
      "Epoch 9283 | Train Loss: 1.2092533111572266 | Test Loss: 1.320162057876587\n",
      "Epoch 9284 | Train Loss: 1.071028232574463 | Test Loss: 1.111204981803894\n",
      "Epoch 9285 | Train Loss: 1.2092434167861938 | Test Loss: 1.3201004266738892\n",
      "Epoch 9286 | Train Loss: 1.0710119009017944 | Test Loss: 1.1113020181655884\n",
      "Epoch 9287 | Train Loss: 1.2092443704605103 | Test Loss: 1.320177435874939\n",
      "Epoch 9288 | Train Loss: 1.0710076093673706 | Test Loss: 1.11118745803833\n",
      "Epoch 9289 | Train Loss: 1.2092469930648804 | Test Loss: 1.3201202154159546\n",
      "Epoch 9290 | Train Loss: 1.070997953414917 | Test Loss: 1.1113407611846924\n",
      "Epoch 9291 | Train Loss: 1.2092478275299072 | Test Loss: 1.320214033126831\n",
      "Epoch 9292 | Train Loss: 1.0709985494613647 | Test Loss: 1.1111739873886108\n",
      "Epoch 9293 | Train Loss: 1.2092376947402954 | Test Loss: 1.3201318979263306\n",
      "Epoch 9294 | Train Loss: 1.0709835290908813 | Test Loss: 1.111362099647522\n",
      "Epoch 9295 | Train Loss: 1.2092461585998535 | Test Loss: 1.3202074766159058\n",
      "Epoch 9296 | Train Loss: 1.0709757804870605 | Test Loss: 1.1111791133880615\n",
      "Epoch 9297 | Train Loss: 1.2092453241348267 | Test Loss: 1.3201583623886108\n",
      "Epoch 9298 | Train Loss: 1.0709642171859741 | Test Loss: 1.11135995388031\n",
      "Epoch 9299 | Train Loss: 1.2092374563217163 | Test Loss: 1.3201929330825806\n",
      "Epoch 9300 | Train Loss: 1.070953130722046 | Test Loss: 1.111191987991333\n",
      "Epoch 9301 | Train Loss: 1.2092207670211792 | Test Loss: 1.3201687335968018\n",
      "Epoch 9302 | Train Loss: 1.0709387063980103 | Test Loss: 1.1113545894622803\n",
      "Epoch 9303 | Train Loss: 1.2092175483703613 | Test Loss: 1.320244550704956\n",
      "Epoch 9304 | Train Loss: 1.0709277391433716 | Test Loss: 1.1111778020858765\n",
      "Epoch 9305 | Train Loss: 1.209201693534851 | Test Loss: 1.3201767206192017\n",
      "Epoch 9306 | Train Loss: 1.0709131956100464 | Test Loss: 1.1113402843475342\n",
      "Epoch 9307 | Train Loss: 1.2091798782348633 | Test Loss: 1.3202592134475708\n",
      "Epoch 9308 | Train Loss: 1.0709025859832764 | Test Loss: 1.111170768737793\n",
      "Epoch 9309 | Train Loss: 1.2091612815856934 | Test Loss: 1.320191740989685\n",
      "Epoch 9310 | Train Loss: 1.0708909034729004 | Test Loss: 1.1113284826278687\n",
      "Epoch 9311 | Train Loss: 1.2091407775878906 | Test Loss: 1.3202714920043945\n",
      "Epoch 9312 | Train Loss: 1.0708794593811035 | Test Loss: 1.1111565828323364\n",
      "Epoch 9313 | Train Loss: 1.209123969078064 | Test Loss: 1.3202719688415527\n",
      "Epoch 9314 | Train Loss: 1.0708696842193604 | Test Loss: 1.1112862825393677\n",
      "Epoch 9315 | Train Loss: 1.2090986967086792 | Test Loss: 1.3202320337295532\n",
      "Epoch 9316 | Train Loss: 1.0708582401275635 | Test Loss: 1.1111571788787842\n",
      "Epoch 9317 | Train Loss: 1.2090673446655273 | Test Loss: 1.3202990293502808\n",
      "Epoch 9318 | Train Loss: 1.070853590965271 | Test Loss: 1.1112202405929565\n",
      "Epoch 9319 | Train Loss: 1.2090424299240112 | Test Loss: 1.3202266693115234\n",
      "Epoch 9320 | Train Loss: 1.0708447694778442 | Test Loss: 1.1111633777618408\n",
      "Epoch 9321 | Train Loss: 1.209025263786316 | Test Loss: 1.3203078508377075\n",
      "Epoch 9322 | Train Loss: 1.0708340406417847 | Test Loss: 1.1111805438995361\n",
      "Epoch 9323 | Train Loss: 1.2090085744857788 | Test Loss: 1.3202646970748901\n",
      "Epoch 9324 | Train Loss: 1.0708200931549072 | Test Loss: 1.1111568212509155\n",
      "Epoch 9325 | Train Loss: 1.2089859247207642 | Test Loss: 1.3203063011169434\n",
      "Epoch 9326 | Train Loss: 1.0708131790161133 | Test Loss: 1.1111546754837036\n",
      "Epoch 9327 | Train Loss: 1.2089706659317017 | Test Loss: 1.3202983140945435\n",
      "Epoch 9328 | Train Loss: 1.070807933807373 | Test Loss: 1.1111515760421753\n",
      "Epoch 9329 | Train Loss: 1.2089591026306152 | Test Loss: 1.3203022480010986\n",
      "Epoch 9330 | Train Loss: 1.07080078125 | Test Loss: 1.1111499071121216\n",
      "Epoch 9331 | Train Loss: 1.2089481353759766 | Test Loss: 1.3202910423278809\n",
      "Epoch 9332 | Train Loss: 1.0707879066467285 | Test Loss: 1.1111611127853394\n",
      "Epoch 9333 | Train Loss: 1.2089462280273438 | Test Loss: 1.3203824758529663\n",
      "Epoch 9334 | Train Loss: 1.070791244506836 | Test Loss: 1.1111443042755127\n",
      "Epoch 9335 | Train Loss: 1.2089390754699707 | Test Loss: 1.3203257322311401\n",
      "Epoch 9336 | Train Loss: 1.070785641670227 | Test Loss: 1.1112006902694702\n",
      "Epoch 9337 | Train Loss: 1.2089370489120483 | Test Loss: 1.3204141855239868\n",
      "Epoch 9338 | Train Loss: 1.0707812309265137 | Test Loss: 1.1111133098602295\n",
      "Epoch 9339 | Train Loss: 1.208948016166687 | Test Loss: 1.3203462362289429\n",
      "Epoch 9340 | Train Loss: 1.070769190788269 | Test Loss: 1.11127507686615\n",
      "Epoch 9341 | Train Loss: 1.208982229232788 | Test Loss: 1.3204216957092285\n",
      "Epoch 9342 | Train Loss: 1.070768117904663 | Test Loss: 1.1111013889312744\n",
      "Epoch 9343 | Train Loss: 1.2090141773223877 | Test Loss: 1.320365309715271\n",
      "Epoch 9344 | Train Loss: 1.0707589387893677 | Test Loss: 1.1113481521606445\n",
      "Epoch 9345 | Train Loss: 1.209047794342041 | Test Loss: 1.320425033569336\n",
      "Epoch 9346 | Train Loss: 1.070755958557129 | Test Loss: 1.1111230850219727\n",
      "Epoch 9347 | Train Loss: 1.2090659141540527 | Test Loss: 1.3203792572021484\n",
      "Epoch 9348 | Train Loss: 1.0707398653030396 | Test Loss: 1.1113877296447754\n",
      "Epoch 9349 | Train Loss: 1.2090965509414673 | Test Loss: 1.3204246759414673\n",
      "Epoch 9350 | Train Loss: 1.0707379579544067 | Test Loss: 1.1111655235290527\n",
      "Epoch 9351 | Train Loss: 1.2091224193572998 | Test Loss: 1.3204001188278198\n",
      "Epoch 9352 | Train Loss: 1.070719599723816 | Test Loss: 1.1114068031311035\n",
      "Epoch 9353 | Train Loss: 1.2091025114059448 | Test Loss: 1.3203965425491333\n",
      "Epoch 9354 | Train Loss: 1.0707098245620728 | Test Loss: 1.1111778020858765\n",
      "Epoch 9355 | Train Loss: 1.209085464477539 | Test Loss: 1.3203843832015991\n",
      "Epoch 9356 | Train Loss: 1.0706924200057983 | Test Loss: 1.1113662719726562\n",
      "Epoch 9357 | Train Loss: 1.209049940109253 | Test Loss: 1.3204035758972168\n",
      "Epoch 9358 | Train Loss: 1.0706889629364014 | Test Loss: 1.1111568212509155\n",
      "Epoch 9359 | Train Loss: 1.2089946269989014 | Test Loss: 1.3203879594802856\n",
      "Epoch 9360 | Train Loss: 1.070673942565918 | Test Loss: 1.11129629611969\n",
      "Epoch 9361 | Train Loss: 1.2089591026306152 | Test Loss: 1.3204246759414673\n",
      "Epoch 9362 | Train Loss: 1.070662021636963 | Test Loss: 1.111163854598999\n",
      "Epoch 9363 | Train Loss: 1.2089139223098755 | Test Loss: 1.320405125617981\n",
      "Epoch 9364 | Train Loss: 1.0706526041030884 | Test Loss: 1.1112140417099\n",
      "Epoch 9365 | Train Loss: 1.2088814973831177 | Test Loss: 1.3204011917114258\n",
      "Epoch 9366 | Train Loss: 1.0706368684768677 | Test Loss: 1.1112042665481567\n",
      "Epoch 9367 | Train Loss: 1.2088637351989746 | Test Loss: 1.3204598426818848\n",
      "Epoch 9368 | Train Loss: 1.0706312656402588 | Test Loss: 1.1111363172531128\n",
      "Epoch 9369 | Train Loss: 1.2088364362716675 | Test Loss: 1.3204004764556885\n",
      "Epoch 9370 | Train Loss: 1.0706138610839844 | Test Loss: 1.111240029335022\n",
      "Epoch 9371 | Train Loss: 1.2088313102722168 | Test Loss: 1.3204658031463623\n",
      "Epoch 9372 | Train Loss: 1.070601463317871 | Test Loss: 1.1110906600952148\n",
      "Epoch 9373 | Train Loss: 1.2088215351104736 | Test Loss: 1.3204340934753418\n",
      "Epoch 9374 | Train Loss: 1.0705831050872803 | Test Loss: 1.1112505197525024\n",
      "Epoch 9375 | Train Loss: 1.2088127136230469 | Test Loss: 1.3204816579818726\n",
      "Epoch 9376 | Train Loss: 1.0705711841583252 | Test Loss: 1.111061453819275\n",
      "Epoch 9377 | Train Loss: 1.2087924480438232 | Test Loss: 1.3204656839370728\n",
      "Epoch 9378 | Train Loss: 1.0705533027648926 | Test Loss: 1.1112478971481323\n",
      "Epoch 9379 | Train Loss: 1.208782434463501 | Test Loss: 1.320489764213562\n",
      "Epoch 9380 | Train Loss: 1.0705430507659912 | Test Loss: 1.111043930053711\n",
      "Epoch 9381 | Train Loss: 1.2087429761886597 | Test Loss: 1.3204419612884521\n",
      "Epoch 9382 | Train Loss: 1.070530652999878 | Test Loss: 1.111208200454712\n",
      "Epoch 9383 | Train Loss: 1.2087140083312988 | Test Loss: 1.3205082416534424\n",
      "Epoch 9384 | Train Loss: 1.0705273151397705 | Test Loss: 1.1110297441482544\n",
      "Epoch 9385 | Train Loss: 1.2086716890335083 | Test Loss: 1.3204587697982788\n",
      "Epoch 9386 | Train Loss: 1.070517659187317 | Test Loss: 1.1111538410186768\n",
      "Epoch 9387 | Train Loss: 1.2086423635482788 | Test Loss: 1.320505142211914\n",
      "Epoch 9388 | Train Loss: 1.070509433746338 | Test Loss: 1.1110209226608276\n",
      "Epoch 9389 | Train Loss: 1.2086161375045776 | Test Loss: 1.3204983472824097\n",
      "Epoch 9390 | Train Loss: 1.0705045461654663 | Test Loss: 1.111124873161316\n",
      "Epoch 9391 | Train Loss: 1.2085950374603271 | Test Loss: 1.3204926252365112\n",
      "Epoch 9392 | Train Loss: 1.0704941749572754 | Test Loss: 1.111017107963562\n",
      "Epoch 9393 | Train Loss: 1.2085747718811035 | Test Loss: 1.3204973936080933\n",
      "Epoch 9394 | Train Loss: 1.0704894065856934 | Test Loss: 1.1110937595367432\n",
      "Epoch 9395 | Train Loss: 1.2085576057434082 | Test Loss: 1.3205127716064453\n",
      "Epoch 9396 | Train Loss: 1.0704834461212158 | Test Loss: 1.111007809638977\n",
      "Epoch 9397 | Train Loss: 1.208536148071289 | Test Loss: 1.3204964399337769\n",
      "Epoch 9398 | Train Loss: 1.0704760551452637 | Test Loss: 1.1110759973526\n",
      "Epoch 9399 | Train Loss: 1.2085282802581787 | Test Loss: 1.3205448389053345\n",
      "Epoch 9400 | Train Loss: 1.070462703704834 | Test Loss: 1.1110109090805054\n",
      "Epoch 9401 | Train Loss: 1.2085168361663818 | Test Loss: 1.3205084800720215\n",
      "Epoch 9402 | Train Loss: 1.070457935333252 | Test Loss: 1.1110597848892212\n",
      "Epoch 9403 | Train Loss: 1.2085061073303223 | Test Loss: 1.320565938949585\n",
      "Epoch 9404 | Train Loss: 1.0704485177993774 | Test Loss: 1.1110063791275024\n",
      "Epoch 9405 | Train Loss: 1.2084943056106567 | Test Loss: 1.3205012083053589\n",
      "Epoch 9406 | Train Loss: 1.0704361200332642 | Test Loss: 1.1110583543777466\n",
      "Epoch 9407 | Train Loss: 1.2084956169128418 | Test Loss: 1.3205815553665161\n",
      "Epoch 9408 | Train Loss: 1.0704303979873657 | Test Loss: 1.1109955310821533\n",
      "Epoch 9409 | Train Loss: 1.2084966897964478 | Test Loss: 1.320509672164917\n",
      "Epoch 9410 | Train Loss: 1.0704225301742554 | Test Loss: 1.1110687255859375\n",
      "Epoch 9411 | Train Loss: 1.2084931135177612 | Test Loss: 1.3206011056900024\n",
      "Epoch 9412 | Train Loss: 1.0704165697097778 | Test Loss: 1.1110024452209473\n",
      "Epoch 9413 | Train Loss: 1.2084914445877075 | Test Loss: 1.3204963207244873\n",
      "Epoch 9414 | Train Loss: 1.0704039335250854 | Test Loss: 1.1110717058181763\n",
      "Epoch 9415 | Train Loss: 1.2084887027740479 | Test Loss: 1.3206202983856201\n",
      "Epoch 9416 | Train Loss: 1.0703964233398438 | Test Loss: 1.11101496219635\n",
      "Epoch 9417 | Train Loss: 1.2084904909133911 | Test Loss: 1.320512056350708\n",
      "Epoch 9418 | Train Loss: 1.0703816413879395 | Test Loss: 1.1110844612121582\n",
      "Epoch 9419 | Train Loss: 1.2084959745407104 | Test Loss: 1.320602297782898\n",
      "Epoch 9420 | Train Loss: 1.0703725814819336 | Test Loss: 1.1110285520553589\n",
      "Epoch 9421 | Train Loss: 1.2084921598434448 | Test Loss: 1.3205286264419556\n",
      "Epoch 9422 | Train Loss: 1.070355772972107 | Test Loss: 1.111095905303955\n",
      "Epoch 9423 | Train Loss: 1.2084839344024658 | Test Loss: 1.3205865621566772\n",
      "Epoch 9424 | Train Loss: 1.0703457593917847 | Test Loss: 1.1110193729400635\n",
      "Epoch 9425 | Train Loss: 1.2084883451461792 | Test Loss: 1.320563793182373\n",
      "Epoch 9426 | Train Loss: 1.0703340768814087 | Test Loss: 1.1110975742340088\n",
      "Epoch 9427 | Train Loss: 1.208463430404663 | Test Loss: 1.3205995559692383\n",
      "Epoch 9428 | Train Loss: 1.0703240633010864 | Test Loss: 1.111000657081604\n",
      "Epoch 9429 | Train Loss: 1.2084616422653198 | Test Loss: 1.3205718994140625\n",
      "Epoch 9430 | Train Loss: 1.0703094005584717 | Test Loss: 1.1111074686050415\n",
      "Epoch 9431 | Train Loss: 1.2084413766860962 | Test Loss: 1.3205875158309937\n",
      "Epoch 9432 | Train Loss: 1.070299506187439 | Test Loss: 1.1109809875488281\n",
      "Epoch 9433 | Train Loss: 1.2084389925003052 | Test Loss: 1.3205652236938477\n",
      "Epoch 9434 | Train Loss: 1.0702903270721436 | Test Loss: 1.1110866069793701\n",
      "Epoch 9435 | Train Loss: 1.2083940505981445 | Test Loss: 1.3205435276031494\n",
      "Epoch 9436 | Train Loss: 1.0702776908874512 | Test Loss: 1.1109662055969238\n",
      "Epoch 9437 | Train Loss: 1.2083731889724731 | Test Loss: 1.3205897808074951\n",
      "Epoch 9438 | Train Loss: 1.0702667236328125 | Test Loss: 1.1110492944717407\n",
      "Epoch 9439 | Train Loss: 1.2083441019058228 | Test Loss: 1.320570707321167\n",
      "Epoch 9440 | Train Loss: 1.0702568292617798 | Test Loss: 1.1109555959701538\n",
      "Epoch 9441 | Train Loss: 1.2083327770233154 | Test Loss: 1.3206318616867065\n",
      "Epoch 9442 | Train Loss: 1.0702470541000366 | Test Loss: 1.1110087633132935\n",
      "Epoch 9443 | Train Loss: 1.2082957029342651 | Test Loss: 1.3205714225769043\n",
      "Epoch 9444 | Train Loss: 1.0702400207519531 | Test Loss: 1.1109321117401123\n",
      "Epoch 9445 | Train Loss: 1.208274245262146 | Test Loss: 1.3206034898757935\n",
      "Epoch 9446 | Train Loss: 1.070232629776001 | Test Loss: 1.1109868288040161\n",
      "Epoch 9447 | Train Loss: 1.208256721496582 | Test Loss: 1.320598840713501\n",
      "Epoch 9448 | Train Loss: 1.0702258348464966 | Test Loss: 1.1109133958816528\n",
      "Epoch 9449 | Train Loss: 1.2082386016845703 | Test Loss: 1.3206238746643066\n",
      "Epoch 9450 | Train Loss: 1.0702179670333862 | Test Loss: 1.1109775304794312\n",
      "Epoch 9451 | Train Loss: 1.208216905593872 | Test Loss: 1.320617437362671\n",
      "Epoch 9452 | Train Loss: 1.0702109336853027 | Test Loss: 1.1109064817428589\n",
      "Epoch 9453 | Train Loss: 1.2082023620605469 | Test Loss: 1.3206735849380493\n",
      "Epoch 9454 | Train Loss: 1.0702046155929565 | Test Loss: 1.1109641790390015\n",
      "Epoch 9455 | Train Loss: 1.2081851959228516 | Test Loss: 1.3206188678741455\n",
      "Epoch 9456 | Train Loss: 1.070198655128479 | Test Loss: 1.1109116077423096\n",
      "Epoch 9457 | Train Loss: 1.2081716060638428 | Test Loss: 1.3207100629806519\n",
      "Epoch 9458 | Train Loss: 1.0701924562454224 | Test Loss: 1.1109483242034912\n",
      "Epoch 9459 | Train Loss: 1.2081689834594727 | Test Loss: 1.320603370666504\n",
      "Epoch 9460 | Train Loss: 1.0701863765716553 | Test Loss: 1.110924482345581\n",
      "Epoch 9461 | Train Loss: 1.2081632614135742 | Test Loss: 1.320723533630371\n",
      "Epoch 9462 | Train Loss: 1.0701794624328613 | Test Loss: 1.110937237739563\n",
      "Epoch 9463 | Train Loss: 1.2081595659255981 | Test Loss: 1.3206279277801514\n",
      "Epoch 9464 | Train Loss: 1.0701748132705688 | Test Loss: 1.1109380722045898\n",
      "Epoch 9465 | Train Loss: 1.2081592082977295 | Test Loss: 1.3207043409347534\n",
      "Epoch 9466 | Train Loss: 1.0701671838760376 | Test Loss: 1.1109381914138794\n",
      "Epoch 9467 | Train Loss: 1.2081573009490967 | Test Loss: 1.3206387758255005\n",
      "Epoch 9468 | Train Loss: 1.0701619386672974 | Test Loss: 1.1109532117843628\n",
      "Epoch 9469 | Train Loss: 1.2081700563430786 | Test Loss: 1.3207508325576782\n",
      "Epoch 9470 | Train Loss: 1.0701541900634766 | Test Loss: 1.1109589338302612\n",
      "Epoch 9471 | Train Loss: 1.2081773281097412 | Test Loss: 1.3206428289413452\n",
      "Epoch 9472 | Train Loss: 1.0701451301574707 | Test Loss: 1.1109867095947266\n",
      "Epoch 9473 | Train Loss: 1.2081842422485352 | Test Loss: 1.3207430839538574\n",
      "Epoch 9474 | Train Loss: 1.0701335668563843 | Test Loss: 1.110958456993103\n",
      "Epoch 9475 | Train Loss: 1.2081891298294067 | Test Loss: 1.3206586837768555\n",
      "Epoch 9476 | Train Loss: 1.0701205730438232 | Test Loss: 1.1110219955444336\n",
      "Epoch 9477 | Train Loss: 1.2082082033157349 | Test Loss: 1.320745825767517\n",
      "Epoch 9478 | Train Loss: 1.0701065063476562 | Test Loss: 1.1109546422958374\n",
      "Epoch 9479 | Train Loss: 1.2082068920135498 | Test Loss: 1.3207416534423828\n",
      "Epoch 9480 | Train Loss: 1.070094108581543 | Test Loss: 1.111050009727478\n",
      "Epoch 9481 | Train Loss: 1.2081984281539917 | Test Loss: 1.3207441568374634\n",
      "Epoch 9482 | Train Loss: 1.0700829029083252 | Test Loss: 1.1109378337860107\n",
      "Epoch 9483 | Train Loss: 1.2081735134124756 | Test Loss: 1.3207316398620605\n",
      "Epoch 9484 | Train Loss: 1.0700666904449463 | Test Loss: 1.1110446453094482\n",
      "Epoch 9485 | Train Loss: 1.208155870437622 | Test Loss: 1.3207498788833618\n",
      "Epoch 9486 | Train Loss: 1.0700571537017822 | Test Loss: 1.110892653465271\n",
      "Epoch 9487 | Train Loss: 1.2081201076507568 | Test Loss: 1.3207411766052246\n",
      "Epoch 9488 | Train Loss: 1.0700416564941406 | Test Loss: 1.1110105514526367\n",
      "Epoch 9489 | Train Loss: 1.2080966234207153 | Test Loss: 1.3207498788833618\n",
      "Epoch 9490 | Train Loss: 1.070033073425293 | Test Loss: 1.1108615398406982\n",
      "Epoch 9491 | Train Loss: 1.2080575227737427 | Test Loss: 1.3207780122756958\n",
      "Epoch 9492 | Train Loss: 1.0700219869613647 | Test Loss: 1.110979676246643\n",
      "Epoch 9493 | Train Loss: 1.208032250404358 | Test Loss: 1.3207758665084839\n",
      "Epoch 9494 | Train Loss: 1.070012092590332 | Test Loss: 1.1108280420303345\n",
      "Epoch 9495 | Train Loss: 1.2079951763153076 | Test Loss: 1.3208303451538086\n",
      "Epoch 9496 | Train Loss: 1.0700030326843262 | Test Loss: 1.1109510660171509\n",
      "Epoch 9497 | Train Loss: 1.2079755067825317 | Test Loss: 1.320739507675171\n",
      "Epoch 9498 | Train Loss: 1.0699951648712158 | Test Loss: 1.1108132600784302\n",
      "Epoch 9499 | Train Loss: 1.2079479694366455 | Test Loss: 1.3208048343658447\n",
      "Epoch 9500 | Train Loss: 1.0699808597564697 | Test Loss: 1.1109189987182617\n",
      "Epoch 9501 | Train Loss: 1.207931399345398 | Test Loss: 1.3207643032073975\n",
      "Epoch 9502 | Train Loss: 1.069974660873413 | Test Loss: 1.1108224391937256\n",
      "Epoch 9503 | Train Loss: 1.2079089879989624 | Test Loss: 1.3208082914352417\n",
      "Epoch 9504 | Train Loss: 1.0699650049209595 | Test Loss: 1.1108840703964233\n",
      "Epoch 9505 | Train Loss: 1.2078981399536133 | Test Loss: 1.3207626342773438\n",
      "Epoch 9506 | Train Loss: 1.0699564218521118 | Test Loss: 1.1108298301696777\n",
      "Epoch 9507 | Train Loss: 1.2078830003738403 | Test Loss: 1.320797085762024\n",
      "Epoch 9508 | Train Loss: 1.0699509382247925 | Test Loss: 1.110866665840149\n",
      "Epoch 9509 | Train Loss: 1.2078694105148315 | Test Loss: 1.320750117301941\n",
      "Epoch 9510 | Train Loss: 1.0699416399002075 | Test Loss: 1.1108274459838867\n",
      "Epoch 9511 | Train Loss: 1.207855463027954 | Test Loss: 1.3207952976226807\n",
      "Epoch 9512 | Train Loss: 1.0699383020401 | Test Loss: 1.1108663082122803\n",
      "Epoch 9513 | Train Loss: 1.2078379392623901 | Test Loss: 1.3207778930664062\n",
      "Epoch 9514 | Train Loss: 1.069931983947754 | Test Loss: 1.1108137369155884\n",
      "Epoch 9515 | Train Loss: 1.2078282833099365 | Test Loss: 1.3208281993865967\n",
      "Epoch 9516 | Train Loss: 1.0699228048324585 | Test Loss: 1.1108700037002563\n",
      "Epoch 9517 | Train Loss: 1.207817792892456 | Test Loss: 1.3208454847335815\n",
      "Epoch 9518 | Train Loss: 1.0699183940887451 | Test Loss: 1.1108120679855347\n",
      "Epoch 9519 | Train Loss: 1.207810640335083 | Test Loss: 1.3208556175231934\n",
      "Epoch 9520 | Train Loss: 1.0699132680892944 | Test Loss: 1.1108652353286743\n",
      "Epoch 9521 | Train Loss: 1.2078050374984741 | Test Loss: 1.320868968963623\n",
      "Epoch 9522 | Train Loss: 1.0699104070663452 | Test Loss: 1.1108183860778809\n",
      "Epoch 9523 | Train Loss: 1.2078028917312622 | Test Loss: 1.3208580017089844\n",
      "Epoch 9524 | Train Loss: 1.069901466369629 | Test Loss: 1.1108696460723877\n",
      "Epoch 9525 | Train Loss: 1.2078062295913696 | Test Loss: 1.320906162261963\n",
      "Epoch 9526 | Train Loss: 1.069903016090393 | Test Loss: 1.110825777053833\n",
      "Epoch 9527 | Train Loss: 1.2078206539154053 | Test Loss: 1.3208824396133423\n",
      "Epoch 9528 | Train Loss: 1.0699007511138916 | Test Loss: 1.110907793045044\n",
      "Epoch 9529 | Train Loss: 1.2078354358673096 | Test Loss: 1.3209325075149536\n",
      "Epoch 9530 | Train Loss: 1.0699081420898438 | Test Loss: 1.110830545425415\n",
      "Epoch 9531 | Train Loss: 1.207857608795166 | Test Loss: 1.3208447694778442\n",
      "Epoch 9532 | Train Loss: 1.0698853731155396 | Test Loss: 1.1109623908996582\n",
      "Epoch 9533 | Train Loss: 1.2078962326049805 | Test Loss: 1.320965051651001\n",
      "Epoch 9534 | Train Loss: 1.0698847770690918 | Test Loss: 1.1108529567718506\n",
      "Epoch 9535 | Train Loss: 1.2079424858093262 | Test Loss: 1.3208770751953125\n",
      "Epoch 9536 | Train Loss: 1.0698777437210083 | Test Loss: 1.1110095977783203\n",
      "Epoch 9537 | Train Loss: 1.2079493999481201 | Test Loss: 1.3209582567214966\n",
      "Epoch 9538 | Train Loss: 1.0698610544204712 | Test Loss: 1.1108795404434204\n",
      "Epoch 9539 | Train Loss: 1.2079851627349854 | Test Loss: 1.3208595514297485\n",
      "Epoch 9540 | Train Loss: 1.0698297023773193 | Test Loss: 1.1110472679138184\n",
      "Epoch 9541 | Train Loss: 1.2080047130584717 | Test Loss: 1.3209340572357178\n",
      "Epoch 9542 | Train Loss: 1.0698155164718628 | Test Loss: 1.1108933687210083\n",
      "Epoch 9543 | Train Loss: 1.2080260515213013 | Test Loss: 1.3208674192428589\n",
      "Epoch 9544 | Train Loss: 1.0698001384735107 | Test Loss: 1.111046314239502\n",
      "Epoch 9545 | Train Loss: 1.2079499959945679 | Test Loss: 1.3208775520324707\n",
      "Epoch 9546 | Train Loss: 1.0697792768478394 | Test Loss: 1.1108795404434204\n",
      "Epoch 9547 | Train Loss: 1.207940936088562 | Test Loss: 1.320837140083313\n",
      "Epoch 9548 | Train Loss: 1.0697660446166992 | Test Loss: 1.1109918355941772\n",
      "Epoch 9549 | Train Loss: 1.2078608274459839 | Test Loss: 1.3208584785461426\n",
      "Epoch 9550 | Train Loss: 1.0697563886642456 | Test Loss: 1.1108554601669312\n",
      "Epoch 9551 | Train Loss: 1.2078135013580322 | Test Loss: 1.320845603942871\n",
      "Epoch 9552 | Train Loss: 1.0697485208511353 | Test Loss: 1.1109013557434082\n",
      "Epoch 9553 | Train Loss: 1.2077181339263916 | Test Loss: 1.3208590745925903\n",
      "Epoch 9554 | Train Loss: 1.0697399377822876 | Test Loss: 1.1108312606811523\n",
      "Epoch 9555 | Train Loss: 1.207679271697998 | Test Loss: 1.3208588361740112\n",
      "Epoch 9556 | Train Loss: 1.069739580154419 | Test Loss: 1.110809326171875\n",
      "Epoch 9557 | Train Loss: 1.2076213359832764 | Test Loss: 1.3208407163619995\n",
      "Epoch 9558 | Train Loss: 1.06972336769104 | Test Loss: 1.1108227968215942\n",
      "Epoch 9559 | Train Loss: 1.2075915336608887 | Test Loss: 1.3208972215652466\n",
      "Epoch 9560 | Train Loss: 1.0697176456451416 | Test Loss: 1.1107642650604248\n",
      "Epoch 9561 | Train Loss: 1.2075655460357666 | Test Loss: 1.3208850622177124\n",
      "Epoch 9562 | Train Loss: 1.0697060823440552 | Test Loss: 1.1108248233795166\n",
      "Epoch 9563 | Train Loss: 1.2075486183166504 | Test Loss: 1.320928931236267\n",
      "Epoch 9564 | Train Loss: 1.069701910018921 | Test Loss: 1.1107439994812012\n",
      "Epoch 9565 | Train Loss: 1.207534909248352 | Test Loss: 1.3208849430084229\n",
      "Epoch 9566 | Train Loss: 1.069684624671936 | Test Loss: 1.1108347177505493\n",
      "Epoch 9567 | Train Loss: 1.2075281143188477 | Test Loss: 1.3209363222122192\n",
      "Epoch 9568 | Train Loss: 1.0696812868118286 | Test Loss: 1.1107245683670044\n",
      "Epoch 9569 | Train Loss: 1.2075194120407104 | Test Loss: 1.3208887577056885\n",
      "Epoch 9570 | Train Loss: 1.069669485092163 | Test Loss: 1.1108473539352417\n",
      "Epoch 9571 | Train Loss: 1.2075154781341553 | Test Loss: 1.320945382118225\n",
      "Epoch 9572 | Train Loss: 1.0696641206741333 | Test Loss: 1.110711693763733\n",
      "Epoch 9573 | Train Loss: 1.2075047492980957 | Test Loss: 1.3208825588226318\n",
      "Epoch 9574 | Train Loss: 1.0696510076522827 | Test Loss: 1.1108386516571045\n",
      "Epoch 9575 | Train Loss: 1.207497477531433 | Test Loss: 1.3209785223007202\n",
      "Epoch 9576 | Train Loss: 1.0696483850479126 | Test Loss: 1.1107115745544434\n",
      "Epoch 9577 | Train Loss: 1.2074902057647705 | Test Loss: 1.3208924531936646\n",
      "Epoch 9578 | Train Loss: 1.069633960723877 | Test Loss: 1.110837697982788\n",
      "Epoch 9579 | Train Loss: 1.2074795961380005 | Test Loss: 1.3209794759750366\n",
      "Epoch 9580 | Train Loss: 1.069625973701477 | Test Loss: 1.1107162237167358\n",
      "Epoch 9581 | Train Loss: 1.2074698209762573 | Test Loss: 1.3209213018417358\n",
      "Epoch 9582 | Train Loss: 1.069610357284546 | Test Loss: 1.1108360290527344\n",
      "Epoch 9583 | Train Loss: 1.2074611186981201 | Test Loss: 1.3209832906723022\n",
      "Epoch 9584 | Train Loss: 1.0696014165878296 | Test Loss: 1.110724925994873\n",
      "Epoch 9585 | Train Loss: 1.207449197769165 | Test Loss: 1.3209662437438965\n",
      "Epoch 9586 | Train Loss: 1.0695902109146118 | Test Loss: 1.1108301877975464\n",
      "Epoch 9587 | Train Loss: 1.207431435585022 | Test Loss: 1.3210073709487915\n",
      "Epoch 9588 | Train Loss: 1.0695816278457642 | Test Loss: 1.1107224225997925\n",
      "Epoch 9589 | Train Loss: 1.2074209451675415 | Test Loss: 1.3209747076034546\n",
      "Epoch 9590 | Train Loss: 1.0695711374282837 | Test Loss: 1.1108288764953613\n",
      "Epoch 9591 | Train Loss: 1.2074097394943237 | Test Loss: 1.3210152387619019\n",
      "Epoch 9592 | Train Loss: 1.0695617198944092 | Test Loss: 1.110726237297058\n",
      "Epoch 9593 | Train Loss: 1.207399606704712 | Test Loss: 1.3209868669509888\n",
      "Epoch 9594 | Train Loss: 1.0695557594299316 | Test Loss: 1.1108112335205078\n",
      "Epoch 9595 | Train Loss: 1.2073791027069092 | Test Loss: 1.3210011720657349\n",
      "Epoch 9596 | Train Loss: 1.0695433616638184 | Test Loss: 1.110731601715088\n",
      "Epoch 9597 | Train Loss: 1.2073639631271362 | Test Loss: 1.3209881782531738\n",
      "Epoch 9598 | Train Loss: 1.0695374011993408 | Test Loss: 1.1107873916625977\n",
      "Epoch 9599 | Train Loss: 1.2073489427566528 | Test Loss: 1.3210126161575317\n",
      "Epoch 9600 | Train Loss: 1.0695269107818604 | Test Loss: 1.1107358932495117\n",
      "Epoch 9601 | Train Loss: 1.207327961921692 | Test Loss: 1.3210078477859497\n",
      "Epoch 9602 | Train Loss: 1.0695226192474365 | Test Loss: 1.110758662223816\n",
      "Epoch 9603 | Train Loss: 1.207313060760498 | Test Loss: 1.3210322856903076\n",
      "Epoch 9604 | Train Loss: 1.0695116519927979 | Test Loss: 1.1107325553894043\n",
      "Epoch 9605 | Train Loss: 1.207292914390564 | Test Loss: 1.3210324048995972\n",
      "Epoch 9606 | Train Loss: 1.0695085525512695 | Test Loss: 1.110733985900879\n",
      "Epoch 9607 | Train Loss: 1.207276701927185 | Test Loss: 1.3210558891296387\n",
      "Epoch 9608 | Train Loss: 1.069495439529419 | Test Loss: 1.1107380390167236\n",
      "Epoch 9609 | Train Loss: 1.2072715759277344 | Test Loss: 1.3210384845733643\n",
      "Epoch 9610 | Train Loss: 1.0694894790649414 | Test Loss: 1.110710620880127\n",
      "Epoch 9611 | Train Loss: 1.2072571516036987 | Test Loss: 1.321061372756958\n",
      "Epoch 9612 | Train Loss: 1.0694773197174072 | Test Loss: 1.1107505559921265\n",
      "Epoch 9613 | Train Loss: 1.207249641418457 | Test Loss: 1.3210514783859253\n",
      "Epoch 9614 | Train Loss: 1.069474458694458 | Test Loss: 1.110698938369751\n",
      "Epoch 9615 | Train Loss: 1.2072426080703735 | Test Loss: 1.3210746049880981\n",
      "Epoch 9616 | Train Loss: 1.0694599151611328 | Test Loss: 1.11077880859375\n",
      "Epoch 9617 | Train Loss: 1.2072319984436035 | Test Loss: 1.321098804473877\n",
      "Epoch 9618 | Train Loss: 1.0694550275802612 | Test Loss: 1.1106828451156616\n",
      "Epoch 9619 | Train Loss: 1.2072192430496216 | Test Loss: 1.3210712671279907\n",
      "Epoch 9620 | Train Loss: 1.0694419145584106 | Test Loss: 1.1107827425003052\n",
      "Epoch 9621 | Train Loss: 1.2072137594223022 | Test Loss: 1.3211113214492798\n",
      "Epoch 9622 | Train Loss: 1.0694400072097778 | Test Loss: 1.1106621026992798\n",
      "Epoch 9623 | Train Loss: 1.2072017192840576 | Test Loss: 1.3210774660110474\n",
      "Epoch 9624 | Train Loss: 1.0694221258163452 | Test Loss: 1.1107882261276245\n",
      "Epoch 9625 | Train Loss: 1.2071946859359741 | Test Loss: 1.3211205005645752\n",
      "Epoch 9626 | Train Loss: 1.0694173574447632 | Test Loss: 1.110656976699829\n",
      "Epoch 9627 | Train Loss: 1.2071837186813354 | Test Loss: 1.321110486984253\n",
      "Epoch 9628 | Train Loss: 1.0694034099578857 | Test Loss: 1.1107919216156006\n",
      "Epoch 9629 | Train Loss: 1.2071830034255981 | Test Loss: 1.321162223815918\n",
      "Epoch 9630 | Train Loss: 1.0694093704223633 | Test Loss: 1.1106441020965576\n",
      "Epoch 9631 | Train Loss: 1.2071765661239624 | Test Loss: 1.321143627166748\n",
      "Epoch 9632 | Train Loss: 1.0693936347961426 | Test Loss: 1.1107980012893677\n",
      "Epoch 9633 | Train Loss: 1.207176685333252 | Test Loss: 1.3212336301803589\n",
      "Epoch 9634 | Train Loss: 1.069393277168274 | Test Loss: 1.1106514930725098\n",
      "Epoch 9635 | Train Loss: 1.2071881294250488 | Test Loss: 1.3211445808410645\n",
      "Epoch 9636 | Train Loss: 1.0693821907043457 | Test Loss: 1.110817551612854\n",
      "Epoch 9637 | Train Loss: 1.2071964740753174 | Test Loss: 1.3212631940841675\n",
      "Epoch 9638 | Train Loss: 1.0693820714950562 | Test Loss: 1.1106576919555664\n",
      "Epoch 9639 | Train Loss: 1.2072032690048218 | Test Loss: 1.3211544752120972\n",
      "Epoch 9640 | Train Loss: 1.0693626403808594 | Test Loss: 1.1108551025390625\n",
      "Epoch 9641 | Train Loss: 1.2072269916534424 | Test Loss: 1.3212631940841675\n",
      "Epoch 9642 | Train Loss: 1.0693590641021729 | Test Loss: 1.1106626987457275\n",
      "Epoch 9643 | Train Loss: 1.207255244255066 | Test Loss: 1.3211727142333984\n",
      "Epoch 9644 | Train Loss: 1.0693501234054565 | Test Loss: 1.1108938455581665\n",
      "Epoch 9645 | Train Loss: 1.2072436809539795 | Test Loss: 1.3212143182754517\n",
      "Epoch 9646 | Train Loss: 1.069339632987976 | Test Loss: 1.1106736660003662\n",
      "Epoch 9647 | Train Loss: 1.2072644233703613 | Test Loss: 1.3211580514907837\n",
      "Epoch 9648 | Train Loss: 1.0693309307098389 | Test Loss: 1.1108945608139038\n",
      "Epoch 9649 | Train Loss: 1.2072389125823975 | Test Loss: 1.3212029933929443\n",
      "Epoch 9650 | Train Loss: 1.0693196058273315 | Test Loss: 1.1106858253479004\n",
      "Epoch 9651 | Train Loss: 1.207247018814087 | Test Loss: 1.3211750984191895\n",
      "Epoch 9652 | Train Loss: 1.0693135261535645 | Test Loss: 1.110857605934143\n",
      "Epoch 9653 | Train Loss: 1.2071853876113892 | Test Loss: 1.3211907148361206\n",
      "Epoch 9654 | Train Loss: 1.0692975521087646 | Test Loss: 1.110696792602539\n",
      "Epoch 9655 | Train Loss: 1.2071675062179565 | Test Loss: 1.3211814165115356\n",
      "Epoch 9656 | Train Loss: 1.069286584854126 | Test Loss: 1.110795259475708\n",
      "Epoch 9657 | Train Loss: 1.2071163654327393 | Test Loss: 1.321187138557434\n",
      "Epoch 9658 | Train Loss: 1.069273829460144 | Test Loss: 1.1106970310211182\n",
      "Epoch 9659 | Train Loss: 1.2070720195770264 | Test Loss: 1.321208119392395\n",
      "Epoch 9660 | Train Loss: 1.0692697763442993 | Test Loss: 1.1107239723205566\n",
      "Epoch 9661 | Train Loss: 1.2070128917694092 | Test Loss: 1.3211839199066162\n",
      "Epoch 9662 | Train Loss: 1.0692578554153442 | Test Loss: 1.1107079982757568\n",
      "Epoch 9663 | Train Loss: 1.2069908380508423 | Test Loss: 1.321212887763977\n",
      "Epoch 9664 | Train Loss: 1.0692535638809204 | Test Loss: 1.1106584072113037\n",
      "Epoch 9665 | Train Loss: 1.2069652080535889 | Test Loss: 1.3211930990219116\n",
      "Epoch 9666 | Train Loss: 1.069237470626831 | Test Loss: 1.110727310180664\n",
      "Epoch 9667 | Train Loss: 1.206953525543213 | Test Loss: 1.3211950063705444\n",
      "Epoch 9668 | Train Loss: 1.0692278146743774 | Test Loss: 1.1106209754943848\n",
      "Epoch 9669 | Train Loss: 1.206946611404419 | Test Loss: 1.3212251663208008\n",
      "Epoch 9670 | Train Loss: 1.0692170858383179 | Test Loss: 1.1107516288757324\n",
      "Epoch 9671 | Train Loss: 1.206949234008789 | Test Loss: 1.321223497390747\n",
      "Epoch 9672 | Train Loss: 1.0692161321640015 | Test Loss: 1.110595941543579\n",
      "Epoch 9673 | Train Loss: 1.2069426774978638 | Test Loss: 1.3212188482284546\n",
      "Epoch 9674 | Train Loss: 1.0691988468170166 | Test Loss: 1.1107672452926636\n",
      "Epoch 9675 | Train Loss: 1.2069464921951294 | Test Loss: 1.3212883472442627\n",
      "Epoch 9676 | Train Loss: 1.0692012310028076 | Test Loss: 1.1105823516845703\n",
      "Epoch 9677 | Train Loss: 1.2069462537765503 | Test Loss: 1.321221113204956\n",
      "Epoch 9678 | Train Loss: 1.0691964626312256 | Test Loss: 1.1107699871063232\n",
      "Epoch 9679 | Train Loss: 1.2069300413131714 | Test Loss: 1.3213181495666504\n",
      "Epoch 9680 | Train Loss: 1.06919264793396 | Test Loss: 1.110581398010254\n",
      "Epoch 9681 | Train Loss: 1.206930160522461 | Test Loss: 1.321195125579834\n",
      "Epoch 9682 | Train Loss: 1.0691797733306885 | Test Loss: 1.1107821464538574\n",
      "Epoch 9683 | Train Loss: 1.2069339752197266 | Test Loss: 1.3213499784469604\n",
      "Epoch 9684 | Train Loss: 1.0691825151443481 | Test Loss: 1.110599160194397\n",
      "Epoch 9685 | Train Loss: 1.2069381475448608 | Test Loss: 1.3212616443634033\n",
      "Epoch 9686 | Train Loss: 1.0691702365875244 | Test Loss: 1.1108132600784302\n",
      "Epoch 9687 | Train Loss: 1.2069380283355713 | Test Loss: 1.3213396072387695\n",
      "Epoch 9688 | Train Loss: 1.069154977798462 | Test Loss: 1.1106328964233398\n",
      "Epoch 9689 | Train Loss: 1.206950068473816 | Test Loss: 1.3212751150131226\n",
      "Epoch 9690 | Train Loss: 1.0691429376602173 | Test Loss: 1.1108144521713257\n",
      "Epoch 9691 | Train Loss: 1.2069482803344727 | Test Loss: 1.3213609457015991\n",
      "Epoch 9692 | Train Loss: 1.0691275596618652 | Test Loss: 1.1106505393981934\n",
      "Epoch 9693 | Train Loss: 1.2069385051727295 | Test Loss: 1.3213061094284058\n",
      "Epoch 9694 | Train Loss: 1.0691112279891968 | Test Loss: 1.1107994318008423\n",
      "Epoch 9695 | Train Loss: 1.206925630569458 | Test Loss: 1.3213486671447754\n",
      "Epoch 9696 | Train Loss: 1.069100022315979 | Test Loss: 1.1106550693511963\n",
      "Epoch 9697 | Train Loss: 1.206898808479309 | Test Loss: 1.3212971687316895\n",
      "Epoch 9698 | Train Loss: 1.0690884590148926 | Test Loss: 1.110764741897583\n",
      "Epoch 9699 | Train Loss: 1.2068662643432617 | Test Loss: 1.3213214874267578\n",
      "Epoch 9700 | Train Loss: 1.0690761804580688 | Test Loss: 1.110648512840271\n",
      "Epoch 9701 | Train Loss: 1.2068324089050293 | Test Loss: 1.3213499784469604\n",
      "Epoch 9702 | Train Loss: 1.0690724849700928 | Test Loss: 1.1106950044631958\n",
      "Epoch 9703 | Train Loss: 1.2067919969558716 | Test Loss: 1.321327805519104\n",
      "Epoch 9704 | Train Loss: 1.0690619945526123 | Test Loss: 1.1106406450271606\n",
      "Epoch 9705 | Train Loss: 1.2067629098892212 | Test Loss: 1.3213788270950317\n",
      "Epoch 9706 | Train Loss: 1.0690580606460571 | Test Loss: 1.1106374263763428\n",
      "Epoch 9707 | Train Loss: 1.20673668384552 | Test Loss: 1.3213462829589844\n",
      "Epoch 9708 | Train Loss: 1.0690432786941528 | Test Loss: 1.1106250286102295\n",
      "Epoch 9709 | Train Loss: 1.2067114114761353 | Test Loss: 1.3214186429977417\n",
      "Epoch 9710 | Train Loss: 1.0690457820892334 | Test Loss: 1.110605001449585\n",
      "Epoch 9711 | Train Loss: 1.2066991329193115 | Test Loss: 1.3213720321655273\n",
      "Epoch 9712 | Train Loss: 1.0690330266952515 | Test Loss: 1.1106361150741577\n",
      "Epoch 9713 | Train Loss: 1.2066961526870728 | Test Loss: 1.32145357131958\n",
      "Epoch 9714 | Train Loss: 1.0690363645553589 | Test Loss: 1.1105751991271973\n",
      "Epoch 9715 | Train Loss: 1.2066919803619385 | Test Loss: 1.3213609457015991\n",
      "Epoch 9716 | Train Loss: 1.0690189599990845 | Test Loss: 1.1106657981872559\n",
      "Epoch 9717 | Train Loss: 1.206713318824768 | Test Loss: 1.3214638233184814\n",
      "Epoch 9718 | Train Loss: 1.0690244436264038 | Test Loss: 1.110567569732666\n",
      "Epoch 9719 | Train Loss: 1.2067339420318604 | Test Loss: 1.3213742971420288\n",
      "Epoch 9720 | Train Loss: 1.069008231163025 | Test Loss: 1.1107138395309448\n",
      "Epoch 9721 | Train Loss: 1.206756353378296 | Test Loss: 1.3214561939239502\n",
      "Epoch 9722 | Train Loss: 1.06900155544281 | Test Loss: 1.110575556755066\n",
      "Epoch 9723 | Train Loss: 1.2067766189575195 | Test Loss: 1.3213835954666138\n",
      "Epoch 9724 | Train Loss: 1.0689798593521118 | Test Loss: 1.11074697971344\n",
      "Epoch 9725 | Train Loss: 1.206783413887024 | Test Loss: 1.3214337825775146\n",
      "Epoch 9726 | Train Loss: 1.0689704418182373 | Test Loss: 1.1106014251708984\n",
      "Epoch 9727 | Train Loss: 1.206791877746582 | Test Loss: 1.3214212656021118\n",
      "Epoch 9728 | Train Loss: 1.0689643621444702 | Test Loss: 1.1107412576675415\n",
      "Epoch 9729 | Train Loss: 1.2067559957504272 | Test Loss: 1.3213963508605957\n",
      "Epoch 9730 | Train Loss: 1.0689525604248047 | Test Loss: 1.110612154006958\n",
      "Epoch 9731 | Train Loss: 1.2067309617996216 | Test Loss: 1.3214092254638672\n",
      "Epoch 9732 | Train Loss: 1.0689460039138794 | Test Loss: 1.1107046604156494\n",
      "Epoch 9733 | Train Loss: 1.2066864967346191 | Test Loss: 1.3214088678359985\n",
      "Epoch 9734 | Train Loss: 1.0689358711242676 | Test Loss: 1.1106046438217163\n",
      "Epoch 9735 | Train Loss: 1.2066539525985718 | Test Loss: 1.3214298486709595\n",
      "Epoch 9736 | Train Loss: 1.0689268112182617 | Test Loss: 1.110655426979065\n",
      "Epoch 9737 | Train Loss: 1.2066150903701782 | Test Loss: 1.3214138746261597\n",
      "Epoch 9738 | Train Loss: 1.0689135789871216 | Test Loss: 1.1106007099151611\n",
      "Epoch 9739 | Train Loss: 1.2065863609313965 | Test Loss: 1.321406364440918\n",
      "Epoch 9740 | Train Loss: 1.068903923034668 | Test Loss: 1.1106070280075073\n",
      "Epoch 9741 | Train Loss: 1.2065644264221191 | Test Loss: 1.3213800191879272\n",
      "Epoch 9742 | Train Loss: 1.0688916444778442 | Test Loss: 1.1106194257736206\n",
      "Epoch 9743 | Train Loss: 1.2065465450286865 | Test Loss: 1.321394920349121\n",
      "Epoch 9744 | Train Loss: 1.068884015083313 | Test Loss: 1.1105626821517944\n",
      "Epoch 9745 | Train Loss: 1.2065314054489136 | Test Loss: 1.3214008808135986\n",
      "Epoch 9746 | Train Loss: 1.0688729286193848 | Test Loss: 1.110625982284546\n",
      "Epoch 9747 | Train Loss: 1.2065229415893555 | Test Loss: 1.321432113647461\n",
      "Epoch 9748 | Train Loss: 1.0688655376434326 | Test Loss: 1.1105263233184814\n",
      "Epoch 9749 | Train Loss: 1.2065105438232422 | Test Loss: 1.321427345275879\n",
      "Epoch 9750 | Train Loss: 1.068851113319397 | Test Loss: 1.1106319427490234\n",
      "Epoch 9751 | Train Loss: 1.2064989805221558 | Test Loss: 1.3214420080184937\n",
      "Epoch 9752 | Train Loss: 1.068842887878418 | Test Loss: 1.1105153560638428\n",
      "Epoch 9753 | Train Loss: 1.2064872980117798 | Test Loss: 1.3214209079742432\n",
      "Epoch 9754 | Train Loss: 1.0688364505767822 | Test Loss: 1.1106171607971191\n",
      "Epoch 9755 | Train Loss: 1.2064696550369263 | Test Loss: 1.3214505910873413\n",
      "Epoch 9756 | Train Loss: 1.06882643699646 | Test Loss: 1.110506534576416\n",
      "Epoch 9757 | Train Loss: 1.2064467668533325 | Test Loss: 1.3214056491851807\n",
      "Epoch 9758 | Train Loss: 1.0688180923461914 | Test Loss: 1.110596776008606\n",
      "Epoch 9759 | Train Loss: 1.206433653831482 | Test Loss: 1.3214874267578125\n",
      "Epoch 9760 | Train Loss: 1.0688109397888184 | Test Loss: 1.1105047464370728\n",
      "Epoch 9761 | Train Loss: 1.2064154148101807 | Test Loss: 1.3214242458343506\n",
      "Epoch 9762 | Train Loss: 1.068803071975708 | Test Loss: 1.1105895042419434\n",
      "Epoch 9763 | Train Loss: 1.2064024209976196 | Test Loss: 1.321494221687317\n",
      "Epoch 9764 | Train Loss: 1.068796992301941 | Test Loss: 1.1105122566223145\n",
      "Epoch 9765 | Train Loss: 1.2063863277435303 | Test Loss: 1.321422815322876\n",
      "Epoch 9766 | Train Loss: 1.0687916278839111 | Test Loss: 1.1105797290802002\n",
      "Epoch 9767 | Train Loss: 1.2063778638839722 | Test Loss: 1.3215223550796509\n",
      "Epoch 9768 | Train Loss: 1.0687834024429321 | Test Loss: 1.110507607460022\n",
      "Epoch 9769 | Train Loss: 1.206366777420044 | Test Loss: 1.3214446306228638\n",
      "Epoch 9770 | Train Loss: 1.0687757730484009 | Test Loss: 1.1105690002441406\n",
      "Epoch 9771 | Train Loss: 1.2063641548156738 | Test Loss: 1.321526288986206\n",
      "Epoch 9772 | Train Loss: 1.0687652826309204 | Test Loss: 1.110529899597168\n",
      "Epoch 9773 | Train Loss: 1.2063546180725098 | Test Loss: 1.3214945793151855\n",
      "Epoch 9774 | Train Loss: 1.0687607526779175 | Test Loss: 1.110568642616272\n",
      "Epoch 9775 | Train Loss: 1.2063472270965576 | Test Loss: 1.3215124607086182\n",
      "Epoch 9776 | Train Loss: 1.068748950958252 | Test Loss: 1.11055326461792\n",
      "Epoch 9777 | Train Loss: 1.206348180770874 | Test Loss: 1.3214943408966064\n",
      "Epoch 9778 | Train Loss: 1.0687373876571655 | Test Loss: 1.1105692386627197\n",
      "Epoch 9779 | Train Loss: 1.2063448429107666 | Test Loss: 1.321557879447937\n",
      "Epoch 9780 | Train Loss: 1.0687276124954224 | Test Loss: 1.1105459928512573\n",
      "Epoch 9781 | Train Loss: 1.2063363790512085 | Test Loss: 1.3215616941452026\n",
      "Epoch 9782 | Train Loss: 1.0687178373336792 | Test Loss: 1.1105769872665405\n",
      "Epoch 9783 | Train Loss: 1.2063357830047607 | Test Loss: 1.3215539455413818\n",
      "Epoch 9784 | Train Loss: 1.068702220916748 | Test Loss: 1.1105313301086426\n",
      "Epoch 9785 | Train Loss: 1.2063264846801758 | Test Loss: 1.3215514421463013\n",
      "Epoch 9786 | Train Loss: 1.0686942338943481 | Test Loss: 1.110573649406433\n",
      "Epoch 9787 | Train Loss: 1.2063086032867432 | Test Loss: 1.3215748071670532\n",
      "Epoch 9788 | Train Loss: 1.0686830282211304 | Test Loss: 1.110522747039795\n",
      "Epoch 9789 | Train Loss: 1.2062962055206299 | Test Loss: 1.3215460777282715\n",
      "Epoch 9790 | Train Loss: 1.068673014640808 | Test Loss: 1.1105648279190063\n",
      "Epoch 9791 | Train Loss: 1.2062737941741943 | Test Loss: 1.3215841054916382\n",
      "Epoch 9792 | Train Loss: 1.068665862083435 | Test Loss: 1.110512614250183\n",
      "Epoch 9793 | Train Loss: 1.2062512636184692 | Test Loss: 1.3216146230697632\n",
      "Epoch 9794 | Train Loss: 1.068660855293274 | Test Loss: 1.1105271577835083\n",
      "Epoch 9795 | Train Loss: 1.206225872039795 | Test Loss: 1.321592926979065\n",
      "Epoch 9796 | Train Loss: 1.0686533451080322 | Test Loss: 1.1105005741119385\n",
      "Epoch 9797 | Train Loss: 1.2061995267868042 | Test Loss: 1.3216100931167603\n",
      "Epoch 9798 | Train Loss: 1.0686440467834473 | Test Loss: 1.110484004020691\n",
      "Epoch 9799 | Train Loss: 1.2061768770217896 | Test Loss: 1.3215914964675903\n",
      "Epoch 9800 | Train Loss: 1.068633794784546 | Test Loss: 1.110485315322876\n",
      "Epoch 9801 | Train Loss: 1.206164002418518 | Test Loss: 1.321616768836975\n",
      "Epoch 9802 | Train Loss: 1.0686283111572266 | Test Loss: 1.1104657649993896\n",
      "Epoch 9803 | Train Loss: 1.2061430215835571 | Test Loss: 1.3216363191604614\n",
      "Epoch 9804 | Train Loss: 1.068625807762146 | Test Loss: 1.110482931137085\n",
      "Epoch 9805 | Train Loss: 1.206132173538208 | Test Loss: 1.3216785192489624\n",
      "Epoch 9806 | Train Loss: 1.0686237812042236 | Test Loss: 1.1104497909545898\n",
      "Epoch 9807 | Train Loss: 1.206122636795044 | Test Loss: 1.3216521739959717\n",
      "Epoch 9808 | Train Loss: 1.0686180591583252 | Test Loss: 1.1105055809020996\n",
      "Epoch 9809 | Train Loss: 1.2061291933059692 | Test Loss: 1.3217248916625977\n",
      "Epoch 9810 | Train Loss: 1.0686194896697998 | Test Loss: 1.110450267791748\n",
      "Epoch 9811 | Train Loss: 1.2061368227005005 | Test Loss: 1.3216469287872314\n",
      "Epoch 9812 | Train Loss: 1.0686149597167969 | Test Loss: 1.1105458736419678\n",
      "Epoch 9813 | Train Loss: 1.2061502933502197 | Test Loss: 1.321759819984436\n",
      "Epoch 9814 | Train Loss: 1.0686182975769043 | Test Loss: 1.110451579093933\n",
      "Epoch 9815 | Train Loss: 1.206186294555664 | Test Loss: 1.3216537237167358\n",
      "Epoch 9816 | Train Loss: 1.0686063766479492 | Test Loss: 1.1106083393096924\n",
      "Epoch 9817 | Train Loss: 1.2062222957611084 | Test Loss: 1.3217720985412598\n",
      "Epoch 9818 | Train Loss: 1.0685995817184448 | Test Loss: 1.1104850769042969\n",
      "Epoch 9819 | Train Loss: 1.2062675952911377 | Test Loss: 1.321663498878479\n",
      "Epoch 9820 | Train Loss: 1.0685831308364868 | Test Loss: 1.1106600761413574\n",
      "Epoch 9821 | Train Loss: 1.2062811851501465 | Test Loss: 1.321746587753296\n",
      "Epoch 9822 | Train Loss: 1.068573236465454 | Test Loss: 1.1105172634124756\n",
      "Epoch 9823 | Train Loss: 1.206318974494934 | Test Loss: 1.321670651435852\n",
      "Epoch 9824 | Train Loss: 1.0685540437698364 | Test Loss: 1.110669493675232\n",
      "Epoch 9825 | Train Loss: 1.2062723636627197 | Test Loss: 1.3217084407806396\n",
      "Epoch 9826 | Train Loss: 1.0685385465621948 | Test Loss: 1.1105378866195679\n",
      "Epoch 9827 | Train Loss: 1.2062764167785645 | Test Loss: 1.3216854333877563\n",
      "Epoch 9828 | Train Loss: 1.0685292482376099 | Test Loss: 1.1106294393539429\n",
      "Epoch 9829 | Train Loss: 1.206206202507019 | Test Loss: 1.3216795921325684\n",
      "Epoch 9830 | Train Loss: 1.068523645401001 | Test Loss: 1.1105257272720337\n",
      "Epoch 9831 | Train Loss: 1.2061525583267212 | Test Loss: 1.3217194080352783\n",
      "Epoch 9832 | Train Loss: 1.0685186386108398 | Test Loss: 1.1105445623397827\n",
      "Epoch 9833 | Train Loss: 1.2060954570770264 | Test Loss: 1.3216793537139893\n",
      "Epoch 9834 | Train Loss: 1.0685083866119385 | Test Loss: 1.1105084419250488\n",
      "Epoch 9835 | Train Loss: 1.206059455871582 | Test Loss: 1.321756362915039\n",
      "Epoch 9836 | Train Loss: 1.0685073137283325 | Test Loss: 1.1104819774627686\n",
      "Epoch 9837 | Train Loss: 1.206026554107666 | Test Loss: 1.3216516971588135\n",
      "Epoch 9838 | Train Loss: 1.068493366241455 | Test Loss: 1.1105403900146484\n",
      "Epoch 9839 | Train Loss: 1.2060130834579468 | Test Loss: 1.321719765663147\n",
      "Epoch 9840 | Train Loss: 1.0684865713119507 | Test Loss: 1.1104495525360107\n",
      "Epoch 9841 | Train Loss: 1.2060070037841797 | Test Loss: 1.3216521739959717\n",
      "Epoch 9842 | Train Loss: 1.0684701204299927 | Test Loss: 1.1105784177780151\n",
      "Epoch 9843 | Train Loss: 1.2060143947601318 | Test Loss: 1.3217222690582275\n",
      "Epoch 9844 | Train Loss: 1.0684630870819092 | Test Loss: 1.110434889793396\n",
      "Epoch 9845 | Train Loss: 1.2060133218765259 | Test Loss: 1.321675181388855\n",
      "Epoch 9846 | Train Loss: 1.0684492588043213 | Test Loss: 1.110596776008606\n",
      "Epoch 9847 | Train Loss: 1.2060152292251587 | Test Loss: 1.3217225074768066\n",
      "Epoch 9848 | Train Loss: 1.0684435367584229 | Test Loss: 1.1104239225387573\n",
      "Epoch 9849 | Train Loss: 1.2059979438781738 | Test Loss: 1.3216931819915771\n",
      "Epoch 9850 | Train Loss: 1.068428635597229 | Test Loss: 1.1106184720993042\n",
      "Epoch 9851 | Train Loss: 1.2060047388076782 | Test Loss: 1.3216968774795532\n",
      "Epoch 9852 | Train Loss: 1.0684192180633545 | Test Loss: 1.1104331016540527\n",
      "Epoch 9853 | Train Loss: 1.205981731414795 | Test Loss: 1.3216867446899414\n",
      "Epoch 9854 | Train Loss: 1.0684078931808472 | Test Loss: 1.110607624053955\n",
      "Epoch 9855 | Train Loss: 1.2059597969055176 | Test Loss: 1.3216980695724487\n",
      "Epoch 9856 | Train Loss: 1.0683996677398682 | Test Loss: 1.1104198694229126\n",
      "Epoch 9857 | Train Loss: 1.2059352397918701 | Test Loss: 1.3216756582260132\n",
      "Epoch 9858 | Train Loss: 1.0683907270431519 | Test Loss: 1.1105564832687378\n",
      "Epoch 9859 | Train Loss: 1.2058961391448975 | Test Loss: 1.321722388267517\n",
      "Epoch 9860 | Train Loss: 1.0683835744857788 | Test Loss: 1.1103965044021606\n",
      "Epoch 9861 | Train Loss: 1.2058579921722412 | Test Loss: 1.3217321634292603\n",
      "Epoch 9862 | Train Loss: 1.0683788061141968 | Test Loss: 1.1104985475540161\n",
      "Epoch 9863 | Train Loss: 1.2058311700820923 | Test Loss: 1.3217412233352661\n",
      "Epoch 9864 | Train Loss: 1.0683650970458984 | Test Loss: 1.110398530960083\n",
      "Epoch 9865 | Train Loss: 1.205802321434021 | Test Loss: 1.3217512369155884\n",
      "Epoch 9866 | Train Loss: 1.068359136581421 | Test Loss: 1.1104564666748047\n",
      "Epoch 9867 | Train Loss: 1.2057764530181885 | Test Loss: 1.3217440843582153\n",
      "Epoch 9868 | Train Loss: 1.0683475732803345 | Test Loss: 1.1104034185409546\n",
      "Epoch 9869 | Train Loss: 1.2057589292526245 | Test Loss: 1.321752667427063\n",
      "Epoch 9870 | Train Loss: 1.0683388710021973 | Test Loss: 1.110432744026184\n",
      "Epoch 9871 | Train Loss: 1.2057430744171143 | Test Loss: 1.3217391967773438\n",
      "Epoch 9872 | Train Loss: 1.0683302879333496 | Test Loss: 1.1104031801223755\n",
      "Epoch 9873 | Train Loss: 1.2057279348373413 | Test Loss: 1.3217754364013672\n",
      "Epoch 9874 | Train Loss: 1.068320631980896 | Test Loss: 1.110414981842041\n",
      "Epoch 9875 | Train Loss: 1.2057182788848877 | Test Loss: 1.321775197982788\n",
      "Epoch 9876 | Train Loss: 1.0683108568191528 | Test Loss: 1.1103954315185547\n",
      "Epoch 9877 | Train Loss: 1.2057008743286133 | Test Loss: 1.321776270866394\n",
      "Epoch 9878 | Train Loss: 1.0683082342147827 | Test Loss: 1.1103997230529785\n",
      "Epoch 9879 | Train Loss: 1.205688238143921 | Test Loss: 1.321785569190979\n",
      "Epoch 9880 | Train Loss: 1.068298101425171 | Test Loss: 1.1103763580322266\n",
      "Epoch 9881 | Train Loss: 1.2056797742843628 | Test Loss: 1.3217898607254028\n",
      "Epoch 9882 | Train Loss: 1.068293571472168 | Test Loss: 1.1103975772857666\n",
      "Epoch 9883 | Train Loss: 1.205665111541748 | Test Loss: 1.3217824697494507\n",
      "Epoch 9884 | Train Loss: 1.068284034729004 | Test Loss: 1.1103636026382446\n",
      "Epoch 9885 | Train Loss: 1.205658197402954 | Test Loss: 1.3218170404434204\n",
      "Epoch 9886 | Train Loss: 1.0682753324508667 | Test Loss: 1.1103943586349487\n",
      "Epoch 9887 | Train Loss: 1.205650806427002 | Test Loss: 1.3218055963516235\n",
      "Epoch 9888 | Train Loss: 1.0682638883590698 | Test Loss: 1.110368251800537\n",
      "Epoch 9889 | Train Loss: 1.2056427001953125 | Test Loss: 1.3218104839324951\n",
      "Epoch 9890 | Train Loss: 1.0682594776153564 | Test Loss: 1.1103864908218384\n",
      "Epoch 9891 | Train Loss: 1.205634355545044 | Test Loss: 1.3218339681625366\n",
      "Epoch 9892 | Train Loss: 1.0682483911514282 | Test Loss: 1.1103798151016235\n",
      "Epoch 9893 | Train Loss: 1.2056269645690918 | Test Loss: 1.3218269348144531\n",
      "Epoch 9894 | Train Loss: 1.0682419538497925 | Test Loss: 1.1103731393814087\n",
      "Epoch 9895 | Train Loss: 1.2056156396865845 | Test Loss: 1.3218557834625244\n",
      "Epoch 9896 | Train Loss: 1.0682319402694702 | Test Loss: 1.1103852987289429\n",
      "Epoch 9897 | Train Loss: 1.2056142091751099 | Test Loss: 1.3218210935592651\n",
      "Epoch 9898 | Train Loss: 1.0682320594787598 | Test Loss: 1.1103707551956177\n",
      "Epoch 9899 | Train Loss: 1.2056050300598145 | Test Loss: 1.3218870162963867\n",
      "Epoch 9900 | Train Loss: 1.0682240724563599 | Test Loss: 1.1103835105895996\n",
      "Epoch 9901 | Train Loss: 1.2056074142456055 | Test Loss: 1.3218389749526978\n",
      "Epoch 9902 | Train Loss: 1.0682235956192017 | Test Loss: 1.1103954315185547\n",
      "Epoch 9903 | Train Loss: 1.2056169509887695 | Test Loss: 1.3219361305236816\n",
      "Epoch 9904 | Train Loss: 1.068216323852539 | Test Loss: 1.110394835472107\n",
      "Epoch 9905 | Train Loss: 1.205625295639038 | Test Loss: 1.3218765258789062\n",
      "Epoch 9906 | Train Loss: 1.0682108402252197 | Test Loss: 1.1104357242584229\n",
      "Epoch 9907 | Train Loss: 1.2056453227996826 | Test Loss: 1.3219245672225952\n",
      "Epoch 9908 | Train Loss: 1.0682010650634766 | Test Loss: 1.1104308366775513\n",
      "Epoch 9909 | Train Loss: 1.2056530714035034 | Test Loss: 1.321907639503479\n",
      "Epoch 9910 | Train Loss: 1.0681971311569214 | Test Loss: 1.110437273979187\n",
      "Epoch 9911 | Train Loss: 1.2056621313095093 | Test Loss: 1.32192862033844\n",
      "Epoch 9912 | Train Loss: 1.0681812763214111 | Test Loss: 1.110461711883545\n",
      "Epoch 9913 | Train Loss: 1.2056686878204346 | Test Loss: 1.3219176530838013\n",
      "Epoch 9914 | Train Loss: 1.068170428276062 | Test Loss: 1.1104406118392944\n",
      "Epoch 9915 | Train Loss: 1.2056628465652466 | Test Loss: 1.3219115734100342\n",
      "Epoch 9916 | Train Loss: 1.0681618452072144 | Test Loss: 1.1104590892791748\n",
      "Epoch 9917 | Train Loss: 1.2056547403335571 | Test Loss: 1.3219558000564575\n",
      "Epoch 9918 | Train Loss: 1.0681654214859009 | Test Loss: 1.1104376316070557\n",
      "Epoch 9919 | Train Loss: 1.2056331634521484 | Test Loss: 1.321928858757019\n",
      "Epoch 9920 | Train Loss: 1.068153977394104 | Test Loss: 1.110444188117981\n",
      "Epoch 9921 | Train Loss: 1.2056137323379517 | Test Loss: 1.3220241069793701\n",
      "Epoch 9922 | Train Loss: 1.0681484937667847 | Test Loss: 1.110414981842041\n",
      "Epoch 9923 | Train Loss: 1.2056082487106323 | Test Loss: 1.3219778537750244\n",
      "Epoch 9924 | Train Loss: 1.0681524276733398 | Test Loss: 1.1104563474655151\n",
      "Epoch 9925 | Train Loss: 1.205610752105713 | Test Loss: 1.3220841884613037\n",
      "Epoch 9926 | Train Loss: 1.068160891532898 | Test Loss: 1.110391616821289\n",
      "Epoch 9927 | Train Loss: 1.2056200504302979 | Test Loss: 1.322011113166809\n",
      "Epoch 9928 | Train Loss: 1.0681430101394653 | Test Loss: 1.110535979270935\n",
      "Epoch 9929 | Train Loss: 1.2056524753570557 | Test Loss: 1.3221083879470825\n",
      "Epoch 9930 | Train Loss: 1.0681231021881104 | Test Loss: 1.1104116439819336\n",
      "Epoch 9931 | Train Loss: 1.2056984901428223 | Test Loss: 1.3220405578613281\n",
      "Epoch 9932 | Train Loss: 1.0681087970733643 | Test Loss: 1.1105574369430542\n",
      "Epoch 9933 | Train Loss: 1.2056856155395508 | Test Loss: 1.3220750093460083\n",
      "Epoch 9934 | Train Loss: 1.0680856704711914 | Test Loss: 1.1104261875152588\n",
      "Epoch 9935 | Train Loss: 1.205682396888733 | Test Loss: 1.3220196962356567\n",
      "Epoch 9936 | Train Loss: 1.068068504333496 | Test Loss: 1.110528588294983\n",
      "Epoch 9937 | Train Loss: 1.2056591510772705 | Test Loss: 1.3220648765563965\n",
      "Epoch 9938 | Train Loss: 1.0680484771728516 | Test Loss: 1.1103955507278442\n",
      "Epoch 9939 | Train Loss: 1.2056339979171753 | Test Loss: 1.3220384120941162\n",
      "Epoch 9940 | Train Loss: 1.0680413246154785 | Test Loss: 1.110467791557312\n",
      "Epoch 9941 | Train Loss: 1.205552101135254 | Test Loss: 1.3220473527908325\n",
      "Epoch 9942 | Train Loss: 1.0680279731750488 | Test Loss: 1.1103507280349731\n",
      "Epoch 9943 | Train Loss: 1.205506443977356 | Test Loss: 1.3220072984695435\n",
      "Epoch 9944 | Train Loss: 1.0680222511291504 | Test Loss: 1.1104097366333008\n",
      "Epoch 9945 | Train Loss: 1.2054494619369507 | Test Loss: 1.3220361471176147\n",
      "Epoch 9946 | Train Loss: 1.0680104494094849 | Test Loss: 1.11033296585083\n",
      "Epoch 9947 | Train Loss: 1.2054282426834106 | Test Loss: 1.3220148086547852\n",
      "Epoch 9948 | Train Loss: 1.0680019855499268 | Test Loss: 1.1103581190109253\n",
      "Epoch 9949 | Train Loss: 1.2053802013397217 | Test Loss: 1.3220137357711792\n",
      "Epoch 9950 | Train Loss: 1.0679904222488403 | Test Loss: 1.1103432178497314\n",
      "Epoch 9951 | Train Loss: 1.2053539752960205 | Test Loss: 1.3220248222351074\n",
      "Epoch 9952 | Train Loss: 1.067984700202942 | Test Loss: 1.1103062629699707\n",
      "Epoch 9953 | Train Loss: 1.2053272724151611 | Test Loss: 1.3220305442810059\n",
      "Epoch 9954 | Train Loss: 1.0679734945297241 | Test Loss: 1.1103523969650269\n",
      "Epoch 9955 | Train Loss: 1.2053117752075195 | Test Loss: 1.3220430612564087\n",
      "Epoch 9956 | Train Loss: 1.067966103553772 | Test Loss: 1.110276460647583\n",
      "Epoch 9957 | Train Loss: 1.2052916288375854 | Test Loss: 1.3220393657684326\n",
      "Epoch 9958 | Train Loss: 1.0679558515548706 | Test Loss: 1.1103540658950806\n",
      "Epoch 9959 | Train Loss: 1.2052781581878662 | Test Loss: 1.3220560550689697\n",
      "Epoch 9960 | Train Loss: 1.06795072555542 | Test Loss: 1.1102652549743652\n",
      "Epoch 9961 | Train Loss: 1.2052669525146484 | Test Loss: 1.3220844268798828\n",
      "Epoch 9962 | Train Loss: 1.067941665649414 | Test Loss: 1.1103705167770386\n",
      "Epoch 9963 | Train Loss: 1.2052571773529053 | Test Loss: 1.3221241235733032\n",
      "Epoch 9964 | Train Loss: 1.0679399967193604 | Test Loss: 1.1102745532989502\n",
      "Epoch 9965 | Train Loss: 1.2052483558654785 | Test Loss: 1.3220995664596558\n",
      "Epoch 9966 | Train Loss: 1.0679309368133545 | Test Loss: 1.110382080078125\n",
      "Epoch 9967 | Train Loss: 1.2052440643310547 | Test Loss: 1.322169542312622\n",
      "Epoch 9968 | Train Loss: 1.0679357051849365 | Test Loss: 1.1102632284164429\n",
      "Epoch 9969 | Train Loss: 1.2052398920059204 | Test Loss: 1.3221136331558228\n",
      "Epoch 9970 | Train Loss: 1.0679209232330322 | Test Loss: 1.1104209423065186\n",
      "Epoch 9971 | Train Loss: 1.2052550315856934 | Test Loss: 1.3222240209579468\n",
      "Epoch 9972 | Train Loss: 1.0679200887680054 | Test Loss: 1.1102485656738281\n",
      "Epoch 9973 | Train Loss: 1.2052596807479858 | Test Loss: 1.3221312761306763\n",
      "Epoch 9974 | Train Loss: 1.0679045915603638 | Test Loss: 1.1104601621627808\n",
      "Epoch 9975 | Train Loss: 1.2052762508392334 | Test Loss: 1.3221795558929443\n",
      "Epoch 9976 | Train Loss: 1.0678964853286743 | Test Loss: 1.110257625579834\n",
      "Epoch 9977 | Train Loss: 1.2052937746047974 | Test Loss: 1.3221542835235596\n",
      "Epoch 9978 | Train Loss: 1.067882776260376 | Test Loss: 1.1104744672775269\n",
      "Epoch 9979 | Train Loss: 1.2052836418151855 | Test Loss: 1.3221858739852905\n",
      "Epoch 9980 | Train Loss: 1.0678706169128418 | Test Loss: 1.1102664470672607\n",
      "Epoch 9981 | Train Loss: 1.2052963972091675 | Test Loss: 1.3221757411956787\n",
      "Epoch 9982 | Train Loss: 1.0678592920303345 | Test Loss: 1.110464096069336\n",
      "Epoch 9983 | Train Loss: 1.205268383026123 | Test Loss: 1.322199821472168\n",
      "Epoch 9984 | Train Loss: 1.067849040031433 | Test Loss: 1.1102683544158936\n",
      "Epoch 9985 | Train Loss: 1.2052534818649292 | Test Loss: 1.3221793174743652\n",
      "Epoch 9986 | Train Loss: 1.0678411722183228 | Test Loss: 1.1104369163513184\n",
      "Epoch 9987 | Train Loss: 1.2052133083343506 | Test Loss: 1.3222019672393799\n",
      "Epoch 9988 | Train Loss: 1.0678331851959229 | Test Loss: 1.1102546453475952\n",
      "Epoch 9989 | Train Loss: 1.2051904201507568 | Test Loss: 1.322165608406067\n",
      "Epoch 9990 | Train Loss: 1.067827820777893 | Test Loss: 1.1104050874710083\n",
      "Epoch 9991 | Train Loss: 1.2051554918289185 | Test Loss: 1.3221920728683472\n",
      "Epoch 9992 | Train Loss: 1.0678167343139648 | Test Loss: 1.1102468967437744\n",
      "Epoch 9993 | Train Loss: 1.2051297426223755 | Test Loss: 1.3222119808197021\n",
      "Epoch 9994 | Train Loss: 1.0678160190582275 | Test Loss: 1.1103519201278687\n",
      "Epoch 9995 | Train Loss: 1.2050917148590088 | Test Loss: 1.3221851587295532\n",
      "Epoch 9996 | Train Loss: 1.0678044557571411 | Test Loss: 1.1102701425552368\n",
      "Epoch 9997 | Train Loss: 1.205074429512024 | Test Loss: 1.3222291469573975\n",
      "Epoch 9998 | Train Loss: 1.0678001642227173 | Test Loss: 1.1102880239486694\n",
      "Epoch 9999 | Train Loss: 1.2050552368164062 | Test Loss: 1.3221956491470337\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if minimax_is_first: train_data, train_labels, test_data, test_labels = minimax_first\n",
    "    else:\n",
    "        train_data, train_labels, test_data, test_labels = minimax_second\n",
    "\n",
    "    train_data.to(cfg.device)\n",
    "    train_labels.to(cfg.device)\n",
    "    test_data.to(cfg.device)\n",
    "    test_labels.to(cfg.device)\n",
    "\n",
    "    for batch in range(0, len(train_data), batch_size):\n",
    "        input_batch = train_data[batch : batch + batch_size]\n",
    "        label_batch = train_labels[batch : batch + batch_size]\n",
    "\n",
    "        logits_batch = model(input_batch)\n",
    "\n",
    "        logits_batch = remove_alternating_indices(logits_batch, odd = not minimax_is_first).to(cfg.device)\n",
    "        label_batch = remove_alternating_indices(label_batch, odd = not minimax_is_first).to(cfg.device)\n",
    "        train_loss = loss_fn(train.rearrange(logits_batch), train.rearrange(label_batch))\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            # test inference runs for every update on the whole test set\n",
    "            test_logits = model(test_data)\n",
    "            logits_batch = remove_alternating_indices(test_logits, odd = not minimax_is_first).to('cuda')\n",
    "            label_batch = remove_alternating_indices(test_labels, odd = not minimax_is_first).to('cuda')\n",
    "            test_loss = loss_fn(train.rearrange(logits_batch), train.rearrange(label_batch))\n",
    "        \n",
    "        minimax_is_first = not minimax_is_first\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch} | Train Loss: {train_loss.item()} | Test Loss: {test_loss.item()}\"\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(batch seq) token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_alternating_indices(t: torch.Tensor, dim: int=0, odd:bool = True) -> torch.Tensor:\n",
    "    indices = [index for index in range(t.shape[1]) if (index + odd) % 2 != 0 ]\n",
    "    indices = torch.tensor(indices).to(t.device)\n",
    "    print(\"indices device is: \", indices.device)\n",
    "    print(\"t device is: \", t.device)\n",
    "    return torch.index_select(t, 1, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "b = remove_alternating_indices(a, odd= True)\n",
    "print(b)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = game.Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 3, 7, 6, 1, 4, 2, 5, 9, 9, 9]\n",
      "Invalid game\n",
      "|   | O | O |\n",
      "| X | X | X |\n",
      "| X | O |   |\n"
     ]
    }
   ],
   "source": [
    "a = evals._sample_game(model, 1)\n",
    "print(a)\n",
    "game.play_game(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9, device='cuda:0')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(torch.tensor(seq))[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [10,3,5,4,0,2,8,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| O |   | X |\n",
      "| X | X | O |\n",
      "| X |   | O |\n"
     ]
    }
   ],
   "source": [
    "board.make_move(seq[-1])\n",
    "board.draw_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| X | O | O |\n",
      "| X | X | X |\n",
      "| X | O | O |\n"
     ]
    }
   ],
   "source": [
    "game.play_game(samples[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:11<00:00, 85.32it/s]\n"
     ]
    }
   ],
   "source": [
    "samples = evals.sample_games(model, 1, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:00, 22113.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'_check_played_repeat_moves': 0.043,\n",
       " '_check_played_after_player_victory': 0.08,\n",
       " '_check_played_after_draw_game': 0.0,\n",
       " 'inappropriate_end_state': 0.392,\n",
       " '_check_if_illegal_moves': 0.509}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evals.eval_model(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
