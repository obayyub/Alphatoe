{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookedRootModule,\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import numpy as np\n",
    "import torch as t\n",
    "import tqdm\n",
    "#functional\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from functools import partial\n",
    "import einops\n",
    "import circuitsvis as cv\n",
    "from alphatoe.game import Board, apply_best_moves, get_best_moves, generate_all_games, tree_walk\n",
    "from typing import Optional, Any\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor: Tensor, flat: bool=False):\n",
    "    if type(tensor)!=torch.Tensor:\n",
    "        return tensor\n",
    "    if flat:\n",
    "        return tensor.flatten().detach().cpu().numpy()\n",
    "    else:\n",
    "        return tensor.detach().cpu().numpy()\n",
    "\n",
    "def imshow(tensor: Tensor, xaxis: Optional[str]=None, yaxis: Optional[str]=None, animation_name: str='Snapshot', **kwargs: Any):\n",
    "    tensor = torch.squeeze(tensor)\n",
    "    px.imshow(to_numpy(tensor, flat=False),aspect='auto', \n",
    "              labels={'x':xaxis, 'y':yaxis, 'animation_name':animation_name}, \n",
    "              **kwargs).show()\n",
    "# Set default colour scheme\n",
    "imshow = partial(imshow, color_continuous_scale='Blues')\n",
    "# Creates good defaults for showing divergent colour scales (ie with both \n",
    "# positive and negative values, where 0 is white)\n",
    "imshow_div = partial(imshow, color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "# Presets a bunch of defaults to imshow to make it suitable for showing heatmaps \n",
    "# of activations with x axis being input 1 and y axis being input 2.\n",
    "inputs_heatmap = partial(imshow, xaxis='Input 1', yaxis='Input 2', color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
    "\n",
    "def line(x, y=None, hover=None, xaxis='', yaxis='', **kwargs):\n",
    "    if type(y)==torch.Tensor:\n",
    "        y = to_numpy(y, flat=True)\n",
    "    if type(x)==torch.Tensor:\n",
    "        x = to_numpy(x, flat=True)\n",
    "    fig = px.line(x, y=y, hover_name=hover, **kwargs)\n",
    "    fig.update_layout(xaxis_title=xaxis, yaxis_title=yaxis)\n",
    "    fig.show()\n",
    "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
    "    if type(lines_list)==torch.Tensor:\n",
    "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
    "    if x is None:\n",
    "        x=np.arange(len(lines_list[0]))\n",
    "    fig = go.Figure(layout={'title':title})\n",
    "    fig.update_xaxes(title=xaxis)\n",
    "    fig.update_yaxes(title=yaxis)\n",
    "    for c, line in enumerate(lines_list):\n",
    "        if type(line)==torch.Tensor:\n",
    "            line = to_numpy(line)\n",
    "        if labels is not None:\n",
    "            label = labels[c]\n",
    "        else:\n",
    "            label = c\n",
    "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
    "    if log_y:\n",
    "        fig.update_layout(yaxis_type=\"log\")\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Config Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers = 8,\n",
    "    n_heads = 8,\n",
    "    d_model = 128,\n",
    "    d_head = 16,\n",
    "    d_mlp = 512,\n",
    "    act_fn = \"relu\",\n",
    "    normalization_type=None,\n",
    "    d_vocab=11,\n",
    "    d_vocab_out=10,\n",
    "    n_ctx=10,\n",
    "    init_weights=True,\n",
    "    device=\"cuda\",\n",
    "    seed = 1337,\n",
    ")\n",
    "\n",
    "lr = 1e-5\n",
    "weight_decay = 1e-4\n",
    "test_train_split = 0.8\n",
    "epochs = 50\n",
    "batch_size = 8192\n",
    "\n",
    "\n",
    "#     normalization_type='LN',\n",
    "#     d_vocab=11,\n",
    "#     d_vocab_out=10,\n",
    "#     n_ctx=10,\n",
    "#     init_weights=True,\n",
    "#     device=\"cuda\",\n",
    "#     seed = 1337,\n",
    "# )\n",
    "\n",
    "# lr = 1e-5\n",
    "# weight_decay = 1e-4\n",
    "# test_train_split = 0.8\n",
    "# epochs = 40\n",
    "# batch_size = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boards = [Board()]\n",
    "game_list = apply_best_moves(boards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3584"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(game_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 4, 1, 2, 6, 3, 5, 7, 8],\n",
       " [0, 4, 1, 2, 6, 3, 5, 8, 7],\n",
       " [0, 4, 2, 1, 7, 3, 5, 8, 6],\n",
       " [0, 4, 2, 1, 7, 5, 3, 6, 8],\n",
       " [0, 4, 2, 1, 7, 6, 3, 5, 8],\n",
       " [0, 4, 2, 1, 7, 6, 3, 8, 5],\n",
       " [0, 4, 2, 1, 7, 6, 5, 8, 3],\n",
       " [0, 4, 2, 1, 7, 6, 8, 5, 3],\n",
       " [0, 4, 2, 1, 7, 8, 3, 6, 5],\n",
       " [0, 4, 2, 1, 7, 8, 5, 3, 6],\n",
       " [0, 4, 2, 1, 7, 8, 5, 6, 3],\n",
       " [0, 4, 2, 1, 7, 8, 6, 3, 5],\n",
       " [0, 4, 3, 6, 2, 1, 7, 5, 8],\n",
       " [0, 4, 3, 6, 2, 1, 7, 8, 5],\n",
       " [0, 4, 5, 1, 7, 6, 2, 8, 3],\n",
       " [0, 4, 5, 1, 7, 8, 2, 3, 6],\n",
       " [0, 4, 5, 1, 7, 8, 2, 6, 3],\n",
       " [0, 4, 5, 1, 7, 8, 3, 6, 2],\n",
       " [0, 4, 5, 1, 7, 8, 6, 3, 2],\n",
       " [0, 4, 5, 2, 6, 3, 1, 7, 8],\n",
       " [0, 4, 5, 2, 6, 3, 1, 8, 7],\n",
       " [0, 4, 5, 2, 6, 3, 7, 8, 1],\n",
       " [0, 4, 5, 2, 6, 3, 8, 7, 1],\n",
       " [0, 4, 5, 7, 1, 2, 6, 3, 8],\n",
       " [0, 4, 5, 8, 1, 2, 6, 3, 7],\n",
       " [0, 4, 5, 8, 2, 1, 7, 3, 6],\n",
       " [0, 4, 5, 8, 2, 1, 7, 6, 3],\n",
       " [0, 4, 5, 8, 6, 3, 1, 2, 7],\n",
       " [0, 4, 5, 8, 6, 3, 2, 1, 7],\n",
       " [0, 4, 5, 8, 6, 3, 7, 1, 2],\n",
       " [0, 4, 5, 8, 6, 3, 7, 2, 1],\n",
       " [0, 4, 5, 8, 7, 1, 2, 3, 6],\n",
       " [0, 4, 5, 8, 7, 1, 2, 6, 3],\n",
       " [0, 4, 5, 8, 7, 1, 3, 6, 2],\n",
       " [0, 4, 5, 8, 7, 1, 6, 3, 2],\n",
       " [0, 4, 5, 8, 7, 2, 6, 3, 1],\n",
       " [0, 4, 5, 8, 7, 3, 1, 2, 6],\n",
       " [0, 4, 5, 8, 7, 3, 2, 1, 6],\n",
       " [0, 4, 5, 8, 7, 3, 6, 1, 2],\n",
       " [0, 4, 5, 8, 7, 3, 6, 2, 1],\n",
       " [0, 4, 5, 8, 7, 6, 2, 1, 3],\n",
       " [0, 4, 6, 3, 5, 1, 7, 8, 2],\n",
       " [0, 4, 6, 3, 5, 2, 1, 7, 8],\n",
       " [0, 4, 6, 3, 5, 2, 1, 8, 7],\n",
       " [0, 4, 6, 3, 5, 2, 7, 8, 1],\n",
       " [0, 4, 6, 3, 5, 2, 8, 7, 1],\n",
       " [0, 4, 6, 3, 5, 7, 1, 2, 8],\n",
       " [0, 4, 6, 3, 5, 8, 1, 2, 7],\n",
       " [0, 4, 6, 3, 5, 8, 2, 1, 7],\n",
       " [0, 4, 6, 3, 5, 8, 7, 1, 2],\n",
       " [0, 4, 6, 3, 5, 8, 7, 2, 1],\n",
       " [0, 4, 7, 3, 5, 2, 6, 8, 1],\n",
       " [0, 4, 7, 3, 5, 8, 1, 2, 6],\n",
       " [0, 4, 7, 3, 5, 8, 2, 1, 6],\n",
       " [0, 4, 7, 3, 5, 8, 6, 1, 2],\n",
       " [0, 4, 7, 3, 5, 8, 6, 2, 1],\n",
       " [0, 4, 7, 5, 3, 6, 2, 1, 8],\n",
       " [0, 4, 7, 6, 2, 1, 3, 5, 8],\n",
       " [0, 4, 7, 6, 2, 1, 3, 8, 5],\n",
       " [0, 4, 7, 6, 2, 1, 5, 8, 3],\n",
       " [0, 4, 7, 6, 2, 1, 8, 5, 3],\n",
       " [0, 4, 7, 8, 2, 1, 3, 6, 5],\n",
       " [0, 4, 7, 8, 2, 1, 5, 3, 6],\n",
       " [0, 4, 7, 8, 2, 1, 5, 6, 3],\n",
       " [0, 4, 7, 8, 2, 1, 6, 3, 5],\n",
       " [0, 4, 7, 8, 3, 6, 2, 1, 5],\n",
       " [0, 4, 7, 8, 5, 1, 2, 3, 6],\n",
       " [0, 4, 7, 8, 5, 1, 2, 6, 3],\n",
       " [0, 4, 7, 8, 5, 1, 3, 6, 2],\n",
       " [0, 4, 7, 8, 5, 1, 6, 3, 2],\n",
       " [0, 4, 7, 8, 5, 2, 6, 3, 1],\n",
       " [0, 4, 7, 8, 5, 3, 1, 2, 6],\n",
       " [0, 4, 7, 8, 5, 3, 2, 1, 6],\n",
       " [0, 4, 7, 8, 5, 3, 6, 1, 2],\n",
       " [0, 4, 7, 8, 5, 3, 6, 2, 1],\n",
       " [0, 4, 7, 8, 5, 6, 2, 1, 3],\n",
       " [0, 4, 7, 8, 6, 3, 5, 1, 2],\n",
       " [0, 4, 7, 8, 6, 3, 5, 2, 1],\n",
       " [0, 4, 8, 1, 7, 6, 2, 5, 3],\n",
       " [0, 4, 8, 3, 5, 2, 6, 7, 1],\n",
       " [0, 4, 8, 5, 3, 6, 2, 1, 7],\n",
       " [0, 4, 8, 7, 1, 2, 6, 3, 5],\n",
       " [1, 0, 3, 4, 8, 2, 6, 7, 5],\n",
       " [1, 0, 3, 4, 8, 5, 2, 6, 7],\n",
       " [1, 0, 3, 4, 8, 5, 2, 7, 6],\n",
       " [1, 0, 3, 4, 8, 5, 6, 7, 2],\n",
       " [1, 0, 3, 4, 8, 5, 7, 6, 2],\n",
       " [1, 0, 3, 4, 8, 6, 2, 5, 7],\n",
       " [1, 0, 3, 4, 8, 7, 2, 5, 6],\n",
       " [1, 0, 3, 4, 8, 7, 5, 2, 6],\n",
       " [1, 0, 3, 4, 8, 7, 6, 2, 5],\n",
       " [1, 0, 3, 4, 8, 7, 6, 5, 2],\n",
       " [1, 0, 3, 5, 2, 4, 8, 6, 7],\n",
       " [1, 0, 3, 5, 2, 4, 8, 7, 6],\n",
       " [1, 0, 3, 5, 2, 6, 4, 7, 8],\n",
       " [1, 0, 3, 5, 2, 6, 7, 4, 8],\n",
       " [1, 0, 3, 5, 2, 6, 8, 4, 7],\n",
       " [1, 0, 3, 5, 2, 6, 8, 7, 4],\n",
       " [1, 0, 3, 5, 2, 7, 4, 6, 8],\n",
       " [1, 0, 3, 5, 2, 7, 6, 4, 8],\n",
       " [1, 0, 3, 5, 2, 7, 8, 4, 6],\n",
       " [1, 0, 3, 5, 2, 7, 8, 6, 4],\n",
       " [1, 0, 3, 5, 4, 7, 2, 6, 8],\n",
       " [1, 0, 3, 5, 4, 7, 6, 2, 8],\n",
       " [1, 0, 3, 5, 4, 7, 8, 2, 6],\n",
       " [1, 0, 3, 5, 4, 7, 8, 6, 2],\n",
       " [1, 0, 3, 5, 7, 4, 8, 6, 2],\n",
       " [1, 0, 3, 5, 8, 4, 2, 6, 7],\n",
       " [1, 0, 3, 5, 8, 4, 2, 7, 6],\n",
       " [1, 0, 3, 5, 8, 4, 6, 7, 2],\n",
       " [1, 0, 3, 5, 8, 4, 7, 6, 2],\n",
       " [1, 0, 3, 5, 8, 6, 2, 4, 7],\n",
       " [1, 0, 3, 5, 8, 6, 2, 7, 4],\n",
       " [1, 0, 3, 5, 8, 6, 4, 7, 2],\n",
       " [1, 0, 3, 5, 8, 6, 7, 4, 2],\n",
       " [1, 0, 3, 5, 8, 7, 2, 4, 6],\n",
       " [1, 0, 3, 5, 8, 7, 2, 6, 4],\n",
       " [1, 0, 3, 5, 8, 7, 4, 2, 6],\n",
       " [1, 0, 3, 5, 8, 7, 4, 6, 2],\n",
       " [1, 0, 3, 5, 8, 7, 6, 2, 4],\n",
       " [1, 0, 3, 5, 8, 7, 6, 4, 2],\n",
       " [1, 0, 3, 7, 4, 5, 2, 6, 8],\n",
       " [1, 0, 3, 7, 4, 5, 6, 2, 8],\n",
       " [1, 0, 3, 7, 4, 5, 8, 2, 6],\n",
       " [1, 0, 3, 7, 4, 5, 8, 6, 2],\n",
       " [1, 0, 3, 7, 5, 4, 8, 2, 6],\n",
       " [1, 0, 3, 7, 6, 2, 4, 5, 8],\n",
       " [1, 0, 3, 7, 6, 2, 5, 4, 8],\n",
       " [1, 0, 3, 7, 6, 2, 8, 4, 5],\n",
       " [1, 0, 3, 7, 6, 2, 8, 5, 4],\n",
       " [1, 0, 3, 7, 6, 4, 8, 2, 5],\n",
       " [1, 0, 3, 7, 6, 4, 8, 5, 2],\n",
       " [1, 0, 3, 7, 6, 5, 2, 4, 8],\n",
       " [1, 0, 3, 7, 6, 5, 4, 2, 8],\n",
       " [1, 0, 3, 7, 6, 5, 8, 2, 4],\n",
       " [1, 0, 3, 7, 6, 5, 8, 4, 2],\n",
       " [1, 0, 3, 7, 8, 2, 4, 5, 6],\n",
       " [1, 0, 3, 7, 8, 2, 5, 4, 6],\n",
       " [1, 0, 3, 7, 8, 2, 6, 4, 5],\n",
       " [1, 0, 3, 7, 8, 2, 6, 5, 4],\n",
       " [1, 0, 3, 7, 8, 4, 2, 5, 6],\n",
       " [1, 0, 3, 7, 8, 4, 5, 2, 6],\n",
       " [1, 0, 3, 7, 8, 4, 6, 2, 5],\n",
       " [1, 0, 3, 7, 8, 4, 6, 5, 2],\n",
       " [1, 0, 3, 7, 8, 5, 2, 4, 6],\n",
       " [1, 0, 3, 7, 8, 5, 2, 6, 4],\n",
       " [1, 0, 3, 7, 8, 5, 4, 2, 6],\n",
       " [1, 0, 3, 7, 8, 5, 4, 6, 2],\n",
       " [1, 0, 3, 7, 8, 5, 6, 2, 4],\n",
       " [1, 0, 3, 7, 8, 5, 6, 4, 2],\n",
       " [1, 0, 4, 7, 3, 5, 2, 6, 8],\n",
       " [1, 0, 4, 7, 3, 5, 6, 2, 8],\n",
       " [1, 0, 4, 7, 3, 5, 8, 2, 6],\n",
       " [1, 0, 4, 7, 3, 5, 8, 6, 2],\n",
       " [1, 0, 4, 7, 5, 3, 6, 2, 8],\n",
       " [1, 0, 4, 7, 6, 2, 3, 5, 8],\n",
       " [1, 0, 4, 7, 6, 2, 5, 3, 8],\n",
       " [1, 0, 4, 7, 6, 2, 8, 3, 5],\n",
       " [1, 0, 4, 7, 6, 2, 8, 5, 3],\n",
       " [1, 0, 4, 7, 8, 2, 3, 5, 6],\n",
       " [1, 0, 4, 7, 8, 2, 5, 3, 6],\n",
       " [1, 0, 4, 7, 8, 2, 6, 3, 5],\n",
       " [1, 0, 4, 7, 8, 2, 6, 5, 3],\n",
       " [1, 0, 4, 7, 8, 3, 6, 2, 5],\n",
       " [1, 0, 4, 7, 8, 5, 2, 6, 3],\n",
       " [1, 0, 4, 7, 8, 5, 3, 2, 6],\n",
       " [1, 0, 4, 7, 8, 5, 3, 6, 2],\n",
       " [1, 0, 4, 7, 8, 5, 6, 2, 3],\n",
       " [1, 0, 4, 7, 8, 6, 3, 5, 2],\n",
       " [1, 0, 6, 4, 8, 7, 2, 5, 3],\n",
       " [1, 0, 6, 4, 8, 7, 3, 2, 5],\n",
       " [1, 0, 6, 4, 8, 7, 3, 5, 2],\n",
       " [1, 0, 6, 4, 8, 7, 5, 2, 3],\n",
       " [1, 0, 6, 7, 2, 4, 8, 5, 3],\n",
       " [1, 0, 6, 7, 3, 2, 4, 5, 8],\n",
       " [1, 0, 6, 7, 3, 2, 5, 4, 8],\n",
       " [1, 0, 6, 7, 3, 2, 8, 4, 5],\n",
       " [1, 0, 6, 7, 3, 2, 8, 5, 4],\n",
       " [1, 0, 6, 7, 3, 4, 8, 2, 5],\n",
       " [1, 0, 6, 7, 3, 4, 8, 5, 2],\n",
       " [1, 0, 6, 7, 3, 5, 2, 4, 8],\n",
       " [1, 0, 6, 7, 3, 5, 4, 2, 8],\n",
       " [1, 0, 6, 7, 3, 5, 8, 2, 4],\n",
       " [1, 0, 6, 7, 3, 5, 8, 4, 2],\n",
       " [1, 0, 6, 7, 4, 2, 3, 5, 8],\n",
       " [1, 0, 6, 7, 4, 2, 5, 3, 8],\n",
       " [1, 0, 6, 7, 4, 2, 8, 3, 5],\n",
       " [1, 0, 6, 7, 4, 2, 8, 5, 3],\n",
       " [1, 0, 6, 7, 5, 2, 3, 4, 8],\n",
       " [1, 0, 6, 7, 5, 2, 4, 3, 8],\n",
       " [1, 0, 6, 7, 5, 2, 8, 3, 4],\n",
       " [1, 0, 6, 7, 5, 2, 8, 4, 3],\n",
       " [1, 0, 6, 7, 5, 4, 8, 2, 3],\n",
       " [1, 0, 6, 7, 8, 2, 3, 4, 5],\n",
       " [1, 0, 6, 7, 8, 2, 3, 5, 4],\n",
       " [1, 0, 6, 7, 8, 2, 4, 3, 5],\n",
       " [1, 0, 6, 7, 8, 2, 4, 5, 3],\n",
       " [1, 0, 6, 7, 8, 2, 5, 3, 4],\n",
       " [1, 0, 6, 7, 8, 2, 5, 4, 3],\n",
       " [1, 0, 6, 7, 8, 4, 2, 5, 3],\n",
       " [1, 0, 6, 7, 8, 4, 3, 2, 5],\n",
       " [1, 0, 6, 7, 8, 4, 3, 5, 2],\n",
       " [1, 0, 6, 7, 8, 4, 5, 2, 3],\n",
       " [1, 0, 6, 7, 8, 5, 2, 4, 3],\n",
       " [1, 0, 6, 7, 8, 5, 3, 2, 4],\n",
       " [1, 0, 6, 7, 8, 5, 3, 4, 2],\n",
       " [1, 0, 6, 7, 8, 5, 4, 2, 3],\n",
       " [1, 0, 8, 4, 2, 5, 3, 6, 7],\n",
       " [1, 0, 8, 4, 2, 5, 3, 7, 6],\n",
       " [1, 0, 8, 4, 3, 2, 6, 7, 5],\n",
       " [1, 0, 8, 4, 3, 5, 2, 6, 7],\n",
       " [1, 0, 8, 4, 3, 5, 2, 7, 6],\n",
       " [1, 0, 8, 4, 3, 5, 6, 7, 2],\n",
       " [1, 0, 8, 4, 3, 5, 7, 6, 2],\n",
       " [1, 0, 8, 4, 3, 6, 2, 5, 7],\n",
       " [1, 0, 8, 4, 3, 7, 2, 5, 6],\n",
       " [1, 0, 8, 4, 3, 7, 5, 2, 6],\n",
       " [1, 0, 8, 4, 3, 7, 6, 2, 5],\n",
       " [1, 0, 8, 4, 3, 7, 6, 5, 2],\n",
       " [1, 0, 8, 4, 5, 2, 6, 7, 3],\n",
       " [1, 0, 8, 4, 6, 7, 2, 5, 3],\n",
       " [1, 0, 8, 4, 6, 7, 3, 2, 5],\n",
       " [1, 0, 8, 4, 6, 7, 3, 5, 2],\n",
       " [1, 0, 8, 4, 6, 7, 5, 2, 3],\n",
       " [1, 0, 8, 6, 3, 4, 2, 5, 7],\n",
       " [1, 0, 8, 6, 3, 5, 2, 4, 7],\n",
       " [1, 0, 8, 6, 3, 5, 2, 7, 4],\n",
       " [1, 0, 8, 6, 3, 5, 4, 7, 2],\n",
       " [1, 0, 8, 6, 3, 5, 7, 4, 2],\n",
       " [1, 0, 8, 7, 2, 5, 3, 4, 6],\n",
       " [1, 0, 8, 7, 2, 5, 3, 6, 4],\n",
       " [1, 0, 8, 7, 2, 5, 4, 6, 3],\n",
       " [1, 0, 8, 7, 2, 5, 6, 4, 3],\n",
       " [1, 0, 8, 7, 3, 2, 4, 5, 6],\n",
       " [1, 0, 8, 7, 3, 2, 5, 4, 6],\n",
       " [1, 0, 8, 7, 3, 2, 6, 4, 5],\n",
       " [1, 0, 8, 7, 3, 2, 6, 5, 4],\n",
       " [1, 0, 8, 7, 3, 4, 2, 5, 6],\n",
       " [1, 0, 8, 7, 3, 4, 5, 2, 6],\n",
       " [1, 0, 8, 7, 3, 4, 6, 2, 5],\n",
       " [1, 0, 8, 7, 3, 4, 6, 5, 2],\n",
       " [1, 0, 8, 7, 3, 5, 2, 4, 6],\n",
       " [1, 0, 8, 7, 3, 5, 2, 6, 4],\n",
       " [1, 0, 8, 7, 3, 5, 4, 2, 6],\n",
       " [1, 0, 8, 7, 3, 5, 4, 6, 2],\n",
       " [1, 0, 8, 7, 3, 5, 6, 2, 4],\n",
       " [1, 0, 8, 7, 3, 5, 6, 4, 2],\n",
       " [1, 0, 8, 7, 4, 2, 3, 5, 6],\n",
       " [1, 0, 8, 7, 4, 2, 5, 3, 6],\n",
       " [1, 0, 8, 7, 4, 2, 6, 3, 5],\n",
       " [1, 0, 8, 7, 4, 2, 6, 5, 3],\n",
       " [1, 0, 8, 7, 4, 3, 6, 2, 5],\n",
       " [1, 0, 8, 7, 4, 5, 2, 6, 3],\n",
       " [1, 0, 8, 7, 4, 5, 3, 2, 6],\n",
       " [1, 0, 8, 7, 4, 5, 3, 6, 2],\n",
       " [1, 0, 8, 7, 4, 5, 6, 2, 3],\n",
       " [1, 0, 8, 7, 4, 6, 3, 5, 2],\n",
       " [1, 0, 8, 7, 5, 2, 3, 4, 6],\n",
       " [1, 0, 8, 7, 5, 2, 4, 3, 6],\n",
       " [1, 0, 8, 7, 5, 2, 6, 3, 4],\n",
       " [1, 0, 8, 7, 5, 2, 6, 4, 3],\n",
       " [1, 0, 8, 7, 6, 2, 3, 4, 5],\n",
       " [1, 0, 8, 7, 6, 2, 3, 5, 4],\n",
       " [1, 0, 8, 7, 6, 2, 4, 3, 5],\n",
       " [1, 0, 8, 7, 6, 2, 4, 5, 3],\n",
       " [1, 0, 8, 7, 6, 2, 5, 3, 4],\n",
       " [1, 0, 8, 7, 6, 2, 5, 4, 3],\n",
       " [1, 0, 8, 7, 6, 4, 2, 5, 3],\n",
       " [1, 0, 8, 7, 6, 4, 3, 2, 5],\n",
       " [1, 0, 8, 7, 6, 4, 3, 5, 2],\n",
       " [1, 0, 8, 7, 6, 4, 5, 2, 3],\n",
       " [1, 0, 8, 7, 6, 5, 2, 4, 3],\n",
       " [1, 0, 8, 7, 6, 5, 3, 2, 4],\n",
       " [1, 0, 8, 7, 6, 5, 3, 4, 2],\n",
       " [1, 0, 8, 7, 6, 5, 4, 2, 3],\n",
       " [1, 2, 4, 7, 3, 5, 8, 0, 6],\n",
       " [1, 2, 4, 7, 5, 3, 0, 8, 6],\n",
       " [1, 2, 4, 7, 5, 3, 6, 0, 8],\n",
       " [1, 2, 4, 7, 5, 3, 6, 8, 0],\n",
       " [1, 2, 4, 7, 5, 3, 8, 0, 6],\n",
       " [1, 2, 4, 7, 6, 0, 3, 5, 8],\n",
       " [1, 2, 4, 7, 6, 0, 5, 3, 8],\n",
       " [1, 2, 4, 7, 6, 0, 8, 3, 5],\n",
       " [1, 2, 4, 7, 6, 0, 8, 5, 3],\n",
       " [1, 2, 4, 7, 6, 3, 0, 8, 5],\n",
       " [1, 2, 4, 7, 6, 3, 5, 0, 8],\n",
       " [1, 2, 4, 7, 6, 3, 5, 8, 0],\n",
       " [1, 2, 4, 7, 6, 3, 8, 0, 5],\n",
       " [1, 2, 4, 7, 6, 5, 8, 0, 3],\n",
       " [1, 2, 4, 7, 6, 8, 5, 3, 0],\n",
       " [1, 2, 4, 7, 8, 0, 3, 5, 6],\n",
       " [1, 2, 4, 7, 8, 0, 5, 3, 6],\n",
       " [1, 2, 4, 7, 8, 0, 6, 3, 5],\n",
       " [1, 2, 4, 7, 8, 0, 6, 5, 3],\n",
       " [1, 2, 5, 3, 0, 4, 6, 7, 8],\n",
       " [1, 2, 5, 3, 0, 4, 6, 8, 7],\n",
       " [1, 2, 5, 3, 0, 7, 4, 8, 6],\n",
       " [1, 2, 5, 3, 0, 7, 6, 4, 8],\n",
       " [1, 2, 5, 3, 0, 7, 6, 8, 4],\n",
       " [1, 2, 5, 3, 0, 7, 8, 4, 6],\n",
       " [1, 2, 5, 3, 0, 8, 4, 7, 6],\n",
       " [1, 2, 5, 3, 0, 8, 6, 4, 7],\n",
       " [1, 2, 5, 3, 0, 8, 6, 7, 4],\n",
       " [1, 2, 5, 3, 0, 8, 7, 4, 6],\n",
       " [1, 2, 5, 3, 4, 7, 0, 8, 6],\n",
       " [1, 2, 5, 3, 4, 7, 6, 0, 8],\n",
       " [1, 2, 5, 3, 4, 7, 6, 8, 0],\n",
       " [1, 2, 5, 3, 4, 7, 8, 0, 6],\n",
       " [1, 2, 5, 3, 6, 4, 0, 7, 8],\n",
       " [1, 2, 5, 3, 6, 4, 0, 8, 7],\n",
       " [1, 2, 5, 3, 6, 4, 7, 8, 0],\n",
       " [1, 2, 5, 3, 6, 4, 8, 7, 0],\n",
       " [1, 2, 5, 3, 6, 7, 0, 4, 8],\n",
       " [1, 2, 5, 3, 6, 7, 0, 8, 4],\n",
       " [1, 2, 5, 3, 6, 7, 4, 0, 8],\n",
       " [1, 2, 5, 3, 6, 7, 4, 8, 0],\n",
       " [1, 2, 5, 3, 6, 7, 8, 0, 4],\n",
       " [1, 2, 5, 3, 6, 7, 8, 4, 0],\n",
       " [1, 2, 5, 3, 6, 8, 0, 4, 7],\n",
       " [1, 2, 5, 3, 6, 8, 0, 7, 4],\n",
       " [1, 2, 5, 3, 6, 8, 4, 7, 0],\n",
       " [1, 2, 5, 3, 6, 8, 7, 4, 0],\n",
       " [1, 2, 5, 3, 7, 4, 6, 8, 0],\n",
       " [1, 2, 5, 4, 6, 0, 8, 7, 3],\n",
       " [1, 2, 5, 4, 6, 3, 0, 7, 8],\n",
       " [1, 2, 5, 4, 6, 3, 0, 8, 7],\n",
       " [1, 2, 5, 4, 6, 3, 7, 8, 0],\n",
       " [1, 2, 5, 4, 6, 3, 8, 7, 0],\n",
       " [1, 2, 5, 4, 6, 7, 0, 3, 8],\n",
       " [1, 2, 5, 4, 6, 7, 3, 0, 8],\n",
       " [1, 2, 5, 4, 6, 7, 8, 0, 3],\n",
       " [1, 2, 5, 4, 6, 7, 8, 3, 0],\n",
       " [1, 2, 5, 4, 6, 8, 0, 3, 7],\n",
       " [1, 2, 5, 7, 3, 4, 6, 0, 8],\n",
       " [1, 2, 5, 7, 4, 3, 0, 8, 6],\n",
       " [1, 2, 5, 7, 4, 3, 6, 0, 8],\n",
       " [1, 2, 5, 7, 4, 3, 6, 8, 0],\n",
       " [1, 2, 5, 7, 4, 3, 8, 0, 6],\n",
       " [1, 2, 5, 7, 6, 0, 3, 4, 8],\n",
       " [1, 2, 5, 7, 6, 0, 4, 3, 8],\n",
       " [1, 2, 5, 7, 6, 0, 8, 3, 4],\n",
       " [1, 2, 5, 7, 6, 0, 8, 4, 3],\n",
       " [1, 2, 5, 7, 6, 3, 0, 4, 8],\n",
       " [1, 2, 5, 7, 6, 3, 0, 8, 4],\n",
       " [1, 2, 5, 7, 6, 3, 4, 0, 8],\n",
       " [1, 2, 5, 7, 6, 3, 4, 8, 0],\n",
       " [1, 2, 5, 7, 6, 3, 8, 0, 4],\n",
       " [1, 2, 5, 7, 6, 3, 8, 4, 0],\n",
       " [1, 2, 5, 7, 6, 4, 0, 3, 8],\n",
       " [1, 2, 5, 7, 6, 4, 3, 0, 8],\n",
       " [1, 2, 5, 7, 6, 4, 8, 0, 3],\n",
       " [1, 2, 5, 7, 6, 4, 8, 3, 0],\n",
       " [1, 2, 5, 7, 8, 0, 3, 4, 6],\n",
       " [1, 2, 5, 7, 8, 0, 4, 3, 6],\n",
       " [1, 2, 5, 7, 8, 0, 6, 3, 4],\n",
       " [1, 2, 5, 7, 8, 0, 6, 4, 3],\n",
       " [1, 2, 5, 7, 8, 3, 0, 4, 6],\n",
       " [1, 2, 5, 7, 8, 3, 4, 0, 6],\n",
       " [1, 2, 5, 7, 8, 3, 6, 0, 4],\n",
       " [1, 2, 5, 7, 8, 3, 6, 4, 0],\n",
       " [1, 2, 5, 7, 8, 4, 6, 0, 3],\n",
       " [1, 2, 5, 7, 8, 4, 6, 3, 0],\n",
       " [1, 2, 6, 4, 0, 3, 5, 7, 8],\n",
       " [1, 2, 6, 4, 0, 3, 5, 8, 7],\n",
       " [1, 2, 6, 4, 3, 0, 8, 7, 5],\n",
       " [1, 2, 6, 4, 5, 0, 8, 7, 3],\n",
       " [1, 2, 6, 4, 5, 3, 0, 7, 8],\n",
       " [1, 2, 6, 4, 5, 3, 0, 8, 7],\n",
       " [1, 2, 6, 4, 5, 3, 7, 8, 0],\n",
       " [1, 2, 6, 4, 5, 3, 8, 7, 0],\n",
       " [1, 2, 6, 4, 5, 7, 0, 3, 8],\n",
       " [1, 2, 6, 4, 5, 7, 3, 0, 8],\n",
       " [1, 2, 6, 4, 5, 7, 8, 0, 3],\n",
       " [1, 2, 6, 4, 5, 7, 8, 3, 0],\n",
       " [1, 2, 6, 4, 5, 8, 0, 3, 7],\n",
       " [1, 2, 6, 4, 8, 7, 0, 3, 5],\n",
       " [1, 2, 6, 4, 8, 7, 3, 0, 5],\n",
       " [1, 2, 6, 4, 8, 7, 5, 0, 3],\n",
       " [1, 2, 6, 4, 8, 7, 5, 3, 0],\n",
       " [1, 2, 6, 7, 0, 3, 4, 8, 5],\n",
       " [1, 2, 6, 7, 0, 3, 5, 4, 8],\n",
       " [1, 2, 6, 7, 0, 3, 5, 8, 4],\n",
       " [1, 2, 6, 7, 0, 3, 8, 4, 5],\n",
       " [1, 2, 6, 7, 3, 0, 4, 5, 8],\n",
       " [1, 2, 6, 7, 3, 0, 5, 4, 8],\n",
       " [1, 2, 6, 7, 3, 0, 8, 4, 5],\n",
       " [1, 2, 6, 7, 3, 0, 8, 5, 4],\n",
       " [1, 2, 6, 7, 4, 0, 3, 5, 8],\n",
       " [1, 2, 6, 7, 4, 0, 5, 3, 8],\n",
       " [1, 2, 6, 7, 4, 0, 8, 3, 5],\n",
       " [1, 2, 6, 7, 4, 0, 8, 5, 3],\n",
       " [1, 2, 6, 7, 4, 3, 0, 8, 5],\n",
       " [1, 2, 6, 7, 4, 3, 5, 0, 8],\n",
       " [1, 2, 6, 7, 4, 3, 5, 8, 0],\n",
       " [1, 2, 6, 7, 4, 3, 8, 0, 5],\n",
       " [1, 2, 6, 7, 4, 5, 8, 0, 3],\n",
       " [1, 2, 6, 7, 4, 8, 5, 3, 0],\n",
       " [1, 2, 6, 7, 5, 0, 3, 4, 8],\n",
       " [1, 2, 6, 7, 5, 0, 4, 3, 8],\n",
       " [1, 2, 6, 7, 5, 0, 8, 3, 4],\n",
       " [1, 2, 6, 7, 5, 0, 8, 4, 3],\n",
       " [1, 2, 6, 7, 5, 3, 0, 4, 8],\n",
       " [1, 2, 6, 7, 5, 3, 0, 8, 4],\n",
       " [1, 2, 6, 7, 5, 3, 4, 0, 8],\n",
       " [1, 2, 6, 7, 5, 3, 4, 8, 0],\n",
       " [1, 2, 6, 7, 5, 3, 8, 0, 4],\n",
       " [1, 2, 6, 7, 5, 3, 8, 4, 0],\n",
       " [1, 2, 6, 7, 5, 4, 0, 3, 8],\n",
       " [1, 2, 6, 7, 5, 4, 3, 0, 8],\n",
       " [1, 2, 6, 7, 5, 4, 8, 0, 3],\n",
       " [1, 2, 6, 7, 5, 4, 8, 3, 0],\n",
       " [1, 2, 6, 7, 8, 0, 3, 4, 5],\n",
       " [1, 2, 6, 7, 8, 0, 3, 5, 4],\n",
       " [1, 2, 6, 7, 8, 0, 4, 3, 5],\n",
       " [1, 2, 6, 7, 8, 0, 4, 5, 3],\n",
       " [1, 2, 6, 7, 8, 0, 5, 3, 4],\n",
       " [1, 2, 6, 7, 8, 0, 5, 4, 3],\n",
       " [1, 2, 6, 7, 8, 3, 0, 4, 5],\n",
       " [1, 2, 6, 7, 8, 3, 4, 0, 5],\n",
       " [1, 2, 6, 7, 8, 3, 5, 0, 4],\n",
       " [1, 2, 6, 7, 8, 3, 5, 4, 0],\n",
       " [1, 2, 6, 7, 8, 4, 0, 3, 5],\n",
       " [1, 2, 6, 7, 8, 4, 3, 0, 5],\n",
       " [1, 2, 6, 7, 8, 4, 5, 0, 3],\n",
       " [1, 2, 6, 7, 8, 4, 5, 3, 0],\n",
       " [1, 2, 6, 8, 5, 3, 0, 4, 7],\n",
       " [1, 2, 6, 8, 5, 3, 0, 7, 4],\n",
       " [1, 2, 6, 8, 5, 3, 4, 7, 0],\n",
       " [1, 2, 6, 8, 5, 3, 7, 4, 0],\n",
       " [1, 2, 6, 8, 5, 4, 0, 3, 7],\n",
       " [1, 2, 8, 4, 6, 7, 0, 3, 5],\n",
       " [1, 2, 8, 4, 6, 7, 3, 0, 5],\n",
       " [1, 2, 8, 4, 6, 7, 5, 0, 3],\n",
       " [1, 2, 8, 4, 6, 7, 5, 3, 0],\n",
       " [1, 2, 8, 7, 0, 4, 6, 3, 5],\n",
       " [1, 2, 8, 7, 3, 0, 4, 5, 6],\n",
       " [1, 2, 8, 7, 3, 0, 5, 4, 6],\n",
       " [1, 2, 8, 7, 3, 0, 6, 4, 5],\n",
       " [1, 2, 8, 7, 3, 0, 6, 5, 4],\n",
       " [1, 2, 8, 7, 3, 4, 6, 0, 5],\n",
       " [1, 2, 8, 7, 4, 0, 3, 5, 6],\n",
       " [1, 2, 8, 7, 4, 0, 5, 3, 6],\n",
       " [1, 2, 8, 7, 4, 0, 6, 3, 5],\n",
       " [1, 2, 8, 7, 4, 0, 6, 5, 3],\n",
       " [1, 2, 8, 7, 5, 0, 3, 4, 6],\n",
       " [1, 2, 8, 7, 5, 0, 4, 3, 6],\n",
       " [1, 2, 8, 7, 5, 0, 6, 3, 4],\n",
       " [1, 2, 8, 7, 5, 0, 6, 4, 3],\n",
       " [1, 2, 8, 7, 5, 3, 0, 4, 6],\n",
       " [1, 2, 8, 7, 5, 3, 4, 0, 6],\n",
       " [1, 2, 8, 7, 5, 3, 6, 0, 4],\n",
       " [1, 2, 8, 7, 5, 3, 6, 4, 0],\n",
       " [1, 2, 8, 7, 5, 4, 6, 0, 3],\n",
       " [1, 2, 8, 7, 5, 4, 6, 3, 0],\n",
       " [1, 2, 8, 7, 6, 0, 3, 4, 5],\n",
       " [1, 2, 8, 7, 6, 0, 3, 5, 4],\n",
       " [1, 2, 8, 7, 6, 0, 4, 3, 5],\n",
       " [1, 2, 8, 7, 6, 0, 4, 5, 3],\n",
       " [1, 2, 8, 7, 6, 0, 5, 3, 4],\n",
       " [1, 2, 8, 7, 6, 0, 5, 4, 3],\n",
       " [1, 2, 8, 7, 6, 3, 0, 4, 5],\n",
       " [1, 2, 8, 7, 6, 3, 4, 0, 5],\n",
       " [1, 2, 8, 7, 6, 3, 5, 0, 4],\n",
       " [1, 2, 8, 7, 6, 3, 5, 4, 0],\n",
       " [1, 2, 8, 7, 6, 4, 0, 3, 5],\n",
       " [1, 2, 8, 7, 6, 4, 3, 0, 5],\n",
       " [1, 2, 8, 7, 6, 4, 5, 0, 3],\n",
       " [1, 2, 8, 7, 6, 4, 5, 3, 0],\n",
       " [1, 4, 0, 2, 6, 3, 5, 7, 8],\n",
       " [1, 4, 0, 2, 6, 3, 5, 8, 7],\n",
       " [1, 4, 2, 0, 8, 5, 3, 6, 7],\n",
       " [1, 4, 2, 0, 8, 5, 3, 7, 6],\n",
       " [1, 4, 3, 0, 8, 2, 6, 7, 5],\n",
       " [1, 4, 3, 0, 8, 5, 2, 6, 7],\n",
       " [1, 4, 3, 0, 8, 5, 2, 7, 6],\n",
       " [1, 4, 3, 0, 8, 5, 6, 7, 2],\n",
       " [1, 4, 3, 0, 8, 5, 7, 6, 2],\n",
       " [1, 4, 3, 0, 8, 6, 2, 5, 7],\n",
       " [1, 4, 3, 0, 8, 7, 2, 5, 6],\n",
       " [1, 4, 3, 0, 8, 7, 5, 2, 6],\n",
       " [1, 4, 3, 0, 8, 7, 6, 2, 5],\n",
       " [1, 4, 3, 0, 8, 7, 6, 5, 2],\n",
       " [1, 4, 3, 2, 6, 0, 8, 7, 5],\n",
       " [1, 4, 3, 6, 2, 0, 8, 5, 7],\n",
       " [1, 4, 5, 0, 8, 2, 6, 7, 3],\n",
       " [1, 4, 5, 2, 6, 0, 8, 7, 3],\n",
       " [1, 4, 5, 2, 6, 3, 0, 7, 8],\n",
       " [1, 4, 5, 2, 6, 3, 0, 8, 7],\n",
       " [1, 4, 5, 2, 6, 3, 7, 8, 0],\n",
       " [1, 4, 5, 2, 6, 3, 8, 7, 0],\n",
       " [1, 4, 5, 2, 6, 7, 0, 3, 8],\n",
       " [1, 4, 5, 2, 6, 7, 3, 0, 8],\n",
       " [1, 4, 5, 2, 6, 7, 8, 0, 3],\n",
       " [1, 4, 5, 2, 6, 7, 8, 3, 0],\n",
       " [1, 4, 5, 2, 6, 8, 0, 3, 7],\n",
       " [1, 4, 5, 8, 0, 2, 6, 3, 7],\n",
       " [1, 4, 6, 0, 8, 7, 2, 5, 3],\n",
       " [1, 4, 6, 0, 8, 7, 3, 2, 5],\n",
       " [1, 4, 6, 0, 8, 7, 3, 5, 2],\n",
       " [1, 4, 6, 0, 8, 7, 5, 2, 3],\n",
       " [1, 4, 6, 2, 0, 3, 5, 7, 8],\n",
       " [1, 4, 6, 2, 0, 3, 5, 8, 7],\n",
       " [1, 4, 6, 2, 3, 0, 8, 7, 5],\n",
       " [1, 4, 6, 2, 5, 0, 8, 7, 3],\n",
       " [1, 4, 6, 2, 5, 3, 0, 7, 8],\n",
       " [1, 4, 6, 2, 5, 3, 0, 8, 7],\n",
       " [1, 4, 6, 2, 5, 3, 7, 8, 0],\n",
       " [1, 4, 6, 2, 5, 3, 8, 7, 0],\n",
       " [1, 4, 6, 2, 5, 7, 0, 3, 8],\n",
       " [1, 4, 6, 2, 5, 7, 3, 0, 8],\n",
       " [1, 4, 6, 2, 5, 7, 8, 0, 3],\n",
       " [1, 4, 6, 2, 5, 7, 8, 3, 0],\n",
       " [1, 4, 6, 2, 5, 8, 0, 3, 7],\n",
       " [1, 4, 6, 2, 8, 7, 0, 3, 5],\n",
       " [1, 4, 6, 2, 8, 7, 3, 0, 5],\n",
       " [1, 4, 6, 2, 8, 7, 5, 0, 3],\n",
       " [1, 4, 6, 2, 8, 7, 5, 3, 0],\n",
       " [1, 4, 6, 3, 5, 2, 0, 7, 8],\n",
       " [1, 4, 6, 3, 5, 2, 0, 8, 7],\n",
       " [1, 4, 6, 3, 5, 2, 7, 8, 0],\n",
       " [1, 4, 6, 3, 5, 2, 8, 7, 0],\n",
       " [1, 4, 6, 3, 5, 8, 0, 2, 7],\n",
       " [1, 4, 6, 5, 3, 0, 8, 7, 2],\n",
       " [1, 4, 8, 0, 2, 5, 3, 6, 7],\n",
       " [1, 4, 8, 0, 2, 5, 3, 7, 6],\n",
       " [1, 4, 8, 0, 3, 2, 6, 7, 5],\n",
       " [1, 4, 8, 0, 3, 5, 2, 6, 7],\n",
       " [1, 4, 8, 0, 3, 5, 2, 7, 6],\n",
       " [1, 4, 8, 0, 3, 5, 6, 7, 2],\n",
       " [1, 4, 8, 0, 3, 5, 7, 6, 2],\n",
       " [1, 4, 8, 0, 3, 6, 2, 5, 7],\n",
       " [1, 4, 8, 0, 3, 7, 2, 5, 6],\n",
       " [1, 4, 8, 0, 3, 7, 5, 2, 6],\n",
       " [1, 4, 8, 0, 3, 7, 6, 2, 5],\n",
       " [1, 4, 8, 0, 3, 7, 6, 5, 2],\n",
       " [1, 4, 8, 0, 5, 2, 6, 7, 3],\n",
       " [1, 4, 8, 0, 6, 7, 2, 5, 3],\n",
       " [1, 4, 8, 0, 6, 7, 3, 2, 5],\n",
       " [1, 4, 8, 0, 6, 7, 3, 5, 2],\n",
       " [1, 4, 8, 0, 6, 7, 5, 2, 3],\n",
       " [1, 4, 8, 2, 6, 7, 0, 3, 5],\n",
       " [1, 4, 8, 2, 6, 7, 3, 0, 5],\n",
       " [1, 4, 8, 2, 6, 7, 5, 0, 3],\n",
       " [1, 4, 8, 2, 6, 7, 5, 3, 0],\n",
       " [1, 4, 8, 3, 5, 2, 6, 7, 0],\n",
       " [1, 4, 8, 5, 3, 0, 2, 6, 7],\n",
       " [1, 4, 8, 5, 3, 0, 2, 7, 6],\n",
       " [1, 4, 8, 5, 3, 0, 6, 7, 2],\n",
       " [1, 4, 8, 5, 3, 0, 7, 6, 2],\n",
       " [1, 4, 8, 5, 3, 6, 2, 0, 7],\n",
       " [1, 7, 0, 2, 6, 3, 4, 8, 5],\n",
       " [1, 7, 0, 2, 6, 3, 5, 4, 8],\n",
       " [1, 7, 0, 2, 6, 3, 5, 8, 4],\n",
       " [1, 7, 0, 2, 6, 3, 8, 4, 5],\n",
       " [1, 7, 0, 2, 8, 4, 6, 3, 5],\n",
       " [1, 7, 2, 0, 6, 4, 8, 5, 3],\n",
       " [1, 7, 2, 0, 8, 5, 3, 4, 6],\n",
       " [1, 7, 2, 0, 8, 5, 3, 6, 4],\n",
       " [1, 7, 2, 0, 8, 5, 4, 6, 3],\n",
       " [1, 7, 2, 0, 8, 5, 6, 4, 3],\n",
       " [1, 7, 3, 0, 4, 5, 2, 6, 8],\n",
       " [1, 7, 3, 0, 4, 5, 6, 2, 8],\n",
       " [1, 7, 3, 0, 4, 5, 8, 2, 6],\n",
       " [1, 7, 3, 0, 4, 5, 8, 6, 2],\n",
       " [1, 7, 3, 0, 5, 4, 8, 2, 6],\n",
       " [1, 7, 3, 0, 6, 2, 4, 5, 8],\n",
       " [1, 7, 3, 0, 6, 2, 5, 4, 8],\n",
       " [1, 7, 3, 0, 6, 2, 8, 4, 5],\n",
       " [1, 7, 3, 0, 6, 2, 8, 5, 4],\n",
       " [1, 7, 3, 0, 6, 4, 8, 2, 5],\n",
       " [1, 7, 3, 0, 6, 4, 8, 5, 2],\n",
       " [1, 7, 3, 0, 6, 5, 2, 4, 8],\n",
       " [1, 7, 3, 0, 6, 5, 4, 2, 8],\n",
       " [1, 7, 3, 0, 6, 5, 8, 2, 4],\n",
       " [1, 7, 3, 0, 6, 5, 8, 4, 2],\n",
       " [1, 7, 3, 0, 8, 2, 4, 5, 6],\n",
       " [1, 7, 3, 0, 8, 2, 5, 4, 6],\n",
       " [1, 7, 3, 0, 8, 2, 6, 4, 5],\n",
       " [1, 7, 3, 0, 8, 2, 6, 5, 4],\n",
       " [1, 7, 3, 0, 8, 4, 2, 5, 6],\n",
       " [1, 7, 3, 0, 8, 4, 5, 2, 6],\n",
       " [1, 7, 3, 0, 8, 4, 6, 2, 5],\n",
       " [1, 7, 3, 0, 8, 4, 6, 5, 2],\n",
       " [1, 7, 3, 0, 8, 5, 2, 4, 6],\n",
       " [1, 7, 3, 0, 8, 5, 2, 6, 4],\n",
       " [1, 7, 3, 0, 8, 5, 4, 2, 6],\n",
       " [1, 7, 3, 0, 8, 5, 4, 6, 2],\n",
       " [1, 7, 3, 0, 8, 5, 6, 2, 4],\n",
       " [1, 7, 3, 0, 8, 5, 6, 4, 2],\n",
       " [1, 7, 3, 2, 4, 5, 8, 0, 6],\n",
       " [1, 7, 3, 2, 5, 4, 6, 0, 8],\n",
       " [1, 7, 3, 2, 6, 0, 4, 5, 8],\n",
       " [1, 7, 3, 2, 6, 0, 5, 4, 8],\n",
       " [1, 7, 3, 2, 6, 0, 8, 4, 5],\n",
       " [1, 7, 3, 2, 6, 0, 8, 5, 4],\n",
       " [1, 7, 3, 2, 8, 0, 4, 5, 6],\n",
       " [1, 7, 3, 2, 8, 0, 5, 4, 6],\n",
       " [1, 7, 3, 2, 8, 0, 6, 4, 5],\n",
       " [1, 7, 3, 2, 8, 0, 6, 5, 4],\n",
       " [1, 7, 3, 2, 8, 4, 6, 0, 5],\n",
       " [1, 7, 4, 0, 3, 5, 2, 6, 8],\n",
       " [1, 7, 4, 0, 3, 5, 6, 2, 8],\n",
       " [1, 7, 4, 0, 3, 5, 8, 2, 6],\n",
       " [1, 7, 4, 0, 3, 5, 8, 6, 2],\n",
       " [1, 7, 4, 0, 5, 3, 6, 2, 8],\n",
       " [1, 7, 4, 0, 6, 2, 3, 5, 8],\n",
       " [1, 7, 4, 0, 6, 2, 5, 3, 8],\n",
       " [1, 7, 4, 0, 6, 2, 8, 3, 5],\n",
       " [1, 7, 4, 0, 6, 2, 8, 5, 3],\n",
       " [1, 7, 4, 0, 8, 2, 3, 5, 6],\n",
       " [1, 7, 4, 0, 8, 2, 5, 3, 6],\n",
       " [1, 7, 4, 0, 8, 2, 6, 3, 5],\n",
       " [1, 7, 4, 0, 8, 2, 6, 5, 3],\n",
       " [1, 7, 4, 0, 8, 3, 6, 2, 5],\n",
       " [1, 7, 4, 0, 8, 5, 2, 6, 3],\n",
       " [1, 7, 4, 0, 8, 5, 3, 2, 6],\n",
       " [1, 7, 4, 0, 8, 5, 3, 6, 2],\n",
       " [1, 7, 4, 0, 8, 5, 6, 2, 3],\n",
       " [1, 7, 4, 0, 8, 6, 3, 5, 2],\n",
       " [1, 7, 4, 2, 3, 5, 8, 0, 6],\n",
       " [1, 7, 4, 2, 5, 3, 0, 8, 6],\n",
       " [1, 7, 4, 2, 5, 3, 6, 0, 8],\n",
       " [1, 7, 4, 2, 5, 3, 6, 8, 0],\n",
       " [1, 7, 4, 2, 5, 3, 8, 0, 6],\n",
       " [1, 7, 4, 2, 6, 0, 3, 5, 8],\n",
       " [1, 7, 4, 2, 6, 0, 5, 3, 8],\n",
       " [1, 7, 4, 2, 6, 0, 8, 3, 5],\n",
       " [1, 7, 4, 2, 6, 0, 8, 5, 3],\n",
       " [1, 7, 4, 2, 6, 3, 0, 8, 5],\n",
       " [1, 7, 4, 2, 6, 3, 5, 0, 8],\n",
       " [1, 7, 4, 2, 6, 3, 5, 8, 0],\n",
       " [1, 7, 4, 2, 6, 3, 8, 0, 5],\n",
       " [1, 7, 4, 2, 6, 5, 8, 0, 3],\n",
       " [1, 7, 4, 2, 6, 8, 5, 3, 0],\n",
       " [1, 7, 4, 2, 8, 0, 3, 5, 6],\n",
       " [1, 7, 4, 2, 8, 0, 5, 3, 6],\n",
       " [1, 7, 4, 2, 8, 0, 6, 3, 5],\n",
       " [1, 7, 4, 2, 8, 0, 6, 5, 3],\n",
       " [1, 7, 4, 6, 8, 0, 3, 5, 2],\n",
       " [1, 7, 4, 8, 6, 2, 5, 3, 0],\n",
       " [1, 7, 5, 0, 3, 4, 8, 2, 6],\n",
       " [1, 7, 5, 0, 4, 3, 6, 2, 8],\n",
       " [1, 7, 5, 0, 6, 2, 3, 4, 8],\n",
       " [1, 7, 5, 0, 6, 2, 4, 3, 8],\n",
       " [1, 7, 5, 0, 6, 2, 8, 3, 4],\n",
       " [1, 7, 5, 0, 6, 2, 8, 4, 3],\n",
       " [1, 7, 5, 0, 6, 4, 8, 2, 3],\n",
       " [1, 7, 5, 0, 8, 2, 3, 4, 6],\n",
       " [1, 7, 5, 0, 8, 2, 4, 3, 6],\n",
       " [1, 7, 5, 0, 8, 2, 6, 3, 4],\n",
       " [1, 7, 5, 0, 8, 2, 6, 4, 3],\n",
       " [1, 7, 5, 2, 3, 4, 6, 0, 8],\n",
       " [1, 7, 5, 2, 4, 3, 0, 8, 6],\n",
       " [1, 7, 5, 2, 4, 3, 6, 0, 8],\n",
       " [1, 7, 5, 2, 4, 3, 6, 8, 0],\n",
       " [1, 7, 5, 2, 4, 3, 8, 0, 6],\n",
       " [1, 7, 5, 2, 6, 0, 3, 4, 8],\n",
       " [1, 7, 5, 2, 6, 0, 4, 3, 8],\n",
       " [1, 7, 5, 2, 6, 0, 8, 3, 4],\n",
       " [1, 7, 5, 2, 6, 0, 8, 4, 3],\n",
       " [1, 7, 5, 2, 6, 3, 0, 4, 8],\n",
       " [1, 7, 5, 2, 6, 3, 0, 8, 4],\n",
       " [1, 7, 5, 2, 6, 3, 4, 0, 8],\n",
       " [1, 7, 5, 2, 6, 3, 4, 8, 0],\n",
       " [1, 7, 5, 2, 6, 3, 8, 0, 4],\n",
       " [1, 7, 5, 2, 6, 3, 8, 4, 0],\n",
       " [1, 7, 5, 2, 6, 4, 0, 3, 8],\n",
       " [1, 7, 5, 2, 6, 4, 3, 0, 8],\n",
       " [1, 7, 5, 2, 6, 4, 8, 0, 3],\n",
       " [1, 7, 5, 2, 6, 4, 8, 3, 0],\n",
       " [1, 7, 5, 2, 8, 0, 3, 4, 6],\n",
       " [1, 7, 5, 2, 8, 0, 4, 3, 6],\n",
       " [1, 7, 5, 2, 8, 0, 6, 3, 4],\n",
       " [1, 7, 5, 2, 8, 0, 6, 4, 3],\n",
       " [1, 7, 5, 2, 8, 3, 0, 4, 6],\n",
       " [1, 7, 5, 2, 8, 3, 4, 0, 6],\n",
       " [1, 7, 5, 2, 8, 3, 6, 0, 4],\n",
       " [1, 7, 5, 2, 8, 3, 6, 4, 0],\n",
       " [1, 7, 5, 2, 8, 4, 6, 0, 3],\n",
       " [1, 7, 5, 2, 8, 4, 6, 3, 0],\n",
       " [1, 7, 6, 0, 2, 4, 8, 5, 3],\n",
       " [1, 7, 6, 0, 3, 2, 4, 5, 8],\n",
       " [1, 7, 6, 0, 3, 2, 5, 4, 8],\n",
       " [1, 7, 6, 0, 3, 2, 8, 4, 5],\n",
       " [1, 7, 6, 0, 3, 2, 8, 5, 4],\n",
       " [1, 7, 6, 0, 3, 4, 8, 2, 5],\n",
       " [1, 7, 6, 0, 3, 4, 8, 5, 2],\n",
       " [1, 7, 6, 0, 3, 5, 2, 4, 8],\n",
       " [1, 7, 6, 0, 3, 5, 4, 2, 8],\n",
       " [1, 7, 6, 0, 3, 5, 8, 2, 4],\n",
       " [1, 7, 6, 0, 3, 5, 8, 4, 2],\n",
       " [1, 7, 6, 0, 4, 2, 3, 5, 8],\n",
       " [1, 7, 6, 0, 4, 2, 5, 3, 8],\n",
       " [1, 7, 6, 0, 4, 2, 8, 3, 5],\n",
       " [1, 7, 6, 0, 4, 2, 8, 5, 3],\n",
       " [1, 7, 6, 0, 5, 2, 3, 4, 8],\n",
       " [1, 7, 6, 0, 5, 2, 4, 3, 8],\n",
       " [1, 7, 6, 0, 5, 2, 8, 3, 4],\n",
       " [1, 7, 6, 0, 5, 2, 8, 4, 3],\n",
       " [1, 7, 6, 0, 5, 4, 8, 2, 3],\n",
       " [1, 7, 6, 0, 8, 2, 3, 4, 5],\n",
       " [1, 7, 6, 0, 8, 2, 3, 5, 4],\n",
       " [1, 7, 6, 0, 8, 2, 4, 3, 5],\n",
       " [1, 7, 6, 0, 8, 2, 4, 5, 3],\n",
       " [1, 7, 6, 0, 8, 2, 5, 3, 4],\n",
       " [1, 7, 6, 0, 8, 2, 5, 4, 3],\n",
       " [1, 7, 6, 0, 8, 4, 2, 5, 3],\n",
       " [1, 7, 6, 0, 8, 4, 3, 2, 5],\n",
       " [1, 7, 6, 0, 8, 4, 3, 5, 2],\n",
       " [1, 7, 6, 0, 8, 4, 5, 2, 3],\n",
       " [1, 7, 6, 0, 8, 5, 2, 4, 3],\n",
       " [1, 7, 6, 0, 8, 5, 3, 2, 4],\n",
       " [1, 7, 6, 0, 8, 5, 3, 4, 2],\n",
       " [1, 7, 6, 0, 8, 5, 4, 2, 3],\n",
       " [1, 7, 6, 2, 0, 3, 4, 8, 5],\n",
       " [1, 7, 6, 2, 0, 3, 5, 4, 8],\n",
       " [1, 7, 6, 2, 0, 3, 5, 8, 4],\n",
       " [1, 7, 6, 2, 0, 3, 8, 4, 5],\n",
       " [1, 7, 6, 2, 3, 0, 4, 5, 8],\n",
       " [1, 7, 6, 2, 3, 0, 5, 4, 8],\n",
       " [1, 7, 6, 2, 3, 0, 8, 4, 5],\n",
       " [1, 7, 6, 2, 3, 0, 8, 5, 4],\n",
       " [1, 7, 6, 2, 4, 0, 3, 5, 8],\n",
       " [1, 7, 6, 2, 4, 0, 5, 3, 8],\n",
       " [1, 7, 6, 2, 4, 0, 8, 3, 5],\n",
       " [1, 7, 6, 2, 4, 0, 8, 5, 3],\n",
       " [1, 7, 6, 2, 4, 3, 0, 8, 5],\n",
       " [1, 7, 6, 2, 4, 3, 5, 0, 8],\n",
       " [1, 7, 6, 2, 4, 3, 5, 8, 0],\n",
       " [1, 7, 6, 2, 4, 3, 8, 0, 5],\n",
       " [1, 7, 6, 2, 4, 5, 8, 0, 3],\n",
       " [1, 7, 6, 2, 4, 8, 5, 3, 0],\n",
       " [1, 7, 6, 2, 5, 0, 3, 4, 8],\n",
       " [1, 7, 6, 2, 5, 0, 4, 3, 8],\n",
       " [1, 7, 6, 2, 5, 0, 8, 3, 4],\n",
       " [1, 7, 6, 2, 5, 0, 8, 4, 3],\n",
       " [1, 7, 6, 2, 5, 3, 0, 4, 8],\n",
       " [1, 7, 6, 2, 5, 3, 0, 8, 4],\n",
       " [1, 7, 6, 2, 5, 3, 4, 0, 8],\n",
       " [1, 7, 6, 2, 5, 3, 4, 8, 0],\n",
       " [1, 7, 6, 2, 5, 3, 8, 0, 4],\n",
       " [1, 7, 6, 2, 5, 3, 8, 4, 0],\n",
       " [1, 7, 6, 2, 5, 4, 0, 3, 8],\n",
       " [1, 7, 6, 2, 5, 4, 3, 0, 8],\n",
       " [1, 7, 6, 2, 5, 4, 8, 0, 3],\n",
       " [1, 7, 6, 2, 5, 4, 8, 3, 0],\n",
       " [1, 7, 6, 2, 8, 0, 3, 4, 5],\n",
       " [1, 7, 6, 2, 8, 0, 3, 5, 4],\n",
       " [1, 7, 6, 2, 8, 0, 4, 3, 5],\n",
       " [1, 7, 6, 2, 8, 0, 4, 5, 3],\n",
       " [1, 7, 6, 2, 8, 0, 5, 3, 4],\n",
       " [1, 7, 6, 2, 8, 0, 5, 4, 3],\n",
       " [1, 7, 6, 2, 8, 3, 0, 4, 5],\n",
       " [1, 7, 6, 2, 8, 3, 4, 0, 5],\n",
       " [1, 7, 6, 2, 8, 3, 5, 0, 4],\n",
       " [1, 7, 6, 2, 8, 3, 5, 4, 0],\n",
       " [1, 7, 6, 2, 8, 4, 0, 3, 5],\n",
       " [1, 7, 6, 2, 8, 4, 3, 0, 5],\n",
       " [1, 7, 6, 2, 8, 4, 5, 0, 3],\n",
       " [1, 7, 6, 2, 8, 4, 5, 3, 0],\n",
       " [1, 7, 8, 0, 2, 5, 3, 4, 6],\n",
       " [1, 7, 8, 0, 2, 5, 3, 6, 4],\n",
       " [1, 7, 8, 0, 2, 5, 4, 6, 3],\n",
       " [1, 7, 8, 0, 2, 5, 6, 4, 3],\n",
       " [1, 7, 8, 0, 3, 2, 4, 5, 6],\n",
       " [1, 7, 8, 0, 3, 2, 5, 4, 6],\n",
       " [1, 7, 8, 0, 3, 2, 6, 4, 5],\n",
       " [1, 7, 8, 0, 3, 2, 6, 5, 4],\n",
       " [1, 7, 8, 0, 3, 4, 2, 5, 6],\n",
       " [1, 7, 8, 0, 3, 4, 5, 2, 6],\n",
       " [1, 7, 8, 0, 3, 4, 6, 2, 5],\n",
       " [1, 7, 8, 0, 3, 4, 6, 5, 2],\n",
       " [1, 7, 8, 0, 3, 5, 2, 4, 6],\n",
       " [1, 7, 8, 0, 3, 5, 2, 6, 4],\n",
       " [1, 7, 8, 0, 3, 5, 4, 2, 6],\n",
       " [1, 7, 8, 0, 3, 5, 4, 6, 2],\n",
       " [1, 7, 8, 0, 3, 5, 6, 2, 4],\n",
       " [1, 7, 8, 0, 3, 5, 6, 4, 2],\n",
       " [1, 7, 8, 0, 4, 2, 3, 5, 6],\n",
       " [1, 7, 8, 0, 4, 2, 5, 3, 6],\n",
       " [1, 7, 8, 0, 4, 2, 6, 3, 5],\n",
       " [1, 7, 8, 0, 4, 2, 6, 5, 3],\n",
       " [1, 7, 8, 0, 4, 3, 6, 2, 5],\n",
       " [1, 7, 8, 0, 4, 5, 2, 6, 3],\n",
       " [1, 7, 8, 0, 4, 5, 3, 2, 6],\n",
       " [1, 7, 8, 0, 4, 5, 3, 6, 2],\n",
       " [1, 7, 8, 0, 4, 5, 6, 2, 3],\n",
       " [1, 7, 8, 0, 4, 6, 3, 5, 2],\n",
       " [1, 7, 8, 0, 5, 2, 3, 4, 6],\n",
       " [1, 7, 8, 0, 5, 2, 4, 3, 6],\n",
       " [1, 7, 8, 0, 5, 2, 6, 3, 4],\n",
       " [1, 7, 8, 0, 5, 2, 6, 4, 3],\n",
       " [1, 7, 8, 0, 6, 2, 3, 4, 5],\n",
       " [1, 7, 8, 0, 6, 2, 3, 5, 4],\n",
       " [1, 7, 8, 0, 6, 2, 4, 3, 5],\n",
       " [1, 7, 8, 0, 6, 2, 4, 5, 3],\n",
       " [1, 7, 8, 0, 6, 2, 5, 3, 4],\n",
       " [1, 7, 8, 0, 6, 2, 5, 4, 3],\n",
       " [1, 7, 8, 0, 6, 4, 2, 5, 3],\n",
       " [1, 7, 8, 0, 6, 4, 3, 2, 5],\n",
       " [1, 7, 8, 0, 6, 4, 3, 5, 2],\n",
       " [1, 7, 8, 0, 6, 4, 5, 2, 3],\n",
       " [1, 7, 8, 0, 6, 5, 2, 4, 3],\n",
       " [1, 7, 8, 0, 6, 5, 3, 2, 4],\n",
       " [1, 7, 8, 0, 6, 5, 3, 4, 2],\n",
       " [1, 7, 8, 0, 6, 5, 4, 2, 3],\n",
       " [1, 7, 8, 2, 0, 4, 6, 3, 5],\n",
       " [1, 7, 8, 2, 3, 0, 4, 5, 6],\n",
       " [1, 7, 8, 2, 3, 0, 5, 4, 6],\n",
       " [1, 7, 8, 2, 3, 0, 6, 4, 5],\n",
       " [1, 7, 8, 2, 3, 0, 6, 5, 4],\n",
       " [1, 7, 8, 2, 3, 4, 6, 0, 5],\n",
       " [1, 7, 8, 2, 4, 0, 3, 5, 6],\n",
       " [1, 7, 8, 2, 4, 0, 5, 3, 6],\n",
       " [1, 7, 8, 2, 4, 0, 6, 3, 5],\n",
       " [1, 7, 8, 2, 4, 0, 6, 5, 3],\n",
       " [1, 7, 8, 2, 5, 0, 3, 4, 6],\n",
       " [1, 7, 8, 2, 5, 0, 4, 3, 6],\n",
       " [1, 7, 8, 2, 5, 0, 6, 3, 4],\n",
       " [1, 7, 8, 2, 5, 0, 6, 4, 3],\n",
       " [1, 7, 8, 2, 5, 3, 0, 4, 6],\n",
       " [1, 7, 8, 2, 5, 3, 4, 0, 6],\n",
       " [1, 7, 8, 2, 5, 3, 6, 0, 4],\n",
       " [1, 7, 8, 2, 5, 3, 6, 4, 0],\n",
       " [1, 7, 8, 2, 5, 4, 6, 0, 3],\n",
       " [1, 7, 8, 2, 5, 4, 6, 3, 0],\n",
       " [1, 7, 8, 2, 6, 0, 3, 4, 5],\n",
       " [1, 7, 8, 2, 6, 0, 3, 5, 4],\n",
       " [1, 7, 8, 2, 6, 0, 4, 3, 5],\n",
       " [1, 7, 8, 2, 6, 0, 4, 5, 3],\n",
       " [1, 7, 8, 2, 6, 0, 5, 3, 4],\n",
       " [1, 7, 8, 2, 6, 0, 5, 4, 3],\n",
       " [1, 7, 8, 2, 6, 3, 0, 4, 5],\n",
       " [1, 7, 8, 2, 6, 3, 4, 0, 5],\n",
       " [1, 7, 8, 2, 6, 3, 5, 0, 4],\n",
       " [1, 7, 8, 2, 6, 3, 5, 4, 0],\n",
       " [1, 7, 8, 2, 6, 4, 0, 3, 5],\n",
       " [1, 7, 8, 2, 6, 4, 3, 0, 5],\n",
       " [1, 7, 8, 2, 6, 4, 5, 0, 3],\n",
       " [1, 7, 8, 2, 6, 4, 5, 3, 0],\n",
       " [2, 4, 0, 1, 7, 3, 5, 8, 6],\n",
       " [2, 4, 0, 1, 7, 5, 3, 6, 8],\n",
       " [2, 4, 0, 1, 7, 6, 3, 5, 8],\n",
       " [2, 4, 0, 1, 7, 6, 3, 8, 5],\n",
       " [2, 4, 0, 1, 7, 6, 5, 8, 3],\n",
       " [2, 4, 0, 1, 7, 6, 8, 5, 3],\n",
       " [2, 4, 0, 1, 7, 8, 3, 6, 5],\n",
       " [2, 4, 0, 1, 7, 8, 5, 3, 6],\n",
       " [2, 4, 0, 1, 7, 8, 5, 6, 3],\n",
       " [2, 4, 0, 1, 7, 8, 6, 3, 5],\n",
       " [2, 4, 1, 0, 8, 5, 3, 6, 7],\n",
       " [2, 4, 1, 0, 8, 5, 3, 7, 6],\n",
       " [2, 4, 3, 0, 8, 5, 1, 6, 7],\n",
       " [2, 4, 3, 0, 8, 5, 1, 7, 6],\n",
       " [2, 4, 3, 0, 8, 5, 6, 7, 1],\n",
       " [2, 4, 3, 0, 8, 5, 7, 6, 1],\n",
       " [2, 4, 3, 1, 7, 6, 0, 5, 8],\n",
       " [2, 4, 3, 1, 7, 6, 0, 8, 5],\n",
       " [2, 4, 3, 1, 7, 6, 5, 8, 0],\n",
       " [2, 4, 3, 1, 7, 6, 8, 5, 0],\n",
       " [2, 4, 3, 1, 7, 8, 0, 6, 5],\n",
       " [2, 4, 3, 6, 0, 1, 7, 5, 8],\n",
       " [2, 4, 3, 6, 0, 1, 7, 8, 5],\n",
       " [2, 4, 3, 6, 1, 0, 8, 5, 7],\n",
       " [2, 4, 3, 6, 7, 0, 8, 5, 1],\n",
       " [2, 4, 3, 6, 7, 1, 0, 5, 8],\n",
       " [2, 4, 3, 6, 7, 1, 0, 8, 5],\n",
       " [2, 4, 3, 6, 7, 1, 5, 8, 0],\n",
       " [2, 4, 3, 6, 7, 1, 8, 5, 0],\n",
       " [2, 4, 3, 6, 7, 5, 0, 1, 8],\n",
       " [2, 4, 3, 6, 7, 5, 1, 0, 8],\n",
       " [2, 4, 3, 6, 7, 5, 8, 0, 1],\n",
       " [2, 4, 3, 6, 7, 5, 8, 1, 0],\n",
       " [2, 4, 3, 6, 7, 8, 0, 1, 5],\n",
       " [2, 4, 3, 6, 8, 5, 0, 1, 7],\n",
       " [2, 4, 3, 6, 8, 5, 1, 0, 7],\n",
       " [2, 4, 3, 6, 8, 5, 7, 0, 1],\n",
       " [2, 4, 3, 6, 8, 5, 7, 1, 0],\n",
       " [2, 4, 3, 7, 1, 0, 8, 5, 6],\n",
       " [2, 4, 5, 8, 0, 1, 7, 3, 6],\n",
       " [2, 4, 5, 8, 0, 1, 7, 6, 3],\n",
       " [2, 4, 6, 1, 7, 8, 0, 3, 5],\n",
       " [2, 4, 6, 3, 5, 8, 0, 1, 7],\n",
       " [2, 4, 6, 5, 3, 0, 8, 7, 1],\n",
       " [2, 4, 6, 7, 1, 0, 8, 5, 3],\n",
       " [2, 4, 7, 3, 5, 8, 0, 1, 6],\n",
       " [2, 4, 7, 5, 3, 0, 8, 6, 1],\n",
       " [2, 4, 7, 5, 3, 6, 0, 1, 8],\n",
       " [2, 4, 7, 5, 3, 6, 1, 0, 8],\n",
       " [2, 4, 7, 5, 3, 6, 8, 0, 1],\n",
       " [2, 4, 7, 5, 3, 6, 8, 1, 0],\n",
       " [2, 4, 7, 6, 0, 1, 3, 5, 8],\n",
       " [2, 4, 7, 6, 0, 1, 3, 8, 5],\n",
       " [2, 4, 7, 6, 0, 1, 5, 8, 3],\n",
       " [2, 4, 7, 6, 0, 1, 8, 5, 3],\n",
       " [2, 4, 7, 6, 3, 0, 8, 5, 1],\n",
       " [2, 4, 7, 6, 3, 1, 0, 5, 8],\n",
       " [2, 4, 7, 6, 3, 1, 0, 8, 5],\n",
       " [2, 4, 7, 6, 3, 1, 5, 8, 0],\n",
       " [2, 4, 7, 6, 3, 1, 8, 5, 0],\n",
       " [2, 4, 7, 6, 3, 5, 0, 1, 8],\n",
       " [2, 4, 7, 6, 3, 5, 1, 0, 8],\n",
       " [2, 4, 7, 6, 3, 5, 8, 0, 1],\n",
       " [2, 4, 7, 6, 3, 5, 8, 1, 0],\n",
       " [2, 4, 7, 6, 3, 8, 0, 1, 5],\n",
       " [2, 4, 7, 6, 5, 8, 0, 1, 3],\n",
       " [2, 4, 7, 6, 8, 5, 3, 0, 1],\n",
       " [2, 4, 7, 6, 8, 5, 3, 1, 0],\n",
       " [2, 4, 7, 8, 0, 1, 3, 6, 5],\n",
       " [2, 4, 7, 8, 0, 1, 5, 3, 6],\n",
       " [2, 4, 7, 8, 0, 1, 5, 6, 3],\n",
       " [2, 4, 7, 8, 0, 1, 6, 3, 5],\n",
       " [2, 4, 8, 5, 3, 0, 1, 6, 7],\n",
       " [2, 4, 8, 5, 3, 0, 1, 7, 6],\n",
       " [2, 4, 8, 5, 3, 0, 6, 7, 1],\n",
       " [2, 4, 8, 5, 3, 0, 7, 6, 1],\n",
       " [2, 4, 8, 5, 3, 1, 7, 6, 0],\n",
       " [2, 4, 8, 5, 3, 6, 0, 1, 7],\n",
       " [2, 4, 8, 5, 3, 6, 1, 0, 7],\n",
       " [2, 4, 8, 5, 3, 6, 7, 0, 1],\n",
       " [2, 4, 8, 5, 3, 6, 7, 1, 0],\n",
       " [2, 4, 8, 5, 3, 7, 1, 0, 6],\n",
       " [3, 0, 1, 4, 8, 2, 6, 7, 5],\n",
       " [3, 0, 1, 4, 8, 5, 2, 6, 7],\n",
       " [3, 0, 1, 4, 8, 5, 2, 7, 6],\n",
       " [3, 0, 1, 4, 8, 5, 6, 7, 2],\n",
       " [3, 0, 1, 4, 8, 5, 7, 6, 2],\n",
       " [3, 0, 1, 4, 8, 6, 2, 5, 7],\n",
       " [3, 0, 1, 4, 8, 7, 2, 5, 6],\n",
       " [3, 0, 1, 4, 8, 7, 5, 2, 6],\n",
       " [3, 0, 1, 4, 8, 7, 6, 2, 5],\n",
       " [3, 0, 1, 4, 8, 7, 6, 5, 2],\n",
       " [3, 0, 1, 5, 2, 4, 8, 6, 7],\n",
       " [3, 0, 1, 5, 2, 4, 8, 7, 6],\n",
       " [3, 0, 1, 5, 2, 6, 4, 7, 8],\n",
       " [3, 0, 1, 5, 2, 6, 7, 4, 8],\n",
       " [3, 0, 1, 5, 2, 6, 8, 4, 7],\n",
       " [3, 0, 1, 5, 2, 6, 8, 7, 4],\n",
       " [3, 0, 1, 5, 2, 7, 4, 6, 8],\n",
       " [3, 0, 1, 5, 2, 7, 6, 4, 8],\n",
       " [3, 0, 1, 5, 2, 7, 8, 4, 6],\n",
       " [3, 0, 1, 5, 2, 7, 8, 6, 4],\n",
       " [3, 0, 1, 5, 4, 7, 2, 6, 8],\n",
       " [3, 0, 1, 5, 4, 7, 6, 2, 8],\n",
       " [3, 0, 1, 5, 4, 7, 8, 2, 6],\n",
       " [3, 0, 1, 5, 4, 7, 8, 6, 2],\n",
       " [3, 0, 1, 5, 7, 4, 8, 6, 2],\n",
       " [3, 0, 1, 5, 8, 4, 2, 6, 7],\n",
       " [3, 0, 1, 5, 8, 4, 2, 7, 6],\n",
       " [3, 0, 1, 5, 8, 4, 6, 7, 2],\n",
       " [3, 0, 1, 5, 8, 4, 7, 6, 2],\n",
       " [3, 0, 1, 5, 8, 6, 2, 4, 7],\n",
       " [3, 0, 1, 5, 8, 6, 2, 7, 4],\n",
       " [3, 0, 1, 5, 8, 6, 4, 7, 2],\n",
       " [3, 0, 1, 5, 8, 6, 7, 4, 2],\n",
       " [3, 0, 1, 5, 8, 7, 2, 4, 6],\n",
       " [3, 0, 1, 5, 8, 7, 2, 6, 4],\n",
       " [3, 0, 1, 5, 8, 7, 4, 2, 6],\n",
       " [3, 0, 1, 5, 8, 7, 4, 6, 2],\n",
       " [3, 0, 1, 5, 8, 7, 6, 2, 4],\n",
       " [3, 0, 1, 5, 8, 7, 6, 4, 2],\n",
       " [3, 0, 1, 7, 4, 5, 2, 6, 8],\n",
       " [3, 0, 1, 7, 4, 5, 6, 2, 8],\n",
       " [3, 0, 1, 7, 4, 5, 8, 2, 6],\n",
       " [3, 0, 1, 7, 4, 5, 8, 6, 2],\n",
       " [3, 0, 1, 7, 5, 4, 8, 2, 6],\n",
       " [3, 0, 1, 7, 6, 2, 4, 5, 8],\n",
       " [3, 0, 1, 7, 6, 2, 5, 4, 8],\n",
       " [3, 0, 1, 7, 6, 2, 8, 4, 5],\n",
       " [3, 0, 1, 7, 6, 2, 8, 5, 4],\n",
       " [3, 0, 1, 7, 6, 4, 8, 2, 5],\n",
       " [3, 0, 1, 7, 6, 4, 8, 5, 2],\n",
       " [3, 0, 1, 7, 6, 5, 2, 4, 8],\n",
       " [3, 0, 1, 7, 6, 5, 4, 2, 8],\n",
       " [3, 0, 1, 7, 6, 5, 8, 2, 4],\n",
       " [3, 0, 1, 7, 6, 5, 8, 4, 2],\n",
       " [3, 0, 1, 7, 8, 2, 4, 5, 6],\n",
       " [3, 0, 1, 7, 8, 2, 5, 4, 6],\n",
       " [3, 0, 1, 7, 8, 2, 6, 4, 5],\n",
       " [3, 0, 1, 7, 8, 2, 6, 5, 4],\n",
       " [3, 0, 1, 7, 8, 4, 2, 5, 6],\n",
       " [3, 0, 1, 7, 8, 4, 5, 2, 6],\n",
       " [3, 0, 1, 7, 8, 4, 6, 2, 5],\n",
       " [3, 0, 1, 7, 8, 4, 6, 5, 2],\n",
       " [3, 0, 1, 7, 8, 5, 2, 4, 6],\n",
       " [3, 0, 1, 7, 8, 5, 2, 6, 4],\n",
       " [3, 0, 1, 7, 8, 5, 4, 2, 6],\n",
       " [3, 0, 1, 7, 8, 5, 4, 6, 2],\n",
       " [3, 0, 1, 7, 8, 5, 6, 2, 4],\n",
       " [3, 0, 1, 7, 8, 5, 6, 4, 2],\n",
       " [3, 0, 2, 4, 8, 5, 1, 6, 7],\n",
       " [3, 0, 2, 4, 8, 5, 1, 7, 6],\n",
       " [3, 0, 2, 4, 8, 5, 6, 7, 1],\n",
       " [3, 0, 2, 4, 8, 5, 7, 6, 1],\n",
       " [3, 0, 2, 5, 1, 4, 8, 6, 7],\n",
       " [3, 0, 2, 5, 1, 4, 8, 7, 6],\n",
       " [3, 0, 2, 5, 1, 6, 4, 7, 8],\n",
       " [3, 0, 2, 5, 1, 6, 7, 4, 8],\n",
       " [3, 0, 2, 5, 1, 6, 8, 4, 7],\n",
       " [3, 0, 2, 5, 1, 6, 8, 7, 4],\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[game.moves_played for game in game_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_best_moves(board)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.get_possible_moves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| O | X | O |\n",
      "|   | X |   |\n",
      "|   |   |   |\n"
     ]
    }
   ],
   "source": [
    "board.make_move(2)\n",
    "board.draw_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 6, 0, 4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board.moves_played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 3, 7, 6]\n"
     ]
    }
   ],
   "source": [
    "print(game_list[3].moves_played)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m boards \u001b[39m=\u001b[39m [Board()]\n\u001b[0;32m----> 2\u001b[0m game_list \u001b[39m=\u001b[39m apply_best_moves(boards)\n\u001b[1;32m      3\u001b[0m moves \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([[\u001b[39m10\u001b[39m] \u001b[39m+\u001b[39m game\u001b[39m.\u001b[39mmoves_played \u001b[39m+\u001b[39m [\u001b[39m9\u001b[39m] \u001b[39mfor\u001b[39;00m game \u001b[39min\u001b[39;00m game_list])\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:156\u001b[0m, in \u001b[0;36mapply_best_moves\u001b[0;34m(boards, finished_boards)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m finished_boards\n\u001b[1;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_best_moves(ongoing_boards, finished_boards)\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:156\u001b[0m, in \u001b[0;36mapply_best_moves\u001b[0;34m(boards, finished_boards)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m finished_boards\n\u001b[1;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_best_moves(ongoing_boards, finished_boards)\n",
      "    \u001b[0;31m[... skipping similar frames: apply_best_moves at line 156 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:156\u001b[0m, in \u001b[0;36mapply_best_moves\u001b[0;34m(boards, finished_boards)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[39mreturn\u001b[39;00m finished_boards\n\u001b[1;32m    155\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[39mreturn\u001b[39;00m apply_best_moves(ongoing_boards, finished_boards)\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:150\u001b[0m, in \u001b[0;36mapply_best_moves\u001b[0;34m(boards, finished_boards)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m get_best_moves(board):\n\u001b[0;32m--> 150\u001b[0m         _board \u001b[39m=\u001b[39m deepcopy(board)\n\u001b[1;32m    151\u001b[0m         _board\u001b[39m.\u001b[39mmake_move(move)\n\u001b[1;32m    152\u001b[0m         ongoing_boards\u001b[39m.\u001b[39mappend(_board)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/copy.py:265\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m args:\n\u001b[1;32m    264\u001b[0m     args \u001b[39m=\u001b[39m (deepcopy(arg, memo) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)\n\u001b[0;32m--> 265\u001b[0m y \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m deep:\n\u001b[1;32m    267\u001b[0m     memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/copy.py:264\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m deep \u001b[39m=\u001b[39m memo \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m deep \u001b[39mand\u001b[39;00m args:\n\u001b[0;32m--> 264\u001b[0m     args \u001b[39m=\u001b[39m (deepcopy(arg, memo) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)\n\u001b[1;32m    265\u001b[0m y \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m deep:\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "boards = [Board()]\n",
    "game_list = apply_best_moves(boards)\n",
    "moves = np.array([[10] + game.moves_played + [9] for game in game_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boards = [Board()]\n",
    "game_list = generate_all_games(boards)\n",
    "moves = np.array([[10] + game.moves_played + ([9] * (cfg.n_ctx - len(game.moves_played))) for game in game_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10  0  1  3  2  6  9  9  9  9  9]\n"
     ]
    }
   ],
   "source": [
    "print(moves[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we're on the 1th loop!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Not a valid move nerd!!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m boards \u001b[39m=\u001b[39m [Board()]\n\u001b[0;32m----> 2\u001b[0m game_list \u001b[39m=\u001b[39m apply_best_moves(boards)\n\u001b[1;32m      3\u001b[0m moves \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([game\u001b[39m.\u001b[39mmoves_played \u001b[39m+\u001b[39m [\u001b[39m9\u001b[39m] \u001b[39mfor\u001b[39;00m game \u001b[39min\u001b[39;00m game_list])\n\u001b[1;32m      4\u001b[0m data \u001b[39m=\u001b[39m moves\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:108\u001b[0m, in \u001b[0;36mapply_best_moves\u001b[0;34m(boards)\u001b[0m\n\u001b[1;32m    106\u001b[0m new_boards \u001b[39m=\u001b[39m []\n\u001b[1;32m    107\u001b[0m \u001b[39mfor\u001b[39;00m board \u001b[39min\u001b[39;00m boards:\n\u001b[0;32m--> 108\u001b[0m     \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m get_best_moves(board):\n\u001b[1;32m    109\u001b[0m         _board \u001b[39m=\u001b[39m deepcopy(board)\n\u001b[1;32m    110\u001b[0m         new_boards\u001b[39m.\u001b[39mappend(_board\u001b[39m.\u001b[39mmake_move(move))\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:132\u001b[0m, in \u001b[0;36mget_best_moves\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m board\u001b[39m.\u001b[39mget_possible_moves():\n\u001b[1;32m    131\u001b[0m     board\u001b[39m.\u001b[39mmake_move(move)\n\u001b[0;32m--> 132\u001b[0m     score \u001b[39m=\u001b[39m minimax(board)\n\u001b[1;32m    133\u001b[0m     board\u001b[39m.\u001b[39mundo()\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m board\u001b[39m.\u001b[39mis_maximizer \u001b[39m&\u001b[39m (score \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m bestScore):\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:175\u001b[0m, in \u001b[0;36mminimax\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m board\u001b[39m.\u001b[39mget_possible_moves():\n\u001b[1;32m    174\u001b[0m     board\u001b[39m.\u001b[39mmake_move(move)\n\u001b[0;32m--> 175\u001b[0m     scores\u001b[39m.\u001b[39mappend(minimax(board))\n\u001b[1;32m    176\u001b[0m     board\u001b[39m.\u001b[39mundo()\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(scores) \u001b[39mif\u001b[39;00m board\u001b[39m.\u001b[39mis_maximizer \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(scores)\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:175\u001b[0m, in \u001b[0;36mminimax\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m board\u001b[39m.\u001b[39mget_possible_moves():\n\u001b[1;32m    174\u001b[0m     board\u001b[39m.\u001b[39mmake_move(move)\n\u001b[0;32m--> 175\u001b[0m     scores\u001b[39m.\u001b[39mappend(minimax(board))\n\u001b[1;32m    176\u001b[0m     board\u001b[39m.\u001b[39mundo()\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(scores) \u001b[39mif\u001b[39;00m board\u001b[39m.\u001b[39mis_maximizer \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(scores)\n",
      "    \u001b[0;31m[... skipping similar frames: minimax at line 175 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:175\u001b[0m, in \u001b[0;36mminimax\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m board\u001b[39m.\u001b[39mget_possible_moves():\n\u001b[1;32m    174\u001b[0m     board\u001b[39m.\u001b[39mmake_move(move)\n\u001b[0;32m--> 175\u001b[0m     scores\u001b[39m.\u001b[39mappend(minimax(board))\n\u001b[1;32m    176\u001b[0m     board\u001b[39m.\u001b[39mundo()\n\u001b[1;32m    178\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mmax\u001b[39m(scores) \u001b[39mif\u001b[39;00m board\u001b[39m.\u001b[39mis_maximizer \u001b[39melse\u001b[39;00m \u001b[39mmin\u001b[39m(scores)\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:174\u001b[0m, in \u001b[0;36mminimax\u001b[0;34m(board)\u001b[0m\n\u001b[1;32m    172\u001b[0m scores \u001b[39m=\u001b[39m []\n\u001b[1;32m    173\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m board\u001b[39m.\u001b[39mget_possible_moves():\n\u001b[0;32m--> 174\u001b[0m     board\u001b[39m.\u001b[39;49mmake_move(move)\n\u001b[1;32m    175\u001b[0m     scores\u001b[39m.\u001b[39mappend(minimax(board))\n\u001b[1;32m    176\u001b[0m     board\u001b[39m.\u001b[39mundo()\n",
      "File \u001b[0;32m~/Code/Tic-Tac-Transformer/src/game.py:43\u001b[0m, in \u001b[0;36mBoard.make_move\u001b[0;34m(self, move)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_move\u001b[39m(\u001b[39mself\u001b[39m, move: \u001b[39mint\u001b[39m):\n\u001b[1;32m     42\u001b[0m     \u001b[39mif\u001b[39;00m move \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_possible_moves():\n\u001b[0;32m---> 43\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot a valid move nerd!!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrid[move] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mturn\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmoves_played\u001b[39m.\u001b[39mappend(move)\n",
      "\u001b[0;31mValueError\u001b[0m: Not a valid move nerd!!"
     ]
    }
   ],
   "source": [
    "boards = [Board()]\n",
    "game_list = apply_best_moves(boards)\n",
    "moves = np.array([game.moves_played + [9] for game in game_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255168\n",
      "255168\n",
      "[10  0  1  3  2  6  9  9  9  9]\n",
      "[0 1 3 2 6 9 9 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "#load npy file\n",
    "# np_data = np.load('data/moves.npy')\n",
    "data = moves[:, :-1]\n",
    "labels = moves[:, 1:]\n",
    "\n",
    "print(len(data))\n",
    "print(len(labels))\n",
    "print(data[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and labels as numpy arrays\n",
    "encoded_labels: Tensor = F.one_hot(t.tensor(labels))\n",
    "data = np.array(data)\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "#data and encoded_labels as tensors\n",
    "data = t.from_numpy(data)\n",
    "encoded_labels = t.from_numpy(encoded_labels).to(t.float)\n",
    "total_data = list(zip(data, encoded_labels))\n",
    "num_samples = len(total_data)\n",
    "train_size = int(test_train_split * num_samples)\n",
    "test_size = num_samples - train_size\n",
    "split_data = list(t.utils.data.random_split(total_data, [train_size, test_size]))\n",
    "train_pairs = split_data[0]\n",
    "test_pairs= split_data[1]\n",
    "train_data, train_labels = zip(*train_pairs)\n",
    "test_data, test_labels = zip(*test_pairs)\n",
    "\n",
    "train_data = t.stack(train_data).to(cfg.device)\n",
    "train_labels = t.stack(train_labels).to(cfg.device)\n",
    "test_data = t.stack(test_data).to(cfg.device)\n",
    "test_labels = t.stack(test_labels).to(cfg.device)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test train split\n",
    "train_data = data[:int(len(data)*test_train_split)]\n",
    "train_labels = encoded_labels[:int(len(data)*test_train_split)]\n",
    "test_data = data[int(len(data)*test_train_split):]\n",
    "test_labels = encoded_labels[int(len(data)*test_train_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits: Tensor, labels: Tensor):\n",
    "    return t.nn.functional.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_is_matched(input: list[int], seq: list[int]) -> bool:\n",
    "    return all(seq[i] == input[i] for i in range(len(seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seqs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/obayyub/p/Tic-Tac-Transformer/model.ipynb Cell 26\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/obayyub/p/Tic-Tac-Transformer/model.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m [seq \u001b[39mfor\u001b[39;00m seq \u001b[39min\u001b[39;00m seqs \u001b[39mif\u001b[39;00m seq_is_matched(seq, [\u001b[39m4\u001b[39m])]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seqs' is not defined"
     ]
    }
   ],
   "source": [
    "[seq for seq in seqs if seq_is_matched(seq, [4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Epoch 0 | Train Loss: 2.809720993041992 | Test Loss: 2.7057554721832275\n",
      "Epoch 0 | Train Loss: 2.709939956665039 | Test Loss: 2.6214585304260254\n",
      "Epoch 0 | Train Loss: 2.6275126934051514 | Test Loss: 2.5577471256256104\n",
      "Epoch 0 | Train Loss: 2.5600898265838623 | Test Loss: 2.5083353519439697\n",
      "Epoch 0 | Train Loss: 2.5101170539855957 | Test Loss: 2.468353271484375\n",
      "Epoch 0 | Train Loss: 2.4703221321105957 | Test Loss: 2.434781074523926\n",
      "Epoch 0 | Train Loss: 2.4407336711883545 | Test Loss: 2.406076431274414\n",
      "Epoch 0 | Train Loss: 2.4066998958587646 | Test Loss: 2.380523204803467\n",
      "Epoch 0 | Train Loss: 2.383831739425659 | Test Loss: 2.3570046424865723\n",
      "Epoch 0 | Train Loss: 2.358985185623169 | Test Loss: 2.3352038860321045\n",
      "Epoch 0 | Train Loss: 2.333693027496338 | Test Loss: 2.3150100708007812\n",
      "Epoch 0 | Train Loss: 2.3151962757110596 | Test Loss: 2.2963321208953857\n",
      "Epoch 0 | Train Loss: 2.2956178188323975 | Test Loss: 2.278759717941284\n",
      "Epoch 0 | Train Loss: 2.279012680053711 | Test Loss: 2.2620551586151123\n",
      "Epoch 0 | Train Loss: 2.2610695362091064 | Test Loss: 2.2460505962371826\n",
      "Epoch 0 | Train Loss: 2.2481603622436523 | Test Loss: 2.2307045459747314\n",
      "Epoch 0 | Train Loss: 2.228414297103882 | Test Loss: 2.2160942554473877\n",
      "Epoch 0 | Train Loss: 2.2171175479888916 | Test Loss: 2.202044725418091\n",
      "Epoch 0 | Train Loss: 2.205108404159546 | Test Loss: 2.1887197494506836\n",
      "Epoch 0 | Train Loss: 2.1878738403320312 | Test Loss: 2.175842046737671\n",
      "Epoch 0 | Train Loss: 2.17578125 | Test Loss: 2.1633894443511963\n",
      "Epoch 0 | Train Loss: 2.1639955043792725 | Test Loss: 2.151346445083618\n",
      "Epoch 0 | Train Loss: 2.153862953186035 | Test Loss: 2.1396539211273193\n",
      "Epoch 0 | Train Loss: 2.141052007675171 | Test Loss: 2.1284077167510986\n",
      "Epoch 0 | Train Loss: 2.1277921199798584 | Test Loss: 2.117582321166992\n",
      "Epoch 1 | Train Loss: 2.120605707168579 | Test Loss: 2.10711669921875\n",
      "Epoch 1 | Train Loss: 2.11041259765625 | Test Loss: 2.0969650745391846\n",
      "Epoch 1 | Train Loss: 2.0996148586273193 | Test Loss: 2.0871739387512207\n",
      "Epoch 1 | Train Loss: 2.0889132022857666 | Test Loss: 2.077772617340088\n",
      "Epoch 1 | Train Loss: 2.07867693901062 | Test Loss: 2.068723678588867\n",
      "Epoch 1 | Train Loss: 2.0694048404693604 | Test Loss: 2.059943914413452\n",
      "Epoch 1 | Train Loss: 2.0637362003326416 | Test Loss: 2.051433563232422\n",
      "Epoch 1 | Train Loss: 2.0504817962646484 | Test Loss: 2.043137550354004\n",
      "Epoch 1 | Train Loss: 2.046142578125 | Test Loss: 2.0350050926208496\n",
      "Epoch 1 | Train Loss: 2.0356853008270264 | Test Loss: 2.0270183086395264\n",
      "Epoch 1 | Train Loss: 2.0268609523773193 | Test Loss: 2.019193649291992\n",
      "Epoch 1 | Train Loss: 2.0184326171875 | Test Loss: 2.0115299224853516\n",
      "Epoch 1 | Train Loss: 2.0103986263275146 | Test Loss: 2.0040087699890137\n",
      "Epoch 1 | Train Loss: 2.0052990913391113 | Test Loss: 1.996598720550537\n",
      "Epoch 1 | Train Loss: 1.99468195438385 | Test Loss: 1.9892849922180176\n",
      "Epoch 1 | Train Loss: 1.989404320716858 | Test Loss: 1.9820630550384521\n",
      "Epoch 1 | Train Loss: 1.9820889234542847 | Test Loss: 1.9748951196670532\n",
      "Epoch 1 | Train Loss: 1.9762343168258667 | Test Loss: 1.9677748680114746\n",
      "Epoch 1 | Train Loss: 1.9696394205093384 | Test Loss: 1.9607198238372803\n",
      "Epoch 1 | Train Loss: 1.9578298330307007 | Test Loss: 1.9537047147750854\n",
      "Epoch 1 | Train Loss: 1.9551359415054321 | Test Loss: 1.9467227458953857\n",
      "Epoch 1 | Train Loss: 1.9477112293243408 | Test Loss: 1.9397871494293213\n",
      "Epoch 1 | Train Loss: 1.94268000125885 | Test Loss: 1.9328895807266235\n",
      "Epoch 1 | Train Loss: 1.9329556226730347 | Test Loss: 1.9260274171829224\n",
      "Epoch 1 | Train Loss: 1.926891803741455 | Test Loss: 1.9192084074020386\n",
      "Epoch 2 | Train Loss: 1.9221057891845703 | Test Loss: 1.9124298095703125\n",
      "Epoch 2 | Train Loss: 1.9164146184921265 | Test Loss: 1.9056715965270996\n",
      "Epoch 2 | Train Loss: 1.9079856872558594 | Test Loss: 1.898939609527588\n",
      "Epoch 2 | Train Loss: 1.8989803791046143 | Test Loss: 1.8922350406646729\n",
      "Epoch 2 | Train Loss: 1.8938941955566406 | Test Loss: 1.8855570554733276\n",
      "Epoch 2 | Train Loss: 1.8855984210968018 | Test Loss: 1.878910779953003\n",
      "Epoch 2 | Train Loss: 1.8822656869888306 | Test Loss: 1.872273564338684\n",
      "Epoch 2 | Train Loss: 1.8717470169067383 | Test Loss: 1.865655779838562\n",
      "Epoch 2 | Train Loss: 1.8683747053146362 | Test Loss: 1.859055519104004\n",
      "Epoch 2 | Train Loss: 1.8599255084991455 | Test Loss: 1.8524621725082397\n",
      "Epoch 2 | Train Loss: 1.852245569229126 | Test Loss: 1.8458794355392456\n",
      "Epoch 2 | Train Loss: 1.8442119359970093 | Test Loss: 1.8392996788024902\n",
      "Epoch 2 | Train Loss: 1.8387359380722046 | Test Loss: 1.832724928855896\n",
      "Epoch 2 | Train Loss: 1.8333481550216675 | Test Loss: 1.8261604309082031\n",
      "Epoch 2 | Train Loss: 1.823637843132019 | Test Loss: 1.8195961713790894\n",
      "Epoch 2 | Train Loss: 1.8185471296310425 | Test Loss: 1.813019871711731\n",
      "Epoch 2 | Train Loss: 1.812895655632019 | Test Loss: 1.8064326047897339\n",
      "Epoch 2 | Train Loss: 1.8068536520004272 | Test Loss: 1.799838662147522\n",
      "Epoch 2 | Train Loss: 1.8013794422149658 | Test Loss: 1.7932395935058594\n",
      "Epoch 2 | Train Loss: 1.7902058362960815 | Test Loss: 1.7866382598876953\n",
      "Epoch 2 | Train Loss: 1.7880665063858032 | Test Loss: 1.7800267934799194\n",
      "Epoch 2 | Train Loss: 1.7817776203155518 | Test Loss: 1.7734107971191406\n",
      "Epoch 2 | Train Loss: 1.7762142419815063 | Test Loss: 1.766789197921753\n",
      "Epoch 2 | Train Loss: 1.766497015953064 | Test Loss: 1.7601686716079712\n",
      "Epoch 2 | Train Loss: 1.7612559795379639 | Test Loss: 1.7535390853881836\n",
      "Epoch 3 | Train Loss: 1.7558501958847046 | Test Loss: 1.74689781665802\n",
      "Epoch 3 | Train Loss: 1.7509689331054688 | Test Loss: 1.7402452230453491\n",
      "Epoch 3 | Train Loss: 1.7418714761734009 | Test Loss: 1.7335760593414307\n",
      "Epoch 3 | Train Loss: 1.7329586744308472 | Test Loss: 1.7268849611282349\n",
      "Epoch 3 | Train Loss: 1.7283838987350464 | Test Loss: 1.7201787233352661\n",
      "Epoch 3 | Train Loss: 1.719629168510437 | Test Loss: 1.713447093963623\n",
      "Epoch 3 | Train Loss: 1.7158854007720947 | Test Loss: 1.706704020500183\n",
      "Epoch 3 | Train Loss: 1.7063028812408447 | Test Loss: 1.6999452114105225\n",
      "Epoch 3 | Train Loss: 1.702654480934143 | Test Loss: 1.6931865215301514\n",
      "Epoch 3 | Train Loss: 1.6935971975326538 | Test Loss: 1.6864246129989624\n",
      "Epoch 3 | Train Loss: 1.6863418817520142 | Test Loss: 1.6796538829803467\n",
      "Epoch 3 | Train Loss: 1.6775044202804565 | Test Loss: 1.6728616952896118\n",
      "Epoch 3 | Train Loss: 1.6722151041030884 | Test Loss: 1.6660547256469727\n",
      "Epoch 3 | Train Loss: 1.6661571264266968 | Test Loss: 1.65923273563385\n",
      "Epoch 3 | Train Loss: 1.6560440063476562 | Test Loss: 1.652381420135498\n",
      "Epoch 3 | Train Loss: 1.6507257223129272 | Test Loss: 1.6455059051513672\n",
      "Epoch 3 | Train Loss: 1.6448830366134644 | Test Loss: 1.6386123895645142\n",
      "Epoch 3 | Train Loss: 1.6383708715438843 | Test Loss: 1.631712555885315\n",
      "Epoch 3 | Train Loss: 1.6328085660934448 | Test Loss: 1.624795913696289\n",
      "Epoch 3 | Train Loss: 1.621561050415039 | Test Loss: 1.6178627014160156\n",
      "Epoch 3 | Train Loss: 1.6186916828155518 | Test Loss: 1.6109105348587036\n",
      "Epoch 3 | Train Loss: 1.6127922534942627 | Test Loss: 1.603936791419983\n",
      "Epoch 3 | Train Loss: 1.6063076257705688 | Test Loss: 1.596924901008606\n",
      "Epoch 3 | Train Loss: 1.5965787172317505 | Test Loss: 1.589881420135498\n",
      "Epoch 3 | Train Loss: 1.591052770614624 | Test Loss: 1.5828107595443726\n",
      "Epoch 4 | Train Loss: 1.5842704772949219 | Test Loss: 1.5757156610488892\n",
      "Epoch 4 | Train Loss: 1.5803812742233276 | Test Loss: 1.5685993432998657\n",
      "Epoch 4 | Train Loss: 1.5694806575775146 | Test Loss: 1.5614805221557617\n",
      "Epoch 4 | Train Loss: 1.5603039264678955 | Test Loss: 1.5543586015701294\n",
      "Epoch 4 | Train Loss: 1.5560694932937622 | Test Loss: 1.547222375869751\n",
      "Epoch 4 | Train Loss: 1.5461896657943726 | Test Loss: 1.5400665998458862\n",
      "Epoch 4 | Train Loss: 1.5421491861343384 | Test Loss: 1.5328943729400635\n",
      "Epoch 4 | Train Loss: 1.532292366027832 | Test Loss: 1.525700569152832\n",
      "Epoch 4 | Train Loss: 1.5287020206451416 | Test Loss: 1.5184963941574097\n",
      "Epoch 4 | Train Loss: 1.5187047719955444 | Test Loss: 1.511275291442871\n",
      "Epoch 4 | Train Loss: 1.5116302967071533 | Test Loss: 1.5040452480316162\n",
      "Epoch 4 | Train Loss: 1.5013818740844727 | Test Loss: 1.4968042373657227\n",
      "Epoch 4 | Train Loss: 1.496119499206543 | Test Loss: 1.4895517826080322\n",
      "Epoch 4 | Train Loss: 1.4895665645599365 | Test Loss: 1.482289433479309\n",
      "Epoch 4 | Train Loss: 1.4790890216827393 | Test Loss: 1.4750200510025024\n",
      "Epoch 4 | Train Loss: 1.4735925197601318 | Test Loss: 1.4677493572235107\n",
      "Epoch 4 | Train Loss: 1.4671295881271362 | Test Loss: 1.4604634046554565\n",
      "Epoch 4 | Train Loss: 1.4600375890731812 | Test Loss: 1.4531662464141846\n",
      "Epoch 4 | Train Loss: 1.4543077945709229 | Test Loss: 1.445851445198059\n",
      "Epoch 4 | Train Loss: 1.4427134990692139 | Test Loss: 1.438511610031128\n",
      "Epoch 4 | Train Loss: 1.4390324354171753 | Test Loss: 1.4311562776565552\n",
      "Epoch 4 | Train Loss: 1.4333552122116089 | Test Loss: 1.4237885475158691\n",
      "Epoch 4 | Train Loss: 1.4257348775863647 | Test Loss: 1.4164128303527832\n",
      "Epoch 4 | Train Loss: 1.4158766269683838 | Test Loss: 1.4090248346328735\n",
      "Epoch 4 | Train Loss: 1.40950608253479 | Test Loss: 1.4016249179840088\n",
      "Epoch 5 | Train Loss: 1.4024966955184937 | Test Loss: 1.394207239151001\n",
      "Epoch 5 | Train Loss: 1.3988784551620483 | Test Loss: 1.386773705482483\n",
      "Epoch 5 | Train Loss: 1.3877941370010376 | Test Loss: 1.3793330192565918\n",
      "Epoch 5 | Train Loss: 1.3780978918075562 | Test Loss: 1.3718914985656738\n",
      "Epoch 5 | Train Loss: 1.373323678970337 | Test Loss: 1.3644320964813232\n",
      "Epoch 5 | Train Loss: 1.3633731603622437 | Test Loss: 1.3569492101669312\n",
      "Epoch 5 | Train Loss: 1.3592733144760132 | Test Loss: 1.349462628364563\n",
      "Epoch 5 | Train Loss: 1.3486770391464233 | Test Loss: 1.3419654369354248\n",
      "Epoch 5 | Train Loss: 1.3454887866973877 | Test Loss: 1.334459662437439\n",
      "Epoch 5 | Train Loss: 1.3342660665512085 | Test Loss: 1.3269528150558472\n",
      "Epoch 5 | Train Loss: 1.327667236328125 | Test Loss: 1.3194289207458496\n",
      "Epoch 5 | Train Loss: 1.316607117652893 | Test Loss: 1.311886191368103\n",
      "Epoch 5 | Train Loss: 1.3109402656555176 | Test Loss: 1.3043254613876343\n",
      "Epoch 5 | Train Loss: 1.3041112422943115 | Test Loss: 1.2967520952224731\n",
      "Epoch 5 | Train Loss: 1.2939033508300781 | Test Loss: 1.2891795635223389\n",
      "Epoch 5 | Train Loss: 1.2885102033615112 | Test Loss: 1.281610369682312\n",
      "Epoch 5 | Train Loss: 1.280868411064148 | Test Loss: 1.274038553237915\n",
      "Epoch 5 | Train Loss: 1.2736490964889526 | Test Loss: 1.2664679288864136\n",
      "Epoch 5 | Train Loss: 1.267554521560669 | Test Loss: 1.2589004039764404\n",
      "Epoch 5 | Train Loss: 1.255190134048462 | Test Loss: 1.2513320446014404\n",
      "Epoch 5 | Train Loss: 1.251255989074707 | Test Loss: 1.2437630891799927\n",
      "Epoch 5 | Train Loss: 1.245657205581665 | Test Loss: 1.2361993789672852\n",
      "Epoch 5 | Train Loss: 1.237523078918457 | Test Loss: 1.2286362648010254\n",
      "Epoch 5 | Train Loss: 1.2274458408355713 | Test Loss: 1.2210766077041626\n",
      "Epoch 5 | Train Loss: 1.220441460609436 | Test Loss: 1.2135202884674072\n",
      "Epoch 6 | Train Loss: 1.2135523557662964 | Test Loss: 1.2059766054153442\n",
      "Epoch 6 | Train Loss: 1.2101370096206665 | Test Loss: 1.1984504461288452\n",
      "Epoch 6 | Train Loss: 1.199493646621704 | Test Loss: 1.1909449100494385\n",
      "Epoch 6 | Train Loss: 1.189619541168213 | Test Loss: 1.1834462881088257\n",
      "Epoch 6 | Train Loss: 1.1840837001800537 | Test Loss: 1.1759552955627441\n",
      "Epoch 6 | Train Loss: 1.1745407581329346 | Test Loss: 1.1684739589691162\n",
      "Epoch 6 | Train Loss: 1.1704031229019165 | Test Loss: 1.1610153913497925\n",
      "Epoch 6 | Train Loss: 1.1599200963974 | Test Loss: 1.153571605682373\n",
      "Epoch 6 | Train Loss: 1.1568820476531982 | Test Loss: 1.1461503505706787\n",
      "Epoch 6 | Train Loss: 1.1456310749053955 | Test Loss: 1.1387447118759155\n",
      "Epoch 6 | Train Loss: 1.1393966674804688 | Test Loss: 1.1313539743423462\n",
      "Epoch 6 | Train Loss: 1.1286373138427734 | Test Loss: 1.1239761114120483\n",
      "Epoch 6 | Train Loss: 1.1229215860366821 | Test Loss: 1.116632103919983\n",
      "Epoch 6 | Train Loss: 1.1158618927001953 | Test Loss: 1.1093024015426636\n",
      "Epoch 6 | Train Loss: 1.1068572998046875 | Test Loss: 1.1019871234893799\n",
      "Epoch 6 | Train Loss: 1.1015887260437012 | Test Loss: 1.0946890115737915\n",
      "Epoch 6 | Train Loss: 1.0933362245559692 | Test Loss: 1.0874007940292358\n",
      "Epoch 6 | Train Loss: 1.0867807865142822 | Test Loss: 1.0801340341567993\n",
      "Epoch 6 | Train Loss: 1.080963134765625 | Test Loss: 1.0728976726531982\n",
      "Epoch 6 | Train Loss: 1.0689767599105835 | Test Loss: 1.0656973123550415\n",
      "Epoch 6 | Train Loss: 1.0649529695510864 | Test Loss: 1.0585227012634277\n",
      "Epoch 6 | Train Loss: 1.0599759817123413 | Test Loss: 1.0513670444488525\n",
      "Epoch 6 | Train Loss: 1.0525137186050415 | Test Loss: 1.0442347526550293\n",
      "Epoch 6 | Train Loss: 1.0428669452667236 | Test Loss: 1.0371190309524536\n",
      "Epoch 6 | Train Loss: 1.035283088684082 | Test Loss: 1.0300228595733643\n",
      "Epoch 7 | Train Loss: 1.0294135808944702 | Test Loss: 1.022947907447815\n",
      "Epoch 7 | Train Loss: 1.0269181728363037 | Test Loss: 1.0158958435058594\n",
      "Epoch 7 | Train Loss: 1.0173993110656738 | Test Loss: 1.0088658332824707\n",
      "Epoch 7 | Train Loss: 1.0084823369979858 | Test Loss: 1.0018550157546997\n",
      "Epoch 7 | Train Loss: 1.0022028684616089 | Test Loss: 0.9948722720146179\n",
      "Epoch 7 | Train Loss: 0.993304431438446 | Test Loss: 0.9879167079925537\n",
      "Epoch 7 | Train Loss: 0.9899484515190125 | Test Loss: 0.980980396270752\n",
      "Epoch 7 | Train Loss: 0.9801169633865356 | Test Loss: 0.9740669131278992\n",
      "Epoch 7 | Train Loss: 0.9765207171440125 | Test Loss: 0.9671778678894043\n",
      "Epoch 7 | Train Loss: 0.9660341143608093 | Test Loss: 0.9603157639503479\n",
      "Epoch 7 | Train Loss: 0.9612275958061218 | Test Loss: 0.9534738659858704\n",
      "Epoch 7 | Train Loss: 0.9511514902114868 | Test Loss: 0.9466604590415955\n",
      "Epoch 7 | Train Loss: 0.9456373453140259 | Test Loss: 0.9398780465126038\n",
      "Epoch 7 | Train Loss: 0.938838005065918 | Test Loss: 0.9331268668174744\n",
      "Epoch 7 | Train Loss: 0.9313018918037415 | Test Loss: 0.9264137744903564\n",
      "Epoch 7 | Train Loss: 0.9262847304344177 | Test Loss: 0.9197242856025696\n",
      "Epoch 7 | Train Loss: 0.9186415076255798 | Test Loss: 0.9130686521530151\n",
      "Epoch 7 | Train Loss: 0.9118366241455078 | Test Loss: 0.90644371509552\n",
      "Epoch 7 | Train Loss: 0.9071716666221619 | Test Loss: 0.8998476266860962\n",
      "Epoch 7 | Train Loss: 0.8967193961143494 | Test Loss: 0.8932870626449585\n",
      "Epoch 7 | Train Loss: 0.8926456570625305 | Test Loss: 0.8867524862289429\n",
      "Epoch 7 | Train Loss: 0.8876091241836548 | Test Loss: 0.880243182182312\n",
      "Epoch 7 | Train Loss: 0.8811718821525574 | Test Loss: 0.8737646341323853\n",
      "Epoch 7 | Train Loss: 0.8720365762710571 | Test Loss: 0.8673164248466492\n",
      "Epoch 7 | Train Loss: 0.8645796775817871 | Test Loss: 0.8608953952789307\n",
      "Epoch 8 | Train Loss: 0.8603849411010742 | Test Loss: 0.8545000553131104\n",
      "Epoch 8 | Train Loss: 0.8585699200630188 | Test Loss: 0.8481248617172241\n",
      "Epoch 8 | Train Loss: 0.8503279685974121 | Test Loss: 0.8417849540710449\n",
      "Epoch 8 | Train Loss: 0.8422260284423828 | Test Loss: 0.8354752063751221\n",
      "Epoch 8 | Train Loss: 0.835784912109375 | Test Loss: 0.8291943073272705\n",
      "Epoch 8 | Train Loss: 0.8279604315757751 | Test Loss: 0.8229511976242065\n",
      "Epoch 8 | Train Loss: 0.8246532678604126 | Test Loss: 0.8167142271995544\n",
      "Epoch 8 | Train Loss: 0.8158052563667297 | Test Loss: 0.810497522354126\n",
      "Epoch 8 | Train Loss: 0.8120414614677429 | Test Loss: 0.8043147325515747\n",
      "Epoch 8 | Train Loss: 0.8027755618095398 | Test Loss: 0.7981643080711365\n",
      "Epoch 8 | Train Loss: 0.7989053130149841 | Test Loss: 0.7920323610305786\n",
      "Epoch 8 | Train Loss: 0.7899541258811951 | Test Loss: 0.7859392166137695\n",
      "Epoch 8 | Train Loss: 0.7851510047912598 | Test Loss: 0.7798784375190735\n",
      "Epoch 8 | Train Loss: 0.7788621783256531 | Test Loss: 0.773865282535553\n",
      "Epoch 8 | Train Loss: 0.7727290391921997 | Test Loss: 0.7678971290588379\n",
      "Epoch 8 | Train Loss: 0.7674376368522644 | Test Loss: 0.7619613409042358\n",
      "Epoch 8 | Train Loss: 0.7611474990844727 | Test Loss: 0.7560558915138245\n",
      "Epoch 8 | Train Loss: 0.7543792724609375 | Test Loss: 0.7501758337020874\n",
      "Epoch 8 | Train Loss: 0.7510084509849548 | Test Loss: 0.7443307042121887\n",
      "Epoch 8 | Train Loss: 0.7420864105224609 | Test Loss: 0.738517165184021\n",
      "Epoch 8 | Train Loss: 0.738060474395752 | Test Loss: 0.7327330708503723\n",
      "Epoch 8 | Train Loss: 0.7327654957771301 | Test Loss: 0.7269933819770813\n",
      "Epoch 8 | Train Loss: 0.7274026274681091 | Test Loss: 0.7212759852409363\n",
      "Epoch 8 | Train Loss: 0.7193423509597778 | Test Loss: 0.715600311756134\n",
      "Epoch 8 | Train Loss: 0.7116705179214478 | Test Loss: 0.7099524140357971\n",
      "Epoch 9 | Train Loss: 0.7097768783569336 | Test Loss: 0.7043389678001404\n",
      "Epoch 9 | Train Loss: 0.7076236009597778 | Test Loss: 0.6987570524215698\n",
      "Epoch 9 | Train Loss: 0.7009926438331604 | Test Loss: 0.6932195425033569\n",
      "Epoch 9 | Train Loss: 0.693109929561615 | Test Loss: 0.6877333521842957\n",
      "Epoch 9 | Train Loss: 0.6874114274978638 | Test Loss: 0.682288408279419\n",
      "Epoch 9 | Train Loss: 0.681181788444519 | Test Loss: 0.6768712997436523\n",
      "Epoch 9 | Train Loss: 0.6778626441955566 | Test Loss: 0.671473503112793\n",
      "Epoch 9 | Train Loss: 0.6693955659866333 | Test Loss: 0.6660988330841064\n",
      "Epoch 9 | Train Loss: 0.6664566993713379 | Test Loss: 0.6607766151428223\n",
      "Epoch 9 | Train Loss: 0.658876359462738 | Test Loss: 0.6555222868919373\n",
      "Epoch 9 | Train Loss: 0.6558884978294373 | Test Loss: 0.6503053903579712\n",
      "Epoch 9 | Train Loss: 0.6480535268783569 | Test Loss: 0.6451337337493896\n",
      "Epoch 9 | Train Loss: 0.6440576910972595 | Test Loss: 0.6400013566017151\n",
      "Epoch 9 | Train Loss: 0.6392080187797546 | Test Loss: 0.6349166035652161\n",
      "Epoch 9 | Train Loss: 0.6337485313415527 | Test Loss: 0.6298783421516418\n",
      "Epoch 9 | Train Loss: 0.6291672587394714 | Test Loss: 0.6248741745948792\n",
      "Epoch 9 | Train Loss: 0.6245344877243042 | Test Loss: 0.6199231147766113\n",
      "Epoch 9 | Train Loss: 0.6178361773490906 | Test Loss: 0.6150259971618652\n",
      "Epoch 9 | Train Loss: 0.6157955527305603 | Test Loss: 0.6101660132408142\n",
      "Epoch 9 | Train Loss: 0.608303427696228 | Test Loss: 0.6053406000137329\n",
      "Epoch 9 | Train Loss: 0.6049532294273376 | Test Loss: 0.60056072473526\n",
      "Epoch 9 | Train Loss: 0.5999603271484375 | Test Loss: 0.595853865146637\n",
      "Epoch 9 | Train Loss: 0.5949479341506958 | Test Loss: 0.5911838412284851\n",
      "Epoch 9 | Train Loss: 0.5899679064750671 | Test Loss: 0.5865553617477417\n",
      "Epoch 9 | Train Loss: 0.5821078419685364 | Test Loss: 0.5819587111473083\n",
      "Epoch 10 | Train Loss: 0.5811200141906738 | Test Loss: 0.5773965120315552\n",
      "Epoch 10 | Train Loss: 0.5796095132827759 | Test Loss: 0.57286536693573\n",
      "Epoch 10 | Train Loss: 0.5743395686149597 | Test Loss: 0.5683560967445374\n",
      "Epoch 10 | Train Loss: 0.5675061941146851 | Test Loss: 0.5638928413391113\n",
      "Epoch 10 | Train Loss: 0.562650203704834 | Test Loss: 0.5594871640205383\n",
      "Epoch 10 | Train Loss: 0.5576659440994263 | Test Loss: 0.5551310181617737\n",
      "Epoch 10 | Train Loss: 0.5553655624389648 | Test Loss: 0.550774872303009\n",
      "Epoch 10 | Train Loss: 0.5480448007583618 | Test Loss: 0.5464272499084473\n",
      "Epoch 10 | Train Loss: 0.5458937287330627 | Test Loss: 0.5421394109725952\n",
      "Epoch 10 | Train Loss: 0.5400266051292419 | Test Loss: 0.5379182696342468\n",
      "Epoch 10 | Train Loss: 0.5382472276687622 | Test Loss: 0.5337266325950623\n",
      "Epoch 10 | Train Loss: 0.5311651229858398 | Test Loss: 0.5295699834823608\n",
      "Epoch 10 | Train Loss: 0.5280467867851257 | Test Loss: 0.5254478454589844\n",
      "Epoch 10 | Train Loss: 0.5248160362243652 | Test Loss: 0.5213748216629028\n",
      "Epoch 10 | Train Loss: 0.5203617215156555 | Test Loss: 0.5173560380935669\n",
      "Epoch 10 | Train Loss: 0.5165301561355591 | Test Loss: 0.5133762359619141\n",
      "Epoch 10 | Train Loss: 0.512948215007782 | Test Loss: 0.5094034075737\n",
      "Epoch 10 | Train Loss: 0.5076016783714294 | Test Loss: 0.5054797530174255\n",
      "Epoch 10 | Train Loss: 0.506658673286438 | Test Loss: 0.5016050338745117\n",
      "Epoch 10 | Train Loss: 0.5008507966995239 | Test Loss: 0.4977721571922302\n",
      "Epoch 10 | Train Loss: 0.4975510537624359 | Test Loss: 0.4939843714237213\n",
      "Epoch 10 | Train Loss: 0.49331244826316833 | Test Loss: 0.49025917053222656\n",
      "Epoch 10 | Train Loss: 0.4883675277233124 | Test Loss: 0.4865545928478241\n",
      "Epoch 10 | Train Loss: 0.4854700565338135 | Test Loss: 0.48288777470588684\n",
      "Epoch 10 | Train Loss: 0.47827574610710144 | Test Loss: 0.479225218296051\n",
      "Epoch 11 | Train Loss: 0.478757381439209 | Test Loss: 0.47562718391418457\n",
      "Epoch 11 | Train Loss: 0.47729364037513733 | Test Loss: 0.47207918763160706\n",
      "Epoch 11 | Train Loss: 0.47272273898124695 | Test Loss: 0.468549907207489\n",
      "Epoch 11 | Train Loss: 0.4673982560634613 | Test Loss: 0.4650631546974182\n",
      "Epoch 11 | Train Loss: 0.46366986632347107 | Test Loss: 0.4616253972053528\n",
      "Epoch 11 | Train Loss: 0.45960626006126404 | Test Loss: 0.45820656418800354\n",
      "Epoch 11 | Train Loss: 0.4577338397502899 | Test Loss: 0.45483270287513733\n",
      "Epoch 11 | Train Loss: 0.4516647458076477 | Test Loss: 0.451485812664032\n",
      "Epoch 11 | Train Loss: 0.451020210981369 | Test Loss: 0.44817864894866943\n",
      "Epoch 11 | Train Loss: 0.44607996940612793 | Test Loss: 0.44490906596183777\n",
      "Epoch 11 | Train Loss: 0.44512444734573364 | Test Loss: 0.44166281819343567\n",
      "Epoch 11 | Train Loss: 0.43907052278518677 | Test Loss: 0.43848907947540283\n",
      "Epoch 11 | Train Loss: 0.4364778995513916 | Test Loss: 0.4353441298007965\n",
      "Epoch 11 | Train Loss: 0.43473339080810547 | Test Loss: 0.4322264492511749\n",
      "Epoch 11 | Train Loss: 0.4315340220928192 | Test Loss: 0.4291209578514099\n",
      "Epoch 11 | Train Loss: 0.427924245595932 | Test Loss: 0.42602410912513733\n",
      "Epoch 11 | Train Loss: 0.4252242147922516 | Test Loss: 0.42296770215034485\n",
      "Epoch 11 | Train Loss: 0.42133474349975586 | Test Loss: 0.41996729373931885\n",
      "Epoch 11 | Train Loss: 0.421680748462677 | Test Loss: 0.4169989228248596\n",
      "Epoch 11 | Train Loss: 0.4173555374145508 | Test Loss: 0.414058119058609\n",
      "Epoch 11 | Train Loss: 0.4141562879085541 | Test Loss: 0.4111575186252594\n",
      "Epoch 11 | Train Loss: 0.4099847376346588 | Test Loss: 0.408282995223999\n",
      "Epoch 11 | Train Loss: 0.40512892603874207 | Test Loss: 0.40539759397506714\n",
      "Epoch 11 | Train Loss: 0.4048338830471039 | Test Loss: 0.4025748074054718\n",
      "Epoch 11 | Train Loss: 0.39835092425346375 | Test Loss: 0.3997877538204193\n",
      "Epoch 12 | Train Loss: 0.39926156401634216 | Test Loss: 0.3970378041267395\n",
      "Epoch 12 | Train Loss: 0.3978136479854584 | Test Loss: 0.39427629113197327\n",
      "Epoch 12 | Train Loss: 0.3945181667804718 | Test Loss: 0.39156144857406616\n",
      "Epoch 12 | Train Loss: 0.3904227316379547 | Test Loss: 0.3889107406139374\n",
      "Epoch 12 | Train Loss: 0.3874359130859375 | Test Loss: 0.38626188039779663\n",
      "Epoch 12 | Train Loss: 0.38462409377098083 | Test Loss: 0.38361531496047974\n",
      "Epoch 12 | Train Loss: 0.38271379470825195 | Test Loss: 0.38105037808418274\n",
      "Epoch 12 | Train Loss: 0.37767231464385986 | Test Loss: 0.37850597500801086\n",
      "Epoch 12 | Train Loss: 0.37785494327545166 | Test Loss: 0.3759794533252716\n",
      "Epoch 12 | Train Loss: 0.3740485608577728 | Test Loss: 0.3734542727470398\n",
      "Epoch 12 | Train Loss: 0.3740396499633789 | Test Loss: 0.3709650933742523\n",
      "Epoch 12 | Train Loss: 0.36829879879951477 | Test Loss: 0.36854052543640137\n",
      "Epoch 12 | Train Loss: 0.36617884039878845 | Test Loss: 0.3661207854747772\n",
      "Epoch 12 | Train Loss: 0.36551886796951294 | Test Loss: 0.3637239933013916\n",
      "Epoch 12 | Train Loss: 0.3638518452644348 | Test Loss: 0.3613165616989136\n",
      "Epoch 12 | Train Loss: 0.3595464825630188 | Test Loss: 0.3589356243610382\n",
      "Epoch 12 | Train Loss: 0.3583734929561615 | Test Loss: 0.3566095530986786\n",
      "Epoch 12 | Train Loss: 0.3547612726688385 | Test Loss: 0.3543119430541992\n",
      "Epoch 12 | Train Loss: 0.3559376895427704 | Test Loss: 0.352022647857666\n",
      "Epoch 12 | Train Loss: 0.3530054986476898 | Test Loss: 0.3497845530509949\n",
      "Epoch 12 | Train Loss: 0.34995561838150024 | Test Loss: 0.3476093113422394\n",
      "Epoch 12 | Train Loss: 0.3465057909488678 | Test Loss: 0.345441609621048\n",
      "Epoch 12 | Train Loss: 0.34142762422561646 | Test Loss: 0.3432202637195587\n",
      "Epoch 12 | Train Loss: 0.343079537153244 | Test Loss: 0.34106844663619995\n",
      "Epoch 12 | Train Loss: 0.33795052766799927 | Test Loss: 0.339000403881073\n",
      "Epoch 13 | Train Loss: 0.33873286843299866 | Test Loss: 0.33693209290504456\n",
      "Epoch 13 | Train Loss: 0.33730724453926086 | Test Loss: 0.3348314166069031\n",
      "Epoch 13 | Train Loss: 0.33518028259277344 | Test Loss: 0.3327457010746002\n",
      "Epoch 13 | Train Loss: 0.332165002822876 | Test Loss: 0.3307288885116577\n",
      "Epoch 13 | Train Loss: 0.32931646704673767 | Test Loss: 0.32872310280799866\n",
      "Epoch 13 | Train Loss: 0.3273817002773285 | Test Loss: 0.3267139792442322\n",
      "Epoch 13 | Train Loss: 0.32596760988235474 | Test Loss: 0.32477447390556335\n",
      "Epoch 13 | Train Loss: 0.3213613033294678 | Test Loss: 0.32288092374801636\n",
      "Epoch 13 | Train Loss: 0.322164386510849 | Test Loss: 0.32098421454429626\n",
      "Epoch 13 | Train Loss: 0.31954964995384216 | Test Loss: 0.3190506100654602\n",
      "Epoch 13 | Train Loss: 0.32003337144851685 | Test Loss: 0.31713926792144775\n",
      "Epoch 13 | Train Loss: 0.31436070799827576 | Test Loss: 0.3153120279312134\n",
      "Epoch 13 | Train Loss: 0.312896192073822 | Test Loss: 0.3135032057762146\n",
      "Epoch 13 | Train Loss: 0.31269603967666626 | Test Loss: 0.3117035925388336\n",
      "Epoch 13 | Train Loss: 0.3122926354408264 | Test Loss: 0.3098810315132141\n",
      "Epoch 13 | Train Loss: 0.30789217352867126 | Test Loss: 0.30810415744781494\n",
      "Epoch 13 | Train Loss: 0.307935506105423 | Test Loss: 0.3063478469848633\n",
      "Epoch 13 | Train Loss: 0.3048500716686249 | Test Loss: 0.3045773506164551\n",
      "Epoch 13 | Train Loss: 0.30603939294815063 | Test Loss: 0.30279743671417236\n",
      "Epoch 13 | Train Loss: 0.30416980385780334 | Test Loss: 0.3010784387588501\n",
      "Epoch 13 | Train Loss: 0.3011098802089691 | Test Loss: 0.2994506061077118\n",
      "Epoch 13 | Train Loss: 0.2980964183807373 | Test Loss: 0.297854483127594\n",
      "Epoch 13 | Train Loss: 0.2937873303890228 | Test Loss: 0.29615285992622375\n",
      "Epoch 13 | Train Loss: 0.2962450087070465 | Test Loss: 0.294494092464447\n",
      "Epoch 13 | Train Loss: 0.29240307211875916 | Test Loss: 0.2929491102695465\n",
      "Epoch 14 | Train Loss: 0.29268553853034973 | Test Loss: 0.2914009988307953\n",
      "Epoch 14 | Train Loss: 0.291952520608902 | Test Loss: 0.2897917330265045\n",
      "Epoch 14 | Train Loss: 0.29002997279167175 | Test Loss: 0.28820741176605225\n",
      "Epoch 14 | Train Loss: 0.28818342089653015 | Test Loss: 0.28667765855789185\n",
      "Epoch 14 | Train Loss: 0.2853880822658539 | Test Loss: 0.2851739525794983\n",
      "Epoch 14 | Train Loss: 0.2841606140136719 | Test Loss: 0.28365230560302734\n",
      "Epoch 14 | Train Loss: 0.2833395004272461 | Test Loss: 0.28216850757598877\n",
      "Epoch 14 | Train Loss: 0.2789805233478546 | Test Loss: 0.28073734045028687\n",
      "Epoch 14 | Train Loss: 0.2801089882850647 | Test Loss: 0.27932581305503845\n",
      "Epoch 14 | Train Loss: 0.27753350138664246 | Test Loss: 0.277866005897522\n",
      "Epoch 14 | Train Loss: 0.2792561948299408 | Test Loss: 0.2763904333114624\n",
      "Epoch 14 | Train Loss: 0.2736336290836334 | Test Loss: 0.27498483657836914\n",
      "Epoch 14 | Train Loss: 0.27243801951408386 | Test Loss: 0.2736199200153351\n",
      "Epoch 14 | Train Loss: 0.27284690737724304 | Test Loss: 0.272266149520874\n",
      "Epoch 14 | Train Loss: 0.27309155464172363 | Test Loss: 0.27090930938720703\n",
      "Epoch 14 | Train Loss: 0.2690657675266266 | Test Loss: 0.2695808708667755\n",
      "Epoch 14 | Train Loss: 0.26926952600479126 | Test Loss: 0.2682460844516754\n",
      "Epoch 14 | Train Loss: 0.2669994831085205 | Test Loss: 0.2669150233268738\n",
      "Epoch 14 | Train Loss: 0.26850685477256775 | Test Loss: 0.2655865252017975\n",
      "Epoch 14 | Train Loss: 0.267285019159317 | Test Loss: 0.2643025815486908\n",
      "Epoch 14 | Train Loss: 0.26418566703796387 | Test Loss: 0.2630919814109802\n",
      "Epoch 14 | Train Loss: 0.2616073787212372 | Test Loss: 0.2618959844112396\n",
      "Epoch 14 | Train Loss: 0.25776946544647217 | Test Loss: 0.26058968901634216\n",
      "Epoch 14 | Train Loss: 0.260447233915329 | Test Loss: 0.2593490183353424\n",
      "Epoch 14 | Train Loss: 0.2574920952320099 | Test Loss: 0.2582297921180725\n",
      "Epoch 15 | Train Loss: 0.2578926980495453 | Test Loss: 0.2570931017398834\n",
      "Epoch 15 | Train Loss: 0.2578549087047577 | Test Loss: 0.25587838888168335\n",
      "Epoch 15 | Train Loss: 0.25606057047843933 | Test Loss: 0.2546853721141815\n",
      "Epoch 15 | Train Loss: 0.25490590929985046 | Test Loss: 0.2535421550273895\n",
      "Epoch 15 | Train Loss: 0.2522229850292206 | Test Loss: 0.2524178922176361\n",
      "Epoch 15 | Train Loss: 0.2515032887458801 | Test Loss: 0.25126713514328003\n",
      "Epoch 15 | Train Loss: 0.2510205805301666 | Test Loss: 0.25014224648475647\n",
      "Epoch 15 | Train Loss: 0.2471090853214264 | Test Loss: 0.24906256794929504\n",
      "Epoch 15 | Train Loss: 0.24836678802967072 | Test Loss: 0.24799911677837372\n",
      "Epoch 15 | Train Loss: 0.24592964351177216 | Test Loss: 0.2469218671321869\n",
      "Epoch 15 | Train Loss: 0.2484298199415207 | Test Loss: 0.24581463634967804\n",
      "Epoch 15 | Train Loss: 0.2432345449924469 | Test Loss: 0.24477356672286987\n",
      "Epoch 15 | Train Loss: 0.24233916401863098 | Test Loss: 0.2437877058982849\n",
      "Epoch 15 | Train Loss: 0.2431778758764267 | Test Loss: 0.24278342723846436\n",
      "Epoch 15 | Train Loss: 0.2433936595916748 | Test Loss: 0.24173806607723236\n",
      "Epoch 15 | Train Loss: 0.2402774840593338 | Test Loss: 0.24070729315280914\n",
      "Epoch 15 | Train Loss: 0.24031367897987366 | Test Loss: 0.23969101905822754\n",
      "Epoch 15 | Train Loss: 0.23855482041835785 | Test Loss: 0.23871056735515594\n",
      "Epoch 15 | Train Loss: 0.2404986172914505 | Test Loss: 0.23772728443145752\n",
      "Epoch 15 | Train Loss: 0.2395923137664795 | Test Loss: 0.23676493763923645\n",
      "Epoch 15 | Train Loss: 0.2366354912519455 | Test Loss: 0.2358548492193222\n",
      "Epoch 15 | Train Loss: 0.23432783782482147 | Test Loss: 0.23496071994304657\n",
      "Epoch 15 | Train Loss: 0.23081515729427338 | Test Loss: 0.2339945137500763\n",
      "Epoch 15 | Train Loss: 0.23381665349006653 | Test Loss: 0.23303475975990295\n",
      "Epoch 15 | Train Loss: 0.23140031099319458 | Test Loss: 0.23218154907226562\n",
      "Epoch 16 | Train Loss: 0.23146763443946838 | Test Loss: 0.2313569337129593\n",
      "Epoch 16 | Train Loss: 0.23210707306861877 | Test Loss: 0.230452761054039\n",
      "Epoch 16 | Train Loss: 0.23050403594970703 | Test Loss: 0.22951579093933105\n",
      "Epoch 16 | Train Loss: 0.22987833619117737 | Test Loss: 0.22862949967384338\n",
      "Epoch 16 | Train Loss: 0.2274158000946045 | Test Loss: 0.22779278457164764\n",
      "Epoch 16 | Train Loss: 0.2271028608083725 | Test Loss: 0.22694170475006104\n",
      "Epoch 16 | Train Loss: 0.22664372622966766 | Test Loss: 0.22606903314590454\n",
      "Epoch 16 | Train Loss: 0.22318606078624725 | Test Loss: 0.2252342402935028\n",
      "Epoch 16 | Train Loss: 0.22435565292835236 | Test Loss: 0.22444576025009155\n",
      "Epoch 16 | Train Loss: 0.22205238044261932 | Test Loss: 0.2236519455909729\n",
      "Epoch 16 | Train Loss: 0.22514736652374268 | Test Loss: 0.2227759212255478\n",
      "Epoch 16 | Train Loss: 0.22032813727855682 | Test Loss: 0.22198088467121124\n",
      "Epoch 16 | Train Loss: 0.21974658966064453 | Test Loss: 0.22124223411083221\n",
      "Epoch 16 | Train Loss: 0.2207561582326889 | Test Loss: 0.2204725444316864\n",
      "Epoch 16 | Train Loss: 0.22115258872509003 | Test Loss: 0.21964767575263977\n",
      "Epoch 16 | Train Loss: 0.21841835975646973 | Test Loss: 0.21882928907871246\n",
      "Epoch 16 | Train Loss: 0.21839956939220428 | Test Loss: 0.2180410772562027\n",
      "Epoch 16 | Train Loss: 0.21702991425991058 | Test Loss: 0.21730373799800873\n",
      "Epoch 16 | Train Loss: 0.21918904781341553 | Test Loss: 0.2165669947862625\n",
      "Epoch 16 | Train Loss: 0.2184329330921173 | Test Loss: 0.2158045619726181\n",
      "Epoch 16 | Train Loss: 0.21570110321044922 | Test Loss: 0.21506421267986298\n",
      "Epoch 16 | Train Loss: 0.2135145217180252 | Test Loss: 0.21436339616775513\n",
      "Epoch 16 | Train Loss: 0.2102549523115158 | Test Loss: 0.2136605829000473\n",
      "Epoch 16 | Train Loss: 0.21338753402233124 | Test Loss: 0.21292753517627716\n",
      "Epoch 16 | Train Loss: 0.21142072975635529 | Test Loss: 0.212230384349823\n",
      "Epoch 17 | Train Loss: 0.2110956460237503 | Test Loss: 0.21158328652381897\n",
      "Epoch 17 | Train Loss: 0.21224157512187958 | Test Loss: 0.21090485155582428\n",
      "Epoch 17 | Train Loss: 0.2109140157699585 | Test Loss: 0.2101823389530182\n",
      "Epoch 17 | Train Loss: 0.2107030153274536 | Test Loss: 0.20946571230888367\n",
      "Epoch 17 | Train Loss: 0.208353191614151 | Test Loss: 0.20879364013671875\n",
      "Epoch 17 | Train Loss: 0.2082734853029251 | Test Loss: 0.20814067125320435\n",
      "Epoch 17 | Train Loss: 0.20782141387462616 | Test Loss: 0.2074822038412094\n",
      "Epoch 17 | Train Loss: 0.2048613280057907 | Test Loss: 0.20683184266090393\n",
      "Epoch 17 | Train Loss: 0.2058573216199875 | Test Loss: 0.2061968594789505\n",
      "Epoch 17 | Train Loss: 0.20360277593135834 | Test Loss: 0.2055957317352295\n",
      "Epoch 17 | Train Loss: 0.2070542871952057 | Test Loss: 0.20489494502544403\n",
      "Epoch 17 | Train Loss: 0.20264337956905365 | Test Loss: 0.2042253613471985\n",
      "Epoch 17 | Train Loss: 0.20218968391418457 | Test Loss: 0.20362362265586853\n",
      "Epoch 17 | Train Loss: 0.20331016182899475 | Test Loss: 0.2030559778213501\n",
      "Epoch 17 | Train Loss: 0.2039356678724289 | Test Loss: 0.20244663953781128\n",
      "Epoch 17 | Train Loss: 0.2013901323080063 | Test Loss: 0.20180070400238037\n",
      "Epoch 17 | Train Loss: 0.20119400322437286 | Test Loss: 0.20117047429084778\n",
      "Epoch 17 | Train Loss: 0.20008860528469086 | Test Loss: 0.20059071481227875\n",
      "Epoch 17 | Train Loss: 0.20253919064998627 | Test Loss: 0.2000388652086258\n",
      "Epoch 17 | Train Loss: 0.20185816287994385 | Test Loss: 0.19945038855075836\n",
      "Epoch 17 | Train Loss: 0.19931595027446747 | Test Loss: 0.19885627925395966\n",
      "Epoch 17 | Train Loss: 0.19732719659805298 | Test Loss: 0.19828824698925018\n",
      "Epoch 17 | Train Loss: 0.19417202472686768 | Test Loss: 0.197750523686409\n",
      "Epoch 17 | Train Loss: 0.19735287129878998 | Test Loss: 0.19719281792640686\n",
      "Epoch 17 | Train Loss: 0.19591760635375977 | Test Loss: 0.1966310739517212\n",
      "Epoch 18 | Train Loss: 0.19513478875160217 | Test Loss: 0.19610300660133362\n",
      "Epoch 18 | Train Loss: 0.19653049111366272 | Test Loss: 0.19557511806488037\n",
      "Epoch 18 | Train Loss: 0.1957939863204956 | Test Loss: 0.19502943754196167\n",
      "Epoch 18 | Train Loss: 0.19556596875190735 | Test Loss: 0.19447778165340424\n",
      "Epoch 18 | Train Loss: 0.19350388646125793 | Test Loss: 0.19392989575862885\n",
      "Epoch 18 | Train Loss: 0.19351167976856232 | Test Loss: 0.19341538846492767\n",
      "Epoch 18 | Train Loss: 0.19315943121910095 | Test Loss: 0.19291648268699646\n",
      "Epoch 18 | Train Loss: 0.19047987461090088 | Test Loss: 0.1924179494380951\n",
      "Epoch 18 | Train Loss: 0.19139714539051056 | Test Loss: 0.19190283119678497\n",
      "Epoch 18 | Train Loss: 0.18939296901226044 | Test Loss: 0.19141609966754913\n",
      "Epoch 18 | Train Loss: 0.19277916848659515 | Test Loss: 0.19089923799037933\n",
      "Epoch 18 | Train Loss: 0.18881098926067352 | Test Loss: 0.1903880536556244\n",
      "Epoch 18 | Train Loss: 0.18843363225460052 | Test Loss: 0.18990527093410492\n",
      "Epoch 18 | Train Loss: 0.18960177898406982 | Test Loss: 0.18943986296653748\n",
      "Epoch 18 | Train Loss: 0.19059942662715912 | Test Loss: 0.1889563947916031\n",
      "Epoch 18 | Train Loss: 0.18794584274291992 | Test Loss: 0.1884622573852539\n",
      "Epoch 18 | Train Loss: 0.18778841197490692 | Test Loss: 0.1879851520061493\n",
      "Epoch 18 | Train Loss: 0.18681439757347107 | Test Loss: 0.1875266134738922\n",
      "Epoch 18 | Train Loss: 0.1894323080778122 | Test Loss: 0.18708369135856628\n",
      "Epoch 18 | Train Loss: 0.18893805146217346 | Test Loss: 0.18663138151168823\n",
      "Epoch 18 | Train Loss: 0.18650443851947784 | Test Loss: 0.18618594110012054\n",
      "Epoch 18 | Train Loss: 0.18480956554412842 | Test Loss: 0.18573366105556488\n",
      "Epoch 18 | Train Loss: 0.1816267967224121 | Test Loss: 0.18529486656188965\n",
      "Epoch 18 | Train Loss: 0.18491294980049133 | Test Loss: 0.18485666811466217\n",
      "Epoch 18 | Train Loss: 0.18378061056137085 | Test Loss: 0.18442578613758087\n",
      "Epoch 19 | Train Loss: 0.18271036446094513 | Test Loss: 0.18401019275188446\n",
      "Epoch 19 | Train Loss: 0.184329092502594 | Test Loss: 0.18359269201755524\n",
      "Epoch 19 | Train Loss: 0.18407593667507172 | Test Loss: 0.18316464126110077\n",
      "Epoch 19 | Train Loss: 0.18379749357700348 | Test Loss: 0.18273577094078064\n",
      "Epoch 19 | Train Loss: 0.18180470168590546 | Test Loss: 0.1823028177022934\n",
      "Epoch 19 | Train Loss: 0.1819431036710739 | Test Loss: 0.18189235031604767\n",
      "Epoch 19 | Train Loss: 0.18169093132019043 | Test Loss: 0.18149767816066742\n",
      "Epoch 19 | Train Loss: 0.17916153371334076 | Test Loss: 0.18110860884189606\n",
      "Epoch 19 | Train Loss: 0.1799498349428177 | Test Loss: 0.18070833384990692\n",
      "Epoch 19 | Train Loss: 0.17821820080280304 | Test Loss: 0.1803271323442459\n",
      "Epoch 19 | Train Loss: 0.1816840022802353 | Test Loss: 0.17991940677165985\n",
      "Epoch 19 | Train Loss: 0.1780623346567154 | Test Loss: 0.17951971292495728\n",
      "Epoch 19 | Train Loss: 0.1775842159986496 | Test Loss: 0.17913872003555298\n",
      "Epoch 19 | Train Loss: 0.1787482351064682 | Test Loss: 0.17876629531383514\n",
      "Epoch 19 | Train Loss: 0.18007934093475342 | Test Loss: 0.1783842295408249\n",
      "Epoch 19 | Train Loss: 0.1773655265569687 | Test Loss: 0.17799752950668335\n",
      "Epoch 19 | Train Loss: 0.1772952377796173 | Test Loss: 0.1776183843612671\n",
      "Epoch 19 | Train Loss: 0.17642006278038025 | Test Loss: 0.17724664509296417\n",
      "Epoch 19 | Train Loss: 0.17900000512599945 | Test Loss: 0.17688903212547302\n",
      "Epoch 19 | Train Loss: 0.1787678301334381 | Test Loss: 0.17653171718120575\n",
      "Epoch 19 | Train Loss: 0.17635047435760498 | Test Loss: 0.17618413269519806\n",
      "Epoch 19 | Train Loss: 0.17488446831703186 | Test Loss: 0.17582526803016663\n",
      "Epoch 19 | Train Loss: 0.1716480553150177 | Test Loss: 0.17546942830085754\n",
      "Epoch 19 | Train Loss: 0.17515650391578674 | Test Loss: 0.1751152127981186\n",
      "Epoch 19 | Train Loss: 0.1741078943014145 | Test Loss: 0.17476540803909302\n",
      "Epoch 20 | Train Loss: 0.17286734282970428 | Test Loss: 0.17442525923252106\n",
      "Epoch 20 | Train Loss: 0.17465992271900177 | Test Loss: 0.17409002780914307\n",
      "Epoch 20 | Train Loss: 0.1747129261493683 | Test Loss: 0.17375348508358002\n",
      "Epoch 20 | Train Loss: 0.17446915805339813 | Test Loss: 0.1734112650156021\n",
      "Epoch 20 | Train Loss: 0.17250296473503113 | Test Loss: 0.1730601042509079\n",
      "Epoch 20 | Train Loss: 0.1726885437965393 | Test Loss: 0.17272944748401642\n",
      "Epoch 20 | Train Loss: 0.17258214950561523 | Test Loss: 0.1724119484424591\n",
      "Epoch 20 | Train Loss: 0.17014388740062714 | Test Loss: 0.17210103571414948\n",
      "Epoch 20 | Train Loss: 0.17076964676380157 | Test Loss: 0.17177876830101013\n",
      "Epoch 20 | Train Loss: 0.16934488713741302 | Test Loss: 0.17147296667099\n",
      "Epoch 20 | Train Loss: 0.1728271245956421 | Test Loss: 0.17113713920116425\n",
      "Epoch 20 | Train Loss: 0.1694522202014923 | Test Loss: 0.1708132028579712\n",
      "Epoch 20 | Train Loss: 0.16891585290431976 | Test Loss: 0.1704995483160019\n",
      "Epoch 20 | Train Loss: 0.17005591094493866 | Test Loss: 0.17019495368003845\n",
      "Epoch 20 | Train Loss: 0.17154861986637115 | Test Loss: 0.16989126801490784\n",
      "Epoch 20 | Train Loss: 0.16889719665050507 | Test Loss: 0.16958552598953247\n",
      "Epoch 20 | Train Loss: 0.16886626183986664 | Test Loss: 0.16928008198738098\n",
      "Epoch 20 | Train Loss: 0.1681489795446396 | Test Loss: 0.16898047924041748\n",
      "Epoch 20 | Train Loss: 0.17063495516777039 | Test Loss: 0.16868698596954346\n",
      "Epoch 20 | Train Loss: 0.1705746203660965 | Test Loss: 0.1683950424194336\n",
      "Epoch 20 | Train Loss: 0.16819320619106293 | Test Loss: 0.16811233758926392\n",
      "Epoch 20 | Train Loss: 0.16686205565929413 | Test Loss: 0.16782240569591522\n",
      "Epoch 20 | Train Loss: 0.16358759999275208 | Test Loss: 0.16753670573234558\n",
      "Epoch 20 | Train Loss: 0.16726341843605042 | Test Loss: 0.16725203394889832\n",
      "Epoch 20 | Train Loss: 0.16629914939403534 | Test Loss: 0.16696874797344208\n",
      "Epoch 21 | Train Loss: 0.16493239998817444 | Test Loss: 0.16668865084648132\n",
      "Epoch 21 | Train Loss: 0.1667860895395279 | Test Loss: 0.1664087176322937\n",
      "Epoch 21 | Train Loss: 0.1671285629272461 | Test Loss: 0.16613365709781647\n",
      "Epoch 21 | Train Loss: 0.16689614951610565 | Test Loss: 0.16585633158683777\n",
      "Epoch 21 | Train Loss: 0.16501004993915558 | Test Loss: 0.16556678712368011\n",
      "Epoch 21 | Train Loss: 0.1652362197637558 | Test Loss: 0.16529139876365662\n",
      "Epoch 21 | Train Loss: 0.16513438522815704 | Test Loss: 0.16502821445465088\n",
      "Epoch 21 | Train Loss: 0.16280806064605713 | Test Loss: 0.16477099061012268\n",
      "Epoch 21 | Train Loss: 0.16334831714630127 | Test Loss: 0.16451002657413483\n",
      "Epoch 21 | Train Loss: 0.16208457946777344 | Test Loss: 0.16425390541553497\n",
      "Epoch 21 | Train Loss: 0.16555070877075195 | Test Loss: 0.1639958769083023\n",
      "Epoch 21 | Train Loss: 0.16241376101970673 | Test Loss: 0.16374553740024567\n",
      "Epoch 21 | Train Loss: 0.161861389875412 | Test Loss: 0.16349320113658905\n",
      "Epoch 21 | Train Loss: 0.16299834847450256 | Test Loss: 0.1632348895072937\n",
      "Epoch 21 | Train Loss: 0.16456662118434906 | Test Loss: 0.16297921538352966\n",
      "Epoch 21 | Train Loss: 0.16207991540431976 | Test Loss: 0.1627233326435089\n",
      "Epoch 21 | Train Loss: 0.16192077100276947 | Test Loss: 0.16247212886810303\n",
      "Epoch 21 | Train Loss: 0.16136661171913147 | Test Loss: 0.1622285544872284\n",
      "Epoch 21 | Train Loss: 0.16380396485328674 | Test Loss: 0.1619848608970642\n",
      "Epoch 21 | Train Loss: 0.16391323506832123 | Test Loss: 0.16174530982971191\n",
      "Epoch 21 | Train Loss: 0.16158144176006317 | Test Loss: 0.16151152551174164\n",
      "Epoch 21 | Train Loss: 0.16037994623184204 | Test Loss: 0.1612745076417923\n",
      "Epoch 21 | Train Loss: 0.15708735585212708 | Test Loss: 0.16103841364383698\n",
      "Epoch 21 | Train Loss: 0.16082245111465454 | Test Loss: 0.16080963611602783\n",
      "Epoch 21 | Train Loss: 0.15993519127368927 | Test Loss: 0.16058748960494995\n",
      "Epoch 22 | Train Loss: 0.15847519040107727 | Test Loss: 0.16036665439605713\n",
      "Epoch 22 | Train Loss: 0.1603250801563263 | Test Loss: 0.16014397144317627\n",
      "Epoch 22 | Train Loss: 0.16091781854629517 | Test Loss: 0.15992043912410736\n",
      "Epoch 22 | Train Loss: 0.16072534024715424 | Test Loss: 0.15968944132328033\n",
      "Epoch 22 | Train Loss: 0.15887616574764252 | Test Loss: 0.15944939851760864\n",
      "Epoch 22 | Train Loss: 0.15917907655239105 | Test Loss: 0.15922187268733978\n",
      "Epoch 22 | Train Loss: 0.15906785428524017 | Test Loss: 0.1590074747800827\n",
      "Epoch 22 | Train Loss: 0.1568106859922409 | Test Loss: 0.15879686176776886\n",
      "Epoch 22 | Train Loss: 0.15740202367305756 | Test Loss: 0.158587247133255\n",
      "Epoch 22 | Train Loss: 0.1561882644891739 | Test Loss: 0.15837770700454712\n",
      "Epoch 22 | Train Loss: 0.1596226990222931 | Test Loss: 0.15816865861415863\n",
      "Epoch 22 | Train Loss: 0.15664660930633545 | Test Loss: 0.15796087682247162\n",
      "Epoch 22 | Train Loss: 0.15603655576705933 | Test Loss: 0.1577550321817398\n",
      "Epoch 22 | Train Loss: 0.15724408626556396 | Test Loss: 0.15754543244838715\n",
      "Epoch 22 | Train Loss: 0.15886779129505157 | Test Loss: 0.15733817219734192\n",
      "Epoch 22 | Train Loss: 0.1564604789018631 | Test Loss: 0.1571313738822937\n",
      "Epoch 22 | Train Loss: 0.15624162554740906 | Test Loss: 0.15692053735256195\n",
      "Epoch 22 | Train Loss: 0.15578870475292206 | Test Loss: 0.15671391785144806\n",
      "Epoch 22 | Train Loss: 0.15819430351257324 | Test Loss: 0.15651121735572815\n",
      "Epoch 22 | Train Loss: 0.15851281583309174 | Test Loss: 0.15631544589996338\n",
      "Epoch 22 | Train Loss: 0.15615713596343994 | Test Loss: 0.15612618625164032\n",
      "Epoch 22 | Train Loss: 0.1550818532705307 | Test Loss: 0.15593455731868744\n",
      "Epoch 22 | Train Loss: 0.15178357064723969 | Test Loss: 0.15574032068252563\n",
      "Epoch 22 | Train Loss: 0.155594140291214 | Test Loss: 0.15555042028427124\n",
      "Epoch 22 | Train Loss: 0.1547612100839615 | Test Loss: 0.15536780655384064\n",
      "Epoch 23 | Train Loss: 0.15318281948566437 | Test Loss: 0.1551893651485443\n",
      "Epoch 23 | Train Loss: 0.15506039559841156 | Test Loss: 0.1550123244524002\n",
      "Epoch 23 | Train Loss: 0.1557629555463791 | Test Loss: 0.154830664396286\n",
      "Epoch 23 | Train Loss: 0.15569491684436798 | Test Loss: 0.15463970601558685\n",
      "Epoch 23 | Train Loss: 0.1538378745317459 | Test Loss: 0.15444619953632355\n",
      "Epoch 23 | Train Loss: 0.154220312833786 | Test Loss: 0.15426155924797058\n",
      "Epoch 23 | Train Loss: 0.15411686897277832 | Test Loss: 0.15408554673194885\n",
      "Epoch 23 | Train Loss: 0.15184538066387177 | Test Loss: 0.15390507876873016\n",
      "Epoch 23 | Train Loss: 0.1525593101978302 | Test Loss: 0.15372979640960693\n",
      "Epoch 23 | Train Loss: 0.15139342844486237 | Test Loss: 0.15355753898620605\n",
      "Epoch 23 | Train Loss: 0.15476660430431366 | Test Loss: 0.15338851511478424\n",
      "Epoch 23 | Train Loss: 0.15193608403205872 | Test Loss: 0.15321695804595947\n",
      "Epoch 23 | Train Loss: 0.1512269526720047 | Test Loss: 0.15304382145404816\n",
      "Epoch 23 | Train Loss: 0.1525033861398697 | Test Loss: 0.15286748111248016\n",
      "Epoch 23 | Train Loss: 0.15419770777225494 | Test Loss: 0.152695432305336\n",
      "Epoch 23 | Train Loss: 0.1518743336200714 | Test Loss: 0.15252719819545746\n",
      "Epoch 23 | Train Loss: 0.15161220729351044 | Test Loss: 0.15235379338264465\n",
      "Epoch 23 | Train Loss: 0.1512126922607422 | Test Loss: 0.1521795094013214\n",
      "Epoch 23 | Train Loss: 0.15362155437469482 | Test Loss: 0.15200559794902802\n",
      "Epoch 23 | Train Loss: 0.1540607511997223 | Test Loss: 0.15184415876865387\n",
      "Epoch 23 | Train Loss: 0.151689812541008 | Test Loss: 0.15169218182563782\n",
      "Epoch 23 | Train Loss: 0.15071092545986176 | Test Loss: 0.1515364646911621\n",
      "Epoch 23 | Train Loss: 0.14737963676452637 | Test Loss: 0.15137112140655518\n",
      "Epoch 23 | Train Loss: 0.151251420378685 | Test Loss: 0.15120556950569153\n",
      "Epoch 23 | Train Loss: 0.1505044847726822 | Test Loss: 0.15104857087135315\n",
      "Epoch 24 | Train Loss: 0.14879342913627625 | Test Loss: 0.1508990228176117\n",
      "Epoch 24 | Train Loss: 0.15072481334209442 | Test Loss: 0.15075258910655975\n",
      "Epoch 24 | Train Loss: 0.15145941078662872 | Test Loss: 0.15060201287269592\n",
      "Epoch 24 | Train Loss: 0.1515023112297058 | Test Loss: 0.1504417061805725\n",
      "Epoch 24 | Train Loss: 0.14964856207370758 | Test Loss: 0.1502833217382431\n",
      "Epoch 24 | Train Loss: 0.1501173973083496 | Test Loss: 0.15013395249843597\n",
      "Epoch 24 | Train Loss: 0.15001189708709717 | Test Loss: 0.14999043941497803\n",
      "Epoch 24 | Train Loss: 0.1476970762014389 | Test Loss: 0.14983677864074707\n",
      "Epoch 24 | Train Loss: 0.14854839444160461 | Test Loss: 0.149689182639122\n",
      "Epoch 24 | Train Loss: 0.14741483330726624 | Test Loss: 0.1495480239391327\n",
      "Epoch 24 | Train Loss: 0.15074582397937775 | Test Loss: 0.1494119018316269\n",
      "Epoch 24 | Train Loss: 0.14800333976745605 | Test Loss: 0.14927154779434204\n",
      "Epoch 24 | Train Loss: 0.1472330242395401 | Test Loss: 0.14912423491477966\n",
      "Epoch 24 | Train Loss: 0.1485813409090042 | Test Loss: 0.14897026121616364\n",
      "Epoch 24 | Train Loss: 0.1502820998430252 | Test Loss: 0.1488196700811386\n",
      "Epoch 24 | Train Loss: 0.14807353913784027 | Test Loss: 0.14867490530014038\n",
      "Epoch 24 | Train Loss: 0.14774397015571594 | Test Loss: 0.14853382110595703\n",
      "Epoch 24 | Train Loss: 0.14741478860378265 | Test Loss: 0.14839112758636475\n",
      "Epoch 24 | Train Loss: 0.14984172582626343 | Test Loss: 0.14824357628822327\n",
      "Epoch 24 | Train Loss: 0.1503080576658249 | Test Loss: 0.14810730516910553\n",
      "Epoch 24 | Train Loss: 0.147955060005188 | Test Loss: 0.1479838490486145\n",
      "Epoch 24 | Train Loss: 0.14706294238567352 | Test Loss: 0.14785932004451752\n",
      "Epoch 24 | Train Loss: 0.14367526769638062 | Test Loss: 0.14772261679172516\n",
      "Epoch 24 | Train Loss: 0.14757822453975677 | Test Loss: 0.14757968485355377\n",
      "Epoch 24 | Train Loss: 0.1469515860080719 | Test Loss: 0.14743654429912567\n",
      "Epoch 25 | Train Loss: 0.14517509937286377 | Test Loss: 0.14730390906333923\n",
      "Epoch 25 | Train Loss: 0.14708064496517181 | Test Loss: 0.14717985689640045\n",
      "Epoch 25 | Train Loss: 0.14784973859786987 | Test Loss: 0.1470545381307602\n",
      "Epoch 25 | Train Loss: 0.1480042189359665 | Test Loss: 0.1469229757785797\n",
      "Epoch 25 | Train Loss: 0.14614048600196838 | Test Loss: 0.14678777754306793\n",
      "Epoch 25 | Train Loss: 0.14670495688915253 | Test Loss: 0.14665761590003967\n",
      "Epoch 25 | Train Loss: 0.14656780660152435 | Test Loss: 0.146534264087677\n",
      "Epoch 25 | Train Loss: 0.1442198008298874 | Test Loss: 0.14640465378761292\n",
      "Epoch 25 | Train Loss: 0.14516635239124298 | Test Loss: 0.14628130197525024\n",
      "Epoch 25 | Train Loss: 0.14407087862491608 | Test Loss: 0.146163210272789\n",
      "Epoch 25 | Train Loss: 0.14735481142997742 | Test Loss: 0.14605183899402618\n",
      "Epoch 25 | Train Loss: 0.14469215273857117 | Test Loss: 0.14593549072742462\n",
      "Epoch 25 | Train Loss: 0.14390352368354797 | Test Loss: 0.1458103358745575\n",
      "Epoch 25 | Train Loss: 0.1452879160642624 | Test Loss: 0.1456831991672516\n",
      "Epoch 25 | Train Loss: 0.14703448116779327 | Test Loss: 0.14555755257606506\n",
      "Epoch 25 | Train Loss: 0.14486415684223175 | Test Loss: 0.14543527364730835\n",
      "Epoch 25 | Train Loss: 0.14449992775917053 | Test Loss: 0.1453171968460083\n",
      "Epoch 25 | Train Loss: 0.14422184228897095 | Test Loss: 0.1451977640390396\n",
      "Epoch 25 | Train Loss: 0.14665254950523376 | Test Loss: 0.14506977796554565\n",
      "Epoch 25 | Train Loss: 0.14715032279491425 | Test Loss: 0.1449522078037262\n",
      "Epoch 25 | Train Loss: 0.1448356658220291 | Test Loss: 0.14484956860542297\n",
      "Epoch 25 | Train Loss: 0.14396655559539795 | Test Loss: 0.14475306868553162\n",
      "Epoch 25 | Train Loss: 0.14056825637817383 | Test Loss: 0.1446414440870285\n",
      "Epoch 25 | Train Loss: 0.14443998038768768 | Test Loss: 0.14451825618743896\n",
      "Epoch 25 | Train Loss: 0.14396317303180695 | Test Loss: 0.1443866640329361\n",
      "Epoch 26 | Train Loss: 0.1421515792608261 | Test Loss: 0.14426304399967194\n",
      "Epoch 26 | Train Loss: 0.14399242401123047 | Test Loss: 0.14415664970874786\n",
      "Epoch 26 | Train Loss: 0.14480839669704437 | Test Loss: 0.14405521750450134\n",
      "Epoch 26 | Train Loss: 0.14503955841064453 | Test Loss: 0.14395061135292053\n",
      "Epoch 26 | Train Loss: 0.14316025376319885 | Test Loss: 0.14383630454540253\n",
      "Epoch 26 | Train Loss: 0.14380519092082977 | Test Loss: 0.14372457563877106\n",
      "Epoch 26 | Train Loss: 0.14366231858730316 | Test Loss: 0.14361503720283508\n",
      "Epoch 26 | Train Loss: 0.14129845798015594 | Test Loss: 0.1435033082962036\n",
      "Epoch 26 | Train Loss: 0.14224565029144287 | Test Loss: 0.14340068399906158\n",
      "Epoch 26 | Train Loss: 0.14128398895263672 | Test Loss: 0.14330361783504486\n",
      "Epoch 26 | Train Loss: 0.14450769126415253 | Test Loss: 0.14321133494377136\n",
      "Epoch 26 | Train Loss: 0.14189477264881134 | Test Loss: 0.14311113953590393\n",
      "Epoch 26 | Train Loss: 0.1411091536283493 | Test Loss: 0.14299722015857697\n",
      "Epoch 26 | Train Loss: 0.14250825345516205 | Test Loss: 0.14288432896137238\n",
      "Epoch 26 | Train Loss: 0.14430128037929535 | Test Loss: 0.1427757740020752\n",
      "Epoch 26 | Train Loss: 0.14211086928844452 | Test Loss: 0.1426728516817093\n",
      "Epoch 26 | Train Loss: 0.14173893630504608 | Test Loss: 0.14257507026195526\n",
      "Epoch 26 | Train Loss: 0.1414964199066162 | Test Loss: 0.14247624576091766\n",
      "Epoch 26 | Train Loss: 0.1439088135957718 | Test Loss: 0.14237017929553986\n",
      "Epoch 26 | Train Loss: 0.144463911652565 | Test Loss: 0.1422698199748993\n",
      "Epoch 26 | Train Loss: 0.14218442142009735 | Test Loss: 0.14217974245548248\n",
      "Epoch 26 | Train Loss: 0.14134150743484497 | Test Loss: 0.14209766685962677\n",
      "Epoch 26 | Train Loss: 0.1379011869430542 | Test Loss: 0.1420038789510727\n",
      "Epoch 26 | Train Loss: 0.14180326461791992 | Test Loss: 0.14189881086349487\n",
      "Epoch 26 | Train Loss: 0.14141152799129486 | Test Loss: 0.14178505539894104\n",
      "Epoch 27 | Train Loss: 0.13956426084041595 | Test Loss: 0.14167585968971252\n",
      "Epoch 27 | Train Loss: 0.141383558511734 | Test Loss: 0.14158262312412262\n",
      "Epoch 27 | Train Loss: 0.1422129124403 | Test Loss: 0.14149756729602814\n",
      "Epoch 27 | Train Loss: 0.14250946044921875 | Test Loss: 0.14141058921813965\n",
      "Epoch 27 | Train Loss: 0.1405881941318512 | Test Loss: 0.14131465554237366\n",
      "Epoch 27 | Train Loss: 0.14135773479938507 | Test Loss: 0.14121893048286438\n",
      "Epoch 27 | Train Loss: 0.1411445438861847 | Test Loss: 0.14112478494644165\n",
      "Epoch 27 | Train Loss: 0.1387988179922104 | Test Loss: 0.14103026688098907\n",
      "Epoch 27 | Train Loss: 0.13974188268184662 | Test Loss: 0.14094175398349762\n",
      "Epoch 27 | Train Loss: 0.1389092653989792 | Test Loss: 0.1408587545156479\n",
      "Epoch 27 | Train Loss: 0.14207206666469574 | Test Loss: 0.14078091084957123\n",
      "Epoch 27 | Train Loss: 0.13949277997016907 | Test Loss: 0.14069733023643494\n",
      "Epoch 27 | Train Loss: 0.13871411979198456 | Test Loss: 0.1406012922525406\n",
      "Epoch 27 | Train Loss: 0.1401427835226059 | Test Loss: 0.14050430059432983\n",
      "Epoch 27 | Train Loss: 0.14196015894412994 | Test Loss: 0.14040708541870117\n",
      "Epoch 27 | Train Loss: 0.1397356390953064 | Test Loss: 0.14031308889389038\n",
      "Epoch 27 | Train Loss: 0.13940224051475525 | Test Loss: 0.14022719860076904\n",
      "Epoch 27 | Train Loss: 0.13915064930915833 | Test Loss: 0.14014595746994019\n",
      "Epoch 27 | Train Loss: 0.14155207574367523 | Test Loss: 0.14006222784519196\n",
      "Epoch 27 | Train Loss: 0.14218078553676605 | Test Loss: 0.13997438549995422\n",
      "Epoch 27 | Train Loss: 0.13990774750709534 | Test Loss: 0.13988828659057617\n",
      "Epoch 27 | Train Loss: 0.13909323513507843 | Test Loss: 0.13981035351753235\n",
      "Epoch 27 | Train Loss: 0.1356065720319748 | Test Loss: 0.13973061740398407\n",
      "Epoch 27 | Train Loss: 0.1395386904478073 | Test Loss: 0.13964571058750153\n",
      "Epoch 27 | Train Loss: 0.139214426279068 | Test Loss: 0.13955269753932953\n",
      "Epoch 28 | Train Loss: 0.13731823861598969 | Test Loss: 0.1394595503807068\n",
      "Epoch 28 | Train Loss: 0.13915622234344482 | Test Loss: 0.13937491178512573\n",
      "Epoch 28 | Train Loss: 0.13997888565063477 | Test Loss: 0.1392967700958252\n",
      "Epoch 28 | Train Loss: 0.14032846689224243 | Test Loss: 0.13922083377838135\n",
      "Epoch 28 | Train Loss: 0.13838942348957062 | Test Loss: 0.13914456963539124\n",
      "Epoch 28 | Train Loss: 0.13925249874591827 | Test Loss: 0.13906681537628174\n",
      "Epoch 28 | Train Loss: 0.1389549970626831 | Test Loss: 0.13898666203022003\n",
      "Epoch 28 | Train Loss: 0.13667109608650208 | Test Loss: 0.13890114426612854\n",
      "Epoch 28 | Train Loss: 0.13759124279022217 | Test Loss: 0.13881991803646088\n",
      "Epoch 28 | Train Loss: 0.1368742436170578 | Test Loss: 0.13874776661396027\n",
      "Epoch 28 | Train Loss: 0.13997331261634827 | Test Loss: 0.13868620991706848\n",
      "Epoch 28 | Train Loss: 0.137423574924469 | Test Loss: 0.13861925899982452\n",
      "Epoch 28 | Train Loss: 0.1366506814956665 | Test Loss: 0.13853693008422852\n",
      "Epoch 28 | Train Loss: 0.13811704516410828 | Test Loss: 0.1384475976228714\n",
      "Epoch 28 | Train Loss: 0.13992644846439362 | Test Loss: 0.1383582502603531\n",
      "Epoch 28 | Train Loss: 0.13765446841716766 | Test Loss: 0.13827316462993622\n",
      "Epoch 28 | Train Loss: 0.13740535080432892 | Test Loss: 0.13819818198680878\n",
      "Epoch 28 | Train Loss: 0.13713131844997406 | Test Loss: 0.13812874257564545\n",
      "Epoch 28 | Train Loss: 0.13953788578510284 | Test Loss: 0.1380586326122284\n",
      "Epoch 28 | Train Loss: 0.14019453525543213 | Test Loss: 0.13798300921916962\n",
      "Epoch 28 | Train Loss: 0.1379234790802002 | Test Loss: 0.13790714740753174\n",
      "Epoch 28 | Train Loss: 0.1371571570634842 | Test Loss: 0.1378363072872162\n",
      "Epoch 28 | Train Loss: 0.13363352417945862 | Test Loss: 0.13776591420173645\n",
      "Epoch 28 | Train Loss: 0.13756847381591797 | Test Loss: 0.13769395649433136\n",
      "Epoch 28 | Train Loss: 0.13729816675186157 | Test Loss: 0.13761566579341888\n",
      "Epoch 29 | Train Loss: 0.1353253573179245 | Test Loss: 0.13753676414489746\n",
      "Epoch 29 | Train Loss: 0.13724099099636078 | Test Loss: 0.13746032118797302\n",
      "Epoch 29 | Train Loss: 0.1380583792924881 | Test Loss: 0.13738882541656494\n",
      "Epoch 29 | Train Loss: 0.13843563199043274 | Test Loss: 0.1373206079006195\n",
      "Epoch 29 | Train Loss: 0.1365162581205368 | Test Loss: 0.1372562050819397\n",
      "Epoch 29 | Train Loss: 0.13743172585964203 | Test Loss: 0.13719147443771362\n",
      "Epoch 29 | Train Loss: 0.13705165684223175 | Test Loss: 0.1371249407529831\n",
      "Epoch 29 | Train Loss: 0.13483630120754242 | Test Loss: 0.1370522826910019\n",
      "Epoch 29 | Train Loss: 0.13575135171413422 | Test Loss: 0.1369815319776535\n",
      "Epoch 29 | Train Loss: 0.13512583076953888 | Test Loss: 0.13692320883274078\n",
      "Epoch 29 | Train Loss: 0.13812606036663055 | Test Loss: 0.13687598705291748\n",
      "Epoch 29 | Train Loss: 0.13563521206378937 | Test Loss: 0.13682161271572113\n",
      "Epoch 29 | Train Loss: 0.13483624160289764 | Test Loss: 0.13675199449062347\n",
      "Epoch 29 | Train Loss: 0.1363600194454193 | Test Loss: 0.1366724818944931\n",
      "Epoch 29 | Train Loss: 0.13818922638893127 | Test Loss: 0.13658975064754486\n",
      "Epoch 29 | Train Loss: 0.13586203753948212 | Test Loss: 0.13650907576084137\n",
      "Epoch 29 | Train Loss: 0.13566283881664276 | Test Loss: 0.13644076883792877\n",
      "Epoch 29 | Train Loss: 0.13539011776447296 | Test Loss: 0.1363798975944519\n",
      "Epoch 29 | Train Loss: 0.13777482509613037 | Test Loss: 0.13631896674633026\n",
      "Epoch 29 | Train Loss: 0.13848911225795746 | Test Loss: 0.13624998927116394\n",
      "Epoch 29 | Train Loss: 0.13619332015514374 | Test Loss: 0.13618279993534088\n",
      "Epoch 29 | Train Loss: 0.13548704981803894 | Test Loss: 0.13612116873264313\n",
      "Epoch 29 | Train Loss: 0.13192950189113617 | Test Loss: 0.1360650360584259\n",
      "Epoch 29 | Train Loss: 0.1358642429113388 | Test Loss: 0.13601242005825043\n",
      "Epoch 29 | Train Loss: 0.13564017415046692 | Test Loss: 0.1359550654888153\n",
      "Epoch 30 | Train Loss: 0.13362763822078705 | Test Loss: 0.1358906775712967\n",
      "Epoch 30 | Train Loss: 0.13560672104358673 | Test Loss: 0.1358187049627304\n",
      "Epoch 30 | Train Loss: 0.1364196389913559 | Test Loss: 0.1357465237379074\n",
      "Epoch 30 | Train Loss: 0.136787548661232 | Test Loss: 0.1356804519891739\n",
      "Epoch 30 | Train Loss: 0.13489830493927002 | Test Loss: 0.1356227695941925\n",
      "Epoch 30 | Train Loss: 0.13583391904830933 | Test Loss: 0.13557058572769165\n",
      "Epoch 30 | Train Loss: 0.13541437685489655 | Test Loss: 0.13551506400108337\n",
      "Epoch 30 | Train Loss: 0.13323456048965454 | Test Loss: 0.13545078039169312\n",
      "Epoch 30 | Train Loss: 0.13413946330547333 | Test Loss: 0.1353854387998581\n",
      "Epoch 30 | Train Loss: 0.13360409438610077 | Test Loss: 0.135334774851799\n",
      "Epoch 30 | Train Loss: 0.13652008771896362 | Test Loss: 0.13529722392559052\n",
      "Epoch 30 | Train Loss: 0.13407854735851288 | Test Loss: 0.1352565586566925\n",
      "Epoch 30 | Train Loss: 0.13328026235103607 | Test Loss: 0.13520091772079468\n",
      "Epoch 30 | Train Loss: 0.13481691479682922 | Test Loss: 0.13513104617595673\n",
      "Epoch 30 | Train Loss: 0.13668012619018555 | Test Loss: 0.13505229353904724\n",
      "Epoch 30 | Train Loss: 0.13431254029273987 | Test Loss: 0.13497500121593475\n",
      "Epoch 30 | Train Loss: 0.1341376155614853 | Test Loss: 0.13491122424602509\n",
      "Epoch 30 | Train Loss: 0.13387630879878998 | Test Loss: 0.13485856354236603\n",
      "Epoch 30 | Train Loss: 0.13625161349773407 | Test Loss: 0.13480767607688904\n",
      "Epoch 30 | Train Loss: 0.1369817554950714 | Test Loss: 0.13474595546722412\n",
      "Epoch 30 | Train Loss: 0.13469158113002777 | Test Loss: 0.134684756398201\n",
      "Epoch 30 | Train Loss: 0.13403843343257904 | Test Loss: 0.1346273571252823\n",
      "Epoch 30 | Train Loss: 0.1304580420255661 | Test Loss: 0.1345777064561844\n",
      "Epoch 30 | Train Loss: 0.13438630104064941 | Test Loss: 0.1345360428094864\n",
      "Epoch 30 | Train Loss: 0.13417284190654755 | Test Loss: 0.1344924122095108\n",
      "Epoch 31 | Train Loss: 0.1321653425693512 | Test Loss: 0.1344398409128189\n",
      "Epoch 31 | Train Loss: 0.13414788246154785 | Test Loss: 0.1343764364719391\n",
      "Epoch 31 | Train Loss: 0.13499562442302704 | Test Loss: 0.13431040942668915\n",
      "Epoch 31 | Train Loss: 0.13537165522575378 | Test Loss: 0.1342504471540451\n",
      "Epoch 31 | Train Loss: 0.13346712291240692 | Test Loss: 0.13419751822948456\n",
      "Epoch 31 | Train Loss: 0.13443024456501007 | Test Loss: 0.13415269553661346\n",
      "Epoch 31 | Train Loss: 0.1339617818593979 | Test Loss: 0.13410626351833344\n",
      "Epoch 31 | Train Loss: 0.13181832432746887 | Test Loss: 0.13405060768127441\n",
      "Epoch 31 | Train Loss: 0.1327277421951294 | Test Loss: 0.1339893490076065\n",
      "Epoch 31 | Train Loss: 0.13226868212223053 | Test Loss: 0.1339370757341385\n",
      "Epoch 31 | Train Loss: 0.1351175755262375 | Test Loss: 0.13389642536640167\n",
      "Epoch 31 | Train Loss: 0.13270346820354462 | Test Loss: 0.1338597536087036\n",
      "Epoch 31 | Train Loss: 0.13190597295761108 | Test Loss: 0.13381853699684143\n",
      "Epoch 31 | Train Loss: 0.1334490329027176 | Test Loss: 0.1337652951478958\n",
      "Epoch 31 | Train Loss: 0.13533341884613037 | Test Loss: 0.1337001472711563\n",
      "Epoch 31 | Train Loss: 0.13295546174049377 | Test Loss: 0.13363268971443176\n",
      "Epoch 31 | Train Loss: 0.13280510902404785 | Test Loss: 0.13357284665107727\n",
      "Epoch 31 | Train Loss: 0.13253997266292572 | Test Loss: 0.13352203369140625\n",
      "Epoch 31 | Train Loss: 0.13490839302539825 | Test Loss: 0.13347618281841278\n",
      "Epoch 31 | Train Loss: 0.1356290727853775 | Test Loss: 0.13342231512069702\n",
      "Epoch 31 | Train Loss: 0.13336889445781708 | Test Loss: 0.13336892426013947\n",
      "Epoch 31 | Train Loss: 0.13273735344409943 | Test Loss: 0.1333167850971222\n",
      "Epoch 31 | Train Loss: 0.12917743623256683 | Test Loss: 0.1332702487707138\n",
      "Epoch 31 | Train Loss: 0.13308775424957275 | Test Loss: 0.13323229551315308\n",
      "Epoch 31 | Train Loss: 0.13289406895637512 | Test Loss: 0.13319633901119232\n",
      "Epoch 32 | Train Loss: 0.1308748573064804 | Test Loss: 0.133155956864357\n",
      "Epoch 32 | Train Loss: 0.13284602761268616 | Test Loss: 0.13310419023036957\n",
      "Epoch 32 | Train Loss: 0.1337508112192154 | Test Loss: 0.13304665684700012\n",
      "Epoch 32 | Train Loss: 0.1341240406036377 | Test Loss: 0.1329912394285202\n",
      "Epoch 32 | Train Loss: 0.13220831751823425 | Test Loss: 0.13294127583503723\n",
      "Epoch 32 | Train Loss: 0.13320092856884003 | Test Loss: 0.1329014003276825\n",
      "Epoch 32 | Train Loss: 0.1326868087053299 | Test Loss: 0.13286304473876953\n",
      "Epoch 32 | Train Loss: 0.13056638836860657 | Test Loss: 0.1328197568655014\n",
      "Epoch 32 | Train Loss: 0.13147909939289093 | Test Loss: 0.13276620209217072\n",
      "Epoch 32 | Train Loss: 0.13109563291072845 | Test Loss: 0.13271558284759521\n",
      "Epoch 32 | Train Loss: 0.13388429582118988 | Test Loss: 0.13267318904399872\n",
      "Epoch 32 | Train Loss: 0.13149352371692657 | Test Loss: 0.13263873755931854\n",
      "Epoch 32 | Train Loss: 0.1306988149881363 | Test Loss: 0.13260690867900848\n",
      "Epoch 32 | Train Loss: 0.13224367797374725 | Test Loss: 0.13256803154945374\n",
      "Epoch 32 | Train Loss: 0.13413922488689423 | Test Loss: 0.13251593708992004\n",
      "Epoch 32 | Train Loss: 0.13175295293331146 | Test Loss: 0.13246051967144012\n",
      "Epoch 32 | Train Loss: 0.1316351741552353 | Test Loss: 0.13240791857242584\n",
      "Epoch 32 | Train Loss: 0.1313726007938385 | Test Loss: 0.1323607712984085\n",
      "Epoch 32 | Train Loss: 0.1337365359067917 | Test Loss: 0.13231749832630157\n",
      "Epoch 32 | Train Loss: 0.13444256782531738 | Test Loss: 0.13226808607578278\n",
      "Epoch 32 | Train Loss: 0.13220788538455963 | Test Loss: 0.132219597697258\n",
      "Epoch 32 | Train Loss: 0.13158462941646576 | Test Loss: 0.13217172026634216\n",
      "Epoch 32 | Train Loss: 0.12804540991783142 | Test Loss: 0.13212735950946808\n",
      "Epoch 32 | Train Loss: 0.13194529712200165 | Test Loss: 0.13208939135074615\n",
      "Epoch 32 | Train Loss: 0.1317751407623291 | Test Loss: 0.13205714523792267\n",
      "Epoch 33 | Train Loss: 0.1297639161348343 | Test Loss: 0.13202376663684845\n",
      "Epoch 33 | Train Loss: 0.13170325756072998 | Test Loss: 0.1319827437400818\n",
      "Epoch 33 | Train Loss: 0.13265009224414825 | Test Loss: 0.13193577527999878\n",
      "Epoch 33 | Train Loss: 0.13302306830883026 | Test Loss: 0.13188740611076355\n",
      "Epoch 33 | Train Loss: 0.1311020851135254 | Test Loss: 0.13184097409248352\n",
      "Epoch 33 | Train Loss: 0.13212279975414276 | Test Loss: 0.1318042278289795\n",
      "Epoch 33 | Train Loss: 0.1315581202507019 | Test Loss: 0.13177143037319183\n",
      "Epoch 33 | Train Loss: 0.12945321202278137 | Test Loss: 0.13173888623714447\n",
      "Epoch 33 | Train Loss: 0.1303446739912033 | Test Loss: 0.13169576227664948\n",
      "Epoch 33 | Train Loss: 0.13006246089935303 | Test Loss: 0.13164445757865906\n",
      "Epoch 33 | Train Loss: 0.1327933669090271 | Test Loss: 0.13159598410129547\n",
      "Epoch 33 | Train Loss: 0.13043352961540222 | Test Loss: 0.1315559595823288\n",
      "Epoch 33 | Train Loss: 0.12963297963142395 | Test Loss: 0.1315249651670456\n",
      "Epoch 33 | Train Loss: 0.13116227090358734 | Test Loss: 0.13149574398994446\n",
      "Epoch 33 | Train Loss: 0.133087158203125 | Test Loss: 0.1314576119184494\n",
      "Epoch 33 | Train Loss: 0.13068850338459015 | Test Loss: 0.13141538202762604\n",
      "Epoch 33 | Train Loss: 0.13060301542282104 | Test Loss: 0.13137365877628326\n",
      "Epoch 33 | Train Loss: 0.13035020232200623 | Test Loss: 0.1313328742980957\n",
      "Epoch 33 | Train Loss: 0.13269083201885223 | Test Loss: 0.13129188120365143\n",
      "Epoch 33 | Train Loss: 0.13335692882537842 | Test Loss: 0.13124854862689972\n",
      "Epoch 33 | Train Loss: 0.13118091225624084 | Test Loss: 0.13120782375335693\n",
      "Epoch 33 | Train Loss: 0.1305779367685318 | Test Loss: 0.13116756081581116\n",
      "Epoch 33 | Train Loss: 0.127048522233963 | Test Loss: 0.1311240941286087\n",
      "Epoch 33 | Train Loss: 0.13094578683376312 | Test Loss: 0.13108395040035248\n",
      "Epoch 33 | Train Loss: 0.13079512119293213 | Test Loss: 0.13105368614196777\n",
      "Epoch 34 | Train Loss: 0.12878084182739258 | Test Loss: 0.1310274451971054\n",
      "Epoch 34 | Train Loss: 0.13070182502269745 | Test Loss: 0.1309988647699356\n",
      "Epoch 34 | Train Loss: 0.1316869705915451 | Test Loss: 0.13096030056476593\n",
      "Epoch 34 | Train Loss: 0.132043719291687 | Test Loss: 0.1309165209531784\n",
      "Epoch 34 | Train Loss: 0.13014593720436096 | Test Loss: 0.13086923956871033\n",
      "Epoch 34 | Train Loss: 0.13118238747119904 | Test Loss: 0.13083067536354065\n",
      "Epoch 34 | Train Loss: 0.1305701583623886 | Test Loss: 0.13079917430877686\n",
      "Epoch 34 | Train Loss: 0.12846098840236664 | Test Loss: 0.13077229261398315\n",
      "Epoch 34 | Train Loss: 0.12935571372509003 | Test Loss: 0.13073846697807312\n",
      "Epoch 34 | Train Loss: 0.1291305422782898 | Test Loss: 0.13069365918636322\n",
      "Epoch 34 | Train Loss: 0.13182587921619415 | Test Loss: 0.13064678013324738\n",
      "Epoch 34 | Train Loss: 0.12951244413852692 | Test Loss: 0.13060657680034637\n",
      "Epoch 34 | Train Loss: 0.12867644429206848 | Test Loss: 0.13057664036750793\n",
      "Epoch 34 | Train Loss: 0.13020306825637817 | Test Loss: 0.13055334985256195\n",
      "Epoch 34 | Train Loss: 0.13214224576950073 | Test Loss: 0.13052773475646973\n",
      "Epoch 34 | Train Loss: 0.12974999845027924 | Test Loss: 0.1304977387189865\n",
      "Epoch 34 | Train Loss: 0.1297047883272171 | Test Loss: 0.13046343624591827\n",
      "Epoch 34 | Train Loss: 0.1294567883014679 | Test Loss: 0.13042309880256653\n",
      "Epoch 34 | Train Loss: 0.13177378475666046 | Test Loss: 0.13038167357444763\n",
      "Epoch 34 | Train Loss: 0.13243578374385834 | Test Loss: 0.13034403324127197\n",
      "Epoch 34 | Train Loss: 0.13027437031269073 | Test Loss: 0.13031212985515594\n",
      "Epoch 34 | Train Loss: 0.1296926736831665 | Test Loss: 0.13028141856193542\n",
      "Epoch 34 | Train Loss: 0.12615345418453217 | Test Loss: 0.13024291396141052\n",
      "Epoch 34 | Train Loss: 0.13005352020263672 | Test Loss: 0.13019925355911255\n",
      "Epoch 34 | Train Loss: 0.12993106245994568 | Test Loss: 0.13016365468502045\n",
      "Epoch 35 | Train Loss: 0.12791967391967773 | Test Loss: 0.13013727962970734\n",
      "Epoch 35 | Train Loss: 0.12981723248958588 | Test Loss: 0.13011503219604492\n",
      "Epoch 35 | Train Loss: 0.13081367313861847 | Test Loss: 0.13008637726306915\n",
      "Epoch 35 | Train Loss: 0.13117006421089172 | Test Loss: 0.13005122542381287\n",
      "Epoch 35 | Train Loss: 0.12928643822669983 | Test Loss: 0.13000674545764923\n",
      "Epoch 35 | Train Loss: 0.13033847510814667 | Test Loss: 0.12996543943881989\n",
      "Epoch 35 | Train Loss: 0.12969131767749786 | Test Loss: 0.12993210554122925\n",
      "Epoch 35 | Train Loss: 0.1275801807641983 | Test Loss: 0.12990781664848328\n",
      "Epoch 35 | Train Loss: 0.12848234176635742 | Test Loss: 0.12988196313381195\n",
      "Epoch 35 | Train Loss: 0.12829773128032684 | Test Loss: 0.1298462152481079\n",
      "Epoch 35 | Train Loss: 0.13096897304058075 | Test Loss: 0.12980617582798004\n",
      "Epoch 35 | Train Loss: 0.12868626415729523 | Test Loss: 0.12976856529712677\n",
      "Epoch 35 | Train Loss: 0.12782740592956543 | Test Loss: 0.1297382265329361\n",
      "Epoch 35 | Train Loss: 0.1293579787015915 | Test Loss: 0.1297120749950409\n",
      "Epoch 35 | Train Loss: 0.13129714131355286 | Test Loss: 0.12968868017196655\n",
      "Epoch 35 | Train Loss: 0.1289060413837433 | Test Loss: 0.129665806889534\n",
      "Epoch 35 | Train Loss: 0.12888316810131073 | Test Loss: 0.1296427696943283\n",
      "Epoch 35 | Train Loss: 0.12864217162132263 | Test Loss: 0.12961144745349884\n",
      "Epoch 35 | Train Loss: 0.13094688951969147 | Test Loss: 0.1295725256204605\n",
      "Epoch 35 | Train Loss: 0.1316068023443222 | Test Loss: 0.12953318655490875\n",
      "Epoch 35 | Train Loss: 0.129444420337677 | Test Loss: 0.1294991374015808\n",
      "Epoch 35 | Train Loss: 0.12890109419822693 | Test Loss: 0.1294695883989334\n",
      "Epoch 35 | Train Loss: 0.12533165514469147 | Test Loss: 0.12944002449512482\n",
      "Epoch 35 | Train Loss: 0.12924151122570038 | Test Loss: 0.12940384447574615\n",
      "Epoch 35 | Train Loss: 0.12914496660232544 | Test Loss: 0.12937238812446594\n",
      "Epoch 36 | Train Loss: 0.12715215981006622 | Test Loss: 0.12934425473213196\n",
      "Epoch 36 | Train Loss: 0.12903830409049988 | Test Loss: 0.12932047247886658\n",
      "Epoch 36 | Train Loss: 0.13002273440361023 | Test Loss: 0.1292971968650818\n",
      "Epoch 36 | Train Loss: 0.13038741052150726 | Test Loss: 0.12927033007144928\n",
      "Epoch 36 | Train Loss: 0.12849926948547363 | Test Loss: 0.1292329579591751\n",
      "Epoch 36 | Train Loss: 0.12957796454429626 | Test Loss: 0.129194438457489\n",
      "Epoch 36 | Train Loss: 0.12891177833080292 | Test Loss: 0.12916260957717896\n",
      "Epoch 36 | Train Loss: 0.1268051713705063 | Test Loss: 0.1291400045156479\n",
      "Epoch 36 | Train Loss: 0.127701997756958 | Test Loss: 0.1291166990995407\n",
      "Epoch 36 | Train Loss: 0.12754423916339874 | Test Loss: 0.12908431887626648\n",
      "Epoch 36 | Train Loss: 0.13022172451019287 | Test Loss: 0.12904849648475647\n",
      "Epoch 36 | Train Loss: 0.12793032824993134 | Test Loss: 0.12901587784290314\n",
      "Epoch 36 | Train Loss: 0.12707805633544922 | Test Loss: 0.1289893090724945\n",
      "Epoch 36 | Train Loss: 0.12859070301055908 | Test Loss: 0.12896493077278137\n",
      "Epoch 36 | Train Loss: 0.130563423037529 | Test Loss: 0.12894372642040253\n",
      "Epoch 36 | Train Loss: 0.12816393375396729 | Test Loss: 0.12892316281795502\n",
      "Epoch 36 | Train Loss: 0.12815463542938232 | Test Loss: 0.12890465557575226\n",
      "Epoch 36 | Train Loss: 0.12790243327617645 | Test Loss: 0.12887950241565704\n",
      "Epoch 36 | Train Loss: 0.1302047073841095 | Test Loss: 0.12884709239006042\n",
      "Epoch 36 | Train Loss: 0.1308618038892746 | Test Loss: 0.12880991399288177\n",
      "Epoch 36 | Train Loss: 0.12870477139949799 | Test Loss: 0.12877532839775085\n",
      "Epoch 36 | Train Loss: 0.12819157540798187 | Test Loss: 0.12874333560466766\n",
      "Epoch 36 | Train Loss: 0.12460844963788986 | Test Loss: 0.12871375679969788\n",
      "Epoch 36 | Train Loss: 0.12847982347011566 | Test Loss: 0.1286829113960266\n",
      "Epoch 36 | Train Loss: 0.12843920290470123 | Test Loss: 0.12865567207336426\n",
      "Epoch 37 | Train Loss: 0.1264660805463791 | Test Loss: 0.1286279708147049\n",
      "Epoch 37 | Train Loss: 0.12834316492080688 | Test Loss: 0.12860102951526642\n",
      "Epoch 37 | Train Loss: 0.1293119341135025 | Test Loss: 0.12857840955257416\n",
      "Epoch 37 | Train Loss: 0.12967272102832794 | Test Loss: 0.12855640053749084\n",
      "Epoch 37 | Train Loss: 0.127769336104393 | Test Loss: 0.1285288780927658\n",
      "Epoch 37 | Train Loss: 0.12888482213020325 | Test Loss: 0.12849785387516022\n",
      "Epoch 37 | Train Loss: 0.12820856273174286 | Test Loss: 0.12846903502941132\n",
      "Epoch 37 | Train Loss: 0.12610545754432678 | Test Loss: 0.12844836711883545\n",
      "Epoch 37 | Train Loss: 0.12700150907039642 | Test Loss: 0.12842847406864166\n",
      "Epoch 37 | Train Loss: 0.12687380611896515 | Test Loss: 0.12840014696121216\n",
      "Epoch 37 | Train Loss: 0.1295352727174759 | Test Loss: 0.12836512923240662\n",
      "Epoch 37 | Train Loss: 0.12726084887981415 | Test Loss: 0.12833330035209656\n",
      "Epoch 37 | Train Loss: 0.12642163038253784 | Test Loss: 0.12830661237239838\n",
      "Epoch 37 | Train Loss: 0.12789402902126312 | Test Loss: 0.1282844841480255\n",
      "Epoch 37 | Train Loss: 0.12990520894527435 | Test Loss: 0.12826620042324066\n",
      "Epoch 37 | Train Loss: 0.12748479843139648 | Test Loss: 0.12824884057044983\n",
      "Epoch 37 | Train Loss: 0.12749972939491272 | Test Loss: 0.12823331356048584\n",
      "Epoch 37 | Train Loss: 0.12723591923713684 | Test Loss: 0.12821319699287415\n",
      "Epoch 37 | Train Loss: 0.12953811883926392 | Test Loss: 0.12818755209445953\n",
      "Epoch 37 | Train Loss: 0.1301838457584381 | Test Loss: 0.1281551867723465\n",
      "Epoch 37 | Train Loss: 0.12805037200450897 | Test Loss: 0.12812219560146332\n",
      "Epoch 37 | Train Loss: 0.12752710282802582 | Test Loss: 0.1280897706747055\n",
      "Epoch 37 | Train Loss: 0.1239648386836052 | Test Loss: 0.1280595362186432\n",
      "Epoch 37 | Train Loss: 0.12780769169330597 | Test Loss: 0.1280316561460495\n",
      "Epoch 37 | Train Loss: 0.1278042048215866 | Test Loss: 0.12800893187522888\n",
      "Epoch 38 | Train Loss: 0.12582555413246155 | Test Loss: 0.12798471748828888\n",
      "Epoch 38 | Train Loss: 0.12771128118038177 | Test Loss: 0.1279601752758026\n",
      "Epoch 38 | Train Loss: 0.12868177890777588 | Test Loss: 0.12793976068496704\n",
      "Epoch 38 | Train Loss: 0.12902949750423431 | Test Loss: 0.12792035937309265\n",
      "Epoch 38 | Train Loss: 0.12711690366268158 | Test Loss: 0.12789694964885712\n",
      "Epoch 38 | Train Loss: 0.12826482951641083 | Test Loss: 0.1278686672449112\n",
      "Epoch 38 | Train Loss: 0.12757475674152374 | Test Loss: 0.12784135341644287\n",
      "Epoch 38 | Train Loss: 0.12547056376934052 | Test Loss: 0.1278226524591446\n",
      "Epoch 38 | Train Loss: 0.12636373937129974 | Test Loss: 0.12780453264713287\n",
      "Epoch 38 | Train Loss: 0.12626171112060547 | Test Loss: 0.12777875363826752\n",
      "Epoch 38 | Train Loss: 0.12889409065246582 | Test Loss: 0.1277482658624649\n",
      "Epoch 38 | Train Loss: 0.12663206458091736 | Test Loss: 0.1277218908071518\n",
      "Epoch 38 | Train Loss: 0.12582139670848846 | Test Loss: 0.12769490480422974\n",
      "Epoch 38 | Train Loss: 0.12726031243801117 | Test Loss: 0.12767136096954346\n",
      "Epoch 38 | Train Loss: 0.12931190431118011 | Test Loss: 0.12765084207057953\n",
      "Epoch 38 | Train Loss: 0.12686818838119507 | Test Loss: 0.1276327222585678\n",
      "Epoch 38 | Train Loss: 0.12689395248889923 | Test Loss: 0.1276167631149292\n",
      "Epoch 38 | Train Loss: 0.12662948668003082 | Test Loss: 0.1275978833436966\n",
      "Epoch 38 | Train Loss: 0.12893475592136383 | Test Loss: 0.12757451832294464\n",
      "Epoch 38 | Train Loss: 0.12956130504608154 | Test Loss: 0.12754590809345245\n",
      "Epoch 38 | Train Loss: 0.12743429839611053 | Test Loss: 0.12751887738704681\n",
      "Epoch 38 | Train Loss: 0.12692435085773468 | Test Loss: 0.1274920552968979\n",
      "Epoch 38 | Train Loss: 0.12338066101074219 | Test Loss: 0.12746499478816986\n",
      "Epoch 38 | Train Loss: 0.12722373008728027 | Test Loss: 0.12744063138961792\n",
      "Epoch 38 | Train Loss: 0.12722276151180267 | Test Loss: 0.12742145359516144\n",
      "Epoch 39 | Train Loss: 0.1252627670764923 | Test Loss: 0.127400204539299\n",
      "Epoch 39 | Train Loss: 0.1271398812532425 | Test Loss: 0.12737730145454407\n",
      "Epoch 39 | Train Loss: 0.1281011402606964 | Test Loss: 0.12736007571220398\n",
      "Epoch 39 | Train Loss: 0.1284426748752594 | Test Loss: 0.12734298408031464\n",
      "Epoch 39 | Train Loss: 0.1265580654144287 | Test Loss: 0.12732034921646118\n",
      "Epoch 39 | Train Loss: 0.12767831981182098 | Test Loss: 0.12729419767856598\n",
      "Epoch 39 | Train Loss: 0.1270218789577484 | Test Loss: 0.12727001309394836\n",
      "Epoch 39 | Train Loss: 0.12491500377655029 | Test Loss: 0.1272551566362381\n",
      "Epoch 39 | Train Loss: 0.12578290700912476 | Test Loss: 0.12723898887634277\n",
      "Epoch 39 | Train Loss: 0.12570475041866302 | Test Loss: 0.12721018493175507\n",
      "Epoch 39 | Train Loss: 0.12830498814582825 | Test Loss: 0.12717972695827484\n",
      "Epoch 39 | Train Loss: 0.1260349005460739 | Test Loss: 0.1271592527627945\n",
      "Epoch 39 | Train Loss: 0.12523648142814636 | Test Loss: 0.1271388679742813\n",
      "Epoch 39 | Train Loss: 0.12668979167938232 | Test Loss: 0.127117320895195\n",
      "Epoch 39 | Train Loss: 0.12876439094543457 | Test Loss: 0.1270977258682251\n",
      "Epoch 39 | Train Loss: 0.12630371749401093 | Test Loss: 0.1270807832479477\n",
      "Epoch 39 | Train Loss: 0.12634868919849396 | Test Loss: 0.12706637382507324\n",
      "Epoch 39 | Train Loss: 0.12608878314495087 | Test Loss: 0.12704798579216003\n",
      "Epoch 39 | Train Loss: 0.12836399674415588 | Test Loss: 0.1270247846841812\n",
      "Epoch 39 | Train Loss: 0.1289978325366974 | Test Loss: 0.1269976794719696\n",
      "Epoch 39 | Train Loss: 0.12689504027366638 | Test Loss: 0.1269722580909729\n",
      "Epoch 39 | Train Loss: 0.12636791169643402 | Test Loss: 0.1269472986459732\n",
      "Epoch 39 | Train Loss: 0.12283960729837418 | Test Loss: 0.12692345678806305\n",
      "Epoch 39 | Train Loss: 0.12667888402938843 | Test Loss: 0.12690116465091705\n",
      "Epoch 39 | Train Loss: 0.12668852508068085 | Test Loss: 0.12688510119915009\n",
      "Epoch 40 | Train Loss: 0.12471884489059448 | Test Loss: 0.1268647164106369\n",
      "Epoch 40 | Train Loss: 0.12660656869411469 | Test Loss: 0.12684254348278046\n",
      "Epoch 40 | Train Loss: 0.1275656521320343 | Test Loss: 0.12682577967643738\n",
      "Epoch 40 | Train Loss: 0.12790586054325104 | Test Loss: 0.12680892646312714\n",
      "Epoch 40 | Train Loss: 0.12603552639484406 | Test Loss: 0.1267881691455841\n",
      "Epoch 40 | Train Loss: 0.12714765965938568 | Test Loss: 0.12676426768302917\n",
      "Epoch 40 | Train Loss: 0.12649108469486237 | Test Loss: 0.12674348056316376\n",
      "Epoch 40 | Train Loss: 0.12439634650945663 | Test Loss: 0.12672992050647736\n",
      "Epoch 40 | Train Loss: 0.12525655329227448 | Test Loss: 0.12671279907226562\n",
      "Epoch 40 | Train Loss: 0.1251872032880783 | Test Loss: 0.12668856978416443\n",
      "Epoch 40 | Train Loss: 0.12775063514709473 | Test Loss: 0.12666581571102142\n",
      "Epoch 40 | Train Loss: 0.12552087008953094 | Test Loss: 0.12665176391601562\n",
      "Epoch 40 | Train Loss: 0.12472838163375854 | Test Loss: 0.12663522362709045\n",
      "Epoch 40 | Train Loss: 0.12617500126361847 | Test Loss: 0.1266169250011444\n",
      "Epoch 40 | Train Loss: 0.12826363742351532 | Test Loss: 0.1265990287065506\n",
      "Epoch 40 | Train Loss: 0.1258004903793335 | Test Loss: 0.12658128142356873\n",
      "Epoch 40 | Train Loss: 0.12583096325397491 | Test Loss: 0.1265665739774704\n",
      "Epoch 40 | Train Loss: 0.12557737529277802 | Test Loss: 0.12654906511306763\n",
      "Epoch 40 | Train Loss: 0.12785570323467255 | Test Loss: 0.12652714550495148\n",
      "Epoch 40 | Train Loss: 0.12847672402858734 | Test Loss: 0.12650208175182343\n",
      "Epoch 40 | Train Loss: 0.12639203667640686 | Test Loss: 0.12647771835327148\n",
      "Epoch 40 | Train Loss: 0.12587836384773254 | Test Loss: 0.12645241618156433\n",
      "Epoch 40 | Train Loss: 0.12235565483570099 | Test Loss: 0.12643040716648102\n",
      "Epoch 40 | Train Loss: 0.12618406116962433 | Test Loss: 0.12641122937202454\n",
      "Epoch 40 | Train Loss: 0.12621885538101196 | Test Loss: 0.1263984739780426\n",
      "Epoch 41 | Train Loss: 0.12423961609601974 | Test Loss: 0.126381516456604\n",
      "Epoch 41 | Train Loss: 0.12612351775169373 | Test Loss: 0.12636074423789978\n",
      "Epoch 41 | Train Loss: 0.1270856112241745 | Test Loss: 0.1263447254896164\n",
      "Epoch 41 | Train Loss: 0.12742243707180023 | Test Loss: 0.1263280063867569\n",
      "Epoch 41 | Train Loss: 0.12555788457393646 | Test Loss: 0.1263078898191452\n",
      "Epoch 41 | Train Loss: 0.12667183578014374 | Test Loss: 0.12628646194934845\n",
      "Epoch 41 | Train Loss: 0.12601731717586517 | Test Loss: 0.12626974284648895\n",
      "Epoch 41 | Train Loss: 0.12392338365316391 | Test Loss: 0.12625890970230103\n",
      "Epoch 41 | Train Loss: 0.12477687746286392 | Test Loss: 0.12624135613441467\n",
      "Epoch 41 | Train Loss: 0.12470873445272446 | Test Loss: 0.1262175291776657\n",
      "Epoch 41 | Train Loss: 0.12726259231567383 | Test Loss: 0.1261991560459137\n",
      "Epoch 41 | Train Loss: 0.1250505894422531 | Test Loss: 0.1261879950761795\n",
      "Epoch 41 | Train Loss: 0.124269999563694 | Test Loss: 0.1261713057756424\n",
      "Epoch 41 | Train Loss: 0.1257026195526123 | Test Loss: 0.12615326046943665\n",
      "Epoch 41 | Train Loss: 0.12779664993286133 | Test Loss: 0.12613801658153534\n",
      "Epoch 41 | Train Loss: 0.12533801794052124 | Test Loss: 0.12612318992614746\n",
      "Epoch 41 | Train Loss: 0.12536685168743134 | Test Loss: 0.12611113488674164\n",
      "Epoch 41 | Train Loss: 0.12511634826660156 | Test Loss: 0.1260964274406433\n",
      "Epoch 41 | Train Loss: 0.1273975968360901 | Test Loss: 0.12607797980308533\n",
      "Epoch 41 | Train Loss: 0.12801575660705566 | Test Loss: 0.1260555237531662\n",
      "Epoch 41 | Train Loss: 0.1259455680847168 | Test Loss: 0.12603150308132172\n",
      "Epoch 41 | Train Loss: 0.12541881203651428 | Test Loss: 0.12600576877593994\n",
      "Epoch 41 | Train Loss: 0.12191534042358398 | Test Loss: 0.12598329782485962\n",
      "Epoch 41 | Train Loss: 0.12573173642158508 | Test Loss: 0.1259644329547882\n",
      "Epoch 41 | Train Loss: 0.12579596042633057 | Test Loss: 0.12595082819461823\n",
      "Epoch 42 | Train Loss: 0.12379921972751617 | Test Loss: 0.1259341686964035\n",
      "Epoch 42 | Train Loss: 0.12566445767879486 | Test Loss: 0.12591543793678284\n",
      "Epoch 42 | Train Loss: 0.1266283541917801 | Test Loss: 0.12590114772319794\n",
      "Epoch 42 | Train Loss: 0.12697730958461761 | Test Loss: 0.1258855164051056\n",
      "Epoch 42 | Train Loss: 0.12511372566223145 | Test Loss: 0.12586680054664612\n",
      "Epoch 42 | Train Loss: 0.12623606622219086 | Test Loss: 0.1258479654788971\n",
      "Epoch 42 | Train Loss: 0.12558117508888245 | Test Loss: 0.12583602964878082\n",
      "Epoch 42 | Train Loss: 0.12348945438861847 | Test Loss: 0.12582798302173615\n",
      "Epoch 42 | Train Loss: 0.12432517111301422 | Test Loss: 0.12581093609333038\n",
      "Epoch 42 | Train Loss: 0.12429354339838028 | Test Loss: 0.12578895688056946\n",
      "Epoch 42 | Train Loss: 0.12680350244045258 | Test Loss: 0.12577421963214874\n",
      "Epoch 42 | Train Loss: 0.12461260706186295 | Test Loss: 0.12576527893543243\n",
      "Epoch 42 | Train Loss: 0.12383866310119629 | Test Loss: 0.12574847042560577\n",
      "Epoch 42 | Train Loss: 0.12527307868003845 | Test Loss: 0.12572918832302094\n",
      "Epoch 42 | Train Loss: 0.1273735761642456 | Test Loss: 0.12571290135383606\n",
      "Epoch 42 | Train Loss: 0.12491631507873535 | Test Loss: 0.12569746375083923\n",
      "Epoch 42 | Train Loss: 0.1249522715806961 | Test Loss: 0.12568531930446625\n",
      "Epoch 42 | Train Loss: 0.12469901889562607 | Test Loss: 0.12567268311977386\n",
      "Epoch 42 | Train Loss: 0.12697069346904755 | Test Loss: 0.12565878033638\n",
      "Epoch 42 | Train Loss: 0.12759549915790558 | Test Loss: 0.1256403923034668\n",
      "Epoch 42 | Train Loss: 0.12554079294204712 | Test Loss: 0.12561805546283722\n",
      "Epoch 42 | Train Loss: 0.12499835342168808 | Test Loss: 0.125593364238739\n",
      "Epoch 42 | Train Loss: 0.12151384353637695 | Test Loss: 0.12557075917720795\n",
      "Epoch 42 | Train Loss: 0.12532658874988556 | Test Loss: 0.12555114924907684\n",
      "Epoch 42 | Train Loss: 0.1253969520330429 | Test Loss: 0.1255364865064621\n",
      "Epoch 43 | Train Loss: 0.12340404838323593 | Test Loss: 0.12552034854888916\n",
      "Epoch 43 | Train Loss: 0.1252453327178955 | Test Loss: 0.1255035251379013\n",
      "Epoch 43 | Train Loss: 0.12621961534023285 | Test Loss: 0.12549231946468353\n",
      "Epoch 43 | Train Loss: 0.12657642364501953 | Test Loss: 0.12547975778579712\n",
      "Epoch 43 | Train Loss: 0.12470182031393051 | Test Loss: 0.12546522915363312\n",
      "Epoch 43 | Train Loss: 0.12584176659584045 | Test Loss: 0.12545014917850494\n",
      "Epoch 43 | Train Loss: 0.12518222630023956 | Test Loss: 0.12544173002243042\n",
      "Epoch 43 | Train Loss: 0.12309552729129791 | Test Loss: 0.12543700635433197\n",
      "Epoch 43 | Train Loss: 0.1239071860909462 | Test Loss: 0.1254223734140396\n",
      "Epoch 43 | Train Loss: 0.12390079349279404 | Test Loss: 0.12539827823638916\n",
      "Epoch 43 | Train Loss: 0.12640057504177094 | Test Loss: 0.12538139522075653\n",
      "Epoch 43 | Train Loss: 0.12421887367963791 | Test Loss: 0.12537111341953278\n",
      "Epoch 43 | Train Loss: 0.12344282120466232 | Test Loss: 0.1253533959388733\n",
      "Epoch 43 | Train Loss: 0.12487629801034927 | Test Loss: 0.1253339648246765\n",
      "Epoch 43 | Train Loss: 0.12699680030345917 | Test Loss: 0.12531855702400208\n",
      "Epoch 43 | Train Loss: 0.12450671195983887 | Test Loss: 0.12530413269996643\n",
      "Epoch 43 | Train Loss: 0.12457189708948135 | Test Loss: 0.1252925843000412\n",
      "Epoch 43 | Train Loss: 0.124326191842556 | Test Loss: 0.1252807378768921\n",
      "Epoch 43 | Train Loss: 0.1265970915555954 | Test Loss: 0.12527091801166534\n",
      "Epoch 43 | Train Loss: 0.12720412015914917 | Test Loss: 0.12525667250156403\n",
      "Epoch 43 | Train Loss: 0.12516145408153534 | Test Loss: 0.1252375841140747\n",
      "Epoch 43 | Train Loss: 0.12460410594940186 | Test Loss: 0.12521648406982422\n",
      "Epoch 43 | Train Loss: 0.12114699184894562 | Test Loss: 0.1251969039440155\n",
      "Epoch 43 | Train Loss: 0.12496017664670944 | Test Loss: 0.12517866492271423\n",
      "Epoch 43 | Train Loss: 0.12501861155033112 | Test Loss: 0.12516453862190247\n",
      "Epoch 44 | Train Loss: 0.12303151935338974 | Test Loss: 0.12514765560626984\n",
      "Epoch 44 | Train Loss: 0.12486443668603897 | Test Loss: 0.12513141334056854\n",
      "Epoch 44 | Train Loss: 0.12585484981536865 | Test Loss: 0.12511886656284332\n",
      "Epoch 44 | Train Loss: 0.12620222568511963 | Test Loss: 0.12510524690151215\n",
      "Epoch 44 | Train Loss: 0.1243215948343277 | Test Loss: 0.12509135901927948\n",
      "Epoch 44 | Train Loss: 0.12548300623893738 | Test Loss: 0.12508025765419006\n",
      "Epoch 44 | Train Loss: 0.12481284141540527 | Test Loss: 0.12507754564285278\n",
      "Epoch 44 | Train Loss: 0.12273397296667099 | Test Loss: 0.12507721781730652\n",
      "Epoch 44 | Train Loss: 0.12355871498584747 | Test Loss: 0.12506099045276642\n",
      "Epoch 44 | Train Loss: 0.1235589012503624 | Test Loss: 0.12503643333911896\n",
      "Epoch 44 | Train Loss: 0.126027449965477 | Test Loss: 0.1250174343585968\n",
      "Epoch 44 | Train Loss: 0.12385622411966324 | Test Loss: 0.12500640749931335\n",
      "Epoch 44 | Train Loss: 0.12307651340961456 | Test Loss: 0.12498793005943298\n",
      "Epoch 44 | Train Loss: 0.12450551986694336 | Test Loss: 0.1249689906835556\n",
      "Epoch 44 | Train Loss: 0.12664847075939178 | Test Loss: 0.12495351582765579\n",
      "Epoch 44 | Train Loss: 0.12413330376148224 | Test Loss: 0.12493815273046494\n",
      "Epoch 44 | Train Loss: 0.12422104179859161 | Test Loss: 0.1249258741736412\n",
      "Epoch 44 | Train Loss: 0.12396853417158127 | Test Loss: 0.12491495162248611\n",
      "Epoch 44 | Train Loss: 0.12623222172260284 | Test Loss: 0.1249079629778862\n",
      "Epoch 44 | Train Loss: 0.12684373557567596 | Test Loss: 0.12489832937717438\n",
      "Epoch 44 | Train Loss: 0.1248183399438858 | Test Loss: 0.1248832568526268\n",
      "Epoch 44 | Train Loss: 0.12424707412719727 | Test Loss: 0.12486579269170761\n",
      "Epoch 44 | Train Loss: 0.12080468982458115 | Test Loss: 0.12484743446111679\n",
      "Epoch 44 | Train Loss: 0.12462160736322403 | Test Loss: 0.12482789903879166\n",
      "Epoch 44 | Train Loss: 0.12468135356903076 | Test Loss: 0.12481298297643661\n",
      "Epoch 45 | Train Loss: 0.12269226461648941 | Test Loss: 0.12479813396930695\n",
      "Epoch 45 | Train Loss: 0.12451829761266708 | Test Loss: 0.12478265911340714\n",
      "Epoch 45 | Train Loss: 0.12551318109035492 | Test Loss: 0.1247696503996849\n",
      "Epoch 45 | Train Loss: 0.12584857642650604 | Test Loss: 0.12475580722093582\n",
      "Epoch 45 | Train Loss: 0.1239737793803215 | Test Loss: 0.1247425526380539\n",
      "Epoch 45 | Train Loss: 0.12514904141426086 | Test Loss: 0.12473330646753311\n",
      "Epoch 45 | Train Loss: 0.12447696924209595 | Test Loss: 0.12473114579916\n",
      "Epoch 45 | Train Loss: 0.12240029871463776 | Test Loss: 0.1247316524386406\n",
      "Epoch 45 | Train Loss: 0.1232151985168457 | Test Loss: 0.12471909821033478\n",
      "Epoch 45 | Train Loss: 0.12322299927473068 | Test Loss: 0.12469770759344101\n",
      "Epoch 45 | Train Loss: 0.125680610537529 | Test Loss: 0.1246766522526741\n",
      "Epoch 45 | Train Loss: 0.12352672964334488 | Test Loss: 0.12466277927160263\n",
      "Epoch 45 | Train Loss: 0.12272941321134567 | Test Loss: 0.12464485317468643\n",
      "Epoch 45 | Train Loss: 0.12416491657495499 | Test Loss: 0.12462804466485977\n",
      "Epoch 45 | Train Loss: 0.12632079422473907 | Test Loss: 0.12461399286985397\n",
      "Epoch 45 | Train Loss: 0.12379059940576553 | Test Loss: 0.12459884583950043\n",
      "Epoch 45 | Train Loss: 0.123883917927742 | Test Loss: 0.12458603829145432\n",
      "Epoch 45 | Train Loss: 0.1236424669623375 | Test Loss: 0.12457490712404251\n",
      "Epoch 45 | Train Loss: 0.125901460647583 | Test Loss: 0.12456806004047394\n",
      "Epoch 45 | Train Loss: 0.1265154927968979 | Test Loss: 0.12456011027097702\n",
      "Epoch 45 | Train Loss: 0.12449321895837784 | Test Loss: 0.12454782426357269\n",
      "Epoch 45 | Train Loss: 0.12391539663076401 | Test Loss: 0.12453439831733704\n",
      "Epoch 45 | Train Loss: 0.12048572301864624 | Test Loss: 0.1245184987783432\n",
      "Epoch 45 | Train Loss: 0.12430168688297272 | Test Loss: 0.12449917197227478\n",
      "Epoch 45 | Train Loss: 0.12436363846063614 | Test Loss: 0.12448492646217346\n",
      "Epoch 46 | Train Loss: 0.12236867100000381 | Test Loss: 0.12447159737348557\n",
      "Epoch 46 | Train Loss: 0.12419674545526505 | Test Loss: 0.12445729225873947\n",
      "Epoch 46 | Train Loss: 0.1251942217350006 | Test Loss: 0.12444403767585754\n",
      "Epoch 46 | Train Loss: 0.1255183219909668 | Test Loss: 0.12443043291568756\n",
      "Epoch 46 | Train Loss: 0.12365724891424179 | Test Loss: 0.12441720813512802\n",
      "Epoch 46 | Train Loss: 0.1248377114534378 | Test Loss: 0.12440862506628036\n",
      "Epoch 46 | Train Loss: 0.12415840476751328 | Test Loss: 0.12440711259841919\n",
      "Epoch 46 | Train Loss: 0.12209451198577881 | Test Loss: 0.12440798431634903\n",
      "Epoch 46 | Train Loss: 0.12288928031921387 | Test Loss: 0.12439700216054916\n",
      "Epoch 46 | Train Loss: 0.12291194498538971 | Test Loss: 0.12437750399112701\n",
      "Epoch 46 | Train Loss: 0.1253500133752823 | Test Loss: 0.12435755878686905\n",
      "Epoch 46 | Train Loss: 0.12321610748767853 | Test Loss: 0.12434375286102295\n",
      "Epoch 46 | Train Loss: 0.122408427298069 | Test Loss: 0.12432654947042465\n",
      "Epoch 46 | Train Loss: 0.12385739386081696 | Test Loss: 0.12431099265813828\n",
      "Epoch 46 | Train Loss: 0.12601426243782043 | Test Loss: 0.1242978647351265\n",
      "Epoch 46 | Train Loss: 0.12348698824644089 | Test Loss: 0.12428246438503265\n",
      "Epoch 46 | Train Loss: 0.12357371300458908 | Test Loss: 0.12426944822072983\n",
      "Epoch 46 | Train Loss: 0.12333505600690842 | Test Loss: 0.12425818294286728\n",
      "Epoch 46 | Train Loss: 0.12559941411018372 | Test Loss: 0.12425115704536438\n",
      "Epoch 46 | Train Loss: 0.1262127012014389 | Test Loss: 0.12424426525831223\n",
      "Epoch 46 | Train Loss: 0.12419576942920685 | Test Loss: 0.1242341697216034\n",
      "Epoch 46 | Train Loss: 0.12360932677984238 | Test Loss: 0.12422347068786621\n",
      "Epoch 46 | Train Loss: 0.12019389122724533 | Test Loss: 0.1242097020149231\n",
      "Epoch 46 | Train Loss: 0.12400426715612411 | Test Loss: 0.12419100850820541\n",
      "Epoch 46 | Train Loss: 0.12406893074512482 | Test Loss: 0.12417704612016678\n",
      "Epoch 47 | Train Loss: 0.12207640707492828 | Test Loss: 0.12416534125804901\n",
      "Epoch 47 | Train Loss: 0.12390022724866867 | Test Loss: 0.12415293604135513\n",
      "Epoch 47 | Train Loss: 0.12489757686853409 | Test Loss: 0.12414130568504333\n",
      "Epoch 47 | Train Loss: 0.1252160668373108 | Test Loss: 0.12412971258163452\n",
      "Epoch 47 | Train Loss: 0.12336932867765427 | Test Loss: 0.12411744147539139\n",
      "Epoch 47 | Train Loss: 0.12455391883850098 | Test Loss: 0.12410956621170044\n",
      "Epoch 47 | Train Loss: 0.12387176603078842 | Test Loss: 0.12410933524370193\n",
      "Epoch 47 | Train Loss: 0.12181303650140762 | Test Loss: 0.12411122024059296\n",
      "Epoch 47 | Train Loss: 0.12258946895599365 | Test Loss: 0.12410176545381546\n",
      "Epoch 47 | Train Loss: 0.12261946499347687 | Test Loss: 0.12408396601676941\n",
      "Epoch 47 | Train Loss: 0.1250544786453247 | Test Loss: 0.12406638264656067\n",
      "Epoch 47 | Train Loss: 0.12292909622192383 | Test Loss: 0.12405308336019516\n",
      "Epoch 47 | Train Loss: 0.12211709469556808 | Test Loss: 0.12403587251901627\n",
      "Epoch 47 | Train Loss: 0.12357547134160995 | Test Loss: 0.12402120977640152\n",
      "Epoch 47 | Train Loss: 0.12573695182800293 | Test Loss: 0.12400978803634644\n",
      "Epoch 47 | Train Loss: 0.12321169674396515 | Test Loss: 0.12399474531412125\n",
      "Epoch 47 | Train Loss: 0.12328583002090454 | Test Loss: 0.12398216873407364\n",
      "Epoch 47 | Train Loss: 0.12305543571710587 | Test Loss: 0.12397103011608124\n",
      "Epoch 47 | Train Loss: 0.1253240406513214 | Test Loss: 0.12396321445703506\n",
      "Epoch 47 | Train Loss: 0.125932976603508 | Test Loss: 0.12395595759153366\n",
      "Epoch 47 | Train Loss: 0.12392228841781616 | Test Loss: 0.1239461824297905\n",
      "Epoch 47 | Train Loss: 0.12332304567098618 | Test Loss: 0.12393689900636673\n",
      "Epoch 47 | Train Loss: 0.11991856247186661 | Test Loss: 0.12392538785934448\n",
      "Epoch 47 | Train Loss: 0.1237282082438469 | Test Loss: 0.12390794605016708\n",
      "Epoch 47 | Train Loss: 0.12379516661167145 | Test Loss: 0.12389358133077621\n",
      "Epoch 48 | Train Loss: 0.12180262058973312 | Test Loss: 0.1238798126578331\n",
      "Epoch 48 | Train Loss: 0.12362115830183029 | Test Loss: 0.1238669753074646\n",
      "Epoch 48 | Train Loss: 0.12462253868579865 | Test Loss: 0.1238563060760498\n",
      "Epoch 48 | Train Loss: 0.12494488060474396 | Test Loss: 0.12384714186191559\n",
      "Epoch 48 | Train Loss: 0.12309405952692032 | Test Loss: 0.12384013831615448\n",
      "Epoch 48 | Train Loss: 0.12428479641675949 | Test Loss: 0.12383892387151718\n",
      "Epoch 48 | Train Loss: 0.12360136955976486 | Test Loss: 0.12384391576051712\n",
      "Epoch 48 | Train Loss: 0.12154572457075119 | Test Loss: 0.1238456442952156\n",
      "Epoch 48 | Train Loss: 0.12231595814228058 | Test Loss: 0.12383195012807846\n",
      "Epoch 48 | Train Loss: 0.12235267460346222 | Test Loss: 0.12381060421466827\n",
      "Epoch 48 | Train Loss: 0.12477517127990723 | Test Loss: 0.12379337102174759\n",
      "Epoch 48 | Train Loss: 0.12266472727060318 | Test Loss: 0.12378130853176117\n",
      "Epoch 48 | Train Loss: 0.12184753268957138 | Test Loss: 0.12376616895198822\n",
      "Epoch 48 | Train Loss: 0.12331648170948029 | Test Loss: 0.12375352531671524\n",
      "Epoch 48 | Train Loss: 0.1254855990409851 | Test Loss: 0.12374299764633179\n",
      "Epoch 48 | Train Loss: 0.12295415252447128 | Test Loss: 0.12372645735740662\n",
      "Epoch 48 | Train Loss: 0.12302594631910324 | Test Loss: 0.12371273338794708\n",
      "Epoch 48 | Train Loss: 0.12277950346469879 | Test Loss: 0.1237015351653099\n",
      "Epoch 48 | Train Loss: 0.12505924701690674 | Test Loss: 0.12369375675916672\n",
      "Epoch 48 | Train Loss: 0.125671848654747 | Test Loss: 0.1236867606639862\n",
      "Epoch 48 | Train Loss: 0.12366807460784912 | Test Loss: 0.12367859482765198\n",
      "Epoch 48 | Train Loss: 0.12305903434753418 | Test Loss: 0.1236722394824028\n",
      "Epoch 48 | Train Loss: 0.11966307461261749 | Test Loss: 0.1236635148525238\n",
      "Epoch 48 | Train Loss: 0.12347767502069473 | Test Loss: 0.1236463114619255\n",
      "Epoch 48 | Train Loss: 0.12353876233100891 | Test Loss: 0.12362915277481079\n",
      "Epoch 49 | Train Loss: 0.12154729664325714 | Test Loss: 0.12361261248588562\n",
      "Epoch 49 | Train Loss: 0.12336862087249756 | Test Loss: 0.12359849363565445\n",
      "Epoch 49 | Train Loss: 0.12437941879034042 | Test Loss: 0.1235876977443695\n",
      "Epoch 49 | Train Loss: 0.12470362335443497 | Test Loss: 0.1235785037279129\n",
      "Epoch 49 | Train Loss: 0.12284868210554123 | Test Loss: 0.12357472628355026\n",
      "Epoch 49 | Train Loss: 0.12404691427946091 | Test Loss: 0.12357746809720993\n",
      "Epoch 49 | Train Loss: 0.12335953861474991 | Test Loss: 0.12358534336090088\n",
      "Epoch 49 | Train Loss: 0.12129836529493332 | Test Loss: 0.12358906120061874\n",
      "Epoch 49 | Train Loss: 0.12205882370471954 | Test Loss: 0.1235775426030159\n",
      "Epoch 49 | Train Loss: 0.12209620326757431 | Test Loss: 0.12355832010507584\n",
      "Epoch 49 | Train Loss: 0.12452492862939835 | Test Loss: 0.12354227155447006\n",
      "Epoch 49 | Train Loss: 0.12241921573877335 | Test Loss: 0.12352956086397171\n",
      "Epoch 49 | Train Loss: 0.12160058319568634 | Test Loss: 0.12351413071155548\n",
      "Epoch 49 | Train Loss: 0.12306727468967438 | Test Loss: 0.12350250780582428\n",
      "Epoch 49 | Train Loss: 0.12524805963039398 | Test Loss: 0.12349243462085724\n",
      "Epoch 49 | Train Loss: 0.1227065920829773 | Test Loss: 0.12347564846277237\n",
      "Epoch 49 | Train Loss: 0.12278353422880173 | Test Loss: 0.1234620064496994\n",
      "Epoch 49 | Train Loss: 0.12252549082040787 | Test Loss: 0.123451367020607\n",
      "Epoch 49 | Train Loss: 0.12482142448425293 | Test Loss: 0.12344106286764145\n",
      "Epoch 49 | Train Loss: 0.1254243105649948 | Test Loss: 0.12343328446149826\n",
      "Epoch 49 | Train Loss: 0.123431496322155 | Test Loss: 0.12342631816864014\n",
      "Epoch 49 | Train Loss: 0.1228082925081253 | Test Loss: 0.1234242245554924\n",
      "Epoch 49 | Train Loss: 0.11942785233259201 | Test Loss: 0.12341989576816559\n",
      "Epoch 49 | Train Loss: 0.12324011325836182 | Test Loss: 0.12340494990348816\n",
      "Epoch 49 | Train Loss: 0.1233002170920372 | Test Loss: 0.12338815629482269\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "# ALPHATOE\n",
    "model = HookedTransformer(cfg).to(cfg.device)\n",
    "optimizer = t.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# for epoch in tqdm.tqdm(range(epochs)):\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, len(train_data), batch_size):\n",
    "        train_logits = model(train_data[batch:batch+batch_size])\n",
    "        train_loss = loss_fn(train_logits, train_labels[batch:batch+batch_size])\n",
    "\n",
    "        train_loss.backward()\n",
    "\n",
    "        train_losses.append(train_loss.item())\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with t.inference_mode():\n",
    "            test_logits = model(test_data)\n",
    "            test_loss = loss_fn(test_logits, test_labels)\n",
    "            test_losses.append(test_loss.item())\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {train_loss.item()} | Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.2244, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "tensor(3, device='cuda:0')\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "seq_test = [10,3,1,7]\n",
    "print(t.max(model(t.tensor(seq_test))[0, -1]))\n",
    "print(t.argmax(model(t.tensor(seq_test))[0, -1]))\n",
    "print(model(t.tensor(seq_test))[0, -1].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "0",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "y": [
          2.3766825199127197,
          2.243098497390747,
          2.147594690322876,
          2.0750679969787598,
          2.017258405685425,
          1.9754102230072021,
          1.9378716945648193,
          1.8938804864883423,
          1.8461593389511108,
          1.801153540611267,
          1.7607389688491821,
          1.719709038734436,
          1.6757651567459106,
          1.6326981782913208,
          1.5933970212936401,
          1.5557435750961304,
          1.5171303749084473,
          1.4770243167877197,
          1.436069369316101,
          1.3944423198699951,
          1.3520495891571045,
          1.3098030090332031,
          1.267351746559143,
          1.2248327732086182,
          1.1827198266983032,
          1.1409310102462769,
          1.0995064973831177,
          1.0587974786758423,
          1.018149495124817,
          0.9774212837219238,
          0.93719482421875,
          0.8973662257194519,
          0.8577096462249756,
          0.8190063834190369,
          0.7809986472129822,
          0.743642270565033,
          0.7073895931243896,
          0.6722399592399597,
          0.638526439666748,
          0.6063086986541748,
          0.575282633304596,
          0.5455979704856873,
          0.5171860456466675,
          0.49040189385414124,
          0.46444106101989746,
          0.4406437873840332,
          0.4180130958557129,
          0.3971012532711029,
          0.3774178922176361,
          0.35905033349990845,
          0.3411436975002289,
          0.325325608253479,
          0.3102540969848633,
          0.2965911030769348,
          0.28397661447525024,
          0.27216655015945435,
          0.2613648474216461,
          0.2513301968574524,
          0.24190662801265717,
          0.23316815495491028,
          0.22519390285015106,
          0.2179746776819229,
          0.21117649972438812,
          0.20505620539188385,
          0.19952842593193054,
          0.1941417157649994,
          0.1898326277732849,
          0.18527798354625702,
          0.1810338944196701,
          0.1772795021533966,
          0.17369140684604645,
          0.1705644428730011,
          0.16754387319087982,
          0.16491487622261047,
          0.16251228749752045,
          0.1601942926645279,
          0.15807980298995972,
          0.1559157371520996,
          0.154312863945961,
          0.15265434980392456,
          0.15105432271957397,
          0.14959289133548737,
          0.14813493192195892,
          0.1471344381570816,
          0.14598987996578217,
          0.14462873339653015,
          0.14376425743103027,
          0.14274372160434723,
          0.14180906116962433,
          0.14101456105709076,
          0.14023438096046448,
          0.13936254382133484,
          0.13881482183933258,
          0.13805489242076874,
          0.13742142915725708,
          0.13672257959842682,
          0.136131152510643,
          0.13567402958869934,
          0.13514035940170288,
          0.1346680074930191,
          0.13424664735794067,
          0.13375923037528992,
          0.13334056735038757,
          0.13308562338352203,
          0.13261397182941437,
          0.13227230310440063,
          0.13195592164993286,
          0.13156571984291077,
          0.13138259947299957,
          0.13098491728305817,
          0.1306731104850769,
          0.13045324385166168,
          0.13011083006858826,
          0.129908949136734,
          0.12967781722545624,
          0.12943555414676666,
          0.12918099761009216,
          0.12899203598499298,
          0.12873370945453644,
          0.12863802909851074,
          0.12837918102741241,
          0.12820610404014587,
          0.12804816663265228,
          0.12782511115074158,
          0.1276494413614273,
          0.1274939626455307,
          0.12732204794883728,
          0.12718097865581512,
          0.12706401944160461,
          0.12687084078788757,
          0.12677150964736938,
          0.12661373615264893,
          0.12649324536323547,
          0.12640416622161865,
          0.1262119561433792,
          0.12612706422805786,
          0.1260467916727066,
          0.12585808336734772,
          0.12577015161514282,
          0.12572064995765686,
          0.12558934092521667,
          0.12544681131839752,
          0.1254195272922516,
          0.12522165477275848,
          0.1251934915781021,
          0.1250678300857544,
          0.12494595348834991,
          0.1248895674943924,
          0.12480498850345612,
          0.12468466907739639,
          0.12467289716005325,
          0.12459263950586319,
          0.1244625523686409,
          0.12446795403957367,
          0.12432662397623062,
          0.12421496957540512,
          0.12417152523994446,
          0.12407737970352173,
          0.12403745949268341,
          0.12396295368671417,
          0.12385241687297821,
          0.12384855002164841,
          0.12378422915935516,
          0.12367603182792664,
          0.12372440844774246,
          0.12371271103620529,
          0.12349551171064377,
          0.12353197485208511,
          0.12339400500059128,
          0.1233416348695755,
          0.12334941327571869,
          0.12324462085962296,
          0.1231575533747673,
          0.12316480278968811,
          0.12305299937725067,
          0.12302396446466446,
          0.1230241060256958,
          0.12290588766336441,
          0.12289337813854218,
          0.12285258620977402,
          0.1227385401725769,
          0.1227421686053276,
          0.1226942166686058,
          0.12264102697372437,
          0.122640460729599,
          0.12255144119262695,
          0.12253663688898087,
          0.1225740984082222,
          0.12244506180286407,
          0.12245526909828186,
          0.12255458533763885,
          0.1223139613866806,
          0.12234976887702942,
          0.12230558693408966,
          0.12221413850784302,
          0.12227678298950195,
          0.12221277505159378,
          0.12209614366292953,
          0.12218058109283447,
          0.1221042200922966,
          0.12203095853328705,
          0.1220800057053566,
          0.12196334451436996,
          0.12191268056631088,
          0.12194591015577316,
          0.12185800075531006,
          0.12180592119693756,
          0.12180979549884796,
          0.12174836546182632,
          0.12173545360565186,
          0.12173071503639221,
          0.12165571749210358,
          0.12168478965759277,
          0.12164175510406494,
          0.12158062309026718,
          0.12167223542928696,
          0.12155064195394516,
          0.12151813507080078,
          0.12149982899427414,
          0.12143408507108688,
          0.12146475166082382,
          0.12145056575536728,
          0.12137669324874878,
          0.12140581011772156,
          0.12138185650110245,
          0.12129182368516922,
          0.12130820006132126,
          0.12126059085130692,
          0.12120625376701355,
          0.12123458087444305,
          0.12120376527309418,
          0.12115079909563065,
          0.1211354210972786,
          0.12110085040330887,
          0.12106317281723022,
          0.12105805426836014,
          0.12102461606264114,
          0.12104025483131409,
          0.12103430181741714,
          0.12098214030265808,
          0.12102384865283966,
          0.12096960842609406,
          0.12093254923820496,
          0.12093579024076462,
          0.12090100347995758,
          0.12089741975069046,
          0.12087824195623398,
          0.12082477658987045,
          0.12082652747631073,
          0.1208309531211853
         ]
        },
        {
         "mode": "lines",
         "name": "1",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249
         ],
         "y": [
          2.8080050945281982,
          2.37743878364563,
          2.242861270904541,
          2.1462807655334473,
          2.0747716426849365,
          2.0150792598724365,
          1.9754637479782104,
          1.9352155923843384,
          1.8929462432861328,
          1.8454378843307495,
          1.798885703086853,
          1.7615665197372437,
          1.7215832471847534,
          1.6756536960601807,
          1.6301854848861694,
          1.591797947883606,
          1.5564578771591187,
          1.5193551778793335,
          1.4755833148956299,
          1.4367454051971436,
          1.3959301710128784,
          1.3505833148956299,
          1.3131237030029297,
          1.2706594467163086,
          1.224825143814087,
          1.182677149772644,
          1.1408798694610596,
          1.1016119718551636,
          1.0592880249023438,
          1.0177980661392212,
          0.9775671362876892,
          0.9371286630630493,
          0.895053505897522,
          0.8566175699234009,
          0.8209537863731384,
          0.7793704867362976,
          0.7448383569717407,
          0.7081493735313416,
          0.6728139519691467,
          0.6401405930519104,
          0.6059673428535461,
          0.5742062926292419,
          0.5450895428657532,
          0.5155925750732422,
          0.48695945739746094,
          0.4649779498577118,
          0.4414531886577606,
          0.4192572236061096,
          0.3950992226600647,
          0.3796736001968384,
          0.3576085567474365,
          0.339712917804718,
          0.32445740699768066,
          0.3120124638080597,
          0.30084356665611267,
          0.2828828990459442,
          0.2716478407382965,
          0.26104292273521423,
          0.24947766959667206,
          0.243340402841568,
          0.2355431169271469,
          0.2265046387910843,
          0.21649034321308136,
          0.2109721451997757,
          0.20444755256175995,
          0.19893959164619446,
          0.19276586174964905,
          0.18757449090480804,
          0.18337956070899963,
          0.17841672897338867,
          0.17890039086341858,
          0.17289531230926514,
          0.17229528725147247,
          0.16412030160427094,
          0.1663275957107544,
          0.16123409569263458,
          0.1574077606201172,
          0.15517966449260712,
          0.15757887065410614,
          0.1580035537481308,
          0.15334421396255493,
          0.15008904039859772,
          0.1485394984483719,
          0.14596442878246307,
          0.14840319752693176,
          0.14760343730449677,
          0.1449640840291977,
          0.14187653362751007,
          0.14242736995220184,
          0.1406169980764389,
          0.1404487043619156,
          0.138347789645195,
          0.13727052509784698,
          0.13664136826992035,
          0.13566812872886658,
          0.13908539712429047,
          0.136225625872612,
          0.13769488036632538,
          0.13198979198932648,
          0.1364511251449585,
          0.13344939053058624,
          0.13162153959274292,
          0.1301063895225525,
          0.13494303822517395,
          0.13633720576763153,
          0.1333167552947998,
          0.1313001960515976,
          0.13076014816761017,
          0.12954512238502502,
          0.13276457786560059,
          0.13228514790534973,
          0.13124020397663116,
          0.12862665951251984,
          0.12958310544490814,
          0.1288212090730667,
          0.12912054359912872,
          0.12731991708278656,
          0.12724918127059937,
          0.12677748501300812,
          0.12640024721622467,
          0.13035990297794342,
          0.12796296179294586,
          0.12979304790496826,
          0.1244659423828125,
          0.12907132506370544,
          0.12642233073711395,
          0.12488564103841782,
          0.12348911911249161,
          0.12887659668922424,
          0.13015270233154297,
          0.1275065392255783,
          0.12577739357948303,
          0.12545843422412872,
          0.12434855848550797,
          0.127783864736557,
          0.12754352390766144,
          0.12662984430789948,
          0.12417848408222198,
          0.12537841498851776,
          0.12466259300708771,
          0.12502296268939972,
          0.12339873611927032,
          0.12363679707050323,
          0.12311770021915436,
          0.1229594349861145,
          0.1270655393600464,
          0.12471484392881393,
          0.12643995881080627,
          0.12131829559803009,
          0.12587769329547882,
          0.12332047522068024,
          0.12210772186517715,
          0.12062633037567139,
          0.12610729038715363,
          0.1274273842573166,
          0.12502646446228027,
          0.12324948608875275,
          0.12295923382043839,
          0.12197156250476837,
          0.12536396086215973,
          0.12518177926540375,
          0.12438696622848511,
          0.12190687656402588,
          0.12330164760351181,
          0.12258534878492355,
          0.12301500141620636,
          0.12156889587640762,
          0.12177393585443497,
          0.12116211652755737,
          0.12110910564661026,
          0.12519033253192902,
          0.12299495190382004,
          0.12481977790594101,
          0.11970718950033188,
          0.12417113035917282,
          0.12166740745306015,
          0.1204736977815628,
          0.11908479034900665,
          0.12451737374067307,
          0.12583990395069122,
          0.12355773895978928,
          0.12176044285297394,
          0.12158817052841187,
          0.12050749361515045,
          0.12396373599767685,
          0.1238357350230217,
          0.12316518276929855,
          0.12067241966724396,
          0.12213434278964996,
          0.12137094885110855,
          0.1217108964920044,
          0.12039065361022949,
          0.12052621692419052,
          0.11994265764951706,
          0.11995726078748703,
          0.12411611527204514,
          0.12190039455890656,
          0.12372063845396042,
          0.11859257519245148,
          0.1230611801147461,
          0.12062595039606094,
          0.11945686489343643,
          0.11813177168369293,
          0.1235460564494133,
          0.12480511516332626,
          0.1226939782500267,
          0.12083547562360764,
          0.12064038962125778,
          0.11965620517730713,
          0.1229865700006485,
          0.12288022041320801,
          0.12231471389532089,
          0.11979684978723526,
          0.12123581022024155,
          0.12055826187133789,
          0.12082727253437042,
          0.11954834312200546,
          0.11975874751806259,
          0.11906421184539795,
          0.11907706409692764,
          0.12337090075016022,
          0.12105735391378403,
          0.12299793213605881,
          0.11794954538345337,
          0.12227001041173935,
          0.11994011700153351,
          0.11877176910638809,
          0.11737262457609177,
          0.12281733006238937,
          0.12410900741815567,
          0.1220371350646019,
          0.12017934769392014,
          0.12003438919782639,
          0.11900551617145538,
          0.12245338410139084,
          0.1221892386674881,
          0.12169792503118515,
          0.11916277557611465,
          0.12058339267969131,
          0.11998011916875839,
          0.12024205178022385,
          0.1189141795039177,
          0.11919911205768585,
          0.11852140724658966,
          0.11849541962146759,
          0.12283433973789215,
          0.12048450857400894,
          0.1224217340350151,
          0.11744692176580429,
          0.1217244416475296
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": ""
        },
        "xaxis": {
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "title": {
          "text": ""
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lines([test_losses, train_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 100.0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#difference of lists test_losses and train_losses\n",
    "diff = [test_losses[i] - train_losses[i] for i in range(len(test_losses))]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(diff)\n",
    "#xlim\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = t.tensor([10] * 10).to(cfg.device)\n",
    "out = model(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  13.0404,   60.2860, -115.6297,   50.0891,  -59.7190,  -54.6353,\n",
       "          -29.3990,    9.6894,  -41.1484,  -44.5379]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899
         ],
         "xaxis": "x",
         "y": [
          2.2703514099121094,
          2.2395405769348145,
          2.2085189819335938,
          2.1764636039733887,
          2.142791748046875,
          2.10731840133667,
          2.0705783367156982,
          2.0336356163024902,
          1.9981554746627808,
          1.965922474861145,
          1.937679648399353,
          1.9128265380859375,
          1.8900750875473022,
          1.8686412572860718,
          1.8483537435531616,
          1.8294475078582764,
          1.812081217765808,
          1.7959147691726685,
          1.7800649404525757,
          1.7634681463241577,
          1.7454668283462524,
          1.7260545492172241,
          1.7056715488433838,
          1.6847436428070068,
          1.6626578569412231,
          1.63853919506073,
          1.61211359500885,
          1.583520531654358,
          1.5527479648590088,
          1.519374966621399,
          1.4836188554763794,
          1.4458200931549072,
          1.406359076499939,
          1.364904522895813,
          1.3204925060272217,
          1.2731270790100098,
          1.22353994846344,
          1.1708508729934692,
          1.1150083541870117,
          1.0571264028549194,
          0.9966414570808411,
          0.9336987137794495,
          0.8701422810554504,
          0.8066260814666748,
          0.7441383600234985,
          0.6831145286560059,
          0.6247608065605164,
          0.5698306560516357,
          0.5179246664047241,
          0.46841806173324585,
          0.4207397699356079,
          0.3750752806663513,
          0.33206844329833984,
          0.291416198015213,
          0.25355300307273865,
          0.21901148557662964,
          0.18773473799228668,
          0.15960176289081573,
          0.13492557406425476,
          0.11380327492952347,
          0.09573955833911896,
          0.08052349090576172,
          0.06809712946414948,
          0.05818776786327362,
          0.04973116144537926,
          0.04219410941004753,
          0.035934869199991226,
          0.031008217483758926,
          0.027041928842663765,
          0.023780616000294685,
          0.02103719301521778,
          0.01870683953166008,
          0.016702858731150627,
          0.014957020059227943,
          0.01343599148094654,
          0.012132631614804268,
          0.011043697595596313,
          0.010113473050296307,
          0.00927244033664465,
          0.008504824712872505,
          0.007827955298125744,
          0.007231445051729679,
          0.00669760862365365,
          0.006213771644979715,
          0.005779371131211519,
          0.0053921290673315525,
          0.00504921842366457,
          0.004745654296129942,
          0.004474232438951731,
          0.004229612648487091,
          0.004006821196526289,
          0.003802781691774726,
          0.0036154550034552813,
          0.003442961722612381,
          0.0032842967193573713,
          0.0031389787327498198,
          0.003006163751706481,
          0.0028854282572865486,
          0.002775507280603051,
          0.0026750783436000347,
          0.0025827172212302685,
          0.002497381065040827,
          0.0024180614855140448,
          0.00234392611309886,
          0.0022742105647921562,
          0.0022089567501097918,
          0.0021481411531567574,
          0.002091736299917102,
          0.002039363607764244,
          0.0019905806984752417,
          0.0019450061954557896,
          0.0019021208863705397,
          0.00186159648001194,
          0.0018231758149340749,
          0.0017866413109004498,
          0.0017519420944154263,
          0.0017189469654113054,
          0.001687486656010151,
          0.001657444634474814,
          0.0016287794569507241,
          0.0016013813437893987,
          0.0015751435421407223,
          0.001549951615743339,
          0.0015257014892995358,
          0.0015023474115878344,
          0.0014798303600400686,
          0.001458054524846375,
          0.0014369753189384937,
          0.001416612882167101,
          0.0013969428837299347,
          0.0013779043219983578,
          0.0013594356132671237,
          0.0013415053253993392,
          0.0013240998378023505,
          0.0013071896973997355,
          0.001290717045776546,
          0.0012746559223160148,
          0.001258987351320684,
          0.0012436764081940055,
          0.0012287106364965439,
          0.0012140895705670118,
          0.0011997767724096775,
          0.0011858086800202727,
          0.0011721622431650758,
          0.0011588024208322167,
          0.0011457238579168916,
          0.0011329231783747673,
          0.0011203923495486379,
          0.001108107273466885,
          0.0010960428044199944,
          0.0010841808980330825,
          0.0010725517058745027,
          0.0010611464967951179,
          0.0010499432682991028,
          0.0010389399249106646,
          0.0010281448485329747,
          0.0010175458155572414,
          0.0010071264114230871,
          0.0009968697559088469,
          0.000986766186542809,
          0.000976838287897408,
          0.0009670790750533342,
          0.0009574713767506182,
          0.0009480128646828234,
          0.0009387098252773285,
          0.0009295524214394391,
          0.0009205344831570983,
          0.000911643379367888,
          0.0009028696804307401,
          0.0008942388813011348,
          0.0008857441716827452,
          0.0008773694862611592,
          0.0008691144757904112,
          0.000860988802742213,
          0.0008529868791811168,
          0.000845093687530607,
          0.000837305560708046,
          0.0008296119049191475,
          0.000822033325675875,
          0.0008145676692947745,
          0.00080720434198156,
          0.0007999375811778009,
          0.0007927807746455073,
          0.000785722688306123,
          0.000778756570070982,
          0.0007718782871961594,
          0.0007650773040950298,
          0.000758373353164643,
          0.0007517627673223615,
          0.0007452411227859557,
          0.0007388003286905587,
          0.0007324504549615085,
          0.000726187601685524,
          0.0007200014661066234,
          0.0007138896617107093,
          0.0007078427588567138,
          0.0007018781616352499,
          0.0006959927850402892,
          0.0006901820888742805,
          0.0006844415329396725,
          0.0006787788588553667,
          0.0006731878966093063,
          0.0006676629418507218,
          0.0006621999200433493,
          0.0006567931268364191,
          0.0006514590932056308,
          0.0006461917073465884,
          0.0006409883499145508,
          0.0006358455284498632,
          0.000630769704002887,
          0.0006257587228901684,
          0.000620804843492806,
          0.000615903118159622,
          0.0006110514514148235,
          0.0006062620086595416,
          0.0006015316466800869,
          0.0005968562327325344,
          0.0005922324489802122,
          0.0005876714130863547,
          0.0005831631715409458,
          0.0005787057452835143,
          0.0005742929060943425,
          0.000569922907743603,
          0.0005656053544953465,
          0.0005613414687104523,
          0.000557125371415168,
          0.0005529549089260399,
          0.000548834097571671,
          0.0005447638686746359,
          0.0005407357239164412,
          0.0005367479170672596,
          0.0005327966646291316,
          0.0005288926186040044,
          0.0005250347894616425,
          0.0005212196265347302,
          0.0005174422403797507,
          0.0005137121770530939,
          0.0005100247217342257,
          0.0005063759163022041,
          0.0005027607548981905,
          0.0004991774912923574,
          0.0004956367774866521,
          0.0004921347717754543,
          0.0004886719980277121,
          0.0004852437705267221,
          0.00048185570631176233,
          0.0004785051860380918,
          0.0004751876404043287,
          0.00047190109035000205,
          0.000468641024781391,
          0.00046541940537281334,
          0.00046223122626543045,
          0.0004590777098201215,
          0.0004559553926810622,
          0.00045286849490366876,
          0.0004498138732742518,
          0.0004467903927434236,
          0.0004437931929714978,
          0.0004408188397064805,
          0.000437877926742658,
          0.0004349696100689471,
          0.00043208911665715277,
          0.00042923673754557967,
          0.0004264171584509313,
          0.00042362604290246964,
          0.0004208609461784363,
          0.00041811863775365055,
          0.0004153990594204515,
          0.0004127078573219478,
          0.000410045322496444,
          0.00040740828262642026,
          0.0004047964175697416,
          0.00040221205563284457,
          0.00039965510950423777,
          0.0003971206024289131,
          0.0003946075157728046,
          0.0003921138704754412,
          0.000389645661925897,
          0.0003872033266816288,
          0.00038478177157230675,
          0.00038238507113419473,
          0.00038001281791366637,
          0.0003776629746425897,
          0.0003753348719328642,
          0.00037302623968571424,
          0.0003707318683154881,
          0.0003684640978462994,
          0.0003662157978396863,
          0.00036398976226337254,
          0.00036178308073431253,
          0.0003595988964661956,
          0.00035743621992878616,
          0.0003552911221049726,
          0.0003531624097377062,
          0.0003510491515044123,
          0.00034895632416009903,
          0.00034688482992351055,
          0.00034483091440051794,
          0.00034279562532901764,
          0.000340779748512432,
          0.0003387836623005569,
          0.0003368027973920107,
          0.00033483767765574157,
          0.0003328874008730054,
          0.0003309535386506468,
          0.0003290392633061856,
          0.0003271415189374238,
          0.0003252580645494163,
          0.00032339521567337215,
          0.00032155014923773706,
          0.0003197167534381151,
          0.0003178993647452444,
          0.00031609315192326903,
          0.00031430349918082356,
          0.0003125302609987557,
          0.0003107719530817121,
          0.00030902816797606647,
          0.0003073005354963243,
          0.0003055886772926897,
          0.0003038910508621484,
          0.000302202592138201,
          0.0003005281905643642,
          0.000298868166282773,
          0.0002972227812279016,
          0.0002955911331810057,
          0.00029397165053524077,
          0.0002923674474004656,
          0.00029077869839966297,
          0.0002892013581003994,
          0.00028763432055711746,
          0.0002860776148736477,
          0.00028453487902879715,
          0.0002830049197655171,
          0.0002814882027450949,
          0.00027998225414194167,
          0.0002784915850497782,
          0.0002770129940472543,
          0.0002755463938228786,
          0.00027408829191699624,
          0.00027263900847174227,
          0.0002712029963731766,
          0.00026977804373018444,
          0.00026836697361432016,
          0.000266965595073998,
          0.00026557472301647067,
          0.00026419784990139306,
          0.00026282991166226566,
          0.00026147073367610574,
          0.000260121509199962,
          0.0002587822964414954,
          0.0002574539103079587,
          0.00025613655452616513,
          0.00025482839555479586,
          0.0002535332168918103,
          0.0002522474096622318,
          0.0002509711775928736,
          0.0002497025125194341,
          0.00024844237486831844,
          0.00024719134671613574,
          0.00024595099966973066,
          0.00024472008226439357,
          0.00024349801242351532,
          0.00024228819529525936,
          0.00024108534853439778,
          0.00023989318287931383,
          0.00023870653240010142,
          0.00023752832203172147,
          0.00023635885736439377,
          0.00023519707610830665,
          0.00023404670355375856,
          0.00023290314129553735,
          0.00023176969261839986,
          0.0002306456008227542,
          0.00022952807194087654,
          0.0002284167130710557,
          0.00022731312492396683,
          0.00022621717653237283,
          0.0002251297264592722,
          0.0002240507019450888,
          0.000222981019760482,
          0.00022191782773006707,
          0.00022086358512751758,
          0.00021981667669024318,
          0.000218775006942451,
          0.00021774003107566386,
          0.00021671106514986604,
          0.00021569266391452402,
          0.00021468033082783222,
          0.00021367458975873888,
          0.0002126771432813257,
          0.0002116867690347135,
          0.0002107047475874424,
          0.00020972710626665503,
          0.00020875393238384277,
          0.00020778887846972793,
          0.0002068315225187689,
          0.00020588030747603625,
          0.00020493488409556448,
          0.00020399958884809166,
          0.00020306839724071324,
          0.00020214510732330382,
          0.00020122523710597306,
          0.00020031226449646056,
          0.0001994050689972937,
          0.00019850354874506593,
          0.00019761022122111171,
          0.00019672133203130215,
          0.00019584133406169713,
          0.00019496624008752406,
          0.00019409632659517229,
          0.00019323098240420222,
          0.00019237142987549305,
          0.00019151595188304782,
          0.00019066780805587769,
          0.0001898268674267456,
          0.00018898963753599674,
          0.00018816010560840368,
          0.00018733479373622686,
          0.0001865155209088698,
          0.00018570059910416603,
          0.00018488845671527088,
          0.0001840836921473965,
          0.0001832839334383607,
          0.0001824907521950081,
          0.00018170078692492098,
          0.0001809180248528719,
          0.00018014013767242432,
          0.00017936722724698484,
          0.00017859786748886108,
          0.00017783221846912056,
          0.00017707253573462367,
          0.000176317582372576,
          0.0001755676930770278,
          0.00017482374096289277,
          0.0001740831066854298,
          0.0001733500830596313,
          0.0001726202026475221,
          0.00017189189384225756,
          0.0001711696822894737,
          0.00017045212734956294,
          0.00016973901074379683,
          0.0001690309145487845,
          0.00016832680557854474,
          0.0001676274259807542,
          0.00016693337238393724,
          0.0001662420982029289,
          0.00016555600450374186,
          0.00016487273387610912,
          0.00016419304301962256,
          0.00016351933300029486,
          0.00016285052697639912,
          0.00016218343807850033,
          0.00016152230091392994,
          0.0001608665188541636,
          0.0001602129195816815,
          0.00015956370043568313,
          0.00015891641669441015,
          0.0001582736149430275,
          0.00015763497503940016,
          0.00015700106450822204,
          0.00015636993339285254,
          0.00015574412827845663,
          0.00015512316895183176,
          0.00015450383943971246,
          0.0001538883661851287,
          0.00015327564324252307,
          0.0001526666892459616,
          0.0001520612568128854,
          0.0001514599280199036,
          0.0001508630666648969,
          0.0001502690720371902,
          0.00014967971947044134,
          0.00014909377205185592,
          0.00014851041487418115,
          0.0001479279453633353,
          0.00014735069999005646,
          0.00014677834406029433,
          0.00014620661386288702,
          0.0001456402096664533,
          0.00014507726882584393,
          0.00014451798051595688,
          0.0001439620100427419,
          0.0001434069126844406,
          0.0001428549294359982,
          0.00014230776287149638,
          0.00014176327385939658,
          0.00014122112770564854,
          0.00014068307064007968,
          0.00014014898624736816,
          0.00013961701188236475,
          0.0001390880352118984,
          0.00013856159057468176,
          0.00013803664478473365,
          0.00013751622464042157,
          0.00013699947157874703,
          0.0001364853815175593,
          0.00013597344513982534,
          0.00013546491391025484,
          0.00013495991879608482,
          0.00013445767399389297,
          0.00013395535643212497,
          0.00013345840852707624,
          0.00013296173710841686,
          0.0001324689801549539,
          0.0001319803559454158,
          0.00013149381265975535,
          0.00013101020886097103,
          0.00013052980648353696,
          0.0001300506992265582,
          0.00012957466242369264,
          0.00012910037185065448,
          0.0001286291517317295,
          0.00012816049274988472,
          0.00012769544264301658,
          0.00012723154213745147,
          0.0001267706393264234,
          0.00012631391291506588,
          0.00012585839431267232,
          0.00012540401075966656,
          0.00012495268310885876,
          0.00012450305803213269,
          0.00012405644520185888,
          0.00012361217522993684,
          0.00012317078653723001,
          0.0001227331958943978,
          0.000122297162306495,
          0.00012186289677629247,
          0.00012143031199229881,
          0.00012100039020879194,
          0.00012057104322593659,
          0.00012014565436402336,
          0.00011972244101343676,
          0.00011930118489544839,
          0.00011888358858413994,
          0.00011846734560094774,
          0.00011805156827904284,
          0.00011763966176658869,
          0.00011722950875991955,
          0.00011682001786539331,
          0.00011641505261650309,
          0.00011601150617934763,
          0.0001156078651547432,
          0.00011520938278408721,
          0.00011481294495752081,
          0.00011441731476224959,
          0.00011402335803722963,
          0.00011363098019501194,
          0.00011324074876029044,
          0.00011285315849818289,
          0.00011246812937315553,
          0.00011208317300770432,
          0.00011170162906637415,
          0.0001113230682676658,
          0.00011094583169324324,
          0.00011056895891670138,
          0.0001101940797525458,
          0.00010982179082930088,
          0.00010945076792268082,
          0.00010908194235526025,
          0.00010871562699321657,
          0.00010835030116140842,
          0.00010798784933285788,
          0.00010762727470137179,
          0.00010726747859735042,
          0.00010690993804018945,
          0.0001065531323547475,
          0.00010619939712341875,
          0.00010584726260276511,
          0.00010549685976002365,
          0.000105148101283703,
          0.00010480011405888945,
          0.00010445644147694111,
          0.00010411145922262222,
          0.00010376901627751067,
          0.00010342831956222653,
          0.00010308927448932081,
          0.0001027525941026397,
          0.00010241592826787382,
          0.00010208332241745666,
          0.00010175068018725142,
          0.00010142206883756444,
          0.0001010914784274064,
          0.0001007636237773113,
          0.00010043707879958674,
          0.00010011216363636777,
          0.00009978978778235614,
          0.0000994682777673006,
          0.00009914778638631105,
          0.00009882986341835931,
          0.00009851455979514867,
          0.00009819842671277002,
          0.00009788454917725176,
          0.00009757113002706319,
          0.00009726065763970837,
          0.00009695099288364872,
          0.0000966433944995515,
          0.00009633646550355479,
          0.00009603229409549385,
          0.0000957283700699918,
          0.00009542668703943491,
          0.00009512484393781051,
          0.00009482490713708103,
          0.00009452673111809418,
          0.00009423040319234133,
          0.00009393509390065446,
          0.00009364181460114196,
          0.00009334916830994189,
          0.00009305899584433064,
          0.0000927686269278638,
          0.00009247891284758225,
          0.00009219249477609992,
          0.00009190665878122672,
          0.00009162179776467383,
          0.00009133891580859199,
          0.00009105657954933122,
          0.00009077702998183668,
          0.00009049779328051955,
          0.00009021856385516003,
          0.00008994252129923552,
          0.00008966594032244757,
          0.00008939144754549488,
          0.00008911819895729423,
          0.00008884574344847351,
          0.00008857509237714112,
          0.00008830673323245719,
          0.00008803904347587377,
          0.00008777180482866243,
          0.00008750568667892367,
          0.00008724067447474226,
          0.00008697700104676187,
          0.0000867146736709401,
          0.00008645302295917645,
          0.000086194348114077,
          0.00008593669190304354,
          0.00008567737677367404,
          0.00008542118303012103,
          0.00008516534580849111,
          0.00008491146581945941,
          0.00008465755672659725,
          0.00008440639066975564,
          0.00008415467164013535,
          0.0000839052299852483,
          0.0000836564286146313,
          0.00008340929343830794,
          0.00008316228922922164,
          0.00008291725680464879,
          0.0000826726682134904,
          0.00008242733747465536,
          0.00008218600851250812,
          0.00008194510155590251,
          0.00008170521323336288,
          0.0000814660670584999,
          0.0000812283469713293,
          0.00008099075785139576,
          0.00008075494406512007,
          0.00008051852637436241,
          0.00008028510637814179,
          0.00008005173003766686,
          0.00007981985254446045,
          0.00007958876813063398,
          0.00007935972098493949,
          0.0000791306301834993,
          0.0000789026526035741,
          0.00007867461681598797,
          0.00007844873471185565,
          0.00007822277984814718,
          0.00007799967715982348,
          0.0000777757159085013,
          0.00007755433034617454,
          0.00007733221718808636,
          0.00007711263606324792,
          0.00007689172343816608,
          0.00007667347381357104,
          0.00007645547884749249,
          0.00007623796409461647,
          0.00007602317782584578,
          0.00007580833334941417,
          0.00007559406367363408,
          0.00007538139470852911,
          0.0000751689076423645,
          0.00007495813770219684,
          0.0000747469239286147,
          0.00007453673606505617,
          0.00007432940765284002,
          0.00007412122067762539,
          0.00007391339750029147,
          0.00007370692765107378,
          0.00007350283703999594,
          0.00007329812069656327,
          0.00007309435022762045,
          0.00007289140921784565,
          0.00007268793706316501,
          0.00007248748443089426,
          0.00007228793401736766,
          0.00007208852184703574,
          0.00007188934250734746,
          0.00007169215678004548,
          0.00007149457087507471,
          0.00007129891309887171,
          0.00007110235310392454,
          0.00007090784492902458,
          0.00007071351137710735,
          0.00007052087312331423,
          0.00007032808935036883,
          0.00007013633876340464,
          0.00006994667637627572,
          0.00006975589349167421,
          0.00006956636207178235,
          0.0000693779657012783,
          0.0000691903114784509,
          0.00006900267180753872,
          0.00006881551234982908,
          0.00006863150338176638,
          0.00006844630115665495,
          0.00006826176831964403,
          0.00006807931640651077,
          0.00006789638428017497,
          0.00006771401240257546,
          0.00006753300840500742,
          0.00006735307397320867,
          0.0000671722082188353,
          0.00006699362711515278,
          0.00006681502418359742,
          0.00006663887324975803,
          0.00006646040128543973,
          0.00006628474511671811,
          0.00006611046410398558,
          0.00006593309808522463,
          0.00006575923907803372,
          0.00006558666791534051,
          0.00006541279435623437,
          0.00006524119817186147,
          0.00006506993668153882,
          0.00006490003579529002,
          0.00006472897075582296,
          0.00006455900438595563,
          0.00006438972195610404,
          0.00006422298611141741,
          0.0000640547732473351,
          0.00006388621841324493,
          0.00006372118514264002,
          0.00006355615187203512,
          0.00006338992534438148,
          0.00006322610715869814,
          0.00006306219438556582,
          0.00006289935845416039,
          0.00006273452891036868,
          0.00006257386121433228,
          0.00006241184019017965,
          0.00006225262768566608,
          0.00006209143612068146,
          0.00006193338049342856,
          0.0000617737096035853,
          0.000061615755839739,
          0.00006145717634353787,
          0.0000613012962276116,
          0.00006114276038715616,
          0.0000609880116826389,
          0.000060832542658317834,
          0.00006067817594157532,
          0.00006052486423868686,
          0.00006037066486896947,
          0.00006021603985573165,
          0.000060064725403208286,
          0.00005991220677969977,
          0.00005976149986963719,
          0.00005961009082966484,
          0.00005946042438154109,
          0.000059311481891199946,
          0.00005916235750191845,
          0.00005901376425754279,
          0.000058864236052613705,
          0.00005871767643839121,
          0.00005857040741830133,
          0.00005842433893121779,
          0.00005827758286613971,
          0.00005813304233015515,
          0.00005798911661258899,
          0.00005784438326372765,
          0.000057700050092535093,
          0.000057557681429898366,
          0.000057415007177041844,
          0.00005727259485865943,
          0.00005713083010050468,
          0.00005698936729459092,
          0.00005685014548362233,
          0.00005670926839229651,
          0.0000565696791454684,
          0.00005643088661599904,
          0.00005629241422866471,
          0.0000561540546186734,
          0.000056016349844867364,
          0.00005587952182395384,
          0.000055742275435477495,
          0.0000556067461729981,
          0.000055471416999353096,
          0.00005533589865081012,
          0.00005520130071090534,
          0.00005506748129846528,
          0.00005493345815921202,
          0.00005480086474562995,
          0.00005466671791509725,
          0.000054535645176656544,
          0.00005440476525109261,
          0.00005427300129667856,
          0.0000541422632522881,
          0.00005401071030064486,
          0.000053881762141827494,
          0.000053751191444462165,
          0.000053623287385562435,
          0.00005349344064597972,
          0.00005336624235496856,
          0.000053238611144479364,
          0.000053111041779629886,
          0.00005298515679896809,
          0.00005285854422254488,
          0.000052732459153048694,
          0.00005260774923954159,
          0.00005248177330940962,
          0.000052356688684085384,
          0.00005223318294156343,
          0.000052109982789261267,
          0.000051985945901833475,
          0.000051864171837223694,
          0.00005174168472876772,
          0.00005161904118722305,
          0.00005149667413206771,
          0.000051376267947489396,
          0.000051255196012789384,
          0.000051135706598870456,
          0.00005101603164803237,
          0.000050896447646664456,
          0.000050776889111148193,
          0.00005065797449788079,
          0.000050540165830170736,
          0.00005042129851062782,
          0.00005030533793615177,
          0.00005018713272875175,
          0.00005006990613765083,
          0.00004995435301680118,
          0.00004983928010915406,
          0.000049723614210961387,
          0.000049607919208938256,
          0.00004949361755279824,
          0.000049379017582396045,
          0.00004926435212837532,
          0.00004915124372928403,
          0.00004903860462945886,
          0.000048925492592388764,
          0.00004881285349256359,
          0.000048700003389967605,
          0.00004859014370595105,
          0.0000484778756799642,
          0.000048365298425778747,
          0.00004825624273507856,
          0.00004814544445252977,
          0.00004803640331374481,
          0.00004792683830601163,
          0.00004781710231327452,
          0.00004770937084686011,
          0.00004760175943374634,
          0.00004749251820612699,
          0.00004738548523164354,
          0.000047277691919589415,
          0.00004717012416222133,
          0.00004706411709776148,
          0.00004695865209214389,
          0.000046852510422468185,
          0.00004674650699598715,
          0.00004664259176934138,
          0.00004653623545891605,
          0.00004643200009013526,
          0.00004632792479242198,
          0.00004622337291948497,
          0.00004612081102095544,
          0.00004601804539561272,
          0.00004591475226334296,
          0.000045811910240445286,
          0.000045709151891060174,
          0.00004560881279758178,
          0.000045507018512580544,
          0.00004540707959677093,
          0.000045304757804842666,
          0.00004520450602285564,
          0.00004510373037192039,
          0.00004500426075537689,
          0.00004490549326874316,
          0.00004480564166442491,
          0.00004470654312171973,
          0.000044608521420741454,
          0.00004451032873475924,
          0.00004441173223312944,
          0.00004431409979588352,
          0.00004421750418259762,
          0.00004411974805407226,
          0.000044024262024322525,
          0.00004392766277305782,
          0.0000438314164057374,
          0.000043735602957895026,
          0.00004364071719464846
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line(test_losses, log_y=True)\n",
    "# plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "variable=0<br>index=%{x}<br>value=%{y}<extra></extra>",
         "legendgroup": "0",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639,
          640,
          641,
          642,
          643,
          644,
          645,
          646,
          647,
          648,
          649,
          650,
          651,
          652,
          653,
          654,
          655,
          656,
          657,
          658,
          659,
          660,
          661,
          662,
          663,
          664,
          665,
          666,
          667,
          668,
          669,
          670,
          671,
          672,
          673,
          674,
          675,
          676,
          677,
          678,
          679,
          680,
          681,
          682,
          683,
          684,
          685,
          686,
          687,
          688,
          689,
          690,
          691,
          692,
          693,
          694,
          695,
          696,
          697,
          698,
          699,
          700,
          701,
          702,
          703,
          704,
          705,
          706,
          707,
          708,
          709,
          710,
          711,
          712,
          713,
          714,
          715,
          716,
          717,
          718,
          719,
          720,
          721,
          722,
          723,
          724,
          725,
          726,
          727,
          728,
          729,
          730,
          731,
          732,
          733,
          734,
          735,
          736,
          737,
          738,
          739,
          740,
          741,
          742,
          743,
          744,
          745,
          746,
          747,
          748,
          749,
          750,
          751,
          752,
          753,
          754,
          755,
          756,
          757,
          758,
          759,
          760,
          761,
          762,
          763,
          764,
          765,
          766,
          767,
          768,
          769,
          770,
          771,
          772,
          773,
          774,
          775,
          776,
          777,
          778,
          779,
          780,
          781,
          782,
          783,
          784,
          785,
          786,
          787,
          788,
          789,
          790,
          791,
          792,
          793,
          794,
          795,
          796,
          797,
          798,
          799,
          800,
          801,
          802,
          803,
          804,
          805,
          806,
          807,
          808,
          809,
          810,
          811,
          812,
          813,
          814,
          815,
          816,
          817,
          818,
          819,
          820,
          821,
          822,
          823,
          824,
          825,
          826,
          827,
          828,
          829,
          830,
          831,
          832,
          833,
          834,
          835,
          836,
          837,
          838,
          839,
          840,
          841,
          842,
          843,
          844,
          845,
          846,
          847,
          848,
          849,
          850,
          851,
          852,
          853,
          854,
          855,
          856,
          857,
          858,
          859,
          860,
          861,
          862,
          863,
          864,
          865,
          866,
          867,
          868,
          869,
          870,
          871,
          872,
          873,
          874,
          875,
          876,
          877,
          878,
          879,
          880,
          881,
          882,
          883,
          884,
          885,
          886,
          887,
          888,
          889,
          890,
          891,
          892,
          893,
          894,
          895,
          896,
          897,
          898,
          899
         ],
         "xaxis": "x",
         "y": [
          2.301990032196045,
          2.271068572998047,
          2.23994779586792,
          2.2092206478118896,
          2.176811695098877,
          2.1434266567230225,
          2.1076362133026123,
          2.070155382156372,
          2.0340754985809326,
          1.9980801343917847,
          1.9673292636871338,
          1.9375600814819336,
          1.9140328168869019,
          1.890297293663025,
          1.8703263998031616,
          1.8486641645431519,
          1.8285305500030518,
          1.813177466392517,
          1.7977007627487183,
          1.7850780487060547,
          1.7640475034713745,
          1.7470600605010986,
          1.7282990217208862,
          1.707759141921997,
          1.684143304824829,
          1.6615384817123413,
          1.6412895917892456,
          1.615416407585144,
          1.589840292930603,
          1.5554484128952026,
          1.5207288265228271,
          1.4847495555877686,
          1.447853684425354,
          1.4046601057052612,
          1.3644217252731323,
          1.321488857269287,
          1.273688554763794,
          1.2271744012832642,
          1.1740831136703491,
          1.1145362854003906,
          1.055804967880249,
          0.998585045337677,
          0.9344625473022461,
          0.8709927797317505,
          0.8093199729919434,
          0.745267391204834,
          0.6857129335403442,
          0.6265265345573425,
          0.5715181231498718,
          0.517713725566864,
          0.468394011259079,
          0.42351245880126953,
          0.37480154633522034,
          0.33606210350990295,
          0.291221559047699,
          0.2551015317440033,
          0.22010841965675354,
          0.18895862996578217,
          0.1602020114660263,
          0.13458159565925598,
          0.11567258834838867,
          0.0959727093577385,
          0.08155813813209534,
          0.06833237409591675,
          0.05873221904039383,
          0.04990093410015106,
          0.04259960353374481,
          0.036075372248888016,
          0.031222213059663773,
          0.02733251266181469,
          0.023842643946409225,
          0.021040376275777817,
          0.018808340653777122,
          0.016962168738245964,
          0.014978194609284401,
          0.013585934415459633,
          0.012280702590942383,
          0.011148259043693542,
          0.010222080163657665,
          0.009298953227698803,
          0.008500603027641773,
          0.007950401864945889,
          0.007358204107731581,
          0.006756930146366358,
          0.006286424584686756,
          0.005878004245460033,
          0.005444241221994162,
          0.005101026501506567,
          0.004754047840833664,
          0.004473147448152304,
          0.0042833187617361546,
          0.004071654286235571,
          0.0038156979717314243,
          0.0036514431703835726,
          0.0034920405596494675,
          0.0033131130039691925,
          0.0031851574312895536,
          0.003001949517056346,
          0.0028868354856967926,
          0.002803441369906068,
          0.0027193098794668913,
          0.0025895119179040194,
          0.002522281603887677,
          0.0024464253801852465,
          0.002365282503888011,
          0.0023117659147828817,
          0.0022096955217421055,
          0.0021499854046851397,
          0.0021094840485602617,
          0.002068719593808055,
          0.001994180493056774,
          0.001963801681995392,
          0.0019234666833654046,
          0.0018742409301921725,
          0.001853539957664907,
          0.0017879009246826172,
          0.0017546433955430984,
          0.0017314881552010775,
          0.0017078530509024858,
          0.0016591619933024049,
          0.0016421641921624541,
          0.0016184309497475624,
          0.001583910547196865,
          0.001575800939463079,
          0.0015281018568202853,
          0.0015064658364281058,
          0.0014901889953762293,
          0.001474240212701261,
          0.0014386108377948403,
          0.0014276693109422922,
          0.0014114391524344683,
          0.0013845361536368728,
          0.0013822632608935237,
          0.0013434424763545394,
          0.001328883576206863,
          0.0013156587956473231,
          0.001303959870710969,
          0.0012757342774420977,
          0.0012688622809946537,
          0.0012565639335662127,
          0.0012337457155808806,
          0.0012346971780061722,
          0.00120096979662776,
          0.0011905819410458207,
          0.0011792590375989676,
          0.0011705746874213219,
          0.0011463674018159509,
          0.0011415740009397268,
          0.001132023986428976,
          0.001112615573219955,
          0.0011146877659484744,
          0.001085318741388619,
          0.0010770575609058142,
          0.0010671202326193452,
          0.001060417271219194,
          0.0010392090771347284,
          0.0010360372252762318,
          0.0010282632429152727,
          0.0010110483272001147,
          0.0010136757045984268,
          0.000987728824838996,
          0.0009809726616367698,
          0.0009720977395772934,
          0.0009667680715210736,
          0.0009480101871304214,
          0.0009458890999667346,
          0.0009394578519277275,
          0.0009238924249075353,
          0.0009269340080209076,
          0.0009037085110321641,
          0.0008980572456493974,
          0.0008900508400984108,
          0.0008857574430294335,
          0.0008689725073054433,
          0.0008675504359416664,
          0.0008621698361821473,
          0.0008480476099066436,
          0.0008513244101777673,
          0.0008304223301820457,
          0.00082563137402758,
          0.0008183288737200201,
          0.0008147937478497624,
          0.0007997145876288414,
          0.0007988822762854397,
          0.0007942959782667458,
          0.000781369861215353,
          0.0007847623201087117,
          0.0007658340036869049,
          0.0007617971277795732,
          0.000755111628677696,
          0.0007521556690335274,
          0.000738547823857516,
          0.0007381337927654386,
          0.0007342075114138424,
          0.0007223595748655498,
          0.0007257675752043724,
          0.0007085629622451961,
          0.0007051440188661218,
          0.0006990195834077895,
          0.0006965243956074119,
          0.0006841927533969283,
          0.000684079306665808,
          0.0006807096651755273,
          0.0006698283250443637,
          0.0006731941248290241,
          0.000657488708384335,
          0.0006545586511492729,
          0.0006489493534900248,
          0.0006468213396146894,
          0.000635601463727653,
          0.0006357257370837033,
          0.0006328379386104643,
          0.0006228075362741947,
          0.0006261146045289934,
          0.0006117022130638361,
          0.000609199982136488,
          0.0006040645530447364,
          0.0006022504530847073,
          0.0005919968243688345,
          0.0005923061980865896,
          0.0005898429080843925,
          0.0005805619293823838,
          0.0005838011275045574,
          0.0005705235525965691,
          0.0005683886120095849,
          0.0005636804853565991,
          0.000562127330340445,
          0.0005527297034859657,
          0.0005531699862331152,
          0.0005510695627890527,
          0.0005424552364274859,
          0.0005456233047880232,
          0.0005333537119440734,
          0.0005315309972502291,
          0.000527199765201658,
          0.0005258712917566299,
          0.0005172318196855485,
          0.0005177754792384803,
          0.0005159850115887821,
          0.0005079704569652677,
          0.000511059770360589,
          0.0004996950156055391,
          0.0004981373785994947,
          0.000494147592689842,
          0.0004930117866024375,
          0.0004850531986448914,
          0.0004856671148445457,
          0.00048414740012958646,
          0.0004766701895277947,
          0.0004796783032361418,
          0.00046912222751416266,
          0.0004677890974562615,
          0.00046411162475124,
          0.00046313609345816076,
          0.00045578545541502535,
          0.0004564494302030653,
          0.00045515852980315685,
          0.00044817267917096615,
          0.00045109959319233894,
          0.00044126613647677004,
          0.0004401266050990671,
          0.00043673402979038656,
          0.00043589179404079914,
          0.0004290817305445671,
          0.0004297848790884018,
          0.0004286973853595555,
          0.0004221529816277325,
          0.00042499788105487823,
          0.0004158135561738163,
          0.00041483977111056447,
          0.0004117040953133255,
          0.000410973938414827,
          0.00040465276106260717,
          0.00040537910535931587,
          0.00040447135688737035,
          0.00039832876063883305,
          0.0004010929842479527,
          0.0003924966440536082,
          0.00039166491478681564,
          0.0003887642815243453,
          0.00038812780985608697,
          0.0003822518337983638,
          0.0003829921188298613,
          0.0003822420258074999,
          0.00037646389682777226,
          0.0003791486378759146,
          0.000371084752259776,
          0.0003703770926222205,
          0.0003676869091577828,
          0.0003671359736472368,
          0.00036165924393571913,
          0.00036240750341676176,
          0.00036179713788442314,
          0.0003563494828995317,
          0.0003589499683585018,
          0.00035137261147610843,
          0.0003507730725686997,
          0.0003482690663076937,
          0.0003477955178823322,
          0.00034268127637915313,
          0.0003434311074670404,
          0.00034294271608814597,
          0.00033779931254684925,
          0.0003403176961001009,
          0.0003331846965011209,
          0.000332680472638458,
          0.0003303417470306158,
          0.000329941714880988,
          0.0003251537855248898,
          0.0003259000077378005,
          0.00032551976619288325,
          0.0003206559631507844,
          0.0003230949805583805,
          0.00031636565108783543,
          0.00031594614847563207,
          0.00031375783146359026,
          0.00031342310830950737,
          0.00030893218354322016,
          0.0003096718282904476,
          0.00030937898554839194,
          0.0003047810459975153,
          0.00030713420710526407,
          0.0003007777559105307,
          0.00030043229344300926,
          0.0002983839367516339,
          0.0002981042198371142,
          0.00029388206894509494,
          0.00029461688245646656,
          0.0002944006701000035,
          0.00029004650423303246,
          0.0002923195715993643,
          0.00028630546876229346,
          0.000286028312984854,
          0.00028410652885213494,
          0.0002838734944816679,
          0.0002798969217110425,
          0.0002806284464895725,
          0.0002804772520903498,
          0.0002763477386906743,
          0.00027854222571477294,
          0.00027284384123049676,
          0.0002726298407651484,
          0.00027082403539679945,
          0.0002706336963456124,
          0.00026688427897170186,
          0.00026760660693980753,
          0.0002675143477972597,
          0.00026358963805250823,
          0.00026571014313958585,
          0.0002603039611130953,
          0.0002601470332592726,
          0.00025844815536402166,
          0.00025829518563114107,
          0.00025474984431639314,
          0.0002554673992563039,
          0.00025542356888763607,
          0.0002516921085771173,
          0.0002537420659791678,
          0.00024860582198016346,
          0.00024849988403730094,
          0.00024689818383194506,
          0.0002467793528921902,
          0.00024342343385796994,
          0.00024413126811850816,
          0.0002441310789436102,
          0.00024057865084614605,
          0.00024255788594018668,
          0.00023767542734276503,
          0.00023761378542985767,
          0.00023610061907675117,
          0.00023601397697348148,
          0.00023283051268663257,
          0.0002335268072783947,
          0.0002335682074772194,
          0.00023018075444269925,
          0.00023209214850794524,
          0.00022744557645637542,
          0.00022742045985069126,
          0.0002259925240650773,
          0.00022593094035983086,
          0.000222908376599662,
          0.00022359447029884905,
          0.00022366979101207107,
          0.00022043616627343,
          0.00022228500165510923,
          0.00021785787248518318,
          0.00021786575962323695,
          0.00021651785937137902,
          0.00021647852554451674,
          0.0002136028342647478,
          0.00021427958563435823,
          0.00021438280236907303,
          0.00021129508968442678,
          0.00021308135183062404,
          0.00020885842968709767,
          0.00020889700681436807,
          0.00020762217172887176,
          0.00020760312327183783,
          0.00020486643188633025,
          0.00020553162903524935,
          0.00020566035527735949,
          0.00020270806271582842,
          0.00020443453104235232,
          0.00020040613890159875,
          0.00020046760619152337,
          0.0001992624020203948,
          0.00019926088862121105,
          0.0001966545096365735,
          0.0001973066246137023,
          0.00019745994359254837,
          0.00019463167700450867,
          0.00019630312453955412,
          0.00019245412840973586,
          0.00019253518257755786,
          0.00019139597134198993,
          0.00019141126540489495,
          0.00018892389198299497,
          0.00018956430722028017,
          0.00018973635451402515,
          0.00018702638044487685,
          0.00018864368030335754,
          0.000184961871127598,
          0.00018506291962694377,
          0.0001839831384131685,
          0.00018401276611257344,
          0.00018163624918088317,
          0.00018226401880383492,
          0.0001824552018661052,
          0.0001798549055820331,
          0.00018141807231586426,
          0.0001778965670382604,
          0.00017801379726734012,
          0.00017698899318929762,
          0.00017703097546473145,
          0.00017475987260695547,
          0.0001753754186211154,
          0.00017558271065354347,
          0.00017308622773271054,
          0.00017459677474107593,
          0.00017122688586823642,
          0.00017135788220912218,
          0.00017038661462720484,
          0.0001704374299151823,
          0.0001682633883319795,
          0.00016886909725144506,
          0.00016908835095819086,
          0.00016668780881445855,
          0.0001681517605902627,
          0.00016492295253556222,
          0.0001650651392992586,
          0.00016414219862781465,
          0.00016420264728367329,
          0.00016212051559705287,
          0.00016271538333967328,
          0.0001629468606552109,
          0.00016063857765402645,
          0.00016205667634494603,
          0.0001589602034073323,
          0.00015911283844616264,
          0.0001582356489961967,
          0.0001583019911777228,
          0.0001563067053211853,
          0.0001568914158269763,
          0.0001571332395542413,
          0.0001549104053992778,
          0.00015628435357939452,
          0.00015331286704167724,
          0.00015347350563388318,
          0.00015263927343767136,
          0.00015271086886059493,
          0.00015079951845109463,
          0.0001513726165285334,
          0.00015162226918619126,
          0.0001494823954999447,
          0.0001508150016888976,
          0.0001479599013691768,
          0.00014812688459642231,
          0.00014733619173057377,
          0.0001474116725148633,
          0.0001455760357202962,
          0.00014613995153922588,
          0.00014639760775025934,
          0.000144334597280249,
          0.00014562490105163306,
          0.00014288185047917068,
          0.0001430543779861182,
          0.00014230083615984768,
          0.00014238145377021283,
          0.00014061776164453477,
          0.00014117239334154874,
          0.0001414365106029436,
          0.0001394440623698756,
          0.00014069740427657962,
          0.00013805933122057468,
          0.000138235351187177,
          0.00013751850929111242,
          0.0001376036088913679,
          0.00013590687012765557,
          0.0001364522468065843,
          0.00013672148634213954,
          0.00013479792687576264,
          0.00013601405953522772,
          0.00013347754429560155,
          0.00013365519407670945,
          0.00013297161785885692,
          0.0001330589147983119,
          0.0001314260734943673,
          0.0001319626608164981,
          0.00013223545101936907,
          0.0001303771568927914,
          0.0001315593544859439,
          0.000129115825984627,
          0.00012929728836752474,
          0.0001286463375436142,
          0.00012873606465291232,
          0.00012716064520645887,
          0.00012768949090968817,
          0.00012796740338671952,
          0.00012617082393262535,
          0.00012731879542116076,
          0.000124965314171277,
          0.00012514674745034426,
          0.00012452545342966914,
          0.00012461686856113374,
          0.00012309869634918869,
          0.00012361937842797488,
          0.00012389985204208642,
          0.00012216343020554632,
          0.00012327880540397018,
          0.0001210101690958254,
          0.00012119189341319725,
          0.00012059907021466643,
          0.00012069315562257543,
          0.00011922812700504437,
          0.00011973911750828847,
          0.00012002227595075965,
          0.00011833906319225207,
          0.00011942535638809204,
          0.00011723684292519465,
          0.00011741912749130279,
          0.00011685430217767134,
          0.00011694955173879862,
          0.0001155335339717567,
          0.00011603676102822646,
          0.00011632197856670246,
          0.00011469283344922587,
          0.0001157494043582119,
          0.00011363637167960405,
          0.00011382007505744696,
          0.00011327788524795324,
          0.0001133746700361371,
          0.0001120075976359658,
          0.0001125007911468856,
          0.00011278777674306184,
          0.00011121023271698505,
          0.0001122385510825552,
          0.00011019755766028538,
          0.00011038122465834022,
          0.00010986238339683041,
          0.00010995881166309118,
          0.00010863868374144658,
          0.00010912407014984637,
          0.00010940999345621094,
          0.00010788103099912405,
          0.00010888336692005396,
          0.00010691144416341558,
          0.00010709406342357397,
          0.00010659772669896483,
          0.0001066961485776119,
          0.00010542001109570265,
          0.00010589690646156669,
          0.00010618211672408506,
          0.00010469984408700839,
          0.00010567633580649272,
          0.0001037681577145122,
          0.00010395183926448226,
          0.00010347592615289614,
          0.00010357455175835639,
          0.00010233847569907084,
          0.00010280816786689684,
          0.00010309489880455658,
          0.00010165637650061399,
          0.00010260617273161188,
          0.00010076124453917146,
          0.00010094331082655117,
          0.00010048670083051547,
          0.0001005862868623808,
          0.00009938951552612707,
          0.00009985054202843457,
          0.00010013727296609432,
          0.00009873984527075663,
          0.00009966702054953203,
          0.00009787989984033629,
          0.00009806102752918378,
          0.00009762388799572363,
          0.00009772311750566587,
          0.00009656447218731046,
          0.00009701723320176825,
          0.0000973042770056054,
          0.00009594548464519903,
          0.00009685056284070015,
          0.00009511932148598135,
          0.0000952991031226702,
          0.0000948795277508907,
          0.00009497906285105273,
          0.00009385561861563474,
          0.00009430235513718799,
          0.0000945873252931051,
          0.0000932676630327478,
          0.0000941497492021881,
          0.00009247234993381426,
          0.00009265122207580134,
          0.00009224807581631467,
          0.00009234821482095867,
          0.00009125890210270882,
          0.00009169741679215804,
          0.00009198264160659164,
          0.00009069905354408547,
          0.00009155816223938018,
          0.00008993388473754749,
          0.0000901096427696757,
          0.00008972325304057449,
          0.00008982264262158424,
          0.000088765453256201,
          0.00008919710671762004,
          0.00008948146569309756,
          0.00008823328244034201,
          0.00008907149458536878,
          0.00008749649714445695,
          0.00008767056715441868,
          0.00008729867840884253,
          0.00008739794429857284,
          0.00008637202699901536,
          0.00008679823076818138,
          0.00008708085806574672,
          0.00008586419426137581,
          0.00008668270311318338,
          0.00008515497029293329,
          0.00008532760693924502,
          0.00008497003000229597,
          0.00008506996528012678,
          0.00008407289715250954,
          0.0000844916285132058,
          0.00008477184746880084,
          0.00008358899503946304,
          0.00008438771328656003,
          0.00008290573896374553,
          0.00008307619282277301,
          0.0000827311523607932,
          0.00008282998169306666,
          0.00008186345075955614,
          0.00008227559010265395,
          0.00008255357533926144,
          0.00008140203863149509,
          0.00008218085713451728,
          0.00008074261131696403,
          0.00008090945630101487,
          0.00008057924424065277,
          0.00008067831367952749,
          0.00007973733590915799,
          0.00008014315244508907,
          0.00008042050467338413,
          0.0000792976570664905,
          0.00008005924610188231,
          0.00007866100349929184,
          0.00007882807403802872,
          0.00007850825932109728,
          0.00007860839104978368,
          0.00007769309013383463,
          0.00007809328963048756,
          0.00007836746226530522,
          0.00007727299089310691,
          0.00007801698666298762,
          0.00007665866723982617,
          0.00007682364230277017,
          0.00007651581836398691,
          0.0000766144585213624,
          0.00007572530739707872,
          0.00007611827459186316,
          0.00007639094110345468,
          0.00007532360177719966,
          0.00007605203427374363,
          0.00007473136065527797,
          0.00007489421841455624,
          0.00007459901098627597,
          0.00007469661068171263,
          0.00007383071351796389,
          0.00007421689224429429,
          0.00007448897667927667,
          0.00007344682671828195,
          0.00007415891013806686,
          0.00007287473272299394,
          0.00007303550228243694,
          0.00007274961535586044,
          0.00007284804451046512,
          0.00007200597610790282,
          0.0000723857301636599,
          0.00007265424210345373,
          0.00007163833652157336,
          0.00007233552605612203,
          0.0000710843742126599,
          0.00007124624971766025,
          0.00007096906483639032,
          0.00007106604607542977,
          0.00007024521619314328,
          0.00007062072836561128,
          0.00007088622805895284,
          0.0000698945441399701,
          0.00007057568291202188,
          0.00006935915007488802,
          0.00006951892282813787,
          0.00006925214984221384,
          0.00006934661359991878,
          0.00006854895036667585,
          0.00006891774683026597,
          0.0000691800523782149,
          0.00006821381248300895,
          0.00006887941708555445,
          0.00006769465107936412,
          0.00006785267032682896,
          0.00006759525422239676,
          0.00006768861203454435,
          0.00006691108137601987,
          0.0000672740425216034,
          0.00006753613706678152,
          0.00006659032078459859,
          0.00006724255945300683,
          0.00006608951662201434,
          0.00006624406523769721,
          0.00006599527841899544,
          0.00006608924741158262,
          0.00006533063424285501,
          0.00006568824028363451,
          0.00006594687147298828,
          0.00006502569158328697,
          0.00006566262163687497,
          0.00006453745299950242,
          0.00006469259096775204,
          0.000064452899096068,
          0.0000645443651592359,
          0.0000638034543953836,
          0.00006415706593543291,
          0.00006441424193326384,
          0.0000635113101452589,
          0.00006413574010366574,
          0.00006304005364654586,
          0.00006319398380583152,
          0.00006295914499787614,
          0.00006305065471678972,
          0.00006232900341274217,
          0.00006267743447097018,
          0.00006293020851444453,
          0.00006205063255038112,
          0.00006266161653911695,
          0.00006159243639558554,
          0.00006174383452162147,
          0.00006152060814201832,
          0.00006160731572890654,
          0.000060904480051249266,
          0.00006124709034338593,
          0.000061498794821091,
          0.00006063778346288018,
          0.00006123663479229435,
          0.00006019217107677832,
          0.000060344140365486965,
          0.000060125708841951564,
          0.00006021382432663813,
          0.0000595268102188129,
          0.000059864858485525474,
          0.000060114263760624453,
          0.000059271755162626505,
          0.000059858051827177405,
          0.000058838904806179926,
          0.00005898890594835393,
          0.00005877863077330403,
          0.00005886489088879898,
          0.000058194105804432184,
          0.000058528130466584116,
          0.00005877532021258958,
          0.00005795078686787747,
          0.00005852470712852664,
          0.000057531229685992,
          0.000057680044847074896,
          0.000057475710491416976,
          0.00005756008977186866,
          0.00005690605757990852,
          0.00005723576759919524,
          0.00005747838440584019,
          0.000056671906349947676,
          0.00005723509821109474,
          0.00005626518031931482,
          0.000056412332924082875,
          0.000056214277719845995,
          0.00005629828592645936,
          0.00005565887113334611,
          0.00005598340794676915,
          0.00005622449316433631,
          0.00005543442603084259,
          0.00005598719508270733,
          0.000055039996368577704,
          0.00005518439138540998,
          0.00005499413964571431,
          0.00005507491368916817,
          0.000054452371841762215,
          0.000054772128351032734,
          0.00005500989573192783,
          0.00005423790935310535,
          0.00005477888771565631,
          0.00005385379699873738,
          0.00005399627843871713,
          0.000053811760153621435,
          0.000053891355491941795,
          0.000053282052249414846,
          0.000053598032536683604,
          0.00005383332245401107,
          0.0000530778088432271,
          0.00005360872091841884,
          0.000052702973334817216,
          0.000052846404287265614,
          0.000052666349802166224,
          0.00005274572686175816,
          0.000052149513066979125,
          0.00005246133150649257,
          0.000052693369070766494,
          0.000051953607908217236,
          0.00005247439185041003,
          0.00005158861677045934,
          0.00005172856253921054,
          0.00005155688995728269,
          0.00005163446985534392,
          0.00005105164018459618,
          0.00005135930405231193,
          0.00005158887506695464,
          0.000050863975047832355,
          0.000051374907343415543,
          0.00005050955951446667,
          0.00005064783545094542,
          0.000050481397920520976,
          0.00005055640576756559,
          0.00004998631993657909,
          0.000050290487706661224,
          0.00005051810512668453,
          0.0000498075460200198,
          0.00005030847023590468,
          0.00004946239278069697,
          0.00004959994839737192,
          0.00004943623207509518,
          0.00004951289156451821,
          0.00004895469101029448,
          0.00004925378743791953,
          0.000049478327127872035,
          0.0000487815668748226,
          0.000049275255150860175,
          0.00004844596332986839,
          0.00004858102693106048,
          0.000048424219130538404,
          0.000048499197873752564,
          0.000047953602916095406,
          0.000048247802624246106,
          0.00004846909359912388,
          0.00004778803850058466,
          0.00004827157317777164,
          0.000047460209316341206,
          0.00004759611329063773,
          0.00004744193938677199,
          0.00004751505912281573,
          0.00004698144766734913,
          0.00004727340638055466,
          0.00004749152503791265,
          0.00004682282451540232,
          0.00004729807915282436,
          0.000046502846089424565,
          0.00004663770232582465,
          0.00004648849062505178,
          0.00004656014425563626,
          0.00004603829074767418,
          0.000046326244046213105,
          0.000046541401388822123,
          0.00004588545925798826,
          0.00004635014192899689,
          0.00004557508509606123,
          0.00004570792953018099,
          0.00004556403655442409,
          0.000045633543777512386,
          0.0000451212799816858,
          0.00004540538429864682,
          0.000045619421143783256,
          0.000044976233766647056,
          0.000045432938350131735,
          0.00004467231337912381,
          0.000044805212382925674,
          0.00004466392783797346,
          0.00004473371882340871,
          0.00004423230348038487,
          0.00004451301720109768,
          0.000044723052269546315,
          0.00004409309622133151,
          0.0000445414443674963,
          0.00004379630263429135,
          0.0000439273972006049
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": ""
         },
         "type": "log"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line(train_losses, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0.014752309,
           -0.002199336,
           -0.022274066,
           -0.06302561,
           0.044890065,
           -0.0009941083,
           0.071683705,
           -0.00015690693,
           0.027712185,
           0.08469321,
           -0.0962048,
           -0.03757114,
           0.015599962,
           -0.018267842,
           -0.066812105,
           0.113961294,
           0.09983583,
           -0.0087002115,
           0.022183293,
           0.07278904,
           -0.14684866,
           0.03843585,
           0.105957486,
           0.046239696,
           0.01199106,
           -0.11242635,
           -0.08331568,
           -0.023323644,
           0.035670776,
           -0.06018816,
           0.10513418,
           0.17886025,
           -0.050274726,
           -0.021443218,
           0.076659955,
           0.0138869,
           0.013439135,
           0.08315471,
           -0.07783359,
           -0.019084485,
           -0.03554732,
           -0.06432425,
           0.039224017,
           -0.110145934,
           -0.08443305,
           0.0423576,
           -0.04425829,
           -0.05175177,
           0.113829,
           -0.056757946,
           0.09189604,
           -0.02227875,
           -0.11300123,
           0.14412107,
           0.19991024,
           -0.122450545,
           0.10683246,
           -0.10485492,
           0.060866985,
           -0.015500544,
           0.056496788,
           0.11254506,
           0.11488245,
           -0.026283696,
           -0.06342319,
           0.041749746,
           -0.0000141357095,
           -0.0070778867,
           -0.036104247,
           0.03746629,
           -0.018122233,
           -0.07687488,
           0.14637648,
           -0.042932626,
           0.017913084,
           0.051161684,
           -0.10575777,
           0.061359573,
           0.011049052,
           0.018432505,
           0.17855151,
           -0.08891975,
           0.07456202,
           0.036295116,
           -0.11272181,
           -0.088649005,
           0.04671096,
           -0.012086218,
           0.028058823,
           -0.028669925,
           0.033530314,
           -0.055054344,
           -0.030694168,
           -0.03459711,
           -0.07281302,
           -0.033682737,
           -0.112966165,
           -0.09979864,
           -0.010531002,
           0.06434596,
           -0.0015813474,
           -0.048396118,
           -0.021363905,
           -0.0020921,
           -0.009524214,
           -0.008952779,
           0.024032813,
           0.100725174,
           0.007351654,
           0.18362328,
           -0.048738036,
           -0.14336437,
           -0.019589914,
           0.0017734175,
           0.06942018,
           -0.13674624,
           0.011639224,
           0.036391277,
           -0.12871817,
           0.04169823,
           0.12821244,
           0.019277882,
           0.019242603,
           -0.044941172,
           -0.04457929,
           0.020387404,
           -0.042451013,
           -0.10680266
          ],
          [
           -0.14898442,
           0.13286471,
           0.09748025,
           -0.03507746,
           0.09528829,
           0.12323746,
           -0.00801605,
           -0.122728474,
           0.00067803595,
           0.008186264,
           0.077280834,
           0.048318785,
           0.019078977,
           -0.04911135,
           0.052747164,
           0.059166525,
           0.16640337,
           0.07201371,
           -0.112493,
           -0.022139287,
           -0.009326849,
           -0.025262536,
           -0.04781972,
           0.07964386,
           -0.07093291,
           0.009467075,
           0.0841102,
           -0.013300309,
           -0.075344115,
           -0.04954478,
           0.11595552,
           -0.05315358,
           -0.059028316,
           0.004108684,
           -0.0037659877,
           0.16973542,
           -0.19619593,
           0.040720895,
           -0.060945537,
           -0.015963405,
           0.0029166653,
           0.14166951,
           -0.013800971,
           -0.01634153,
           -0.097017504,
           -0.009245876,
           -0.13619518,
           -0.10045376,
           -0.026985634,
           -0.08158779,
           0.011786248,
           0.015363821,
           0.05228769,
           0.06639244,
           -0.12542096,
           0.09329719,
           0.06530194,
           0.077768646,
           0.047186546,
           -0.094891556,
           -0.051343527,
           0.15290223,
           0.03411392,
           0.09043861,
           0.012214674,
           -0.020971168,
           0.1287862,
           0.10224707,
           0.080043726,
           -0.046540055,
           -0.03480715,
           0.04044979,
           0.0041026142,
           -0.123450324,
           0.0792714,
           0.034387056,
           -0.08847141,
           -0.068915725,
           -0.005851341,
           0.004740282,
           0.022995673,
           -0.04780576,
           -0.023032013,
           -0.034848396,
           0.043182343,
           0.041241758,
           0.09100632,
           0.053560026,
           0.03607448,
           0.05808124,
           -0.019715145,
           -0.05066159,
           -0.043173067,
           0.022384977,
           -0.041077316,
           -0.028775327,
           -0.039686013,
           -0.018842213,
           0.000033943183,
           -0.025197558,
           -0.0034502784,
           0.055917036,
           -0.0022586016,
           -0.029020362,
           0.0518868,
           -0.10586822,
           0.03950973,
           -0.13274,
           -0.0731839,
           0.0021889107,
           -0.030090082,
           -0.029026384,
           0.110623166,
           -0.0053811334,
           -0.05470736,
           -0.08559338,
           0.031779416,
           -0.015949814,
           -0.004769566,
           -0.06040441,
           -0.019388616,
           0.013524798,
           -0.06239094,
           -0.0940246,
           0.082749136,
           -0.13829191,
           -0.03908753,
           0.077760994
          ],
          [
           0.033862956,
           -0.078739524,
           -0.058606848,
           0.13405241,
           0.057475813,
           -0.0065625543,
           0.103204995,
           0.021708895,
           -0.1346174,
           0.013074939,
           -0.08393035,
           -0.0075507504,
           -0.12918767,
           -0.055842113,
           -0.03170245,
           -0.090226494,
           -0.019039178,
           0.06572845,
           -0.0494497,
           0.011396898,
           0.057499446,
           -0.007107801,
           -0.054172713,
           0.07185734,
           -0.053906266,
           -0.11077443,
           -0.19962144,
           -0.20342603,
           -0.017327072,
           0.006093719,
           -0.043753088,
           -0.18403904,
           0.00048297818,
           0.07284097,
           -0.08639285,
           0.016598307,
           -0.047660634,
           -0.023803946,
           -0.09561683,
           -0.07903224,
           0.017164834,
           -0.06096406,
           0.04972345,
           0.03750264,
           0.051119205,
           -0.09484917,
           0.03581879,
           0.008051612,
           0.028098587,
           -0.11438892,
           -0.05654954,
           0.004082696,
           0.07378889,
           -0.021203186,
           0.07396184,
           -0.14418897,
           -0.09898262,
           0.08745571,
           -0.08112901,
           0.037574038,
           -0.09669033,
           0.05971817,
           -0.083195545,
           0.09378606,
           0.15891427,
           -0.01615618,
           0.03701688,
           0.18435596,
           -0.0331078,
           -0.00985925,
           0.030500619,
           -0.021451626,
           -0.09966484,
           -0.0012696797,
           -0.017455732,
           -0.11418187,
           -0.0011696211,
           -0.0053324904,
           -0.04072831,
           -0.03339945,
           -0.017350206,
           -0.1535838,
           0.058363292,
           0.050719403,
           -0.08752393,
           0.044855658,
           -0.1488501,
           -0.034903426,
           -0.049267843,
           -0.05150723,
           0.021137621,
           -0.09213544,
           0.020179035,
           -0.049729053,
           0.07946012,
           -0.059595667,
           0.0237719,
           0.09110586,
           0.092236206,
           -0.024232673,
           -0.15027651,
           0.07482916,
           -0.07311932,
           0.06882286,
           -0.0037272344,
           0.07948868,
           0.0128787905,
           0.03696663,
           0.12109694,
           0.045381315,
           -0.1321703,
           0.08875304,
           -0.024073014,
           -0.1390264,
           0.0407393,
           0.0603654,
           -0.059650574,
           0.032646257,
           -0.06687186,
           -0.05509694,
           0.010335129,
           0.039029498,
           0.001248939,
           -0.06395212,
           0.10676351,
           0.031965267,
           -0.029976245,
           0.0290008
          ],
          [
           0.0063405195,
           -0.0059513054,
           0.0625106,
           -0.03547431,
           0.06991947,
           0.11658829,
           0.027152475,
           0.021601327,
           -0.03082104,
           -0.051282506,
           0.06712607,
           -0.07045371,
           0.095390275,
           -0.15763277,
           0.041944347,
           -0.01129418,
           0.009911668,
           0.0045021796,
           -0.08118686,
           -0.07675572,
           -0.021093087,
           -0.051743615,
           0.011626737,
           -0.09976503,
           0.069460444,
           -0.05416398,
           -0.12258536,
           0.052274156,
           -0.018694373,
           0.021398641,
           0.015461048,
           0.040082283,
           0.023795143,
           0.08662922,
           -0.15377048,
           -0.07374549,
           0.07313229,
           0.23104887,
           -0.26921153,
           -0.037703995,
           0.05156787,
           -0.109088905,
           -0.04224523,
           -0.02239987,
           0.0418965,
           -0.08868316,
           0.07130134,
           0.050788455,
           -0.09801015,
           0.08968737,
           0.0039763898,
           -0.15679358,
           -0.077866964,
           0.1788045,
           0.11340472,
           -0.12993677,
           0.14479423,
           0.010307967,
           0.0055693095,
           -0.01145286,
           0.12157863,
           0.042280648,
           0.04775216,
           0.04111583,
           0.009907022,
           -0.020938557,
           0.052769333,
           -0.064418726,
           -0.11625707,
           -0.06633566,
           -0.10704983,
           0.019721365,
           -0.118535995,
           0.028013654,
           0.036211435,
           -0.050212573,
           -0.073073946,
           0.0017917367,
           -0.00011129086,
           0.0076076863,
           0.01023095,
           -0.110896945,
           -0.14561562,
           -0.08726454,
           0.0011427426,
           0.04590261,
           -0.033727057,
           -0.016894154,
           0.0012031336,
           -0.0015367379,
           -0.0012580513,
           -0.08910896,
           0.08323511,
           -0.114313394,
           0.097508475,
           0.054683108,
           0.049637776,
           -0.025127541,
           -0.074379064,
           0.06413137,
           0.09156549,
           0.040044956,
           -0.03459311,
           -0.038119044,
           0.08676588,
           -0.045314755,
           -0.002528733,
           -0.08406715,
           0.03654299,
           0.028554745,
           -0.043511197,
           0.09118754,
           -0.052535966,
           0.02719249,
           0.0118861655,
           -0.15344895,
           0.1230847,
           -0.0032920497,
           -0.008943698,
           0.072590806,
           -0.0034495792,
           0.022151526,
           0.03902857,
           -0.06058976,
           0.12632899,
           0.0075141517,
           -0.022592362,
           -0.03278441
          ],
          [
           -0.041330203,
           0.13205367,
           0.025020327,
           0.0035379208,
           0.15412028,
           -0.051320393,
           -0.11688143,
           -0.012513233,
           0.08389302,
           0.065168716,
           -0.1678082,
           -0.097068995,
           -0.07204511,
           -0.14492077,
           -0.0074997684,
           -0.05452908,
           0.061948434,
           -0.10749898,
           0.007957359,
           -0.122413985,
           0.14605086,
           -0.036401615,
           -0.005464544,
           -0.020730196,
           0.09198924,
           -0.019404182,
           -0.08452754,
           -0.05986945,
           -0.02050583,
           0.068266496,
           0.064024396,
           0.013580649,
           -0.016389227,
           -0.014279221,
           0.0013206166,
           0.0652323,
           0.06324774,
           -0.010819065,
           0.05383161,
           0.1047092,
           -0.03214215,
           -0.08458384,
           0.021271309,
           0.025851093,
           -0.10791918,
           -0.028102921,
           0.052837115,
           0.024941038,
           0.074434645,
           0.02291016,
           0.005359633,
           -0.010372884,
           0.026934255,
           0.014020709,
           -0.09204371,
           0.124574065,
           0.06881242,
           0.013243023,
           0.033042032,
           0.019480841,
           -0.026598576,
           0.051735617,
           0.056562945,
           -0.03823224,
           -0.07701602,
           0.04094051,
           -0.060245126,
           0.03143453,
           0.015788896,
           -0.16175318,
           0.02596144,
           0.110508494,
           -0.0787361,
           -0.0685294,
           -0.04492598,
           0.030602925,
           0.02274928,
           -0.062649935,
           0.027979847,
           -0.042161092,
           0.00015904443,
           -0.03262091,
           0.0030344438,
           0.12335146,
           -0.1494038,
           -0.13697834,
           -0.057473898,
           0.025339412,
           0.08765759,
           0.0215353,
           0.0018863049,
           0.06492265,
           -0.11448172,
           0.07656665,
           0.038984273,
           -0.03489051,
           0.09131745,
           -0.15391229,
           0.07168845,
           -0.11494643,
           0.025012674,
           -0.07071253,
           0.17412622,
           -0.05171462,
           0.18885934,
           0.07121881,
           -0.030015895,
           -0.011278195,
           -0.00094193313,
           -0.0039953906,
           -0.16413373,
           -0.0029563126,
           -0.0039664237,
           0.03229649,
           0.027079875,
           0.026848264,
           0.003573777,
           0.18847105,
           -0.09523581,
           -0.03365446,
           -0.017945992,
           -0.096263,
           -0.00667267,
           -0.00339467,
           -0.07140714,
           0.0436297,
           0.014037083,
           0.039862216
          ],
          [
           -0.050970864,
           0.009010234,
           0.08992501,
           -0.051782846,
           -0.041731384,
           0.033428628,
           -0.15752676,
           0.019217573,
           0.0072658416,
           -0.21960635,
           -0.09458176,
           -0.022239994,
           -0.048625275,
           -0.051485296,
           0.044361603,
           0.042147912,
           0.007427966,
           -0.13178523,
           0.0959572,
           0.12363244,
           0.04166715,
           -0.089127325,
           -0.08443788,
           0.051506083,
           0.0038562568,
           0.25817108,
           -0.0098843295,
           -0.036419723,
           -0.099316806,
           0.007323368,
           -0.076740764,
           0.034708682,
           -0.050887156,
           -0.05440117,
           -0.01697347,
           0.05243532,
           -0.00000974215,
           -0.08812413,
           0.06388126,
           -0.08796397,
           -0.15382734,
           -0.14297363,
           0.054866638,
           -0.043313082,
           0.07479592,
           -0.026924241,
           -0.015721282,
           -0.13221574,
           0.04122382,
           0.011718642,
           -0.00090237893,
           -0.07608579,
           0.058824763,
           0.02878115,
           -0.1450287,
           0.07457184,
           0.037197493,
           0.039608516,
           -0.03758227,
           -0.00079683855,
           0.01960614,
           -0.14434691,
           -0.08221075,
           -0.014319112,
           0.07797567,
           0.08434972,
           0.037686974,
           -0.060375396,
           0.028083216,
           -0.000005989776,
           0.055614524,
           0.12860641,
           0.1046007,
           -0.09648477,
           0.09171699,
           -0.021883912,
           0.02537751,
           -0.024155144,
           -0.1399842,
           -0.09624381,
           -0.0032747386,
           -0.09740391,
           -0.07214907,
           0.09667033,
           0.029123928,
           0.03861668,
           0.06871662,
           0.05645386,
           0.16772434,
           -0.114905685,
           -0.12920865,
           0.025908755,
           0.008134128,
           0.038862646,
           0.002702624,
           0.006985457,
           0.008498845,
           0.08165752,
           0.052710924,
           0.027589979,
           -0.064375505,
           -0.08925745,
           0.0030001828,
           0.0032622996,
           0.033588517,
           0.07957424,
           0.009315557,
           0.0018604903,
           -0.025983434,
           -0.018270794,
           -0.1848505,
           -0.0075808545,
           -0.07239679,
           0.010796247,
           0.117049165,
           -0.043445487,
           -0.10481062,
           -0.014778385,
           -0.03468563,
           -0.05850474,
           -0.00048561362,
           0.037296753,
           0.04405932,
           -0.107939705,
           -0.05659108,
           0.029401455,
           -0.06476393,
           0.0380444
          ],
          [
           0.023726903,
           0.0780196,
           0.02522304,
           -0.07835848,
           -0.0054707457,
           -0.09116944,
           -0.05402302,
           0.051625,
           0.05297353,
           -0.023034802,
           0.00020359886,
           0.018249858,
           -0.1467489,
           -0.036960017,
           0.009961784,
           0.051162127,
           0.02269293,
           -0.10861997,
           0.04519758,
           -0.016183702,
           -0.013430891,
           0.03154047,
           0.03959418,
           -0.035794802,
           0.011903068,
           -0.13335995,
           0.045820594,
           -0.06474634,
           -0.10239114,
           0.12005616,
           -0.03169407,
           0.03694404,
           -0.070204094,
           0.07116124,
           -0.15870848,
           0.027672287,
           -0.021580713,
           -0.07771718,
           -0.0759437,
           0.15835387,
           -0.0773395,
           -0.089057244,
           0.070091076,
           0.030360132,
           -0.13052803,
           0.01993148,
           -0.016847169,
           -0.109246284,
           -0.047126785,
           0.14658654,
           -0.047587782,
           0.118802,
           -0.05476482,
           0.04356722,
           0.075081006,
           -0.028856825,
           -0.028379295,
           0.014177686,
           0.090358965,
           0.0005642054,
           0.0020398844,
           -0.05249997,
           0.008765151,
           -0.026162125,
           0.10834606,
           0.075750686,
           0.099404685,
           0.11981446,
           0.053889975,
           0.09915462,
           -0.05263951,
           0.10740014,
           -0.06679132,
           0.14876972,
           0.0037947076,
           0.08107521,
           -0.02323634,
           0.017896332,
           0.06235454,
           0.024687184,
           -0.004196225,
           -0.14069514,
           0.10875849,
           -0.08693421,
           0.11140186,
           0.042932775,
           0.005053007,
           0.1175651,
           -0.0754557,
           -0.036083266,
           0.04399749,
           -0.09335605,
           -0.1308876,
           -0.0061259437,
           -0.019834207,
           -0.054428462,
           0.030290749,
           0.03617815,
           0.03915918,
           0.08077602,
           0.008658505,
           0.090277396,
           -0.06585046,
           -0.05496249,
           -0.027153492,
           -0.026319245,
           -0.030475644,
           -0.0061693434,
           0.036686163,
           0.04746251,
           0.16606371,
           0.0528496,
           -0.019305771,
           0.10448271,
           -0.02122185,
           -0.12652074,
           0.03997033,
           -0.027022772,
           0.17222883,
           0.013085254,
           -0.09499217,
           -0.06922001,
           -0.13867716,
           0.0005555211,
           0.0030058268,
           -0.19864541,
           0.07572464,
           -0.07220181
          ],
          [
           0.06025659,
           -0.08952401,
           -0.034732636,
           0.0051879543,
           -0.026185028,
           0.024201425,
           -0.067510776,
           -0.01289628,
           -0.047104087,
           -0.043402825,
           -0.04009965,
           -0.06872842,
           -0.041857503,
           -0.02465963,
           -0.0555916,
           -0.028638218,
           0.06883919,
           -0.04311869,
           0.010362176,
           0.020569755,
           0.013655041,
           0.09950284,
           -0.039638907,
           -0.04123211,
           -0.12529586,
           -0.012344073,
           0.014185579,
           0.012022133,
           0.044352222,
           0.06255738,
           0.08955361,
           -0.1033955,
           0.05490487,
           0.053647492,
           0.09386603,
           -0.04699979,
           -0.06624404,
           0.065130234,
           0.14820817,
           0.06813173,
           0.11362538,
           0.09861613,
           0.03219138,
           -0.0060783937,
           -0.013430308,
           0.08314887,
           0.1475978,
           0.09292465,
           -0.06119256,
           -0.048648186,
           -0.015573397,
           -0.03768588,
           -0.032075044,
           0.0041478234,
           -0.03797824,
           0.094910756,
           0.0059467843,
           0.021003779,
           0.07056773,
           0.00012576066,
           -0.0026290256,
           0.13415967,
           0.14492252,
           0.14680454,
           0.0042979685,
           -0.04518681,
           0.02498858,
           0.12814312,
           -0.029278181,
           0.03645594,
           -0.13318157,
           0.0134702185,
           -0.05049377,
           0.03682718,
           -0.065345354,
           0.038617514,
           -0.05885647,
           0.089236796,
           0.054668654,
           -0.012855604,
           0.11699271,
           0.011075532,
           -0.08975264,
           0.09530008,
           0.037504897,
           -0.06037359,
           0.09983531,
           -0.14104317,
           -0.05909937,
           -0.09260909,
           -0.03328637,
           0.031142049,
           0.01801482,
           -0.01984746,
           -0.001127762,
           0.06980215,
           0.06771964,
           0.025280567,
           0.04462017,
           0.052017577,
           0.012041851,
           0.13465653,
           0.11953862,
           -0.1109269,
           0.020951161,
           -0.14946523,
           0.038448486,
           0.036110017,
           -0.1028196,
           -0.05058396,
           -0.03039593,
           -0.017983843,
           0.060768567,
           0.052262437,
           -0.0071646473,
           0.0662319,
           -0.05225482,
           -0.022792565,
           -0.08634599,
           0.057366207,
           -0.14822596,
           0.0074987533,
           -0.024298593,
           -0.043188535,
           -0.024537984,
           0.05645498,
           -0.0305751,
           0.13140835
          ],
          [
           0.02139169,
           0.013531686,
           -0.0037669293,
           -0.0070491573,
           -0.0026456953,
           0.004197229,
           -0.039021984,
           -0.14049642,
           -0.058559384,
           -0.050591882,
           -0.0048344703,
           0.017650297,
           -0.03471847,
           0.039273724,
           -0.039859474,
           -0.12910074,
           -0.13862509,
           -0.013927994,
           -0.045550153,
           -0.017016018,
           0.12301788,
           -0.08957562,
           -0.014708132,
           0.04991955,
           0.14451602,
           0.0930165,
           0.019003041,
           -0.08962814,
           -0.07325916,
           0.03671598,
           -0.04507493,
           0.08579424,
           -0.029353198,
           0.035717554,
           -0.021819886,
           0.011625213,
           0.10498669,
           0.05304151,
           0.046454802,
           0.03249605,
           -0.041036807,
           0.090832114,
           -0.110136956,
           0.029016132,
           -0.028696628,
           -0.04657193,
           -0.028465385,
           -0.13542058,
           0.06323703,
           0.08552427,
           -0.009125914,
           0.12705866,
           0.060486965,
           0.024251768,
           -0.027463375,
           -0.11519329,
           -0.016653696,
           -0.043696582,
           0.04441375,
           -0.028303748,
           -0.014828527,
           -0.06424329,
           0.016625118,
           -0.05168043,
           -0.120352566,
           -0.101122774,
           -0.096259214,
           -0.04152149,
           0.1352366,
           0.025914948,
           0.024865123,
           0.1162862,
           -0.11796431,
           -0.07693366,
           -0.028982084,
           0.038563937,
           -0.14993706,
           -0.1340593,
           0.059613656,
           0.10078514,
           0.030184824,
           -0.036194753,
           -0.18275087,
           0.10261126,
           0.024318274,
           -0.071111165,
           -0.06475012,
           -0.11862895,
           0.064363435,
           -0.028091593,
           0.10092731,
           -0.024758762,
           0.054217525,
           0.06307222,
           -0.036645904,
           -0.05253858,
           0.040360283,
           0.01396236,
           -0.054696184,
           -0.11382211,
           0.07833608,
           0.14628701,
           -0.027649596,
           0.11644525,
           0.029339576,
           -0.063073315,
           -0.032126207,
           0.018179797,
           -0.058894716,
           0.013530863,
           0.1166636,
           0.14683162,
           -0.06239883,
           0.027653305,
           -0.052624643,
           -0.02382929,
           -0.046246085,
           0.11729749,
           0.040014464,
           0.06140233,
           0.08490512,
           0.034697838,
           0.012435983,
           -0.08991722,
           0.02852424,
           -0.08764032,
           -0.16458382,
           -0.021676844
          ],
          [
           -0.017003078,
           -0.029020015,
           -0.06453707,
           0.019403063,
           -0.009387116,
           0.029436307,
           0.042317413,
           -0.07553502,
           0.054430276,
           0.14849088,
           0.117944226,
           0.048981532,
           -0.039168455,
           -0.0026213804,
           0.07437607,
           -0.10360101,
           0.017107368,
           -0.08292289,
           -0.0135795595,
           -0.043695327,
           -0.017437525,
           0.17003894,
           0.05442568,
           0.009285898,
           -0.090112396,
           0.08605621,
           0.10188356,
           -0.13270485,
           -0.06431724,
           0.053891983,
           -0.03337867,
           -0.013785112,
           0.052083995,
           0.045589544,
           -0.015695123,
           0.07510371,
           -0.0038197532,
           0.0063244104,
           -0.13964795,
           0.05224458,
           0.06841642,
           -0.043002486,
           -0.010229152,
           0.12148912,
           -0.005167954,
           -0.0010853158,
           0.0039342637,
           -0.04174567,
           -0.015206472,
           -0.018905096,
           -0.013351577,
           0.040917303,
           0.07175102,
           0.0492764,
           0.024772676,
           -0.037202742,
           0.04298042,
           0.00799301,
           0.11436326,
           -0.0885676,
           0.07733422,
           -0.081331715,
           0.004326475,
           -0.08078724,
           0.078380086,
           -0.028644344,
           -0.03472116,
           0.0007225073,
           0.10653469,
           -0.07974656,
           -0.000071554314,
           0.108385466,
           0.016855394,
           0.095947064,
           -0.042998973,
           0.029115783,
           0.063369595,
           0.04759481,
           -0.030274564,
           -0.004451013,
           -0.036614235,
           -0.0949039,
           0.02177236,
           -0.073027536,
           -0.06464267,
           -0.12424479,
           -0.049353242,
           0.046615653,
           0.016972471,
           0.07467318,
           -0.046061534,
           -0.012436955,
           -0.072441585,
           0.031540036,
           -0.051822364,
           -0.02005164,
           -0.019931223,
           -0.02952773,
           -0.09093385,
           -0.031165583,
           -0.14536078,
           0.06063487,
           0.085046984,
           -0.044857137,
           -0.12883028,
           0.16276939,
           -0.07337806,
           -0.03397781,
           0.023211695,
           0.09952537,
           -0.07530584,
           -0.049628437,
           0.008029495,
           0.07113078,
           0.15599775,
           0.041253965,
           -0.004917968,
           -0.123053685,
           -0.07084862,
           0.14143945,
           0.019988317,
           0.027109819,
           -0.08110791,
           -0.01113437,
           -0.043661673,
           -0.019022912,
           0.0055792327,
           0.023549598
          ],
          [
           -0.034987047,
           -0.086414166,
           -0.11771526,
           0.1310056,
           -0.047088493,
           0.0760205,
           -0.061560143,
           -0.032407958,
           0.021380575,
           -0.010149799,
           -0.054149695,
           0.040859714,
           -0.097141206,
           -0.07890128,
           -0.04095405,
           0.06328918,
           -0.009567464,
           -0.04069083,
           0.02642972,
           -0.007795023,
           -0.12226817,
           -0.12838693,
           -0.08032079,
           -0.06503689,
           -0.06163592,
           -0.05399302,
           -0.049042244,
           0.102346666,
           -0.010049522,
           0.012294077,
           -0.021952607,
           0.048599467,
           -0.021237584,
           0.12718715,
           -0.00016999066,
           0.022139477,
           -0.009038933,
           0.028862234,
           -0.0055744667,
           -0.12543122,
           -0.027401324,
           -0.01393394,
           -0.04634763,
           -0.0568339,
           -0.057702776,
           -0.021546204,
           0.03007587,
           0.027139165,
           -0.021326182,
           -0.027107444,
           0.035605375,
           -0.018808339,
           -0.044990547,
           0.03606564,
           0.069767706,
           0.04477057,
           0.019034388,
           0.010251825,
           0.019514881,
           -0.04626875,
           -0.114408,
           -0.04127761,
           0.058850024,
           0.040850528,
           -0.05190114,
           -0.0029293166,
           -0.059043556,
           -0.14341417,
           -0.055544972,
           0.030934004,
           -0.067486934,
           -0.10798831,
           0.044861954,
           0.07085552,
           0.0192963,
           0.016322551,
           0.0102340225,
           -0.08720616,
           0.01977224,
           -0.07942097,
           -0.098742716,
           -0.053472407,
           0.118337266,
           -0.009847075,
           0.039478503,
           -0.15526214,
           -0.054192115,
           0.13825142,
           0.066621564,
           0.16463704,
           0.113169305,
           0.08969727,
           0.05881934,
           0.045111474,
           0.06674756,
           -0.05480557,
           -0.1608591,
           -0.0501757,
           -0.01173854,
           0.1627263,
           -0.13724767,
           0.031102559,
           0.021407446,
           0.08153856,
           -0.0025064424,
           -0.04672858,
           0.054642186,
           0.09235664,
           0.07064484,
           -0.039661977,
           0.0837159,
           0.08260215,
           0.009767015,
           0.0036907555,
           0.051747497,
           0.028754558,
           -0.12797432,
           -0.0009107836,
           0.027228624,
           0.15049076,
           0.055611823,
           -0.039596964,
           -0.09727776,
           -0.06785157,
           0.006346223,
           0.0024700898,
           0.12057322,
           0.110510245
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "colorscale": [
          [
           0,
           "rgb(247,251,255)"
          ],
          [
           0.125,
           "rgb(222,235,247)"
          ],
          [
           0.25,
           "rgb(198,219,239)"
          ],
          [
           0.375,
           "rgb(158,202,225)"
          ],
          [
           0.5,
           "rgb(107,174,214)"
          ],
          [
           0.625,
           "rgb(66,146,198)"
          ],
          [
           0.75,
           "rgb(33,113,181)"
          ],
          [
           0.875,
           "rgb(8,81,156)"
          ],
          [
           1,
           "rgb(8,48,107)"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(model.embed.W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding shape is torch.Size([11, 128]), so our vectors of length 128\n"
     ]
    }
   ],
   "source": [
    "# Take the dot product of all the embedding vectors\n",
    "emb = model.embed.W_E\n",
    "vec_count = emb.shape[0]\n",
    "vec_dim = emb.shape[1]\n",
    "print(f\"The embedding shape is {emb.shape}, so our vectors of length {emb.shape[1]}\")\n",
    "\n",
    "dot_products = einops.einsum(emb, emb, \"v2 embs, v1 emb -> v1 v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 11])\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "coloraxis": "coloraxis",
         "hovertemplate": "x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>",
         "name": "0",
         "type": "heatmap",
         "xaxis": "x",
         "yaxis": "y",
         "z": [
          [
           0.028888486,
           -0.008704282,
           -0.28349918,
           -0.0903837,
           -0.028867347,
           -0.20513731,
           0.025150014,
           0.16676904,
           -0.11742266,
           0.030425806,
           -0.07172319
          ],
          [
           -0.008704282,
           0.002622655,
           0.08542008,
           0.027233176,
           0.008697913,
           0.06180916,
           -0.007577857,
           -0.05024856,
           0.03538019,
           -0.009167486,
           0.02161065
          ],
          [
           -0.28349918,
           0.08542008,
           2.7821388,
           0.88698673,
           0.28329173,
           2.0131292,
           -0.2468114,
           -1.6365997,
           1.1523355,
           -0.29858574,
           0.7038606
          ],
          [
           -0.0903837,
           0.027233176,
           0.88698673,
           0.2827844,
           0.09031756,
           0.6418152,
           -0.0786871,
           -0.52177197,
           0.36738148,
           -0.09519353,
           0.22440109
          ],
          [
           -0.028867347,
           0.008697913,
           0.28329173,
           0.09031756,
           0.028846225,
           0.20498721,
           -0.025131611,
           -0.16664702,
           0.11733675,
           -0.030403541,
           0.07167071
          ],
          [
           -0.20513731,
           0.06180916,
           2.0131292,
           0.6418152,
           0.20498721,
           1.4566813,
           -0.1785904,
           -1.184228,
           0.83381903,
           -0.21605383,
           0.50930685
          ],
          [
           0.025150014,
           -0.007577857,
           -0.2468114,
           -0.0786871,
           -0.025131611,
           -0.1785904,
           0.021895338,
           0.14518738,
           -0.10222694,
           0.026488388,
           -0.06244146
          ],
          [
           0.16676904,
           -0.05024856,
           -1.6365997,
           -0.52177197,
           -0.16664702,
           -1.184228,
           0.14518738,
           0.9627335,
           -0.677864,
           0.17564376,
           -0.4140476
          ],
          [
           -0.11742266,
           0.03538019,
           1.1523355,
           0.36738148,
           0.11733675,
           0.83381903,
           -0.10222694,
           -0.677864,
           0.47728643,
           -0.12367139,
           0.29153237
          ],
          [
           0.030425806,
           -0.009167486,
           -0.29858574,
           -0.09519353,
           -0.030403541,
           -0.21605383,
           0.026488388,
           0.17564376,
           -0.12367139,
           0.032044932,
           -0.07553999
          ],
          [
           -0.07172319,
           0.02161065,
           0.7038606,
           0.22440109,
           0.07167071,
           0.50930685,
           -0.06244146,
           -0.4140476,
           0.29153237,
           -0.07553999,
           0.17807151
          ]
         ]
        }
       ],
       "layout": {
        "coloraxis": {
         "cmid": 0,
         "colorscale": [
          [
           0,
           "rgb(103,0,31)"
          ],
          [
           0.1,
           "rgb(178,24,43)"
          ],
          [
           0.2,
           "rgb(214,96,77)"
          ],
          [
           0.3,
           "rgb(244,165,130)"
          ],
          [
           0.4,
           "rgb(253,219,199)"
          ],
          [
           0.5,
           "rgb(247,247,247)"
          ],
          [
           0.6,
           "rgb(209,229,240)"
          ],
          [
           0.7,
           "rgb(146,197,222)"
          ],
          [
           0.8,
           "rgb(67,147,195)"
          ],
          [
           0.9,
           "rgb(33,102,172)"
          ],
          [
           1,
           "rgb(5,48,97)"
          ]
         ]
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ]
        },
        "yaxis": {
         "anchor": "x",
         "autorange": "reversed",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(dot_products.shape)\n",
    "imshow_div(dot_products)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What would your hypothesis around the attention head activations be based on seeing this?\n",
    "+ Jack - My poorly informed guess is that tokens with low dot products and/or low norms won't have any strong attentional interaction\n",
    "+ Omar - I think that corner moves [0, 2, 6, 8] will have similar attention patterns\n",
    "+ Ari - I think same as Omar, plus center attends to everything, middle edges have attention symmetry too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformer_lens.ActivationCache.ActivationCache'>\n",
      "torch.Size([8, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-d222a64c-6714\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-d222a64c-6714\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"10\", \"1\", \"2\", \"5\", \"8\", \"7\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45812541246414185, 0.5418745875358582, 0.0, 0.0, 0.0, 0.0], [0.22398819029331207, 0.2694855332374573, 0.5065262913703918, 0.0, 0.0, 0.0], [0.17355869710445404, 0.18835435807704926, 0.2808837294578552, 0.3572031855583191, 0.0, 0.0], [0.1269979029893875, 0.13958105444908142, 0.24249961972236633, 0.25156325101852417, 0.23935818672180176, 0.0], [0.0990457758307457, 0.12053121626377106, 0.16093873977661133, 0.22442267835140228, 0.1837877631187439, 0.21127384901046753]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4681083559989929, 0.5318916440010071, 0.0, 0.0, 0.0, 0.0], [0.3042255640029907, 0.35106292366981506, 0.3447115421295166, 0.0, 0.0, 0.0], [0.21583274006843567, 0.2623125910758972, 0.26122868061065674, 0.260625958442688, 0.0, 0.0], [0.16870172321796417, 0.19197826087474823, 0.22148297727108002, 0.23696300387382507, 0.18087412416934967, 0.0], [0.1503811627626419, 0.17610394954681396, 0.17247708141803741, 0.17722801864147186, 0.14403246343135834, 0.1797773838043213]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4752345681190491, 0.5247654318809509, 0.0, 0.0, 0.0, 0.0], [0.35157713294029236, 0.3767607510089874, 0.2716621458530426, 0.0, 0.0, 0.0], [0.21788854897022247, 0.2475658655166626, 0.19214384257793427, 0.34240174293518066, 0.0, 0.0], [0.17064471542835236, 0.18243300914764404, 0.14766493439674377, 0.23293939232826233, 0.2663179636001587, 0.0], [0.1342678666114807, 0.14604060351848602, 0.11000017821788788, 0.16950426995754242, 0.21473285555839539, 0.22545425593852997]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4937058389186859, 0.5062941908836365, 0.0, 0.0, 0.0, 0.0], [0.32408398389816284, 0.3302459716796875, 0.34567001461982727, 0.0, 0.0, 0.0], [0.19183191657066345, 0.19340123236179352, 0.1891317218542099, 0.42563512921333313, 0.0, 0.0], [0.11048267036676407, 0.12187877297401428, 0.11438468098640442, 0.3168584406375885, 0.3363954424858093, 0.0], [0.08901658654212952, 0.10130669921636581, 0.09910479187965393, 0.2473156899213791, 0.2429945170879364, 0.22026167809963226]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5774500966072083, 0.42254993319511414, 0.0, 0.0, 0.0, 0.0], [0.2723369300365448, 0.19073911011219025, 0.5369239449501038, 0.0, 0.0, 0.0], [0.20105022192001343, 0.14507363736629486, 0.3386540412902832, 0.3152221441268921, 0.0, 0.0], [0.17045089602470398, 0.0982857272028923, 0.2620142698287964, 0.21883347630500793, 0.2504156827926636, 0.0], [0.11563804000616074, 0.08774539083242416, 0.19550427794456482, 0.1805626004934311, 0.2262844294309616, 0.19426529109477997]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5989135503768921, 0.4010865092277527, 0.0, 0.0, 0.0, 0.0], [0.357910692691803, 0.223377987742424, 0.4187113344669342, 0.0, 0.0, 0.0], [0.23573610186576843, 0.16895413398742676, 0.30138707160949707, 0.29392266273498535, 0.0, 0.0], [0.1761287897825241, 0.13383854925632477, 0.20638708770275116, 0.2460632473230362, 0.23758232593536377, 0.0], [0.15075193345546722, 0.11085337400436401, 0.18433155119419098, 0.188791424036026, 0.18833915889263153, 0.17693254351615906]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.589232861995697, 0.4107670783996582, 0.0, 0.0, 0.0, 0.0], [0.27390170097351074, 0.2158406376838684, 0.5102576613426208, 0.0, 0.0, 0.0], [0.17146533727645874, 0.16103732585906982, 0.3197775185108185, 0.34771978855133057, 0.0, 0.0], [0.12572365999221802, 0.11243776977062225, 0.21638505160808563, 0.2739451229572296, 0.2715083956718445, 0.0], [0.10330648720264435, 0.08931203931570053, 0.1733933985233307, 0.2262541800737381, 0.18265725672245026, 0.22507663071155548]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5085398554801941, 0.4914601445198059, 0.0, 0.0, 0.0, 0.0], [0.306155800819397, 0.31936269998550415, 0.3744814991950989, 0.0, 0.0, 0.0], [0.2649987041950226, 0.21907074749469757, 0.2594199776649475, 0.25651055574417114, 0.0, 0.0], [0.18456077575683594, 0.1834363341331482, 0.23037849366664886, 0.21455152332782745, 0.18707290291786194, 0.0], [0.14894025027751923, 0.1545296162366867, 0.18241463601589203, 0.1643138974905014, 0.14793427288532257, 0.20186732709407806]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f78d07cc1c0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = [10,0,2]\n",
    "tokens = [10] + [1,2,5,8,7]\n",
    "str_tokens = [str(token) for token in tokens]\n",
    "logits, cache = model.run_with_cache(torch.tensor(tokens).to('cuda'), remove_batch_dim=True)\n",
    "\n",
    "print(type(cache))\n",
    "attention_pattern = cache[\"pattern\", 3, \"attn\"]\n",
    "print(attention_pattern.shape)\n",
    "cv.attention.attention_patterns(tokens=str_tokens, attention=attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_sequence(model: HookedTransformer, seq: Optional[list[int]]= None, temperature: float = 1) -> list[int]:\n",
    "#     if seq == None:\n",
    "#         seq = [10]\n",
    "#     assert temperature != 0\n",
    "#     model_seq = torch.tensor(seq)\n",
    "#     logit: Tensor = model(model_seq)[0,-1]\n",
    "#     probabilities: Tensor = torch.softmax(logit/temperature, dim=0)\n",
    "#     new_token: int = int(torch.multinomial(probabilities, 1).item())\n",
    "#     seq = seq + [new_token]\n",
    "#     if new_token == 9 or len(seq) == 10:\n",
    "#         return seq\n",
    "#     else:\n",
    "#         return generate_sequence(model, seq)\n",
    "        \n",
    "# def check_sequence(sequence: list[int]) -> bool:\n",
    "#     board = Board()\n",
    "#     seq_len = len(sequence)\n",
    "#     if sequence[-1] != 9:\n",
    "#         return False\n",
    "#     dedup_len = len({token for token in sequence})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = {1,2}\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = generate_sequence(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 3, 1, 1, 3, 7, 0, 6, 3, 6]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Must provide a tokenizer if passing a string to the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:259\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Input is either a batch of tokens ([batch, pos]) or a text string, a string is automatically tokenized to a batch of a single element. The prepend_bos flag only applies when inputting a text string.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[1;32m    249\u001b[0m \u001b[39mreturn_type Optional[str]: The type of output to return. Can be one of: None (return nothing, don't calculate logits), 'logits' (return logits), 'loss' (return cross-entropy loss), 'both' (return logits and loss)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mNote that loss is the standard \"predict the next token\" cross-entropy loss for GPT-2 style language models - if you want a custom loss function, the recommended behaviour is returning the logits and then applying your custom loss function.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mstr\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39minput\u001b[39m) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[1;32m    257\u001b[0m     \u001b[39m# If text, convert to tokens (batch_size=1)\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m--> 259\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mMust provide a tokenizer if passing a string to the model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    261\u001b[0m     \u001b[39m# This is only intended to support passing in a single string\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_tokens(\u001b[39minput\u001b[39m, prepend_bos\u001b[39m=\u001b[39mprepend_bos)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Must provide a tokenizer if passing a string to the model"
     ]
    }
   ],
   "source": [
    "model([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformer_lens.HookedTransformer.HookedTransformer"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tree_walk(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589325.3191094658"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.931568569324174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log2(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:24<00:00, 41.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def sample_game(model: HookedTransformer, temp: float) -> list[int]:\n",
    "    assert temp > 0\n",
    "    seq = [10]\n",
    "    #no grad\n",
    "    with t.no_grad():\n",
    "        for _ in range(8):\n",
    "            logits = model(t.tensor(seq))[0, -1]\n",
    "            probs = t.softmax(logits / temp, dim=0)\n",
    "            token = t.multinomial(probs, num_samples=1).item()\n",
    "            seq.append(token)\n",
    "    return seq\n",
    "\n",
    "def sample_games(model: HookedTransformer, temp: float, num_games: int) -> list[list[int]]:\n",
    "    games=[]\n",
    "    for _ in tqdm.tqdm(range(num_games)):\n",
    "        games.append(sample_game(model, temp))\n",
    "    return games\n",
    "\n",
    "all_games_samples = sample_games(model, temp=1, num_games=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_illegal_moves(game: list[int]) -> bool:\n",
    "    clean_game = [token for token in game if token != 9]\n",
    "    set_length = len(set(clean_game))\n",
    "    return set_length == len(clean_game)\n",
    "\n",
    "def check_illegal_moves_again(games: list[list[int]]) -> list[bool]:\n",
    "    return [check_illegal_moves(game) for game in games]\n",
    "    \n",
    "def error_rate(games: list[list[int]]) -> float:\n",
    "    return check_illegal_moves_again(games).count(False) / len(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate for new model: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Error rate for new model: {error_rate(all_games_samples)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
